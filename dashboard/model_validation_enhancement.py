#!/usr/bin/env python3
"""
SPY ëª¨ë¸ ì •í™•ë„ í–¥ìƒ ë° ê²€ì¦ ê°•í™”
- ì˜¤ë²„í”¼íŒ… ë°©ì§€ ê°•í™”
- ë°ì´í„° ëˆ„ìˆ˜ ì™„ì „ ë°©ì§€
- ëª¨ë¸ ì˜¤ë¥˜ ë¶„ì„
- ì¶”ê°€ ì •í™•ë„ í–¥ìƒ ê¸°ë²•
"""

import json
import numpy as np
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ML libraries
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.svm import SVC
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, validation_curve, learning_curve
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel
from sklearn.pipeline import Pipeline
from sklearn.utils.class_weight import compute_class_weight

import matplotlib.pyplot as plt
import seaborn as sns

# Advanced validation
from sklearn.model_selection import StratifiedKFold, GroupKFold
from sklearn.inspection import permutation_importance

# Deep Learning (if available)
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GRU
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
    from tensorflow.keras.regularizers import l1_l2
    tf.get_logger().setLevel('ERROR')
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False

class ModelValidationEnhancement:
    def __init__(self):
        self.spy_data = None
        self.vix_data = None
        self.enhanced_features = None
        self.models = {}
        self.results = {}
        self.validation_results = {}
        
    def load_and_validate_data(self):
        """ë°ì´í„° ë¡œë“œ ë° í’ˆì§ˆ ê²€ì¦"""
        print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ë° í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        try:
            # ë” ê¸´ ê¸°ê°„ìœ¼ë¡œ í™•ì¥ (2017-2024)
            spy_raw = yf.download('SPY', start='2017-01-01', end='2025-01-01', auto_adjust=True, progress=False)
            vix_raw = yf.download('^VIX', start='2017-01-01', end='2025-01-01', auto_adjust=True, progress=False)
            
            # MultiIndex ì»¬ëŸ¼ ì •ë¦¬
            if isinstance(spy_raw.columns, pd.MultiIndex):
                spy_raw.columns = spy_raw.columns.get_level_values(0)
            if isinstance(vix_raw.columns, pd.MultiIndex):
                vix_raw.columns = vix_raw.columns.get_level_values(0)
                
            # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            print(f"ğŸ“Š SPY ë°ì´í„°: {len(spy_raw)} ì¼")
            print(f"ğŸ“Š VIX ë°ì´í„°: {len(vix_raw)} ì¼")
            
            # ê²°ì¸¡ì¹˜ ê²€ì‚¬
            spy_missing = spy_raw.isnull().sum().sum()
            vix_missing = vix_raw.isnull().sum().sum()
            
            print(f"â“ SPY ê²°ì¸¡ì¹˜: {spy_missing}")
            print(f"â“ VIX ê²°ì¸¡ì¹˜: {vix_missing}")
            
            # ì´ìƒì¹˜ ê²€ì‚¬ (ê·¹ë‹¨ê°’)
            spy_outliers = ((spy_raw['Close'] - spy_raw['Close'].mean()).abs() > 3 * spy_raw['Close'].std()).sum()
            print(f"âš ï¸ SPY ì´ìƒì¹˜ (3Ïƒ ì´ˆê³¼): {spy_outliers}")
            
            self.spy_data = spy_raw
            self.vix_data = vix_raw
            
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def create_leak_proof_features(self):
        """ì™„ì „í•œ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ íŠ¹ì„± ìƒì„±"""
        print("ğŸ”’ ëˆ„ìˆ˜ ë°©ì§€ íŠ¹ì„± ìƒì„± ì¤‘...")
        
        spy_features = self.spy_data.copy()
        
        # ê¸°ë³¸ ìˆ˜ìµë¥  (t+1 ì˜ˆì¸¡ì„ ìœ„í•´ tì‹œì  ë°ì´í„°ë§Œ ì‚¬ìš©)
        spy_features['returns'] = spy_features['Close'].pct_change()
        spy_features['log_returns'] = np.log(spy_features['Close'] / spy_features['Close'].shift(1))
        
        # ê³¼ê±° ìˆ˜ìµë¥  ì‹œë¦¬ì¦ˆ (1-20ì¼ ì „)
        for i in range(1, 21):
            spy_features[f'return_lag_{i}'] = spy_features['returns'].shift(i)
        
        # ê¸°ìˆ ì  ì§€í‘œ (ëª¨ë‘ ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        def safe_rsi(prices, period=14):
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi.shift(1)  # 1ì¼ ì§€ì—°ìœ¼ë¡œ ëˆ„ìˆ˜ ë°©ì§€
            
        spy_features['rsi'] = safe_rsi(spy_features['Close'])
        
        # ì´ë™í‰ê·  (ê³¼ê±°ë§Œ)
        for period in [5, 10, 20, 50, 200]:
            spy_features[f'ma_{period}'] = spy_features['Close'].rolling(period).mean().shift(1)
            spy_features[f'price_to_ma_{period}'] = spy_features['Close'].shift(1) / spy_features[f'ma_{period}']
        
        # ë³¼ë¦°ì € ë°´ë“œ (ê³¼ê±°ë§Œ)
        bb_period = 20
        spy_features['bb_middle'] = spy_features['Close'].rolling(bb_period).mean().shift(1)
        bb_std = spy_features['Close'].rolling(bb_period).std().shift(1)
        spy_features['bb_upper'] = spy_features['bb_middle'] + (bb_std * 2)
        spy_features['bb_lower'] = spy_features['bb_middle'] - (bb_std * 2)
        spy_features['bb_position'] = (spy_features['Close'].shift(1) - spy_features['bb_lower']) / (spy_features['bb_upper'] - spy_features['bb_lower'])
        
        # VIX íŠ¹ì„± (ê³¼ê±°ë§Œ)
        vix_aligned = self.vix_data.reindex(spy_features.index, method='ffill')
        spy_features['vix'] = vix_aligned['Close'].shift(1)  # 1ì¼ ì§€ì—°
        spy_features['vix_change'] = spy_features['vix'].pct_change()
        spy_features['vix_ma_5'] = spy_features['vix'].rolling(5).mean()
        spy_features['vix_signal'] = (spy_features['vix'] <= 20).astype(int)
        
        # ê±°ë˜ëŸ‰ ì§€í‘œ (ê³¼ê±°ë§Œ)
        spy_features['volume_ma'] = spy_features['Volume'].rolling(20).mean().shift(1)
        spy_features['volume_ratio'] = spy_features['Volume'].shift(1) / spy_features['volume_ma']
        
        # ë³€ë™ì„± ì§€í‘œ (ê³¼ê±°ë§Œ)
        for period in [5, 10, 20]:
            spy_features[f'volatility_{period}'] = spy_features['returns'].rolling(period).std().shift(1)
            spy_features[f'returns_mean_{period}'] = spy_features['returns'].rolling(period).mean().shift(1)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜: t+1 ì‹œì ì˜ ìˆ˜ìµë¥  ë°©í–¥
        spy_features['future_return'] = spy_features['Close'].shift(-1) / spy_features['Close'] - 1
        spy_features['target'] = (spy_features['future_return'] > 0).astype(int)
        
        # ë‚ ì§œ íŠ¹ì„± (ìˆœí™˜ì  ì¸ì½”ë”©)
        spy_features['month'] = pd.to_datetime(spy_features.index).month
        spy_features['day_of_week'] = pd.to_datetime(spy_features.index).dayofweek
        spy_features['month_sin'] = np.sin(2 * np.pi * spy_features['month'] / 12)
        spy_features['month_cos'] = np.cos(2 * np.pi * spy_features['month'] / 12)
        spy_features['dow_sin'] = np.sin(2 * np.pi * spy_features['day_of_week'] / 7)
        spy_features['dow_cos'] = np.cos(2 * np.pi * spy_features['day_of_week'] / 7)
        
        self.enhanced_features = spy_features
        print(f"âœ… ëˆ„ìˆ˜ ë°©ì§€ íŠ¹ì„± {len(spy_features.columns)}ê°œ ìƒì„± ì™„ë£Œ")
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (forward fillë§Œ ì‚¬ìš©)
        self.enhanced_features = self.enhanced_features.fillna(method='ffill')
        
        return True
    
    def validate_data_leakage(self):
        """ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦"""
        print("ğŸ” ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦ ì¤‘...")
        
        validation_results = {
            'feature_future_correlation': {},
            'temporal_consistency': True,
            'target_leakage_check': True
        }
        
        # íŠ¹ì„±ê³¼ ë¯¸ë˜ ìˆ˜ìµë¥  ê°„ ìƒê´€ê´€ê³„ ê²€ì‚¬ (ë†’ìœ¼ë©´ ëˆ„ìˆ˜ ì˜ì‹¬)
        if 'future_return' in self.enhanced_features.columns:
            future_returns = self.enhanced_features['future_return'].dropna()
            
            feature_cols = [col for col in self.enhanced_features.columns 
                          if col not in ['target', 'future_return', 'Open', 'High', 'Low', 'Close', 'Volume']]
            
            for feature in feature_cols:
                if self.enhanced_features[feature].dtype in ['float64', 'int64']:
                    # ê°™ì€ ì‹œì  ë°ì´í„°ë¡œ ìƒê´€ê´€ê³„ ê³„ì‚° (ëˆ„ìˆ˜ ê²€ì‚¬)
                    aligned_data = pd.concat([
                        self.enhanced_features[feature],
                        future_returns
                    ], axis=1).dropna()
                    
                    if len(aligned_data) > 100:
                        correlation = aligned_data.iloc[:, 0].corr(aligned_data.iloc[:, 1])
                        validation_results['feature_future_correlation'][feature] = abs(correlation)
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë†’ì€ ìƒê´€ê´€ê³„ (>0.8) ì²´í¬
        high_corr_features = {k: v for k, v in validation_results['feature_future_correlation'].items() 
                            if v > 0.8}
        
        if high_corr_features:
            print("âš ï¸ ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬ íŠ¹ì„±ë“¤:")
            for feature, corr in high_corr_features.items():
                print(f"   {feature}: {corr:.3f}")
            validation_results['target_leakage_check'] = False
        else:
            print("âœ… ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬ í†µê³¼")
            
        return validation_results
    
    def prepare_robust_training_data(self):
        """ê°•ê±´í•œ í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ“Š ê°•ê±´í•œ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # íŠ¹ì„± ì„ íƒ (ëˆ„ìˆ˜ ì—†ëŠ” íŠ¹ì„±ë§Œ)
        exclude_cols = ['target', 'future_return', 'Open', 'High', 'Low', 'Close', 'Volume', 'month', 'day_of_week']
        feature_columns = [col for col in self.enhanced_features.columns 
                          if col not in exclude_cols and 
                          self.enhanced_features[col].dtype in ['float64', 'int64']]
        
        # ë¬´í•œê°’ê³¼ ê²°ì¸¡ê°’ ì²˜ë¦¬
        clean_data = self.enhanced_features.replace([np.inf, -np.inf], np.nan).dropna()
        
        X = clean_data[feature_columns]
        y = clean_data['target']
        
        # ì—„ê²©í•œ ì‹œê³„ì—´ ë¶„í• 
        # 2017-2020: í›ˆë ¨ìš©
        # 2021-2022: ê²€ì¦ìš©  
        # 2023-2024: í…ŒìŠ¤íŠ¸ìš©
        train_mask = X.index < '2021-01-01'
        val_mask = (X.index >= '2021-01-01') & (X.index < '2023-01-01')
        test_mask = X.index >= '2023-01-01'
        
        X_train, y_train = X[train_mask], y[train_mask]
        X_val, y_val = X[val_mask], y[val_mask]
        X_test, y_test = X[test_mask], y[test_mask]
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸
        train_class_dist = y_train.value_counts()
        val_class_dist = y_val.value_counts()
        test_class_dist = y_test.value_counts()
        
        print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(X_train)} ìƒ˜í”Œ")
        print(f"   í´ë˜ìŠ¤ ë¶„í¬: {dict(train_class_dist)}")
        print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(X_val)} ìƒ˜í”Œ")  
        print(f"   í´ë˜ìŠ¤ ë¶„í¬: {dict(val_class_dist)}")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)} ìƒ˜í”Œ")
        print(f"   í´ë˜ìŠ¤ ë¶„í¬: {dict(test_class_dist)}")
        print(f"ğŸ“Š íŠ¹ì„± ìˆ˜: {len(feature_columns)}ê°œ")
        
        return X_train, X_val, X_test, y_train, y_val, y_test, feature_columns
    
    def detect_overfitting_early(self, model, X_train, X_val, y_train, y_val, model_name):
        """ì˜¤ë²„í”¼íŒ… ì¡°ê¸° ê°ì§€"""
        print(f"ğŸ” {model_name} ì˜¤ë²„í”¼íŒ… ê²€ì‚¬ ì¤‘...")
        
        # í•™ìŠµ ê³¡ì„  ë¶„ì„
        train_sizes = np.linspace(0.1, 1.0, 10)
        train_sizes_abs, train_scores, val_scores = learning_curve(
            model, X_train, y_train, 
            train_sizes=train_sizes,
            cv=TimeSeriesSplit(n_splits=3),
            scoring='accuracy',
            n_jobs=-1
        )
        
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        val_mean = np.mean(val_scores, axis=1)
        val_std = np.std(val_scores, axis=1)
        
        # ì˜¤ë²„í”¼íŒ… ì§€í‘œ ê³„ì‚°
        final_gap = train_mean[-1] - val_mean[-1]
        max_gap = np.max(train_mean - val_mean)
        
        overfitting_detected = final_gap > 0.1 or max_gap > 0.15
        
        overfitting_analysis = {
            'final_train_score': train_mean[-1],
            'final_val_score': val_mean[-1],
            'final_gap': final_gap,
            'max_gap': max_gap,
            'overfitting_detected': overfitting_detected,
            'train_sizes': train_sizes_abs,
            'train_scores': train_scores,
            'val_scores': val_scores
        }
        
        if overfitting_detected:
            print(f"âš ï¸ {model_name} ì˜¤ë²„í”¼íŒ… ê°ì§€!")
            print(f"   ìµœì¢… ê²©ì°¨: {final_gap:.3f}")
            print(f"   ìµœëŒ€ ê²©ì°¨: {max_gap:.3f}")
        else:
            print(f"âœ… {model_name} ì˜¤ë²„í”¼íŒ… ì—†ìŒ")
            
        return overfitting_analysis
    
    def train_regularized_models(self, X_train, X_val, X_test, y_train, y_val, y_test, feature_columns):
        """ì •ê·œí™”ê°€ ê°•í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ğŸ¯ ì •ê·œí™” ê°•í™” ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        models_config = {
            'regularized_rf': {
                'model': RandomForestClassifier(
                    n_estimators=200,
                    max_depth=10,  # ë” ì œí•œì 
                    min_samples_split=20,  # ë” ë†’ê²Œ
                    min_samples_leaf=10,   # ë” ë†’ê²Œ
                    max_features=0.3,      # ë” ì œí•œì 
                    class_weight='balanced',
                    random_state=42,
                    n_jobs=-1
                ),
                'use_scaling': False
            },
            'regularized_gb': {
                'model': GradientBoostingClassifier(
                    n_estimators=100,
                    max_depth=6,           # ë” ì œí•œì 
                    learning_rate=0.05,    # ë” ë‚®ê²Œ
                    subsample=0.8,         # ìƒ˜í”Œë§ìœ¼ë¡œ ì •ê·œí™”
                    max_features=0.5,      # íŠ¹ì„± ìƒ˜í”Œë§
                    random_state=42
                ),
                'use_scaling': False
            },
            'ridge_lr': {
                'model': RidgeClassifier(
                    alpha=1.0,             # L2 ì •ê·œí™”
                    class_weight='balanced',
                    random_state=42
                ),
                'use_scaling': True
            },
            'regularized_svm': {
                'model': SVC(
                    C=0.1,                 # ë” ê°•í•œ ì •ê·œí™”
                    kernel='rbf',
                    class_weight='balanced',
                    probability=True,
                    random_state=42
                ),
                'use_scaling': True
            }
        }
        
        results = {}
        
        for name, config in models_config.items():
            print(f"\nğŸ”§ {name} í›ˆë ¨ ì¤‘...")
            
            model = config['model']
            
            # ì ì ˆí•œ ë°ì´í„° ì‚¬ìš©
            if config['use_scaling']:
                X_tr, X_v, X_te = X_train_scaled, X_val_scaled, X_test_scaled
            else:
                X_tr, X_v, X_te = X_train, X_val, X_test
            
            # ëª¨ë¸ í›ˆë ¨
            model.fit(X_tr, y_train)
            
            # ì˜ˆì¸¡
            train_pred = model.predict(X_tr)
            val_pred = model.predict(X_v)
            test_pred = model.predict(X_te)
            
            # ì„±ëŠ¥ ê³„ì‚°
            train_acc = accuracy_score(y_train, train_pred)
            val_acc = accuracy_score(y_val, val_pred)
            test_acc = accuracy_score(y_test, test_pred)
            
            # AUC ê³„ì‚° (í™•ë¥  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                if hasattr(model, 'predict_proba'):
                    test_proba = model.predict_proba(X_te)[:, 1]
                elif hasattr(model, 'decision_function'):
                    test_proba = model.decision_function(X_te)
                else:
                    test_proba = test_pred
                    
                test_auc = roc_auc_score(y_test, test_proba)
            except:
                test_auc = 0.5
            
            # ì˜¤ë²„í”¼íŒ… ê²€ì‚¬
            overfitting_analysis = self.detect_overfitting_early(
                model, X_tr, X_v, y_train, y_val, name
            )
            
            results[name] = {
                'model': model,
                'train_accuracy': train_acc,
                'val_accuracy': val_acc,
                'test_accuracy': test_acc,
                'test_auc': test_auc,
                'overfitting_analysis': overfitting_analysis,
                'scaler': scaler if config['use_scaling'] else None
            }
            
            print(f"   í›ˆë ¨ ì •í™•ë„: {train_acc:.3f}")
            print(f"   ê²€ì¦ ì •í™•ë„: {val_acc:.3f}")
            print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.3f}")
            print(f"   í…ŒìŠ¤íŠ¸ AUC: {test_auc:.3f}")
            
        self.models.update(results)
        return results
    
    def train_enhanced_lstm(self, X_train, X_val, X_test, y_train, y_val, y_test):
        """ê°•í™”ëœ LSTM ëª¨ë¸"""
        if not TENSORFLOW_AVAILABLE:
            print("âš ï¸ TensorFlow ë¯¸ì„¤ì¹˜ë¡œ LSTM ìŠ¤í‚µ")
            return {}
            
        print("ğŸ§  ê°•í™”ëœ LSTM ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        def create_sequences(X, y, seq_length=60):  # ë” ê¸´ ì‹œí€€ìŠ¤
            sequences = []
            targets = []
            
            for i in range(seq_length, len(X)):
                sequences.append(X[i-seq_length:i])
                targets.append(y[i])
                
            return np.array(sequences), np.array(targets)
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = MinMaxScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        seq_length = 60
        X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, seq_length)
        X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val.values, seq_length)
        X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, seq_length)
        
        print(f"   LSTM ì‹œí€€ìŠ¤ shape: {X_train_seq.shape}")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train_seq), y=y_train_seq)
        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
        
        # ê°•í™”ëœ LSTM ëª¨ë¸
        model = Sequential([
            # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´ (ë” ë§ì€ ì •ê·œí™”)
            LSTM(128, return_sequences=True, input_shape=(seq_length, X_train_seq.shape[2]),
                 dropout=0.3, recurrent_dropout=0.3,
                 kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),
            BatchNormalization(),
            
            # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´
            LSTM(64, return_sequences=True, 
                 dropout=0.3, recurrent_dropout=0.3,
                 kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),
            BatchNormalization(),
            
            # ì„¸ ë²ˆì§¸ LSTM ë ˆì´ì–´
            LSTM(32, return_sequences=False,
                 dropout=0.3, recurrent_dropout=0.3,
                 kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),
            BatchNormalization(),
            
            # Dense ë ˆì´ì–´ë“¤
            Dense(32, activation='relu', 
                  kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),
            Dropout(0.4),
            BatchNormalization(),
            
            Dense(16, activation='relu',
                  kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),
            Dropout(0.3),
            
            # ì¶œë ¥ ë ˆì´ì–´
            Dense(1, activation='sigmoid')
        ])
        
        # ì»´íŒŒì¼ (ë” ë‚®ì€ í•™ìŠµë¥ )
        model.compile(
            optimizer=Adam(learning_rate=0.0005),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        # ì½œë°± ì„¤ì • (ë” ì—„ê²©í•œ ì¡°ê¸° ì¢…ë£Œ)
        callbacks = [
            EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7, verbose=1),
            ModelCheckpoint('best_lstm_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
        ]
        
        # í›ˆë ¨
        history = model.fit(
            X_train_seq, y_train_seq,
            epochs=100,  # ë” ë§ì€ ì—í¬í¬
            batch_size=32,
            validation_data=(X_val_seq, y_val_seq),
            callbacks=callbacks,
            class_weight=class_weight_dict,
            verbose=0
        )
        
        # ì˜ˆì¸¡
        train_pred_proba = model.predict(X_train_seq, verbose=0).flatten()
        val_pred_proba = model.predict(X_val_seq, verbose=0).flatten()
        test_pred_proba = model.predict(X_test_seq, verbose=0).flatten()
        
        train_pred = (train_pred_proba > 0.5).astype(int)
        val_pred = (val_pred_proba > 0.5).astype(int)
        test_pred = (test_pred_proba > 0.5).astype(int)
        
        # ì„±ëŠ¥ ê³„ì‚°
        train_acc = accuracy_score(y_train_seq, train_pred)
        val_acc = accuracy_score(y_val_seq, val_pred)
        test_acc = accuracy_score(y_test_seq, test_pred)
        test_auc = roc_auc_score(y_test_seq, test_pred_proba)
        
        # ì˜¤ë²„í”¼íŒ… ë¶„ì„ (íˆìŠ¤í† ë¦¬ ê¸°ë°˜)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        train_accuracy = history.history['accuracy']
        val_accuracy = history.history['val_accuracy']
        
        # ë§ˆì§€ë§‰ 10 ì—í¬í¬ í‰ê· ìœ¼ë¡œ ì˜¤ë²„í”¼íŒ… íŒë‹¨
        final_epochs = 10
        final_train_acc = np.mean(train_accuracy[-final_epochs:])
        final_val_acc = np.mean(val_accuracy[-final_epochs:])
        overfitting_gap = final_train_acc - final_val_acc
        
        lstm_results = {
            'enhanced_lstm': {
                'model': model,
                'scaler': scaler,
                'train_accuracy': train_acc,
                'val_accuracy': val_acc,  
                'test_accuracy': test_acc,
                'test_auc': test_auc,
                'overfitting_gap': overfitting_gap,
                'overfitting_detected': overfitting_gap > 0.1,
                'history': history.history,
                'seq_length': seq_length
            }
        }
        
        print(f"âœ… Enhanced LSTM - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.3f}, AUC: {test_auc:.3f}")
        print(f"   ì˜¤ë²„í”¼íŒ… ê²©ì°¨: {overfitting_gap:.3f}")
        
        self.models.update(lstm_results)
        return lstm_results
    
    def analyze_model_errors(self, X_test, y_test):
        """ëª¨ë¸ ì˜¤ë¥˜ ìƒì„¸ ë¶„ì„"""
        print("ğŸ” ëª¨ë¸ ì˜¤ë¥˜ ìƒì„¸ ë¶„ì„ ì¤‘...")
        
        error_analysis = {}
        
        for model_name, model_data in self.models.items():
            if 'model' not in model_data:
                continue
                
            model = model_data['model']
            
            try:
                # ì˜ˆì¸¡ (LSTMì€ ë³„ë„ ì²˜ë¦¬ í•„ìš”)
                if 'lstm' in model_name.lower():
                    # LSTMì€ ì‹œí€€ìŠ¤ ë°ì´í„° í•„ìš”
                    continue
                    
                # ìŠ¤ì¼€ì¼ë§ ì ìš© ì—¬ë¶€
                if model_data.get('scaler') is not None:
                    X_test_processed = model_data['scaler'].transform(X_test)
                else:
                    X_test_processed = X_test
                    
                test_pred = model.predict(X_test_processed)
                
                # ì˜¤ë¥˜ ë¶„ì„
                errors = (y_test != test_pred)
                error_rate = errors.sum() / len(y_test)
                
                # íŠ¹ì„±ë³„ ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
                error_data = X_test[errors]
                correct_data = X_test[~errors]
                
                feature_error_analysis = {}
                for feature in X_test.columns:
                    if X_test[feature].dtype in ['float64', 'int64']:
                        error_mean = error_data[feature].mean()
                        correct_mean = correct_data[feature].mean()
                        difference = abs(error_mean - correct_mean)
                        
                        feature_error_analysis[feature] = {
                            'error_mean': error_mean,
                            'correct_mean': correct_mean,
                            'difference': difference
                        }
                
                # ê°€ì¥ ë¬¸ì œë˜ëŠ” íŠ¹ì„±ë“¤
                problematic_features = sorted(feature_error_analysis.items(), 
                                           key=lambda x: x[1]['difference'], reverse=True)[:10]
                
                error_analysis[model_name] = {
                    'error_rate': error_rate,
                    'total_errors': errors.sum(),
                    'problematic_features': problematic_features,
                    'confusion_matrix': confusion_matrix(y_test, test_pred).tolist()
                }
                
                print(f"\nğŸ“Š {model_name} ì˜¤ë¥˜ ë¶„ì„:")
                print(f"   ì˜¤ë¥˜ìœ¨: {error_rate:.3f}")
                print(f"   ë¬¸ì œ íŠ¹ì„± Top 3:")
                for i, (feature, analysis) in enumerate(problematic_features[:3]):
                    print(f"     {i+1}. {feature}: ì°¨ì´ {analysis['difference']:.4f}")
                    
            except Exception as e:
                print(f"âŒ {model_name} ì˜¤ë¥˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                continue
        
        return error_analysis
    
    def create_validation_report(self, leak_validation, error_analysis):
        """ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“ ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        report = {
            'validation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data_leakage_validation': leak_validation,
            'model_performance': {},
            'overfitting_analysis': {},
            'error_analysis': error_analysis,
            'recommendations': []
        }
        
        # ëª¨ë¸ ì„±ëŠ¥ ì •ë¦¬
        for model_name, model_data in self.models.items():
            if 'test_accuracy' in model_data:
                report['model_performance'][model_name] = {
                    'test_accuracy': float(model_data['test_accuracy']),
                    'test_auc': float(model_data.get('test_auc', 0)),
                    'overfitting_detected': model_data.get('overfitting_analysis', {}).get('overfitting_detected', False)
                }
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = max(report['model_performance'].keys(), 
                        key=lambda k: report['model_performance'][k]['test_accuracy'])
        best_accuracy = report['model_performance'][best_model]['test_accuracy']
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        
        if leak_validation['target_leakage_check']:
            recommendations.append("âœ… ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ì„±ê³µ")
        else:
            recommendations.append("âš ï¸ ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬ íŠ¹ì„±ë“¤ ì œê±° í•„ìš”")
            
        # ì˜¤ë²„í”¼íŒ… ê¶Œì¥ì‚¬í•­
        overfitting_models = [name for name, data in report['model_performance'].items() 
                            if data['overfitting_detected']]
        if overfitting_models:
            recommendations.append(f"âš ï¸ ì˜¤ë²„í”¼íŒ… ëª¨ë¸ë“¤: {overfitting_models}")
            recommendations.append("ğŸ”§ ì •ê·œí™” ê°•í™” ë˜ëŠ” ëª¨ë¸ ë³µì¡ë„ ê°ì†Œ í•„ìš”")
        else:
            recommendations.append("âœ… ëª¨ë“  ëª¨ë¸ì—ì„œ ì˜¤ë²„í”¼íŒ… í†µì œë¨")
            
        # ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­
        if best_accuracy < 0.60:
            recommendations.append("ğŸ¯ 60% ëŒíŒŒë¥¼ ìœ„í•œ ì¶”ê°€ ê¸°ë²• í•„ìš”")
            recommendations.append("ğŸ”¬ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê°•í™” ë˜ëŠ” ì•™ìƒë¸” ì ìš©")
        
        report['recommendations'] = recommendations
        report['best_model'] = best_model
        report['best_accuracy'] = best_accuracy
        
        # ë³´ê³ ì„œ ì €ì¥
        with open('data/raw/model_validation_enhancement_report.json', 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        print(f"âœ… ê²€ì¦ ë³´ê³ ì„œ ì €ì¥: data/raw/model_validation_enhancement_report.json")
        
        return report
    
    def run_enhanced_validation(self):
        """ì „ì²´ ê°•í™”ëœ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸ”¬ SPY ëª¨ë¸ ê°•í™”ëœ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹œì‘!")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
        if not self.load_and_validate_data():
            return
            
        # 2. ëˆ„ìˆ˜ ë°©ì§€ íŠ¹ì„± ìƒì„±
        if not self.create_leak_proof_features():
            return
            
        # 3. ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦
        leak_validation = self.validate_data_leakage()
        
        # 4. ê°•ê±´í•œ í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        X_train, X_val, X_test, y_train, y_val, y_test, feature_columns = self.prepare_robust_training_data()
        
        # 5. ì •ê·œí™” ê°•í™” ëª¨ë¸ í›ˆë ¨
        regularized_results = self.train_regularized_models(X_train, X_val, X_test, y_train, y_val, y_test, feature_columns)
        
        # 6. ê°•í™”ëœ LSTM í›ˆë ¨
        lstm_results = self.train_enhanced_lstm(X_train, X_val, X_test, y_train, y_val, y_test)
        
        # 7. ëª¨ë¸ ì˜¤ë¥˜ ë¶„ì„
        error_analysis = self.analyze_model_errors(X_test, y_test)
        
        # 8. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        validation_report = self.create_validation_report(leak_validation, error_analysis)
        
        print("\n" + "=" * 60)
        print("ğŸ† ê°•í™”ëœ ê²€ì¦ ê²°ê³¼:")
        print(f"ğŸ“Š ìµœê³  ëª¨ë¸: {validation_report['best_model']}")
        print(f"ğŸ¯ ìµœê³  ì •í™•ë„: {validation_report['best_accuracy']:.1%}")
        
        if leak_validation['target_leakage_check']:
            print("âœ… ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ì„±ê³µ")
        else:
            print("âš ï¸ ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬ì‚¬í•­ ìˆìŒ")
            
        overfitting_count = sum(1 for data in validation_report['model_performance'].values() 
                               if data['overfitting_detected'])
        print(f"ğŸ” ì˜¤ë²„í”¼íŒ… ëª¨ë¸ ìˆ˜: {overfitting_count}")
        
        print("\nğŸ“‹ ì£¼ìš” ê¶Œì¥ì‚¬í•­:")
        for rec in validation_report['recommendations']:
            print(f"   {rec}")
            
        print(f"\nâœ… ê°•í™”ëœ ê²€ì¦ ì™„ë£Œ!")

def main():
    validator = ModelValidationEnhancement()
    validator.run_enhanced_validation()

if __name__ == "__main__":
    main()