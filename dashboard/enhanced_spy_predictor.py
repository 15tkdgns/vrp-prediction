#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ê°ì • ë¶„ì„ì´ í†µí•©ëœ ê°•í™” SPY ì˜ˆì¸¡ ëª¨ë¸
- ê¸°ì¡´ ê²€ì¦ëœ ê¸°ìˆ ì  íŠ¹ì„± + ë‰´ìŠ¤ ê°ì • íŠ¹ì„±
- ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•œ ë³´ìˆ˜ì  ì ‘ê·¼
- 2-3% ì„±ëŠ¥ í–¥ìƒ ëª©í‘œ (53-56% â†’ 55-58%)
"""

import os
import json
import numpy as np
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from sklearn.feature_selection import SelectKBest, f_classif

from news_data_collector import NewsDataCollector
from llm_sentiment_analyzer import LLMSentimentAnalyzer
import asyncio
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedSPYPredictor:
    """
    ë‰´ìŠ¤ ê°ì • ë¶„ì„ì´ í†µí•©ëœ SPY ì˜ˆì¸¡ ëª¨ë¸
    """
    
    def __init__(self):
        self.news_collector = NewsDataCollector()
        self.sentiment_analyzer = LLMSentimentAnalyzer()
        
        # ê²€ì¦ëœ ê¸°ìˆ ì  íŠ¹ì„± (ê¸°ì¡´ ì—°êµ¬ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™)
        self.base_features = [
            'returns_lag1', 'returns_lag2', 'returns_lag3', 
            'price_to_ma50', 'vix_change', 'volatility_20', 'volume_ratio'
        ]
        
        # ìƒˆë¡œìš´ ê°ì • íŠ¹ì„± (ìµœëŒ€ 3ê°œë¡œ ì œí•œ)
        self.sentiment_features = [
            'news_sentiment_1d',      # 1ì¼ ë‰´ìŠ¤ ê°ì • ì ìˆ˜
            'sentiment_momentum',     # ê°ì • ëª¨ë©˜í…€ (3ì¼ ë³€í™”)
            'news_impact_weighted'    # ì˜í–¥ë„ ê°€ì¤‘ ê°ì • ì ìˆ˜
        ]
        
        self.results = {}\n        \n    def load_clean_data(self, start_date='2019-01-01', end_date='2024-12-31'):\n        \"\"\"ê°€ê²© ë°ì´í„° ë¡œë“œ\"\"\"\n        logger.info(f\"ğŸ“¥ SPY ë° VIX ë°ì´í„° ë¡œë“œ ì¤‘... ({start_date} ~ {end_date})\")\n        \n        try:\n            # SPYì™€ VIX ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n            spy_raw = yf.download('SPY', start=start_date, end=end_date, auto_adjust=True, progress=False)\n            vix_raw = yf.download('^VIX', start=start_date, end=end_date, auto_adjust=True, progress=False)\n            \n            # MultiIndex ì»¬ëŸ¼ ì •ë¦¬\n            if isinstance(spy_raw.columns, pd.MultiIndex):\n                spy_raw.columns = spy_raw.columns.get_level_values(0)\n            if isinstance(vix_raw.columns, pd.MultiIndex):\n                vix_raw.columns = vix_raw.columns.get_level_values(0)\n                \n            logger.info(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: SPY {len(spy_raw)}ì¼, VIX {len(vix_raw)}ì¼\")\n            return spy_raw, vix_raw\n            \n        except Exception as e:\n            logger.error(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n            return None, None\n    \n    def create_technical_features(self, spy_data, vix_data):\n        \"\"\"ê²€ì¦ëœ ê¸°ìˆ ì  íŠ¹ì„± ìƒì„± (ê¸°ì¡´ ì—°êµ¬ ê¸°ë°˜)\"\"\"\n        logger.info(\"ğŸ”§ ê¸°ìˆ ì  íŠ¹ì„± ìƒì„± ì¤‘...\")\n        \n        df = pd.DataFrame(index=spy_data.index)\n        \n        # ê¸°ë³¸ ìˆ˜ìµë¥  (1ì¼ ì§€ì—°ìœ¼ë¡œ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n        returns = spy_data['Close'].pct_change()\n        df['returns_lag1'] = returns.shift(1)\n        df['returns_lag2'] = returns.shift(2)\n        df['returns_lag3'] = returns.shift(3)\n        \n        # ì´ë™í‰ê·  ë¹„ìœ¨ (ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)\n        ma50 = spy_data['Close'].rolling(50).mean()\n        df['price_to_ma50'] = (spy_data['Close'].shift(1) / ma50.shift(1) - 1)\n        \n        # VIX íŠ¹ì„± (1ì¼ ì§€ì—°)\n        vix_aligned = vix_data.reindex(spy_data.index, method='ffill')\n        df['vix_change'] = vix_aligned['Close'].pct_change().shift(1)\n        \n        # ë³€ë™ì„± (ê³¼ê±° ë°ì´í„°ë§Œ)\n        df['volatility_20'] = returns.rolling(20).std().shift(1)\n        \n        # ê±°ë˜ëŸ‰ ë¹„ìœ¨ (ê³¼ê±° ë°ì´í„°ë§Œ)\n        volume_ma = spy_data['Volume'].rolling(20).mean()\n        df['volume_ratio'] = (spy_data['Volume'].shift(1) / volume_ma.shift(1))\n        \n        # íƒ€ê²Ÿ: ë‹¤ìŒë‚  ìˆ˜ìµë¥  ë°©í–¥\n        df['target'] = (spy_data['Close'].shift(-1) / spy_data['Close'] - 1 > 0).astype(int)\n        \n        return df.dropna()\n    \n    async def load_or_create_sentiment_data(self, start_date, end_date):\n        \"\"\"ê°ì • ë¶„ì„ ë°ì´í„° ë¡œë“œ ë˜ëŠ” ìƒì„±\"\"\"\n        logger.info(f\"ğŸ“° ê°ì • ë¶„ì„ ë°ì´í„° ì¤€ë¹„ ì¤‘... ({start_date.date()} ~ {end_date.date()})\")\n        \n        sentiment_data = {}\n        missing_dates = []\n        \n        # ê¸°ì¡´ ê°ì • ë¶„ì„ ë°ì´í„° í™•ì¸\n        current_date = start_date\n        while current_date <= end_date:\n            date_str = current_date.strftime('%Y%m%d')\n            sentiment_file = f\"data/raw/sentiment_analysis_{date_str}.json\"\n            \n            if os.path.exists(sentiment_file):\n                try:\n                    with open(sentiment_file, 'r', encoding='utf-8') as f:\n                        data = json.load(f)\n                        sentiment_data[current_date] = data['daily_summary']\n                except Exception as e:\n                    logger.warning(f\"ê°ì • ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({date_str}): {str(e)}\")\n                    missing_dates.append(current_date)\n            else:\n                missing_dates.append(current_date)\n            \n            current_date += timedelta(days=1)\n        \n        logger.info(f\"ğŸ“Š ê¸°ì¡´ ê°ì • ë°ì´í„°: {len(sentiment_data)}ì¼, ëˆ„ë½: {len(missing_dates)}ì¼\")\n        \n        # ëˆ„ë½ëœ ë‚ ì§œì— ëŒ€í•´ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ê°ì • ë¶„ì„ (ìƒ˜í”Œë§Œ)\n        if missing_dates and len(missing_dates) <= 10:  # ìµœëŒ€ 10ì¼ë§Œ ì²˜ë¦¬\n            logger.info(f\"ğŸ¤– ëˆ„ë½ëœ {len(missing_dates)}ì¼ì˜ ê°ì • ë¶„ì„ ì‹¤í–‰ ì¤‘...\")\n            \n            for date in missing_dates[:5]:  # ìµœëŒ€ 5ì¼ë§Œ ì²˜ë¦¬ (API ë¹„ìš© ê³ ë ¤)\n                try:\n                    # ë‰´ìŠ¤ ìˆ˜ì§‘\n                    articles = await self.news_collector.collect_daily_news(date)\n                    \n                    if articles:\n                        # ê°ì • ë¶„ì„\n                        sentiment_results = await self.sentiment_analyzer.analyze_batch(articles, max_concurrent=3)\n                        \n                        # ê²°ê³¼ ì €ì¥\n                        self.sentiment_analyzer.save_sentiment_analysis(date, sentiment_results)\n                        \n                        # ì¼ì¼ ì ìˆ˜ ê³„ì‚°\n                        daily_score = self.sentiment_analyzer.calculate_daily_sentiment_score(sentiment_results)\n                        sentiment_data[date] = daily_score\n                        \n                        logger.info(f\"âœ… {date.date()} ê°ì • ë¶„ì„ ì™„ë£Œ: {daily_score['overall_sentiment']:.3f}\")\n                    else:\n                        # ë‰´ìŠ¤ê°€ ì—†ëŠ” ê²½ìš° ì¤‘ë¦½ê°’\n                        sentiment_data[date] = {\n                            'overall_sentiment': 0.0,\n                            'market_impact': 0.0,\n                            'confidence': 0.0,\n                            'total_articles': 0\n                        }\n                        \n                except Exception as e:\n                    logger.error(f\"ë‚ ì§œ {date.date()} ê°ì • ë¶„ì„ ì‹¤íŒ¨: {str(e)}\")\n                    # ê¸°ë³¸ê°’ ì„¤ì •\n                    sentiment_data[date] = {\n                        'overall_sentiment': 0.0,\n                        'market_impact': 0.0,\n                        'confidence': 0.0,\n                        'total_articles': 0\n                    }\n        \n        return sentiment_data\n    \n    def create_sentiment_features(self, price_index, sentiment_data):\n        \"\"\"ê°ì • ë¶„ì„ ë°ì´í„°ë¡œë¶€í„° íŠ¹ì„± ìƒì„±\"\"\"\n        logger.info(\"ğŸ§  ê°ì • íŠ¹ì„± ìƒì„± ì¤‘...\")\n        \n        # ê°ì • ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n        sentiment_df = pd.DataFrame([\n            {\n                'date': date,\n                'sentiment': data['overall_sentiment'],\n                'impact': data['market_impact'],\n                'confidence': data['confidence']\n            }\n            for date, data in sentiment_data.items()\n        ])\n        \n        if sentiment_df.empty:\n            logger.warning(\"ê°ì • ë°ì´í„°ê°€ ì—†ìŒ. ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •\")\n            # ê°ì • íŠ¹ì„±ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n            sentiment_features = pd.DataFrame(index=price_index)\n            for feature in self.sentiment_features:\n                sentiment_features[feature] = 0.0\n            return sentiment_features\n        \n        sentiment_df.set_index('date', inplace=True)\n        \n        # ê°€ê²© ì¸ë±ìŠ¤ì— ë§ì¶° ì •ë ¬\n        sentiment_aligned = sentiment_df.reindex(price_index, method='ffill').fillna(0)\n        \n        features = pd.DataFrame(index=price_index)\n        \n        # 1ì¼ ë‰´ìŠ¤ ê°ì • ì ìˆ˜ (1ì¼ ì§€ì—°)\n        features['news_sentiment_1d'] = sentiment_aligned['sentiment'].shift(1)\n        \n        # ê°ì • ëª¨ë©˜í…€ (3ì¼ ë³€í™”ìœ¨)\n        sentiment_3d_avg = sentiment_aligned['sentiment'].rolling(3).mean()\n        features['sentiment_momentum'] = (sentiment_3d_avg.shift(1) - sentiment_3d_avg.shift(4))\n        \n        # ì˜í–¥ë„ ê°€ì¤‘ ê°ì • ì ìˆ˜\n        weighted_sentiment = sentiment_aligned['sentiment'] * sentiment_aligned['impact']\n        features['news_impact_weighted'] = weighted_sentiment.shift(1)\n        \n        # ê²°ì¸¡ê°’ ì²˜ë¦¬\n        features = features.fillna(0)\n        \n        logger.info(f\"âœ… ê°ì • íŠ¹ì„± ìƒì„± ì™„ë£Œ: {features.shape[1]}ê°œ íŠ¹ì„±\")\n        return features\n    \n    async def create_enhanced_dataset(self, start_date='2019-01-01', end_date='2024-12-31'):\n        \"\"\"ê¸°ìˆ ì  íŠ¹ì„± + ê°ì • íŠ¹ì„± í†µí•© ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n        logger.info(\"ğŸ”— ê°•í™”ëœ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n        \n        # 1. ê°€ê²© ë°ì´í„° ë¡œë“œ\n        spy_data, vix_data = self.load_clean_data(start_date, end_date)\n        if spy_data is None:\n            return None\n        \n        # 2. ê¸°ìˆ ì  íŠ¹ì„± ìƒì„±\n        tech_features = self.create_technical_features(spy_data, vix_data)\n        \n        # 3. ê°ì • ë¶„ì„ ë°ì´í„° ë¡œë“œ/ìƒì„±\n        start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n        end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n        sentiment_data = await self.load_or_create_sentiment_data(start_dt, end_dt)\n        \n        # 4. ê°ì • íŠ¹ì„± ìƒì„±\n        sentiment_features = self.create_sentiment_features(tech_features.index, sentiment_data)\n        \n        # 5. íŠ¹ì„± í†µí•©\n        # ê¸°ì¡´ ê²€ì¦ëœ íŠ¹ì„±ë§Œ ì„ íƒ\n        selected_tech_features = tech_features[self.base_features + ['target']]\n        \n        # ê°ì • íŠ¹ì„±ê³¼ ê²°í•©\n        enhanced_df = pd.concat([selected_tech_features, sentiment_features], axis=1)\n        enhanced_df = enhanced_df.dropna()\n        \n        logger.info(f\"âœ… ê°•í™”ëœ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {enhanced_df.shape[0]}í–‰, {enhanced_df.shape[1]-1}ê°œ íŠ¹ì„±\")\n        logger.info(f\"ğŸ“Š íŠ¹ì„± ëª©ë¡: {list(enhanced_df.columns[:-1])}\")\n        \n        return enhanced_df\n    \n    def strict_time_split(self, df):\n        \"\"\"ì—„ê²©í•œ ì‹œê°„ ë¶„í•  (ì˜¤ë²„í”¼íŒ… ë°©ì§€)\"\"\"\n        logger.info(\"ğŸ“Š ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í•  ì¤‘...\")\n        \n        # 2019-2021: í›ˆë ¨, 2022: ê²€ì¦, 2023-2024: í…ŒìŠ¤íŠ¸\n        train_mask = df.index < '2022-01-01'\n        val_mask = (df.index >= '2022-01-01') & (df.index < '2023-01-01')\n        test_mask = df.index >= '2023-01-01'\n        \n        feature_cols = [col for col in df.columns if col != 'target']\n        \n        X_train = df.loc[train_mask, feature_cols]\n        y_train = df.loc[train_mask, 'target']\n        \n        X_val = df.loc[val_mask, feature_cols]\n        y_val = df.loc[val_mask, 'target']\n        \n        X_test = df.loc[test_mask, feature_cols] \n        y_test = df.loc[test_mask, 'target']\n        \n        logger.info(f\"ğŸ“Š ë°ì´í„° ë¶„í• : í›ˆë ¨ {len(X_train)}, ê²€ì¦ {len(X_val)}, í…ŒìŠ¤íŠ¸ {len(X_test)}\")\n        logger.info(f\"ğŸ“Š í›ˆë ¨ í´ë˜ìŠ¤ ë¶„í¬: {dict(y_train.value_counts())}\")\n        \n        return X_train, X_val, X_test, y_train, y_val, y_test\n    \n    def train_enhanced_models(self, X_train, X_val, X_test, y_train, y_val, y_test):\n        \"\"\"ê°•í™”ëœ ëª¨ë¸ í›ˆë ¨ (ë³´ìˆ˜ì  ì ‘ê·¼)\"\"\"\n        logger.info(\"ğŸ¯ ê°•í™”ëœ ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n        \n        models = {\n            'enhanced_lr': LogisticRegression(\n                C=0.1,  # ê°•í•œ ì •ê·œí™”\n                class_weight='balanced',\n                random_state=42,\n                max_iter=1000\n            ),\n            'enhanced_rf': RandomForestClassifier(\n                n_estimators=50,      # ì ê²Œ\n                max_depth=6,          # ì–•ê²Œ  \n                min_samples_split=50, # í¬ê²Œ\n                min_samples_leaf=20,  # í¬ê²Œ\n                max_features=0.6,     # ì ë‹¹íˆ\n                class_weight='balanced',\n                random_state=42\n            )\n        }\n        \n        results = {}\n        \n        for name, model in models.items():\n            logger.info(f\"\\nğŸ”§ {name} í›ˆë ¨ ì¤‘...\")\n            \n            # ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ìŠ¤ì¼€ì¼ë§ í•„ìš”\n            if 'lr' in name:\n                scaler = RobustScaler()\n                X_train_proc = scaler.fit_transform(X_train)\n                X_val_proc = scaler.transform(X_val)\n                X_test_proc = scaler.transform(X_test)\n            else:\n                X_train_proc = X_train\n                X_val_proc = X_val\n                X_test_proc = X_test\n                scaler = None\n            \n            # ëª¨ë¸ í›ˆë ¨\n            model.fit(X_train_proc, y_train)\n            \n            # ì˜ˆì¸¡\n            train_pred = model.predict(X_train_proc)\n            val_pred = model.predict(X_val_proc)\n            test_pred = model.predict(X_test_proc)\n            \n            # ì„±ëŠ¥ ê³„ì‚°\n            train_acc = accuracy_score(y_train, train_pred)\n            val_acc = accuracy_score(y_val, val_pred)\n            test_acc = accuracy_score(y_test, test_pred)\n            \n            # AUC ê³„ì‚°\n            if hasattr(model, 'predict_proba'):\n                test_proba = model.predict_proba(X_test_proc)[:, 1]\n                test_auc = roc_auc_score(y_test, test_proba)\n            else:\n                test_auc = 0.5\n            \n            # ì˜¤ë²„í”¼íŒ… ê²€ì‚¬\n            overfitting_gap = train_acc - val_acc\n            \n            results[name] = {\n                'model': model,\n                'scaler': scaler,\n                'train_accuracy': train_acc,\n                'val_accuracy': val_acc,\n                'test_accuracy': test_acc,\n                'test_auc': test_auc,\n                'overfitting_gap': overfitting_gap,\n                'overfitting': overfitting_gap > 0.1\n            }\n            \n            logger.info(f\"   í›ˆë ¨: {train_acc:.3f} | ê²€ì¦: {val_acc:.3f} | í…ŒìŠ¤íŠ¸: {test_acc:.3f}\")\n            logger.info(f\"   AUC: {test_auc:.3f} | ì˜¤ë²„í”¼íŒ… ê°­: {overfitting_gap:.3f}\")\n            \n            # íŠ¹ì„± ì¤‘ìš”ë„ (Random Forestë§Œ)\n            if hasattr(model, 'feature_importances_'):\n                feature_importance = dict(zip(X_train.columns, model.feature_importances_))\n                top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:5]\n                logger.info(f\"   ìƒìœ„ íŠ¹ì„±: {top_features}\")\n        \n        return results\n    \n    def create_enhanced_report(self, results, feature_names):\n        \"\"\"ê°•í™”ëœ ëª¨ë¸ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±\"\"\"\n        logger.info(\"ğŸ“ ê°•í™”ëœ ëª¨ë¸ ë³´ê³ ì„œ ìƒì„± ì¤‘...\")\n        \n        report = {\n            'experiment_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'model_type': 'enhanced_with_sentiment',\n            'total_features': len(feature_names),\n            'technical_features': len(self.base_features),\n            'sentiment_features': len(self.sentiment_features),\n            'feature_list': list(feature_names),\n            'model_performance': {},\n            'best_model': None,\n            'performance_improvement': {},\n            'conclusions': []\n        }\n        \n        # ì„±ëŠ¥ ì •ë¦¬\n        best_acc = 0\n        best_model_name = None\n        \n        for name, data in results.items():\n            report['model_performance'][name] = {\n                'test_accuracy': float(data['test_accuracy']),\n                'test_auc': float(data['test_auc']),\n                'overfitting_gap': float(data['overfitting_gap']),\n                'has_overfitting': bool(data['overfitting'])\n            }\n            \n            if data['test_accuracy'] > best_acc:\n                best_acc = data['test_accuracy']\n                best_model_name = name\n        \n        report['best_model'] = best_model_name\n        report['best_accuracy'] = float(best_acc)\n        \n        # ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ (ê°€ìƒì˜ ë² ì´ìŠ¤ë¼ì¸)\n        baseline_accuracy = 0.535  # ê¸°ì¡´ ì—°êµ¬ì˜ ë³´ìˆ˜ì  ëª¨ë¸ ì„±ëŠ¥\n        improvement = best_acc - baseline_accuracy\n        \n        report['performance_improvement'] = {\n            'baseline_accuracy': baseline_accuracy,\n            'enhanced_accuracy': best_acc,\n            'absolute_improvement': improvement,\n            'relative_improvement': improvement / baseline_accuracy * 100\n        }\n        \n        # ê²°ë¡ \n        conclusions = []\n        if improvement > 0.02:\n            conclusions.append(\"âœ… ë‰´ìŠ¤ ê°ì • ë¶„ì„ìœ¼ë¡œ 2%+ ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±\")\n        elif improvement > 0.01:\n            conclusions.append(\"ğŸ¯ ë‰´ìŠ¤ ê°ì • ë¶„ì„ìœ¼ë¡œ 1%+ ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±\")\n        else:\n            conclusions.append(\"âš ï¸ ë‰´ìŠ¤ ê°ì • ë¶„ì„ì˜ ì„±ëŠ¥ í–¥ìƒ ë¯¸ë¯¸\")\n        \n        if any(data['overfitting'] for data in results.values()):\n            conclusions.append(\"âš ï¸ ì¼ë¶€ ëª¨ë¸ì—ì„œ ì˜¤ë²„í”¼íŒ… ê°ì§€\")\n        else:\n            conclusions.append(\"âœ… ì˜¤ë²„í”¼íŒ… ë°©ì§€ ì„±ê³µ\")\n        \n        report['conclusions'] = conclusions\n        \n        # ì €ì¥\n        filename = 'data/raw/enhanced_spy_predictor_report.json'\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(report, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"ğŸ’¾ ê°•í™”ëœ ëª¨ë¸ ë³´ê³ ì„œ ì €ì¥: {filename}\")\n        return report\n    \n    async def run_enhanced_experiment(self):\n        \"\"\"ê°•í™”ëœ SPY ì˜ˆì¸¡ ì‹¤í—˜ ì‹¤í–‰\"\"\"\n        logger.info(\"ğŸš€ ë‰´ìŠ¤ ê°ì • ë¶„ì„ í†µí•© SPY ì˜ˆì¸¡ ì‹¤í—˜ ì‹œì‘!\")\n        logger.info(\"=\" * 60)\n        \n        try:\n            # 1. ê°•í™”ëœ ë°ì´í„°ì…‹ ìƒì„±\n            enhanced_df = await self.create_enhanced_dataset()\n            if enhanced_df is None:\n                logger.error(\"ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨\")\n                return\n            \n            # 2. ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í• \n            X_train, X_val, X_test, y_train, y_val, y_test = self.strict_time_split(enhanced_df)\n            \n            # 3. ê°•í™”ëœ ëª¨ë¸ í›ˆë ¨\n            results = self.train_enhanced_models(X_train, X_val, X_test, y_train, y_val, y_test)\n            \n            # 4. ë³´ê³ ì„œ ìƒì„±\n            feature_names = [col for col in enhanced_df.columns if col != 'target']\n            report = self.create_enhanced_report(results, feature_names)\n            \n            # 5. ê²°ê³¼ ì¶œë ¥\n            logger.info(\"\\n\" + \"=\" * 60)\n            logger.info(\"ğŸ† ë‰´ìŠ¤ ê°ì • ë¶„ì„ í†µí•© ì‹¤í—˜ ê²°ê³¼:\")\n            logger.info(f\"ğŸ¯ ìµœê³  ëª¨ë¸: {report['best_model']}\")\n            logger.info(f\"ğŸ“Š ìµœê³  ì •í™•ë„: {report['best_accuracy']:.1%}\")\n            logger.info(f\"ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ: +{report['performance_improvement']['absolute_improvement']:.1%}\")\n            logger.info(f\"ğŸ”§ ì´ íŠ¹ì„± ìˆ˜: {report['total_features']}ê°œ (ê¸°ìˆ ì  {report['technical_features']} + ê°ì • {report['sentiment_features']})\")\n            \n            logger.info(\"\\nğŸ“‹ ì£¼ìš” ê²°ë¡ :\")\n            for conclusion in report['conclusions']:\n                logger.info(f\"   {conclusion}\")\n            \n            logger.info(f\"\\nâœ… ê°•í™”ëœ ì‹¤í—˜ ì™„ë£Œ! ë³´ê³ ì„œ: data/raw/enhanced_spy_predictor_report.json\")\n            \n            self.results = results\n            return results\n            \n        except Exception as e:\n            logger.error(f\"âŒ ê°•í™”ëœ ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}\")\n            return None\n\nasync def main():\n    \"\"\"ê°•í™”ëœ SPY ì˜ˆì¸¡ ì‹¤í—˜ ì‹¤í–‰\"\"\"\n    predictor = EnhancedSPYPredictor()\n    await predictor.run_enhanced_experiment()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())