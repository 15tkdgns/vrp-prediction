#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
- News APIë¥¼ í†µí•œ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘
- SPY/S&P 500 ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§
- ê°ì • ë¶„ì„ì„ ìœ„í•œ ì „ì²˜ë¦¬
"""

import os
import json
import requests
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import pandas as pd
import logging
from dataclasses import dataclass

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    title: str
    description: str
    content: str
    url: str
    published_at: datetime
    source: str
    spy_relevance: float = 0.0
    market_impact_potential: float = 0.0

class NewsDataCollector:
    """
    ë‹¤ì¤‘ ì†ŒìŠ¤ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ê¸°
    """
    
    def __init__(self):
        # API í‚¤ë“¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        self.news_api_key = os.getenv('NEWS_API_KEY', 'your_news_api_key_here')
        self.alpha_vantage_key = os.getenv('ALPHA_VANTAGE_KEY', 'your_alpha_vantage_key')
        
        # SPY/ì‹œì¥ ê´€ë ¨ í‚¤ì›Œë“œ
        self.spy_keywords = [
            'S&P 500', 'SPY ETF', 'S&P500', 'SP500',
            'broad market', 'large cap', 'market index',
            'equity market', 'stock market index'
        ]
        
        self.market_keywords = [
            'Federal Reserve', 'Fed', 'interest rates', 'inflation',
            'GDP', 'unemployment', 'earnings', 'economic growth',
            'recession', 'bull market', 'bear market',
            'market volatility', 'VIX', 'market sentiment'
        ]
        
        # ê°ì • ê´€ë ¨ í‚¤ì›Œë“œ
        self.positive_keywords = [
            'rally', 'surge', 'bullish', 'growth', 'strong', 'beat',
            'exceed', 'optimistic', 'positive', 'gains', 'rise', 'up'
        ]
        
        self.negative_keywords = [
            'crash', 'drop', 'fall', 'bearish', 'recession', 'weak',
            'miss', 'decline', 'pessimistic', 'negative', 'losses', 'down'
        ]
        
        # ë‰´ìŠ¤ ì†ŒìŠ¤ ì‹ ë¢°ë„ (0-1)
        self.source_credibility = {
            'reuters': 0.95,
            'bloomberg': 0.95,
            'wsj': 0.90,
            'cnbc': 0.85,
            'marketwatch': 0.80,
            'yahoo-finance': 0.75,
            'seeking-alpha': 0.70,
            'motley-fool': 0.65
        }
    
    def calculate_spy_relevance(self, title: str, description: str) -> float:
        """
        ë‰´ìŠ¤ì˜ SPY ê´€ë ¨ë„ ê³„ì‚° (0-1)
        """
        text = f"{title} {description}".lower()
        
        # ì§ì ‘ ê´€ë ¨ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        spy_score = 0.0
        for keyword in self.spy_keywords:
            if keyword.lower() in text:
                spy_score += 0.3  # ì§ì ‘ ì–¸ê¸‰ ì‹œ ë†’ì€ ì ìˆ˜
        
        # ì‹œì¥ ê´€ë ¨ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        for keyword in self.market_keywords:
            if keyword.lower() in text:
                spy_score += 0.1  # ê°„ì ‘ ê´€ë ¨
        
        return min(spy_score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def calculate_market_impact_potential(self, title: str, description: str, source: str) -> float:
        """
        ì‹œì¥ ì˜í–¥ë„ ì ì¬ë ¥ ê³„ì‚° (0-1)
        """
        text = f"{title} {description}".lower()
        
        # ê¸°ë³¸ ì†ŒìŠ¤ ì‹ ë¢°ë„
        impact_score = self.source_credibility.get(source.lower(), 0.5)
        
        # ê°•í•œ ê°ì • í‘œí˜„ ê°€ì¤‘ì¹˜
        strong_sentiment = 0.0
        for keyword in self.positive_keywords + self.negative_keywords:
            if keyword in text:
                strong_sentiment += 0.1
        
        # Fed, ê²½ì œì§€í‘œ ê´€ë ¨ ì¶”ê°€ ê°€ì¤‘ì¹˜
        high_impact_terms = ['federal reserve', 'fed rate', 'inflation data', 
                           'gdp', 'unemployment rate', 'earnings report']
        for term in high_impact_terms:
            if term in text:
                impact_score += 0.2
        
        return min(impact_score + strong_sentiment, 1.0)
    
    async def collect_news_api_data(self, date: datetime) -> List[NewsArticle]:
        """
        News APIì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
        """
        logger.info(f"News APIì—ì„œ {date.date()} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        
        # í‚¤ì›Œë“œ ì¿¼ë¦¬ êµ¬ì„±
        query = 'S&P 500 OR SPY OR "stock market" OR "Federal Reserve" OR "interest rates"'
        
        url = 'https://newsapi.org/v2/everything'
        params = {
            'q': query,
            'from': date.strftime('%Y-%m-%d'),
            'to': date.strftime('%Y-%m-%d'),
            'language': 'en',
            'sortBy': 'relevancy',
            'pageSize': 100,
            'apiKey': self.news_api_key
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        articles = []\n                        \n                        for item in data.get('articles', []):\n                            if not item.get('title') or not item.get('description'):\n                                continue\n                            \n                            # SPY ê´€ë ¨ë„ ê³„ì‚°\n                            spy_relevance = self.calculate_spy_relevance(\n                                item['title'], item['description']\n                            )\n                            \n                            # ê´€ë ¨ë„ê°€ ë‚®ìœ¼ë©´ ìŠ¤í‚µ\n                            if spy_relevance < 0.2:\n                                continue\n                            \n                            # ì‹œì¥ ì˜í–¥ë„ ê³„ì‚°\n                            market_impact = self.calculate_market_impact_potential(\n                                item['title'], item['description'], \n                                item['source']['name']\n                            )\n                            \n                            article = NewsArticle(\n                                title=item['title'],\n                                description=item['description'] or '',\n                                content=item['content'] or '',\n                                url=item['url'],\n                                published_at=datetime.fromisoformat(item['publishedAt'].replace('Z', '+00:00')),\n                                source=item['source']['name'],\n                                spy_relevance=spy_relevance,\n                                market_impact_potential=market_impact\n                            )\n                            articles.append(article)\n                        \n                        logger.info(f\"News APIì—ì„œ {len(articles)}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘\")\n                        return articles\n                    \n                    else:\n                        logger.error(f\"News API ì˜¤ë¥˜: {response.status}\")\n                        return []\n                        \n        except Exception as e:\n            logger.error(f\"News API ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}\")\n            return []\n    \n    async def collect_alpha_vantage_news(self, date: datetime) -> List[NewsArticle]:\n        \"\"\"\n        Alpha Vantage News & Sentiment APIì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘\n        \"\"\"\n        logger.info(f\"Alpha Vantageì—ì„œ {date.date()} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...\")\n        \n        url = 'https://www.alphavantage.co/query'\n        params = {\n            'function': 'NEWS_SENTIMENT',\n            'tickers': 'SPY',\n            'time_from': date.strftime('%Y%m%dT0000'),\n            'time_to': date.strftime('%Y%m%dT2359'),\n            'limit': 50,\n            'apikey': self.alpha_vantage_key\n        }\n        \n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url, params=params) as response:\n                    if response.status == 200:\n                        data = await response.json()\n                        articles = []\n                        \n                        for item in data.get('feed', []):\n                            if not item.get('title') or not item.get('summary'):\n                                continue\n                            \n                            # SPY ê´€ë ¨ë„ëŠ” ì´ë¯¸ SPY í‹°ì»¤ë¡œ í•„í„°ë§ë¨\n                            spy_relevance = 0.8  # ë†’ì€ ê´€ë ¨ë„\n                            \n                            article = NewsArticle(\n                                title=item['title'],\n                                description=item['summary'],\n                                content=item.get('summary', ''),\n                                url=item['url'],\n                                published_at=datetime.fromisoformat(item['time_published'][:8] + 'T' + item['time_published'][9:]),\n                                source=item.get('source', 'Alpha Vantage'),\n                                spy_relevance=spy_relevance,\n                                market_impact_potential=float(item.get('overall_sentiment_score', 0)) if item.get('overall_sentiment_score') else 0.5\n                            )\n                            articles.append(article)\n                        \n                        logger.info(f\"Alpha Vantageì—ì„œ {len(articles)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘\")\n                        return articles\n                    \n                    else:\n                        logger.error(f\"Alpha Vantage API ì˜¤ë¥˜: {response.status}\")\n                        return []\n                        \n        except Exception as e:\n            logger.error(f\"Alpha Vantage ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}\")\n            return []\n    \n    def filter_and_deduplicate(self, articles: List[NewsArticle]) -> List[NewsArticle]:\n        \"\"\"\n        ë‰´ìŠ¤ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°\n        \"\"\"\n        # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì œê±°\n        seen_titles = set()\n        unique_articles = []\n        \n        for article in articles:\n            title_key = article.title.lower().strip()\n            if title_key not in seen_titles:\n                seen_titles.add(title_key)\n                unique_articles.append(article)\n        \n        # ê´€ë ¨ë„ ë° ì˜í–¥ë„ ê¸°ì¤€ í•„í„°ë§\n        filtered_articles = [\n            article for article in unique_articles\n            if article.spy_relevance >= 0.3 and article.market_impact_potential >= 0.4\n        ]\n        \n        # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n        filtered_articles.sort(key=lambda x: (x.spy_relevance, x.market_impact_potential), reverse=True)\n        \n        return filtered_articles[:20]  # ìƒìœ„ 20ê°œë§Œ ì„ íƒ\n    \n    async def collect_daily_news(self, date: datetime = None) -> List[NewsArticle]:\n        \"\"\"\n        íŠ¹ì • ë‚ ì§œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë©”ì¸ í•¨ìˆ˜)\n        \"\"\"\n        if date is None:\n            date = datetime.now() - timedelta(days=1)  # ì „ë‚  ë‰´ìŠ¤\n        \n        logger.info(f\"ğŸ“° {date.date()} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘\")\n        \n        # ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘\n        tasks = [\n            self.collect_news_api_data(date),\n            self.collect_alpha_vantage_news(date)\n        ]\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # ê²°ê³¼ í†µí•©\n        all_articles = []\n        for result in results:\n            if isinstance(result, list):\n                all_articles.extend(result)\n            else:\n                logger.error(f\"ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {str(result)}\")\n        \n        # í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°\n        filtered_articles = self.filter_and_deduplicate(all_articles)\n        \n        logger.info(f\"âœ… ìµœì¢… {len(filtered_articles)}ê°œ ë‰´ìŠ¤ ì„ ë³„ ì™„ë£Œ\")\n        return filtered_articles\n    \n    def save_news_data(self, articles: List[NewsArticle], date: datetime):\n        \"\"\"\n        ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n        \"\"\"\n        # ë°ì´í„° ì§ë ¬í™”\n        news_data = {\n            'date': date.strftime('%Y-%m-%d'),\n            'collection_time': datetime.now().isoformat(),\n            'total_articles': len(articles),\n            'articles': []\n        }\n        \n        for article in articles:\n            news_data['articles'].append({\n                'title': article.title,\n                'description': article.description,\n                'content': article.content[:500],  # ì²˜ìŒ 500ìë§Œ\n                'url': article.url,\n                'published_at': article.published_at.isoformat(),\n                'source': article.source,\n                'spy_relevance': article.spy_relevance,\n                'market_impact_potential': article.market_impact_potential\n            })\n        \n        # íŒŒì¼ ì €ì¥\n        filename = f\"data/raw/news_data_{date.strftime('%Y%m%d')}.json\"\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\n        \n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(news_data, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"ğŸ’¾ ë‰´ìŠ¤ ë°ì´í„° ì €ì¥: {filename}\")\n    \n    async def run_daily_collection(self):\n        \"\"\"\n        ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰\n        \"\"\"\n        try:\n            # ì „ë‚  ë‰´ìŠ¤ ìˆ˜ì§‘ (ì‹œì¥ ë§ˆê° í›„)\n            yesterday = datetime.now() - timedelta(days=1)\n            articles = await self.collect_daily_news(yesterday)\n            \n            if articles:\n                self.save_news_data(articles, yesterday)\n                \n                # í†µê³„ ì¶œë ¥\n                avg_relevance = sum(a.spy_relevance for a in articles) / len(articles)\n                avg_impact = sum(a.market_impact_potential for a in articles) / len(articles)\n                \n                logger.info(f\"ğŸ“Š ìˆ˜ì§‘ í†µê³„:\")\n                logger.info(f\"   - í‰ê·  SPY ê´€ë ¨ë„: {avg_relevance:.2f}\")\n                logger.info(f\"   - í‰ê·  ì‹œì¥ ì˜í–¥ë„: {avg_impact:.2f}\")\n                logger.info(f\"   - ìƒìœ„ ì†ŒìŠ¤: {[a.source for a in articles[:3]]}\")\n            \n            else:\n                logger.warning(\"ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n                \n        except Exception as e:\n            logger.error(f\"ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}\")\n\ndef main():\n    \"\"\"\n    ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰\n    \"\"\"\n    collector = NewsDataCollector()\n    \n    # ë¹„ë™ê¸° ì‹¤í–‰\n    asyncio.run(collector.run_daily_collection())\n\nif __name__ == \"__main__\":\n    main()