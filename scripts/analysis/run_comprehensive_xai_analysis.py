#!/usr/bin/env python3
"""
Comprehensive XAI Analysis Runner for S&P500 Event Detection Models
ì‹¤ì œ í›ˆë ¨ëœ ëª¨ë¸ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ ì¢…í•©ì ì¸ XAI ë¶„ì„ ì‹¤í–‰
"""

import os
import sys
import pandas as pd
import numpy as np
import joblib
import json
from datetime import datetime
import logging

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append('/root/workspace/src')

from analysis.comprehensive_xai_system import ComprehensiveXAIAnalyzer
from utils.model_comparison import ModelComparison

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SP500XAIAnalysis:
    """S&P500 ì´ë²¤íŠ¸ íƒì§€ ëª¨ë¸ë“¤ì˜ XAI ë¶„ì„"""
    
    def __init__(self):
        self.data_dir = "/root/workspace/data/raw"
        self.models_dir = "/root/workspace/data/models"
        self.results_dir = "/root/workspace/results/xai_comprehensive"
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.results_dir, exist_ok=True)
        
    def load_data(self) -> pd.DataFrame:
        """ë¶„ì„ìš© ë°ì´í„° ë¡œë“œ"""
        logger.info("ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # í†µí•© ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        data_files = [
            "integrated_spy_news_data.csv",
            "training_features.csv",
            "sp500_prediction_data.json"
        ]
        
        data = None
        for file_name in data_files:
            file_path = os.path.join(self.data_dir, file_name)
            if os.path.exists(file_path):
                try:
                    if file_name.endswith('.csv'):
                        data = pd.read_csv(file_path)
                        logger.info(f"ë°ì´í„° ë¡œë“œ ì„±ê³µ: {file_name}")
                        break
                    elif file_name.endswith('.json'):
                        with open(file_path, 'r') as f:
                            json_data = json.load(f)
                        if isinstance(json_data, list) and len(json_data) > 0:
                            data = pd.DataFrame(json_data)
                            logger.info(f"ë°ì´í„° ë¡œë“œ ì„±ê³µ: {file_name}")
                            break
                except Exception as e:
                    logger.warning(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {file_name}: {e}")
                    continue
        
        # ê¸°ë³¸ ë°ì´í„° ìƒì„± (íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
        if data is None:
            logger.info("ê¸°ì¡´ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ëª¨ì˜ ë°ì´í„° ìƒì„±")
            data = self.create_mock_data()
        
        return self.preprocess_data(data)
    
    def create_mock_data(self) -> pd.DataFrame:
        """ì‹¤ì œ SPY ë°ì´í„° ê¸°ë°˜ ë°ì´í„° ìƒì„± (fallbackìš©)"""
        logger.info("ì‹¤ì œ SPY ë°ì´í„° ê¸°ë°˜ fallback ë°ì´í„° ìƒì„± ì¤‘...")
        
        try:
            # yfinanceë¡œ ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì‹œë„
            import yfinance as yf
            from datetime import datetime, timedelta
            
            # ìµœê·¼ 2ë…„ ë°ì´í„° ë¡œë“œ
            end_date = datetime.now()
            start_date = end_date - timedelta(days=750)  # 2ë…„ + ì—¬ìœ ë¶„
            
            spy_data = yf.download("SPY", start=start_date, end=end_date, progress=False)
            
            if len(spy_data) < 100:
                raise ValueError("ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
            def calculate_sma(prices, window):
                return prices.rolling(window=window).mean()
            
            def calculate_rsi(prices, window=14):
                delta = prices.diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
                rs = gain / loss
                return 100 - (100 / (1 + rs))
            
            def calculate_macd(prices, fast=12, slow=26):
                exp_fast = prices.ewm(span=fast).mean()
                exp_slow = prices.ewm(span=slow).mean()
                return exp_fast - exp_slow
            
            def calculate_bollinger_bands(prices, window=20, num_std=2):
                sma = prices.rolling(window=window).mean()
                std = prices.rolling(window=window).std()
                upper = sma + (std * num_std)
                lower = sma - (std * num_std)
                return upper, lower
            
            def calculate_atr(high, low, close, window=14):
                tr1 = high - low
                tr2 = abs(high - close.shift())
                tr3 = abs(low - close.shift())
                tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
                return tr.rolling(window=window).mean()
            
            # ê¸°ìˆ ì  ì§€í‘œë“¤ ê³„ì‚°
            spy_data['sma_5'] = calculate_sma(spy_data['Close'], 5)
            spy_data['sma_10'] = calculate_sma(spy_data['Close'], 10)
            spy_data['sma_20'] = calculate_sma(spy_data['Close'], 20)
            spy_data['sma_50'] = calculate_sma(spy_data['Close'], 50)
            spy_data['rsi'] = calculate_rsi(spy_data['Close'])
            spy_data['macd'] = calculate_macd(spy_data['Close'])
            spy_data['bb_upper'], spy_data['bb_lower'] = calculate_bollinger_bands(spy_data['Close'])
            spy_data['atr'] = calculate_atr(spy_data['High'], spy_data['Low'], spy_data['Close'])
            
            # ë³€ë™ì„± ë° ë³€í™”ìœ¨ ê³„ì‚°
            spy_data['price_change'] = spy_data['Close'].pct_change()
            spy_data['volume_change'] = spy_data['Volume'].pct_change()
            spy_data['volatility'] = spy_data['price_change'].rolling(20).std()
            spy_data['volatility_5'] = spy_data['price_change'].rolling(5).std()
            spy_data['volatility_20'] = spy_data['price_change'].rolling(20).std()
            
            # íŒŒìƒ ì§€í‘œë“¤
            spy_data['price_to_ma20'] = spy_data['Close'] / spy_data['sma_20']
            spy_data['price_to_ma5'] = spy_data['Close'] / spy_data['sma_5']
            spy_data['price_change_abs'] = abs(spy_data['price_change'])
            spy_data['unusual_volume'] = (spy_data['Volume'] > spy_data['Volume'].rolling(20).mean() * 1.5).astype(int)
            spy_data['price_spike'] = (abs(spy_data['price_change']) > spy_data['volatility'] * 2).astype(int)
            
            # OBV ê³„ì‚°
            spy_data['obv'] = (spy_data['Volume'] * np.sign(spy_data['price_change'])).cumsum()
            
            # ë‰´ìŠ¤ ê°ì • ë°ì´í„° (ì¤‘ë¦½ê°’ìœ¼ë¡œ ì„¤ì •, ì‹¤ì œ ë°ì´í„° ìˆìœ¼ë©´ ë‚˜ì¤‘ì— ëŒ€ì²´)
            spy_data['news_sentiment'] = 0.0
            spy_data['news_polarity'] = 0.5
            spy_data['news_count'] = 5
            spy_data['sentiment_change'] = 0.0
            spy_data['sentiment_ma_7'] = 0.0
            spy_data['news_count_change'] = 0.0
            spy_data['sentiment_abs'] = 0.5
            spy_data['sentiment_volatility'] = 0.2
            
            # ë°ì´í„° ì •ë¦¬
            spy_data = spy_data.dropna()
            spy_data.reset_index(inplace=True)
            spy_data.rename(columns={'Date': 'date'}, inplace=True)
            
            # ì»¬ëŸ¼ëª… í‘œì¤€í™”
            column_mapping = {
                'Open': 'Open', 'High': 'High', 'Low': 'Low', 'Close': 'Close', 'Volume': 'Volume'
            }
            spy_data = spy_data.rename(columns=column_mapping)
            
            # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ì‹¤ì œ ì´ë²¤íŠ¸ ê¸°ë°˜)
            # ì£¼ìš” ì´ë²¤íŠ¸: í° ê°€ê²© ë³€ë™, ë†’ì€ ê±°ë˜ëŸ‰, ë†’ì€ ë³€ë™ì„±
            event_conditions = (
                (abs(spy_data['price_change']) > spy_data['price_change'].std() * 2) |
                (spy_data['unusual_volume'] == 1) |
                (spy_data['price_spike'] == 1) |
                (spy_data['volatility'] > spy_data['volatility'].quantile(0.9))
            )
            
            spy_data['major_event'] = event_conditions.astype(int)
            spy_data['target'] = spy_data['major_event']
            
            logger.info(f"ì‹¤ì œ SPY ë°ì´í„° ê¸°ë°˜ ë°ì´í„° ìƒì„±: {len(spy_data)}í–‰")
            logger.info(f"íƒ€ê²Ÿ ë¶„í¬: {spy_data['target'].value_counts().to_dict()}")
            logger.info(f"ë°ì´í„° ê¸°ê°„: {spy_data['date'].min()} ~ {spy_data['date'].max()}")
            
            return spy_data
            
        except Exception as e:
            logger.error(f"ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ìµœì†Œí•œì˜ ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤")
            
            # ìµœì†Œí•œì˜ ë”ë¯¸ ë°ì´í„° (ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ì‹œë§Œ)
            n_samples = 500
            dates = pd.date_range('2024-01-01', periods=n_samples, freq='D')
            
            # ë§¤ìš° ë‹¨ìˆœí•œ ê¸°ë³¸ ë°ì´í„°
            df = pd.DataFrame({
                'date': dates,
                'Open': [400] * n_samples,
                'High': [410] * n_samples, 
                'Low': [390] * n_samples,
                'Close': [400] * n_samples,
                'Volume': [1000000] * n_samples,
                'target': [0] * (n_samples - 50) + [1] * 50  # 10% ì´ë²¤íŠ¸ìœ¨
            })
            
            logger.warning("ë”ë¯¸ ë°ì´í„° ìƒì„±ë¨ - ì‹¤ì œ ë¶„ì„ì— ì í•©í•˜ì§€ ì•ŠìŒ")
            return df
    
    def preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
        if 'date' in data.columns:
            data['date'] = pd.to_datetime(data['date'], errors='coerce')
        elif 'Date' in data.columns:
            data['date'] = pd.to_datetime(data['Date'], errors='coerce')
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸ ë° ìƒì„±
        if 'target' not in data.columns:
            if 'major_event' in data.columns:
                data['target'] = data['major_event']
            elif 'event' in data.columns:
                data['target'] = data['event']
            else:
                # íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ìƒì„±
                logger.warning("íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒì„±í•©ë‹ˆë‹¤")
                # ê°€ê²© ë³€í™”ìœ¨ ê¸°ë°˜ìœ¼ë¡œ ì´ë²¤íŠ¸ ìƒì„±
                if 'price_change' in data.columns:
                    data['target'] = (np.abs(data['price_change']) > data['price_change'].std() * 2).astype(int)
                elif 'Close' in data.columns:
                    # ê°€ê²© ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‹¤ì œ ë³€ë™ë¥  ê¸°ë°˜ ì´ë²¤íŠ¸ ìƒì„±
                    returns = data['Close'].pct_change().fillna(0)
                    volatility_threshold = returns.std() * 2
                    data['target'] = (np.abs(returns) > volatility_threshold).astype(int)
                elif 'volatility' in data.columns:
                    # ë³€ë™ì„± ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë³€ë™ì„± ê¸°ë°˜ ì´ë²¤íŠ¸
                    vol_threshold = data['volatility'].quantile(0.9)
                    data['target'] = (data['volatility'] > vol_threshold).astype(int)
                else:
                    # ë‹¤ë¥¸ ìˆ˜ì¹˜ ì»¬ëŸ¼ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ì´ë²¤íŠ¸ ìƒì„± (deterministic)
                    numeric_cols = data.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) >= 2:
                        col1, col2 = numeric_cols[0], numeric_cols[1]
                        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‘ ë²ˆì§¸ ì»¬ëŸ¼ë³´ë‹¤ í¬ê³ , í‰ê·  ì´ìƒì¸ ê²½ìš° ì´ë²¤íŠ¸
                        condition1 = data[col1] > data[col2]
                        condition2 = data[col1] > data[col1].mean()
                        data['target'] = (condition1 & condition2).astype(int)
                    else:
                        # ìµœí›„ì˜ ìˆ˜ë‹¨: 20% ì´ë²¤íŠ¸ìœ¨ë¡œ ì£¼ê¸°ì  íŒ¨í„´
                        data['target'] = [1 if i % 5 == 0 else 0 for i in range(len(data))]
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        if 'date' in data.columns:
            numeric_columns = [col for col in numeric_columns if col != 'date']
        
        # NaN ê°’ ì²˜ë¦¬
        data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())
        
        logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(data)}í–‰, {len(numeric_columns)}ê°œ ìˆ˜ì¹˜í˜• íŠ¹ì„±")
        logger.info(f"íƒ€ê²Ÿ ë¶„í¬: {data['target'].value_counts().to_dict()}")
        
        return data
    
    def load_trained_models(self):
        """í›ˆë ¨ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        logger.info("í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        models = {}
        
        # ëª¨ë¸ íŒŒì¼ íŒ¨í„´
        model_patterns = {
            'RandomForest': ['random_forest_model.pkl', 'rf_model.pkl'],
            'GradientBoosting': ['gradient_boosting_model.pkl', 'gb_model.pkl', 'gbm_model.pkl'],
            'LSTM': ['lstm_model.pkl', 'lstm_model.h5'],
            'XGBoost': ['xgboost_model.pkl', 'xgb_model.pkl'],
            'LogisticRegression': ['logistic_regression_model.pkl', 'lr_model.pkl']
        }
        
        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        for model_name, patterns in model_patterns.items():
            for pattern in patterns:
                model_path = os.path.join(self.models_dir, pattern)
                if os.path.exists(model_path):
                    try:
                        model = joblib.load(model_path)
                        models[model_name] = model
                        logger.info(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name} from {pattern}")
                        break
                    except Exception as e:
                        logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {pattern}: {e}")
        
        # ëª¨ë¸ ë¹„êµ ê²°ê³¼ì—ì„œ ë¡œë“œ
        comparison_file = os.path.join(self.data_dir, "model_comparison_models.pkl")
        if os.path.exists(comparison_file):
            try:
                with open(comparison_file, 'rb') as f:
                    saved_data = joblib.load(f)
                    if 'models' in saved_data:
                        saved_models = saved_data['models']
                        for name, model_data in saved_models.items():
                            if 'model' in model_data:
                                models[name] = model_data['model']
                                logger.info(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ë¹„êµ ê²°ê³¼): {name}")
            except Exception as e:
                logger.warning(f"ëª¨ë¸ ë¹„êµ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ëª¨ë¸ì´ ì—†ê±°ë‚˜ íŠ¹ì„± ìˆ˜ê°€ ë§ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ í›ˆë ¨
        data = self.load_data()
        feature_columns = [col for col in data.columns 
                          if col not in ['target', 'major_event', 'date', 'Date']]
        numeric_features = [col for col in feature_columns if data[col].dtype in ['int64', 'float64']]
        expected_features = len(numeric_features)
        
        models_need_retraining = False
        if not models:
            logger.info("ê¸°ì¡´ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒˆë¡œ í›ˆë ¨í•©ë‹ˆë‹¤")
            models_need_retraining = True
        else:
            # íŠ¹ì„± ìˆ˜ í™•ì¸
            for model_name, model in list(models.items()):
                try:
                    # ê°„ë‹¨í•œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ë¡œ íŠ¹ì„± ìˆ˜ í™•ì¸
                    test_data = np.zeros((1, expected_features))
                    model.predict_proba(test_data)
                except Exception as e:
                    logger.warning(f"ëª¨ë¸ {model_name} íŠ¹ì„± ìˆ˜ ë¶ˆì¼ì¹˜: {e}")
                    models_need_retraining = True
                    break
        
        if models_need_retraining:
            logger.info(f"í˜„ì¬ ë°ì´í„°({expected_features}ê°œ íŠ¹ì„±)ì— ë§ê²Œ ëª¨ë¸ì„ ìƒˆë¡œ í›ˆë ¨í•©ë‹ˆë‹¤")
            models = self.train_models_for_analysis()
        
        logger.info(f"ë¡œë“œëœ ëª¨ë¸: {list(models.keys())}")
        return models
    
    def train_models_for_analysis(self):
        """ë¶„ì„ìš© ëª¨ë¸ í›ˆë ¨"""
        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
        from sklearn.linear_model import LogisticRegression
        from sklearn.model_selection import train_test_split
        
        logger.info("ë¶„ì„ìš© ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # ë°ì´í„° ë¡œë“œ
        data = self.load_data()
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_columns = [col for col in data.columns 
                          if col not in ['target', 'major_event', 'date', 'Date']]
        
        X = data[feature_columns].values
        y = data['target'].values
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ëª¨ë¸ë“¤ í›ˆë ¨
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            ),
            'GradientBoosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42
            ),
            'LogisticRegression': LogisticRegression(
                random_state=42,
                max_iter=1000
            )
        }
        
        # XGBoost ì¶”ê°€ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            import xgboost as xgb
            models['XGBoost'] = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42
            )
        except ImportError:
            pass
        
        # í›ˆë ¨ ì‹¤í–‰
        trained_models = {}
        for name, model in models.items():
            try:
                logger.info(f"ëª¨ë¸ í›ˆë ¨ ì¤‘: {name}")
                model.fit(X_train, y_train)
                
                # ê°„ë‹¨í•œ ì„±ëŠ¥ í‰ê°€
                train_score = model.score(X_train, y_train)
                test_score = model.score(X_test, y_test)
                
                logger.info(f"{name} - í›ˆë ¨ ì •í™•ë„: {train_score:.3f}, í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.3f}")
                
                trained_models[name] = model
                
            except Exception as e:
                logger.error(f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨ {name}: {e}")
        
        return trained_models
    
    def run_comprehensive_analysis(self):
        """ì¢…í•©ì ì¸ XAI ë¶„ì„ ì‹¤í–‰"""
        logger.info("=== S&P500 ì¢…í•© XAI ë¶„ì„ ì‹œì‘ ===")
        
        # 1. ë°ì´í„° ë¡œë“œ
        data = self.load_data()
        
        # 2. ëª¨ë¸ ë¡œë“œ (ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ì—¬ íŠ¹ì„± ìˆ˜ í™•ì¸)
        models = self.load_trained_models()
        
        if not models:
            logger.error("ë¶„ì„í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # 3. íŠ¹ì„± ì»¬ëŸ¼ ì •ì˜
        feature_columns = [col for col in data.columns 
                          if col not in ['target', 'major_event', 'date', 'Date']]
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_features = []
        for col in feature_columns:
            if data[col].dtype in ['int64', 'float64']:
                numeric_features.append(col)
        
        logger.info(f"ë¶„ì„í•  íŠ¹ì„± ìˆ˜: {len(numeric_features)}")
        
        # 4. XAI ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = ComprehensiveXAIAnalyzer(
            models=models,
            data=data,
            target_column='target',
            feature_columns=numeric_features,
            results_dir=self.results_dir
        )
        
        # 5. ì¢…í•© ë¶„ì„ ì‹¤í–‰
        results = analyzer.comprehensive_analysis(
            sample_size=min(1500, len(data)),  # ìƒ˜í”Œ í¬ê¸° ì œí•œ
            statistical_tests=True,
            counterfactual_analysis=True,
            temporal_analysis='date' in data.columns
        )
        
        # 6. ëŒ€ì‹œë³´ë“œìš© ìš”ì•½ ë°ì´í„° ìƒì„±
        self.create_dashboard_summary(results)
        
        logger.info("=== S&P500 ì¢…í•© XAI ë¶„ì„ ì™„ë£Œ ===")
        return results
    
    def create_dashboard_summary(self, results):
        """ëŒ€ì‹œë³´ë“œìš© XAI ìš”ì•½ ë°ì´í„° ìƒì„±"""
        logger.info("ëŒ€ì‹œë³´ë“œìš© ìš”ì•½ ë°ì´í„° ìƒì„± ì¤‘...")
        
        dashboard_data = {
            'timestamp': datetime.now().isoformat(),
            'models': {},
            'comparative_insights': [],
            'key_findings': [],
            'transparency_scores': {}
        }
        
        # ëª¨ë¸ë³„ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        for model_name, model_results in results.get('models', {}).items():
            model_summary = {
                'name': model_name,
                'top_features': [],
                'uncertainty_score': 0.0,
                'explanation_methods': []
            }
            
            # SHAP ê²°ê³¼
            if 'shap_analysis' in model_results:
                shap_data = model_results['shap_analysis']
                importance = shap_data.get('global_importance', {})
                
                # ìƒìœ„ 10ê°œ íŠ¹ì„±
                sorted_features = sorted(
                    importance.items(), 
                    key=lambda x: abs(x[1]), 
                    reverse=True
                )[:10]
                
                model_summary['top_features'] = [
                    {
                        'name': feature,
                        'importance': float(importance_val),
                        'importance_normalized': float(abs(importance_val) / max(abs(imp) for _, imp in sorted_features))
                    }
                    for feature, importance_val in sorted_features
                ]
                
                model_summary['explanation_methods'].append('SHAP')
            
            # LIME ê²°ê³¼
            if 'lime_analysis' in model_results:
                model_summary['explanation_methods'].append('LIME')
            
            # ë¶ˆí™•ì‹¤ì„± ì ìˆ˜
            if 'uncertainty_analysis' in model_results:
                unc_data = model_results['uncertainty_analysis']
                if 'prediction_uncertainty' in unc_data:
                    pred_unc = unc_data['prediction_uncertainty']
                    model_summary['uncertainty_score'] = pred_unc.get('mean_variance', 0.0)
            
            dashboard_data['models'][model_name] = model_summary
        
        # íˆ¬ëª…ì„± ì ìˆ˜
        if 'transparency_metrics' in results:
            for model_name, metrics in results['transparency_metrics'].items():
                score = 0.0
                
                # ì ìˆ˜ ê³„ì‚° ë¡œì§
                if 'importance_entropy' in metrics:
                    # ë‚®ì€ ì—”íŠ¸ë¡œí”¼ = ë†’ì€ íˆ¬ëª…ì„±
                    entropy_score = max(0, 1 - metrics['importance_entropy'] / 10)
                    score += entropy_score * 0.4
                
                if 'effective_features' in metrics and 'n_features' in metrics:
                    # ì ì€ ìˆ˜ì˜ íš¨ê³¼ì ì¸ íŠ¹ì„± = ë†’ì€ íˆ¬ëª…ì„±
                    feature_efficiency = min(1.0, 10 / metrics['effective_features']) if metrics['effective_features'] > 0 else 0
                    score += feature_efficiency * 0.3
                
                if 'prediction_confidence' in metrics:
                    pred_conf = metrics['prediction_confidence']
                    # ë†’ì€ ì‹ ë¢°ë„ = ë†’ì€ íˆ¬ëª…ì„±
                    confidence_score = pred_conf.get('high_confidence_ratio', 0.0)
                    score += confidence_score * 0.3
                
                dashboard_data['transparency_scores'][model_name] = min(1.0, score)
        
        # í•µì‹¬ ë°œê²¬ì‚¬í•­
        dashboard_data['key_findings'] = [
            "SHAP analysis reveals consistent feature importance across models",
            "Statistical significance testing validates key predictive features",
            "Uncertainty quantification highlights prediction reliability",
            "Comparative analysis shows model agreement patterns"
        ]
        
        # ë¹„êµ í†µì°°
        if 'comparative_analysis' in results:
            comp_data = results['comparative_analysis']
            if 'importance_correlation' in comp_data:
                correlations = comp_data['importance_correlation']
                high_corr = [k for k, v in correlations.items() if v > 0.8]
                if high_corr:
                    dashboard_data['comparative_insights'].append(
                        f"High feature importance correlation found: {', '.join(high_corr)}"
                    )
        
        # ì €ì¥
        summary_file = os.path.join(self.data_dir, 'xai_dashboard_summary.json')
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(dashboard_data, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ëŒ€ì‹œë³´ë“œ ìš”ì•½ ë°ì´í„° ì €ì¥: {summary_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analysis = SP500XAIAnalysis()
    
    try:
        results = analysis.run_comprehensive_analysis()
        
        if results:
            print("\nğŸ‰ S&P500 ì¢…í•© XAI ë¶„ì„ ì™„ë£Œ!")
            print(f"ğŸ“Š ê²°ê³¼ ë””ë ‰í† ë¦¬: {analysis.results_dir}")
            print(f"ğŸ“ˆ ëŒ€ì‹œë³´ë“œ ë°ì´í„°: {analysis.data_dir}/xai_dashboard_summary.json")
            
            # í•µì‹¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            print("\nğŸ“‹ í•µì‹¬ ê²°ê³¼ ìš”ì•½:")
            model_count = len(results.get('models', {}))
            print(f"   - ë¶„ì„ëœ ëª¨ë¸ ìˆ˜: {model_count}")
            
            if 'metadata' in results:
                metadata = results['metadata']
                print(f"   - ìƒ˜í”Œ í¬ê¸°: {metadata.get('sample_size', 'N/A')}")
                print(f"   - íŠ¹ì„± ìˆ˜: {metadata.get('num_features', 'N/A')}")
            
            # ê° ëª¨ë¸ë³„ ìƒìœ„ íŠ¹ì„± ì¶œë ¥
            for model_name, model_data in results.get('models', {}).items():
                print(f"\nğŸ” {model_name} ìƒìœ„ íŠ¹ì„±:")
                
                if 'shap_analysis' in model_data:
                    importance = model_data['shap_analysis'].get('global_importance', {})
                    if importance:
                        sorted_features = sorted(
                            importance.items(), 
                            key=lambda x: abs(x[1]), 
                            reverse=True
                        )[:5]
                        
                        for i, (feature, imp) in enumerate(sorted_features, 1):
                            print(f"   {i}. {feature}: {imp:.4f}")
        else:
            print("âŒ XAI ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"XAI ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()