import pandas as pd
import numpy as np
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.features.llm_feature_extractor_improved import EnhancedLLMFeatureExtractor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class XAIAnalyzer:
    """
    XAI (Explainable AI) ë¶„ì„ì„ ìœ„í•œ ì •ì  ë¶„ì„ê¸°
    Chain-of-Thought ì¶”ë¡ ê³¼ Attention Visualizationì„ í¬í•¨í•œ ì¢…í•©ì ì¸ XAI ë¶„ì„ ìˆ˜í–‰
    """
    
    def __init__(self, data_path: str = "data/raw", output_path: str = "data/processed"):
        """
        XAI ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            data_path: ì›ë³¸ ë°ì´í„° ê²½ë¡œ
            output_path: ë¶„ì„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        """
        self.data_path = Path(data_path)
        self.output_path = Path(output_path)
        self.output_path.mkdir(exist_ok=True)
        
        # Enhanced LLM Feature Extractor ì´ˆê¸°í™”
        self.llm_extractor = EnhancedLLMFeatureExtractor()
        
        logger.info("ğŸš€ XAI Analyzer initialized")
    
    def load_news_data(self, filename: str = "news_sentiment_data.csv") -> pd.DataFrame:
        """
        ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
        
        Args:
            filename: ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ëª…
            
        Returns:
            ë¡œë“œëœ ë‰´ìŠ¤ ë°ì´í„°í”„ë ˆì„
        """
        file_path = self.data_path / filename
        
        if not file_path.exists():
            logger.error(f"âŒ News data file not found: {file_path}")
            return pd.DataFrame()
        
        try:
            df = pd.read_csv(file_path)
            logger.info(f"ğŸ“° Loaded {len(df)} news articles from {filename}")
            
            # ê¸°ë³¸ ë°ì´í„° ì •ì œ
            if 'title' in df.columns:
                df = df.dropna(subset=['title'])  # ì œëª©ì´ ì—†ëŠ” í–‰ ì œê±°
                df = df[df['title'].str.len() > 10]  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œê±°
                logger.info(f"ğŸ“ After cleaning: {len(df)} articles")
            
            return df
        
        except Exception as e:
            logger.error(f"âŒ Error loading news data: {e}")
            return pd.DataFrame()
    
    def analyze_sample_data(self, df: pd.DataFrame, sample_size: int = 10) -> pd.DataFrame:
        """
        ìƒ˜í”Œ ë°ì´í„°ì— ëŒ€í•œ XAI ë¶„ì„ ìˆ˜í–‰ (í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            df: ë‰´ìŠ¤ ë°ì´í„°í”„ë ˆì„
            sample_size: ë¶„ì„í•  ìƒ˜í”Œ ê°œìˆ˜
            
        Returns:
            XAI ë¶„ì„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
        """
        if df.empty:
            logger.warning("âš ï¸ Empty dataframe provided for analysis")
            return pd.DataFrame()
        
        # ìƒ˜í”Œ ì„ íƒ (ë‹¤ì–‘ì„±ì„ ìœ„í•´ ê³ ë¥´ê²Œ ë¶„í¬ëœ ìƒ˜í”Œ)
        if len(df) > sample_size:
            indices = np.linspace(0, len(df) - 1, sample_size, dtype=int)
            sample_df = df.iloc[indices].copy()
        else:
            sample_df = df.copy()
        
        logger.info(f"ğŸ§  Analyzing {len(sample_df)} sample articles with Enhanced LLM...")
        
        # Enhanced LLM íŠ¹ì„± ì¶”ì¶œ
        xai_results = self.llm_extractor.extract_enhanced_features(sample_df)
        
        return xai_results
    
    def create_xai_summary(self, xai_df: pd.DataFrame) -> Dict[str, Any]:
        """
        XAI ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±
        
        Args:
            xai_df: XAI ë¶„ì„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            ìš”ì•½ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if xai_df.empty:
            return {"error": "No XAI data available"}
        
        try:
            summary = {
                "metadata": {
                    "analysis_date": datetime.now().isoformat(),
                    "total_articles_analyzed": len(xai_df),
                    "successful_analyses": len(xai_df[xai_df['processing_status'] == 'success']),
                    "failed_analyses": len(xai_df[xai_df['processing_status'] != 'success']),
                    "model_used": xai_df['model_used'].iloc[0] if not xai_df.empty else "unknown"
                },
                
                "sentiment_analysis": {
                    "sentiment_score_stats": {
                        "mean": float(xai_df['llm_sentiment_score'].mean()),
                        "std": float(xai_df['llm_sentiment_score'].std()),
                        "min": float(xai_df['llm_sentiment_score'].min()),
                        "max": float(xai_df['llm_sentiment_score'].max()),
                        "median": float(xai_df['llm_sentiment_score'].median())
                    },
                    "market_sentiment_distribution": xai_df['market_sentiment'].value_counts().to_dict(),
                    "event_category_distribution": xai_df['event_category'].value_counts().to_dict()
                },
                
                "uncertainty_analysis": {
                    "uncertainty_score_stats": {
                        "mean": float(xai_df['uncertainty_score'].mean()),
                        "std": float(xai_df['uncertainty_score'].std()),
                        "min": float(xai_df['uncertainty_score'].min()),
                        "max": float(xai_df['uncertainty_score'].max()),
                        "median": float(xai_df['uncertainty_score'].median())
                    }
                },
                
                "attention_analysis": {
                    "avg_tokens_per_article": float(xai_df['attention_tokens'].apply(len).mean()),
                    "total_unique_tokens": len(set([token for tokens in xai_df['attention_tokens'] for token in tokens])),
                    "articles_with_attention": len(xai_df[xai_df['attention_tokens'].apply(len) > 0])
                }
            }
            
            return summary
        
        except Exception as e:
            logger.error(f"âŒ Error creating XAI summary: {e}")
            return {"error": str(e)}
    
    def create_dashboard_data(self, xai_df: pd.DataFrame) -> Dict[str, Any]:
        """
        ëŒ€ì‹œë³´ë“œìš© XAI ë°ì´í„° ìƒì„±
        
        Args:
            xai_df: XAI ë¶„ì„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° êµ¬ì¡°
        """
        if xai_df.empty:
            return {"error": "No XAI data available"}
        
        try:
            # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ë°ì´í„°ë§Œ í•„í„°ë§
            successful_df = xai_df[xai_df['processing_status'] == 'success'].copy()
            
            dashboard_data = {
                "xai_samples": []
            }
            
            for idx, row in successful_df.iterrows():
                # Chain-of-Thought ì¶”ë¡  ë‹¨ê³„ íŒŒì‹±
                reasoning_steps = self._parse_reasoning_steps(row['reasoning_chain'])
                
                # ì–´í…ì…˜ ë°ì´í„° êµ¬ì¡°í™”
                attention_data = {
                    "tokens": row['attention_tokens'] if isinstance(row['attention_tokens'], list) else [],
                    "weights": row['attention_weights'] if isinstance(row['attention_weights'], list) else []
                }
                
                sample_data = {
                    "id": f"sample_{idx}",
                    "title": row['title'],
                    "date": row['date'],
                    "sentiment_score": float(row['llm_sentiment_score']),
                    "uncertainty_score": float(row['uncertainty_score']),
                    "market_sentiment": row['market_sentiment'],
                    "event_category": row['event_category'],
                    "reasoning_steps": reasoning_steps,
                    "attention": attention_data,
                    "model_used": row['model_used']
                }
                
                dashboard_data["xai_samples"].append(sample_data)
            
            return dashboard_data
        
        except Exception as e:
            logger.error(f"âŒ Error creating dashboard data: {e}")
            return {"error": str(e)}
    
    def _parse_reasoning_steps(self, reasoning_chain: str) -> List[Dict[str, str]]:
        """
        ì¶”ë¡  ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ íŒŒì‹±
        
        Args:
            reasoning_chain: ì›ë³¸ ì¶”ë¡  ì²´ì¸ í…ìŠ¤íŠ¸
            
        Returns:
            ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì • ë¦¬ìŠ¤íŠ¸
        """
        if not reasoning_chain or reasoning_chain.strip() == "":
            return []
        
        try:
            steps = []
            current_step = None
            current_content = []
            
            for line in reasoning_chain.split('\n'):
                line = line.strip()
                if not line:
                    continue
                
                # STEPìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ ì°¾ê¸°
                if line.startswith('STEP'):
                    # ì´ì „ ë‹¨ê³„ ì €ì¥
                    if current_step and current_content:
                        steps.append({
                            "step": current_step,
                            "content": '\n'.join(current_content)
                        })
                    
                    # ìƒˆ ë‹¨ê³„ ì‹œì‘
                    current_step = line
                    current_content = []
                
                elif current_step:
                    current_content.append(line)
            
            # ë§ˆì§€ë§‰ ë‹¨ê³„ ì €ì¥
            if current_step and current_content:
                steps.append({
                    "step": current_step,
                    "content": '\n'.join(current_content)
                })
            
            return steps
        
        except Exception as e:
            logger.warning(f"âš ï¸ Error parsing reasoning steps: {e}")
            return [{"step": "Raw Content", "content": reasoning_chain}]
    
    def save_xai_analysis(self, xai_df: pd.DataFrame, summary: Dict, dashboard_data: Dict, 
                         filename_prefix: str = "xai_analysis") -> Dict[str, str]:
        """
        XAI ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            xai_df: XAI ë¶„ì„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
            summary: ë¶„ì„ ìš”ì•½
            dashboard_data: ëŒ€ì‹œë³´ë“œìš© ë°ì´í„°
            filename_prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        saved_files = {}
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # 1. ì „ì²´ XAI ë¶„ì„ ê²°ê³¼ CSV
            csv_path = self.output_path / f"{filename_prefix}_{timestamp}.csv"
            xai_df.to_csv(csv_path, index=False)
            saved_files['csv'] = str(csv_path)
            logger.info(f"ğŸ’¾ Saved XAI CSV data to: {csv_path}")
            
            # 2. ë¶„ì„ ìš”ì•½ JSON
            summary_path = self.output_path / f"{filename_prefix}_summary_{timestamp}.json"
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            saved_files['summary'] = str(summary_path)
            logger.info(f"ğŸ“Š Saved XAI summary to: {summary_path}")
            
            # 3. ëŒ€ì‹œë³´ë“œìš© JSON (ì •ì  íŒŒì¼ëª… - ëŒ€ì‹œë³´ë“œì—ì„œ ì°¸ì¡°í•˜ê¸° ìœ„í•´)
            dashboard_path = self.output_path / "xai_dashboard_data.json"
            with open(dashboard_path, 'w', encoding='utf-8') as f:
                json.dump(dashboard_data, f, indent=2, ensure_ascii=False)
            saved_files['dashboard'] = str(dashboard_path)
            logger.info(f"ğŸ“± Saved dashboard data to: {dashboard_path}")
            
            return saved_files
        
        except Exception as e:
            logger.error(f"âŒ Error saving XAI analysis: {e}")
            return {"error": str(e)}
    
    def run_complete_analysis(self, sample_size: int = 10, 
                            news_file: str = "news_sentiment_data.csv") -> Dict[str, Any]:
        """
        ì™„ì „í•œ XAI ë¶„ì„ ì‹¤í–‰
        
        Args:
            sample_size: ë¶„ì„í•  ìƒ˜í”Œ ê°œìˆ˜
            news_file: ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ëª…
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ì •ë³´
        """
        logger.info("ğŸš€ Starting complete XAI analysis...")
        
        try:
            # 1. ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
            news_df = self.load_news_data(news_file)
            if news_df.empty:
                return {"error": "Failed to load news data"}
            
            # 2. XAI ë¶„ì„ ìˆ˜í–‰
            xai_df = self.analyze_sample_data(news_df, sample_size)
            if xai_df.empty:
                return {"error": "XAI analysis failed"}
            
            # 3. ìš”ì•½ í†µê³„ ìƒì„±
            summary = self.create_xai_summary(xai_df)
            
            # 4. ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±
            dashboard_data = self.create_dashboard_data(xai_df)
            
            # 5. ê²°ê³¼ ì €ì¥
            saved_files = self.save_xai_analysis(xai_df, summary, dashboard_data)
            
            logger.info("âœ… Complete XAI analysis finished successfully!")
            
            return {
                "status": "success",
                "articles_analyzed": len(xai_df),
                "successful_analyses": len(xai_df[xai_df['processing_status'] == 'success']),
                "saved_files": saved_files,
                "summary": summary
            }
        
        except Exception as e:
            logger.error(f"âŒ Complete XAI analysis failed: {e}")
            return {"error": str(e)}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸŒŸ Starting XAI Analysis System")
    
    # XAI ë¶„ì„ê¸° ìƒì„±
    analyzer = XAIAnalyzer()
    
    # ì™„ì „í•œ ë¶„ì„ ì‹¤í–‰ (ìƒ˜í”Œ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥)
    result = analyzer.run_complete_analysis(sample_size=5)  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 5ê°œë§Œ ë¶„ì„
    
    # ê²°ê³¼ ì¶œë ¥
    if "error" in result:
        logger.error(f"âŒ Analysis failed: {result['error']}")
    else:
        logger.info("âœ… XAI Analysis completed successfully!")
        logger.info(f"ğŸ“ˆ Articles analyzed: {result['articles_analyzed']}")
        logger.info(f"âœ¨ Successful analyses: {result['successful_analyses']}")
        logger.info(f"ğŸ’¾ Files saved: {list(result['saved_files'].keys())}")
        
        # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
        if 'summary' in result:
            summary = result['summary']
            logger.info(f"ğŸ“Š Average sentiment score: {summary['sentiment_analysis']['sentiment_score_stats']['mean']:.3f}")
            logger.info(f"ğŸ¯ Market sentiment distribution: {summary['sentiment_analysis']['market_sentiment_distribution']}")


if __name__ == "__main__":
    main()