#!/usr/bin/env python3
"""
ğŸ“ íš¨ê³¼ í¬ê¸° ë¶„ì„ê¸°
í•™ìˆ  ë…¼ë¬¸ì„ ìœ„í•œ ì‹¤ìš©ì  ìœ ì˜ì„± í‰ê°€ ë„êµ¬

ì£¼ìš” ê¸°ëŠ¥:
- Cohen's d ë° ë³€í˜•ë“¤
- ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ íš¨ê³¼ í¬ê¸°
- ê¸ˆìœµ íŠ¹í™” íš¨ê³¼ í¬ê¸° ì§€í‘œ
- íš¨ê³¼ í¬ê¸° í•´ì„ ê°€ì´ë“œ
"""

import numpy as np
import pandas as pd
import scipy.stats as stats
from typing import Dict, List, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

class EffectSizeAnalyzer:
    """
    ë‹¤ì–‘í•œ íš¨ê³¼ í¬ê¸° ì§€í‘œ ê³„ì‚° ë° í•´ì„ í´ë˜ìŠ¤

    í†µê³„ì  ìœ ì˜ì„±ì„ ë„˜ì–´ ì‹¤ìš©ì  ìœ ì˜ì„±ì„ í‰ê°€í•˜ëŠ” ë„êµ¬
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.cohen_guidelines = {
            'd': {'small': 0.2, 'medium': 0.5, 'large': 0.8},
            'r': {'small': 0.1, 'medium': 0.3, 'large': 0.5},
            'eta_squared': {'small': 0.01, 'medium': 0.06, 'large': 0.14},
            'omega_squared': {'small': 0.01, 'medium': 0.06, 'large': 0.14}
        }

        # ê¸ˆìœµ íŠ¹í™” í•´ì„ ê¸°ì¤€
        self.financial_guidelines = {
            'sharpe_ratio_diff': {'negligible': 0.1, 'small': 0.3, 'medium': 0.5, 'large': 0.7},
            'mdd_reduction': {'negligible': 0.01, 'small': 0.05, 'medium': 0.10, 'large': 0.20},
            'accuracy_improvement': {'negligible': 0.01, 'small': 0.02, 'medium': 0.05, 'large': 0.10}
        }

    def cohens_d_paired(self, before: np.ndarray, after: np.ndarray) -> Dict:
        """
        ìŒì²´ ë°ì´í„°ì˜ Cohen's d ê³„ì‚°

        Args:
            before: ì²˜ì¹˜ ì „ ë°ì´í„°
            after: ì²˜ì¹˜ í›„ ë°ì´í„°

        Returns:
            Cohen's d ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if len(before) != len(after):
            raise ValueError("beforeì™€ after ë°°ì—´ì˜ ê¸¸ì´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

        # ì°¨ì´ ì ìˆ˜
        diff = after - before
        mean_diff = np.mean(diff)
        std_diff = np.std(diff, ddof=1)

        # ìŒì²´ Cohen's d
        cohens_d = mean_diff / std_diff if std_diff != 0 else 0

        # í¸í–¥ ë³´ì •ëœ Cohen's d (Hedges' g)
        n = len(diff)
        correction_factor = 1 - (3 / (4 * n - 9)) if n > 3 else 1
        hedges_g = cohens_d * correction_factor

        # íš¨ê³¼ í¬ê¸° í•´ì„
        interpretation = self._interpret_cohens_d(abs(cohens_d))

        return {
            'effect_size_type': "Cohen's d (paired)",
            'cohens_d': cohens_d,
            'hedges_g': hedges_g,
            'interpretation': interpretation,
            'mean_difference': mean_diff,
            'std_difference': std_diff,
            'sample_size': n,
            'practical_significance': abs(cohens_d) >= 0.2  # ì‹¤ìš©ì  ìœ ì˜ì„± ì„ê³„ê°’
        }

    def cohens_d_independent(self, group1: np.ndarray, group2: np.ndarray) -> Dict:
        """
        ë…ë¦½í‘œë³¸ì˜ Cohen's d ê³„ì‚°

        Args:
            group1: ê·¸ë£¹ 1 ë°ì´í„°
            group2: ê·¸ë£¹ 2 ë°ì´í„°

        Returns:
            Cohen's d ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        n1, n2 = len(group1), len(group2)
        mean1, mean2 = np.mean(group1), np.mean(group2)
        var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)

        # í•©ë™ í‘œì¤€í¸ì°¨ (pooled standard deviation)
        pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))

        # Cohen's d
        cohens_d = (mean1 - mean2) / pooled_std if pooled_std != 0 else 0

        # í¸í–¥ ë³´ì •ëœ Cohen's d (Hedges' g)
        df = n1 + n2 - 2
        correction_factor = 1 - (3 / (4 * df - 1)) if df > 1 else 1
        hedges_g = cohens_d * correction_factor

        # Glass's delta (ê·¸ë£¹ 2ì˜ í‘œì¤€í¸ì°¨ ì‚¬ìš©)
        glass_delta = (mean1 - mean2) / np.sqrt(var2) if var2 != 0 else 0

        # íš¨ê³¼ í¬ê¸° í•´ì„
        interpretation = self._interpret_cohens_d(abs(cohens_d))

        return {
            'effect_size_type': "Cohen's d (independent)",
            'cohens_d': cohens_d,
            'hedges_g': hedges_g,
            'glass_delta': glass_delta,
            'interpretation': interpretation,
            'group1_mean': mean1,
            'group2_mean': mean2,
            'mean_difference': mean1 - mean2,
            'pooled_std': pooled_std,
            'sample_size_1': n1,
            'sample_size_2': n2,
            'practical_significance': abs(cohens_d) >= 0.2
        }

    def correlation_effect_size(self, correlation: float, n: int) -> Dict:
        """
        ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ íš¨ê³¼ í¬ê¸° ë¶„ì„

        Args:
            correlation: ìƒê´€ê³„ìˆ˜
            n: í‘œë³¸ í¬ê¸°

        Returns:
            ìƒê´€ê³„ìˆ˜ íš¨ê³¼ í¬ê¸° ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if abs(correlation) > 1:
            raise ValueError("ìƒê´€ê³„ìˆ˜ëŠ” -1ê³¼ 1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤")

        # rÂ²: ì„¤ëª…ëœ ë¶„ì‚°ì˜ ë¹„ìœ¨
        r_squared = correlation ** 2

        # Cohen's ê¸°ì¤€ìœ¼ë¡œ í•´ì„
        r_interpretation = self._interpret_correlation(abs(correlation))

        # ìƒê´€ê³„ìˆ˜ë¥¼ Cohen's dë¡œ ë³€í™˜
        # d = 2r / sqrt(1 - rÂ²)
        if abs(correlation) < 1:
            equivalent_cohens_d = (2 * correlation) / np.sqrt(1 - correlation**2)
        else:
            equivalent_cohens_d = np.inf if correlation > 0 else -np.inf

        return {
            'effect_size_type': 'Correlation Effect Size',
            'correlation': correlation,
            'r_squared': r_squared,
            'variance_explained_percent': r_squared * 100,
            'interpretation': r_interpretation,
            'equivalent_cohens_d': equivalent_cohens_d,
            'sample_size': n,
            'practical_significance': abs(correlation) >= 0.1
        }

    def eta_squared(self, groups: List[np.ndarray]) -> Dict:
        """
        Eta Squared (Î·Â²) ê³„ì‚° - ANOVA íš¨ê³¼ í¬ê¸°

        Args:
            groups: ê·¸ë£¹ë³„ ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            Eta squared ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if len(groups) < 2:
            raise ValueError("ìµœì†Œ 2ê°œ ê·¸ë£¹ì´ í•„ìš”í•©ë‹ˆë‹¤")

        # ì „ì²´ ë°ì´í„° ê²°í•©
        all_data = np.concatenate(groups)
        grand_mean = np.mean(all_data)
        total_n = len(all_data)

        # ì´ ì œê³±í•© (Total Sum of Squares)
        ss_total = np.sum((all_data - grand_mean) ** 2)

        # ì§‘ë‹¨ ê°„ ì œê³±í•© (Between-group Sum of Squares)
        ss_between = 0
        for group in groups:
            group_mean = np.mean(group)
            group_n = len(group)
            ss_between += group_n * (group_mean - grand_mean) ** 2

        # ì§‘ë‹¨ ë‚´ ì œê³±í•© (Within-group Sum of Squares)
        ss_within = ss_total - ss_between

        # Eta Squared
        eta_squared = ss_between / ss_total if ss_total != 0 else 0

        # Partial Eta Squared (ë¶€ë¶„ ì—íƒ€ ì œê³±)
        partial_eta_squared = ss_between / (ss_between + ss_within) if (ss_between + ss_within) != 0 else 0

        # Omega Squared (í¸í–¥ ë³´ì •ëœ íš¨ê³¼ í¬ê¸°)
        k = len(groups)  # ê·¸ë£¹ ìˆ˜
        df_between = k - 1
        df_within = total_n - k
        ms_within = ss_within / df_within if df_within > 0 else 0

        omega_squared = (ss_between - df_between * ms_within) / (ss_total + ms_within) if ss_total + ms_within != 0 else 0
        omega_squared = max(0, omega_squared)  # ìŒìˆ˜ ë°©ì§€

        # í•´ì„
        eta_interpretation = self._interpret_eta_squared(eta_squared)

        return {
            'effect_size_type': 'Eta Squared (ANOVA)',
            'eta_squared': eta_squared,
            'partial_eta_squared': partial_eta_squared,
            'omega_squared': omega_squared,
            'interpretation': eta_interpretation,
            'ss_total': ss_total,
            'ss_between': ss_between,
            'ss_within': ss_within,
            'n_groups': k,
            'total_n': total_n,
            'practical_significance': eta_squared >= 0.01
        }

    def financial_effect_sizes(self, baseline_metrics: Dict, improved_metrics: Dict) -> Dict:
        """
        ê¸ˆìœµ ëª¨ë¸ íŠ¹í™” íš¨ê³¼ í¬ê¸° ê³„ì‚°

        Args:
            baseline_metrics: ê¸°ì¤€ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ
            improved_metrics: ê°œì„  ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ

        Returns:
            ê¸ˆìœµ íŠ¹í™” íš¨ê³¼ í¬ê¸° ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {}

        # 1. ìƒ¤í”„ ë¹„ìœ¨ ê°œì„  íš¨ê³¼
        if 'sharpe_ratio' in baseline_metrics and 'sharpe_ratio' in improved_metrics:
            sharpe_diff = improved_metrics['sharpe_ratio'] - baseline_metrics['sharpe_ratio']
            sharpe_improvement_pct = (sharpe_diff / abs(baseline_metrics['sharpe_ratio'])) * 100 if baseline_metrics['sharpe_ratio'] != 0 else 0

            results['sharpe_ratio_improvement'] = {
                'absolute_difference': sharpe_diff,
                'relative_improvement_percent': sharpe_improvement_pct,
                'interpretation': self._interpret_sharpe_improvement(abs(sharpe_diff)),
                'practical_significance': abs(sharpe_diff) >= 0.1
            }

        # 2. MDD ê°ì†Œ íš¨ê³¼
        if 'max_drawdown' in baseline_metrics and 'max_drawdown' in improved_metrics:
            mdd_reduction = baseline_metrics['max_drawdown'] - improved_metrics['max_drawdown']
            mdd_reduction_pct = (mdd_reduction / baseline_metrics['max_drawdown']) * 100 if baseline_metrics['max_drawdown'] != 0 else 0

            results['mdd_reduction'] = {
                'absolute_reduction': mdd_reduction,
                'relative_reduction_percent': mdd_reduction_pct,
                'interpretation': self._interpret_mdd_reduction(mdd_reduction),
                'practical_significance': mdd_reduction >= 0.01
            }

        # 3. ì •í™•ë„ ê°œì„  íš¨ê³¼
        if 'accuracy' in baseline_metrics and 'accuracy' in improved_metrics:
            accuracy_improvement = improved_metrics['accuracy'] - baseline_metrics['accuracy']
            accuracy_improvement_pct = (accuracy_improvement / baseline_metrics['accuracy']) * 100 if baseline_metrics['accuracy'] != 0 else 0

            results['accuracy_improvement'] = {
                'absolute_improvement': accuracy_improvement,
                'relative_improvement_percent': accuracy_improvement_pct,
                'interpretation': self._interpret_accuracy_improvement(accuracy_improvement),
                'practical_significance': accuracy_improvement >= 0.01
            }

        # 4. ìˆ˜ìµë¥  ê°œì„  íš¨ê³¼
        if 'mean_return' in baseline_metrics and 'mean_return' in improved_metrics:
            return_improvement = improved_metrics['mean_return'] - baseline_metrics['mean_return']
            return_improvement_pct = (return_improvement / abs(baseline_metrics['mean_return'])) * 100 if baseline_metrics['mean_return'] != 0 else 0

            results['return_improvement'] = {
                'absolute_improvement': return_improvement,
                'relative_improvement_percent': return_improvement_pct,
                'annualized_improvement': return_improvement * 252,  # ì¼ì¼ â†’ ì—°ê°„ í™˜ì‚°
                'practical_significance': abs(return_improvement) >= 0.0001  # 0.01% ì¼ì¼ ìˆ˜ìµë¥ 
            }

        # 5. ë³€ë™ì„± ë³€í™” íš¨ê³¼
        if 'volatility' in baseline_metrics and 'volatility' in improved_metrics:
            volatility_change = improved_metrics['volatility'] - baseline_metrics['volatility']
            volatility_change_pct = (volatility_change / baseline_metrics['volatility']) * 100 if baseline_metrics['volatility'] != 0 else 0

            results['volatility_change'] = {
                'absolute_change': volatility_change,
                'relative_change_percent': volatility_change_pct,
                'interpretation': 'Volatility reduced' if volatility_change < 0 else 'Volatility increased',
                'practical_significance': abs(volatility_change) >= 0.001  # 0.1% ë³€ë™ì„± ë³€í™”
            }

        return {
            'effect_size_type': 'Financial Model Comparison',
            'baseline_metrics': baseline_metrics,
            'improved_metrics': improved_metrics,
            'effect_sizes': results,
            'overall_improvement': self._assess_overall_improvement(results)
        }

    def model_performance_effect_size(self, y_true: np.ndarray, y_pred1: np.ndarray, y_pred2: np.ndarray) -> Dict:
        """
        ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ íš¨ê³¼ í¬ê¸° ë¹„êµ

        Args:
            y_true: ì‹¤ì œ ê°’
            y_pred1: ëª¨ë¸ 1 ì˜ˆì¸¡ ê°’
            y_pred2: ëª¨ë¸ 2 ì˜ˆì¸¡ ê°’

        Returns:
            ëª¨ë¸ ì„±ëŠ¥ íš¨ê³¼ í¬ê¸° ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ì˜¤ì°¨ ê³„ì‚°
        errors1 = np.abs(y_true - y_pred1)
        errors2 = np.abs(y_true - y_pred2)

        # ìŒì²´ Cohen's d (ì˜¤ì°¨ ê°ì†Œ)
        error_reduction_cohens_d = self.cohens_d_paired(errors1, errors2)

        # ì„¤ëª…ë ¥ ë¹„êµ (RÂ²)
        def calculate_r2(y_t, y_p):
            ss_res = np.sum((y_t - y_p) ** 2)
            ss_tot = np.sum((y_t - np.mean(y_t)) ** 2)
            return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

        r2_model1 = calculate_r2(y_true, y_pred1)
        r2_model2 = calculate_r2(y_true, y_pred2)
        r2_improvement = r2_model2 - r2_model1

        # ìƒê´€ê³„ìˆ˜ ë¹„êµ
        corr1 = np.corrcoef(y_true, y_pred1)[0, 1] if len(y_true) > 1 else 0
        corr2 = np.corrcoef(y_true, y_pred2)[0, 1] if len(y_true) > 1 else 0

        return {
            'effect_size_type': 'Model Performance Comparison',
            'error_reduction_cohens_d': error_reduction_cohens_d['cohens_d'],
            'error_reduction_interpretation': error_reduction_cohens_d['interpretation'],
            'r2_model1': r2_model1,
            'r2_model2': r2_model2,
            'r2_improvement': r2_improvement,
            'correlation_model1': corr1,
            'correlation_model2': corr2,
            'correlation_improvement': corr2 - corr1,
            'mean_absolute_error_model1': np.mean(errors1),
            'mean_absolute_error_model2': np.mean(errors2),
            'mae_reduction_percent': ((np.mean(errors1) - np.mean(errors2)) / np.mean(errors1)) * 100 if np.mean(errors1) != 0 else 0,
            'sample_size': len(y_true),
            'practical_significance': abs(error_reduction_cohens_d['cohens_d']) >= 0.2
        }

    def _interpret_cohens_d(self, d: float) -> str:
        """Cohen's d í•´ì„"""
        if d < 0.2:
            return "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼"
        elif d < 0.5:
            return "ì‘ì€ íš¨ê³¼"
        elif d < 0.8:
            return "ì¤‘ê°„ íš¨ê³¼"
        else:
            return "í° íš¨ê³¼"

    def _interpret_correlation(self, r: float) -> str:
        """ìƒê´€ê³„ìˆ˜ íš¨ê³¼ í¬ê¸° í•´ì„"""
        if r < 0.1:
            return "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼"
        elif r < 0.3:
            return "ì‘ì€ íš¨ê³¼"
        elif r < 0.5:
            return "ì¤‘ê°„ íš¨ê³¼"
        else:
            return "í° íš¨ê³¼"

    def _interpret_eta_squared(self, eta_sq: float) -> str:
        """Eta squared í•´ì„"""
        if eta_sq < 0.01:
            return "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼"
        elif eta_sq < 0.06:
            return "ì‘ì€ íš¨ê³¼"
        elif eta_sq < 0.14:
            return "ì¤‘ê°„ íš¨ê³¼"
        else:
            return "í° íš¨ê³¼"

    def _interpret_sharpe_improvement(self, diff: float) -> str:
        """ìƒ¤í”„ ë¹„ìœ¨ ê°œì„  í•´ì„"""
        if diff < 0.1:
            return "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ê°œì„ "
        elif diff < 0.3:
            return "ì‘ì€ ê°œì„ "
        elif diff < 0.5:
            return "ì¤‘ê°„ ê°œì„ "
        else:
            return "í° ê°œì„ "

    def _interpret_mdd_reduction(self, reduction: float) -> str:
        """MDD ê°ì†Œ í•´ì„"""
        if reduction < 0.01:
            return "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ê°ì†Œ"
        elif reduction < 0.05:
            return "ì‘ì€ ê°ì†Œ"
        elif reduction < 0.10:
            return "ì¤‘ê°„ ê°ì†Œ"
        else:
            return "í° ê°ì†Œ"

    def _interpret_accuracy_improvement(self, improvement: float) -> str:
        """ì •í™•ë„ ê°œì„  í•´ì„"""
        if improvement < 0.01:
            return "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ê°œì„ "
        elif improvement < 0.02:
            return "ì‘ì€ ê°œì„ "
        elif improvement < 0.05:
            return "ì¤‘ê°„ ê°œì„ "
        else:
            return "í° ê°œì„ "

    def _assess_overall_improvement(self, results: Dict) -> str:
        """ì „ë°˜ì  ê°œì„  í‰ê°€"""
        significant_improvements = 0
        total_metrics = 0

        for metric, result in results.items():
            total_metrics += 1
            if result.get('practical_significance', False):
                significant_improvements += 1

        if significant_improvements == 0:
            return "ì‹¤ìš©ì  ê°œì„  ì—†ìŒ"
        elif significant_improvements / total_metrics < 0.5:
            return "ë¶€ë¶„ì  ê°œì„ "
        elif significant_improvements / total_metrics < 0.8:
            return "ìƒë‹¹í•œ ê°œì„ "
        else:
            return "ì „ë©´ì  ê°œì„ "

def main():
    """í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸ“ íš¨ê³¼ í¬ê¸° ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)

    # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë°ì´í„°
    model1_accuracy = np.random.normal(0.75, 0.05, 30)
    model2_accuracy = np.random.normal(0.78, 0.05, 30)

    # ì˜ˆì¸¡ ì„±ëŠ¥ ë°ì´í„°
    y_true = np.random.normal(0, 1, 100)
    y_pred1 = y_true + np.random.normal(0, 0.5, 100)
    y_pred2 = y_true + np.random.normal(0, 0.4, 100)

    analyzer = EffectSizeAnalyzer()

    # 1. ìŒì²´ Cohen's d
    print("\n1. ëª¨ë¸ ì •í™•ë„ ê°œì„ ì˜ Cohen's d")
    print("-" * 40)
    paired_d = analyzer.cohens_d_paired(model1_accuracy, model2_accuracy)
    print(f"Cohen's d: {paired_d['cohens_d']:.3f}")
    print(f"Hedges' g: {paired_d['hedges_g']:.3f}")
    print(f"í•´ì„: {paired_d['interpretation']}")
    print(f"ì‹¤ìš©ì  ìœ ì˜ì„±: {paired_d['practical_significance']}")

    # 2. ë…ë¦½í‘œë³¸ Cohen's d
    print("\n2. ë…ë¦½í‘œë³¸ Cohen's d")
    print("-" * 40)
    independent_d = analyzer.cohens_d_independent(model1_accuracy[:15], model2_accuracy[:15])
    print(f"Cohen's d: {independent_d['cohens_d']:.3f}")
    print(f"í•´ì„: {independent_d['interpretation']}")

    # 3. ìƒê´€ê³„ìˆ˜ íš¨ê³¼ í¬ê¸°
    print("\n3. ìƒê´€ê³„ìˆ˜ íš¨ê³¼ í¬ê¸°")
    print("-" * 40)
    corr_effect = analyzer.correlation_effect_size(0.65, n=50)
    print(f"ìƒê´€ê³„ìˆ˜: {corr_effect['correlation']:.3f}")
    print(f"ì„¤ëª…ëœ ë¶„ì‚°: {corr_effect['variance_explained_percent']:.1f}%")
    print(f"í•´ì„: {corr_effect['interpretation']}")

    # 4. ëª¨ë¸ ì„±ëŠ¥ íš¨ê³¼ í¬ê¸°
    print("\n4. ëª¨ë¸ ì˜ˆì¸¡ ì„±ëŠ¥ íš¨ê³¼ í¬ê¸°")
    print("-" * 40)
    model_effect = analyzer.model_performance_effect_size(y_true, y_pred1, y_pred2)
    print(f"ì˜¤ì°¨ ê°ì†Œ Cohen's d: {model_effect['error_reduction_cohens_d']:.3f}")
    print(f"RÂ² ê°œì„ : {model_effect['r2_improvement']:.3f}")
    print(f"MAE ê°ì†Œ: {model_effect['mae_reduction_percent']:.1f}%")

    # 5. ê¸ˆìœµ íŠ¹í™” íš¨ê³¼ í¬ê¸°
    print("\n5. ê¸ˆìœµ ëª¨ë¸ íš¨ê³¼ í¬ê¸°")
    print("-" * 40)
    baseline_metrics = {
        'sharpe_ratio': 1.2,
        'max_drawdown': 0.15,
        'accuracy': 0.65,
        'mean_return': 0.0008
    }

    improved_metrics = {
        'sharpe_ratio': 1.5,
        'max_drawdown': 0.12,
        'accuracy': 0.68,
        'mean_return': 0.0010
    }

    financial_effect = analyzer.financial_effect_sizes(baseline_metrics, improved_metrics)
    print(f"ìƒ¤í”„ ë¹„ìœ¨ ê°œì„ : {financial_effect['effect_sizes']['sharpe_ratio_improvement']['absolute_difference']:.2f}")
    print(f"MDD ê°ì†Œ: {financial_effect['effect_sizes']['mdd_reduction']['absolute_reduction']:.3f}")
    print(f"ì „ë°˜ì  ê°œì„ : {financial_effect['overall_improvement']}")

if __name__ == "__main__":
    main()