#!/usr/bin/env python3
"""
ëª¨ë¸ ì„±ëŠ¥ ë¬¸ì œì  ì§„ë‹¨ ë° ê°œì„  ë°©ì•ˆ ë¶„ì„
í˜„ì¬ RÂ² = -0.0209 ë¬¸ì œ ì›ì¸ ê·œëª…
"""

import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from scipy import stats
import warnings
import os

warnings.filterwarnings('ignore')

def load_spy_data():
    """SPY ë°ì´í„° ë¡œë“œ"""
    print("ğŸ“Š SPY ë°ì´í„° ë¡œë“œ ì¤‘...")
    spy = yf.download('SPY', start='2015-01-01', end='2024-12-31', progress=False)
    spy['returns'] = spy['Close'].pct_change()
    spy = spy.dropna()
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(spy)} ê´€ì¸¡ì¹˜")
    return spy

def analyze_basic_statistics(data):
    """ê¸°ë³¸ í†µê³„ ë¶„ì„"""
    print("\nğŸ“ˆ ê¸°ë³¸ í†µê³„ ë¶„ì„")
    print("=" * 50)

    returns = data['returns']

    # ê¸°ë³¸ í†µê³„ëŸ‰
    stats_dict = {
        'Mean': returns.mean(),
        'Std': returns.std(),
        'Skewness': returns.skew(),
        'Kurtosis': returns.kurtosis(),
        'Min': returns.min(),
        'Max': returns.max(),
        'Jarque-Bera p-value': stats.jarque_bera(returns)[1]
    }

    for key, value in stats_dict.items():
        print(f"{key:20}: {value:.6f}")

    return stats_dict

def create_enhanced_features(data):
    """í–¥ìƒëœ íŠ¹ì„± ìƒì„±"""
    print("\nğŸ”§ í–¥ìƒëœ íŠ¹ì„± ìƒì„± ì¤‘...")

    features = pd.DataFrame(index=data.index)
    returns = data['returns']
    prices = data['Close']
    volume = data['Volume']

    # 1. ê¸°ë³¸ ë³€ë™ì„± íŠ¹ì„±
    for window in [5, 10, 20, 50]:
        features[f'volatility_{window}'] = returns.rolling(window).std()
        features[f'realized_vol_{window}'] = features[f'volatility_{window}'] * np.sqrt(252)

    # 2. ìˆ˜ìµë¥  í†µê³„
    for window in [5, 10, 20]:
        features[f'mean_return_{window}'] = returns.rolling(window).mean()
        features[f'skew_{window}'] = returns.rolling(window).skew()
        features[f'kurt_{window}'] = returns.rolling(window).kurt()

    # 3. ë˜ê·¸ ë³€ìˆ˜
    for lag in [1, 2, 3, 5]:
        features[f'return_lag_{lag}'] = returns.shift(lag)
        features[f'vol_lag_{lag}'] = features['volatility_5'].shift(lag)

    # 4. ë³¼ë¥¨ ê¸°ë°˜ íŠ¹ì„± (ìƒˆë¡œ ì¶”ê°€)
    volume_sma_20 = volume.rolling(20).mean()
    features['volume_ratio'] = volume / (volume_sma_20 + 1e-8)
    features['price_volume'] = np.log(prices * volume + 1e-8)

    # 5. ê°€ê²© ëª¨ë©˜í…€ íŠ¹ì„± (ìƒˆë¡œ ì¶”ê°€)
    for window in [5, 10, 20]:
        features[f'momentum_{window}'] = returns.rolling(window).sum()
        features[f'rsi_{window}'] = calculate_rsi(prices, window)

    # 6. ë³€ë™ì„± ë¹„ìœ¨
    features['vol_ratio_5_20'] = features['volatility_5'] / (features['volatility_20'] + 1e-8)
    features['vol_ratio_10_50'] = features['volatility_10'] / (features['volatility_50'] + 1e-8)

    # 7. ê³ ê¸‰ í†µê³„ íŠ¹ì„±
    features['returns_zscore'] = (returns - returns.rolling(20).mean()) / (returns.rolling(20).std() + 1e-8)
    features['vol_zscore'] = (features['volatility_5'] - features['volatility_5'].rolling(50).mean()) / (features['volatility_5'].rolling(50).std() + 1e-8)

    print(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
    return features

def calculate_rsi(prices, window=14):
    """RSI ê³„ì‚°"""
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

def create_future_volatility_targets(data):
    """ë¯¸ë˜ ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„±"""
    print("\nğŸ¯ ë¯¸ë˜ ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„± ì¤‘...")

    targets = pd.DataFrame(index=data.index)
    returns = data['returns']

    # ë‹¤ì–‘í•œ ê¸°ê°„ì˜ ë¯¸ë˜ ë³€ë™ì„±
    for window in [1, 3, 5, 10, 20]:
        vol_values = []
        for i in range(len(returns)):
            if i + window < len(returns):
                future_window = returns.iloc[i+1:i+1+window]
                vol_values.append(future_window.std())
            else:
                vol_values.append(np.nan)
        targets[f'target_vol_{window}d'] = vol_values

    print(f"âœ… íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ: {len(targets.columns)}ê°œ")
    return targets

def analyze_feature_target_correlation(features, targets):
    """íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„"""
    print("\nğŸ” íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„")
    print("=" * 50)

    # ê²°í•© ë°ì´í„° (NaN ì²˜ë¦¬ ê°œì„ )
    combined = pd.concat([features, targets], axis=1)

    # ê° ì»¬ëŸ¼ë³„ NaN ê°œìˆ˜ í™•ì¸
    nan_counts = combined.isnull().sum()
    print(f"\nNaN ê°œìˆ˜ í™•ì¸:")
    for col, count in nan_counts.head(10).items():
        print(f"  {col}: {count}")

    # ìµœì†Œ 200ê°œ ìƒ˜í”Œì´ ë‚¨ë„ë¡ dropna
    combined_clean = combined.dropna(thresh=len(combined.columns) * 0.7)  # 70% ì´ìƒ ë°ì´í„°ê°€ ìˆëŠ” í–‰ë§Œ
    print(f"\në°ì´í„° ì •ë¦¬ í›„ ìƒ˜í”Œ ìˆ˜: {len(combined_clean)}")

    if len(combined_clean) < 100:
        print("âš ï¸ ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ë³´ë‹¤ ê´€ëŒ€í•œ ê¸°ì¤€ ì ìš©")
        combined_clean = combined.dropna(subset=['target_vol_5d'])  # íƒ€ê²Ÿë§Œ í•„ìˆ˜
        combined_clean = combined_clean.fillna(combined_clean.median())  # ë‚˜ë¨¸ì§€ëŠ” ì¤‘ê°„ê°’ìœ¼ë¡œ ì±„ì›€
        print(f"ìˆ˜ì • í›„ ìƒ˜í”Œ ìˆ˜: {len(combined_clean)}")

    # ê° íƒ€ê²Ÿì— ëŒ€í•œ ìµœê³  ìƒê´€ê´€ê³„ íŠ¹ì„± ì°¾ê¸°
    for target_col in targets.columns:
        if target_col in combined_clean.columns and not combined_clean[target_col].isnull().all():
            # ìœ íš¨í•œ íŠ¹ì„±ë§Œ ì„ íƒ
            valid_features = []
            for feature in features.columns:
                if feature in combined_clean.columns and not combined_clean[feature].isnull().all():
                    valid_features.append(feature)

            if valid_features:
                correlations = combined_clean[valid_features].corrwith(combined_clean[target_col]).abs().sort_values(ascending=False)
                correlations = correlations.dropna()  # NaN ìƒê´€ê´€ê³„ ì œê±°

                print(f"\n{target_col} ì˜ˆì¸¡ì„ ìœ„í•œ ìƒìœ„ 10ê°œ íŠ¹ì„±:")
                for i, (feature, corr) in enumerate(correlations.head(10).items()):
                    print(f"  {i+1:2d}. {feature:25}: {corr:.4f}")

    return combined_clean

def test_multiple_models(X, y):
    """ë‹¤ì–‘í•œ ëª¨ë¸ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¤– ë‹¤ì–‘í•œ ëª¨ë¸ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # NaN ì²˜ë¦¬ - ì™„ì „í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
    combined_data = pd.concat([X, y], axis=1).dropna()
    print(f"NaN ì œê±° í›„ ìƒ˜í”Œ ìˆ˜: {len(combined_data)}")

    if len(combined_data) < 100:
        print("âš ï¸ ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
        return {}

    X_clean = combined_data[X.columns]
    y_clean = combined_data[y.name]

    # ì‹œê°„ ìˆœì„œ ë¶„í• 
    split_idx = int(len(X_clean) * 0.8)
    X_train, X_test = X_clean.iloc[:split_idx], X_clean.iloc[split_idx:]
    y_train, y_test = y_clean.iloc[:split_idx], y_clean.iloc[split_idx:]

    print(f"í›ˆë ¨ ì„¸íŠ¸: {len(X_train)} ìƒ˜í”Œ")
    print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(X_test)} ìƒ˜í”Œ")

    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    models = {
        'Ridge (Î±=0.01)': Ridge(alpha=0.01),
        'Ridge (Î±=0.1)': Ridge(alpha=0.1),
        'Ridge (Î±=1.0)': Ridge(alpha=1.0),
        'Ridge (Î±=10.0)': Ridge(alpha=10.0),
        'Ridge (Î±=100.0)': Ridge(alpha=100.0)
    }

    results = {}

    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)

        results[name] = {'r2': r2, 'mse': mse}
        print(f"{name:15}: RÂ² = {r2:7.4f}, MSE = {mse:.6f}")

    return results

def analyze_target_predictability(targets):
    """íƒ€ê²Ÿ ì˜ˆì¸¡ ê°€ëŠ¥ì„± ë¶„ì„"""
    print("\nğŸ¯ íƒ€ê²Ÿ ì˜ˆì¸¡ ê°€ëŠ¥ì„± ë¶„ì„")
    print("=" * 50)

    for col in targets.columns:
        target_data = targets[col].dropna()

        # ìê¸°ìƒê´€ ë¶„ì„
        autocorr_1 = target_data.autocorr(lag=1) if len(target_data) > 1 else 0
        autocorr_5 = target_data.autocorr(lag=5) if len(target_data) > 5 else 0

        # ê¸°ë³¸ í†µê³„
        mean_val = target_data.mean()
        std_val = target_data.std()
        cv = std_val / mean_val if mean_val != 0 else float('inf')

        print(f"{col}:")
        print(f"  Mean: {mean_val:.6f}, Std: {std_val:.6f}, CV: {cv:.4f}")
        print(f"  Autocorr(1): {autocorr_1:.4f}, Autocorr(5): {autocorr_5:.4f}")

def main():
    """ë©”ì¸ ì§„ë‹¨ í•¨ìˆ˜"""
    print("ğŸ”¬ ëª¨ë¸ ì„±ëŠ¥ ë¬¸ì œì  ì§„ë‹¨ ì‹œì‘")
    print("=" * 60)

    # 1. ë°ì´í„° ë¡œë“œ
    spy_data = load_spy_data()

    # 2. ê¸°ë³¸ í†µê³„ ë¶„ì„
    basic_stats = analyze_basic_statistics(spy_data)

    # 3. í–¥ìƒëœ íŠ¹ì„± ìƒì„±
    features = create_enhanced_features(spy_data)

    # 4. íƒ€ê²Ÿ ìƒì„±
    targets = create_future_volatility_targets(spy_data)

    # 5. íƒ€ê²Ÿ ì˜ˆì¸¡ ê°€ëŠ¥ì„± ë¶„ì„
    analyze_target_predictability(targets)

    # 6. íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„
    combined = analyze_feature_target_correlation(features, targets)

    # 7. 5ì¼ ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    if 'target_vol_5d' in combined.columns:
        print(f"\nğŸ“Š 5ì¼ ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ìƒ˜í”Œ ìˆ˜: {len(combined)})")
        X = combined[features.columns]
        y = combined['target_vol_5d']
        results = test_multiple_models(X, y)

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_model = max(results.items(), key=lambda x: x[1]['r2'])
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model[0]} (RÂ² = {best_model[1]['r2']:.4f})")

    # 8. ê²°ê³¼ ì €ì¥
    os.makedirs('results', exist_ok=True)

    diagnosis_results = {
        'basic_statistics': basic_stats,
        'feature_count': len(features.columns),
        'target_count': len(targets.columns),
        'sample_count': len(combined) if 'combined' in locals() else 0,
        'model_results': results if 'results' in locals() else {}
    }

    import json
    with open('results/model_diagnosis.json', 'w') as f:
        json.dump(diagnosis_results, f, indent=2, default=str)

    print(f"\nğŸ’¾ ì§„ë‹¨ ê²°ê³¼ ì €ì¥: results/model_diagnosis.json")
    print("=" * 60)

if __name__ == "__main__":
    main()