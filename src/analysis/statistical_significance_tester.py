#!/usr/bin/env python3
"""
ğŸ“Š í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ ì‹œìŠ¤í…œ
í•™ìˆ  ë…¼ë¬¸ ì‘ì„±ì„ ìœ„í•œ ì—„ê²©í•œ í†µê³„ì  ê²€ì¦ ë„êµ¬

ì£¼ìš” ê¸°ëŠ¥:
- ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ ìœ ì˜ì„± ê²€ì •
- ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
- íš¨ê³¼ í¬ê¸° ì¸¡ì •
- ë‹¤ì¤‘ ë¹„êµ ë³´ì •
- ê²€ì •ë ¥ ë¶„ì„
"""

import numpy as np
import pandas as pd
import scipy.stats as stats
from scipy.stats import ttest_rel, wilcoxon, mannwhitneyu
from statsmodels.stats.multitest import multipletests
from statsmodels.stats.power import ttest_power
from typing import Dict, List, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

class StatisticalSignificanceTester:
    """
    í•™ìˆ  ë…¼ë¬¸ì„ ìœ„í•œ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ í´ë˜ìŠ¤

    ê¸ˆìœµ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµì— íŠ¹í™”ëœ í†µê³„ ê²€ì • ë„êµ¬
    """

    def __init__(self, alpha: float = 0.05, power: float = 0.8):
        """
        ì´ˆê¸°í™”

        Args:
            alpha: ìœ ì˜ìˆ˜ì¤€ (ê¸°ë³¸ê°’: 0.05)
            power: ê²€ì •ë ¥ (ê¸°ë³¸ê°’: 0.8)
        """
        self.alpha = alpha
        self.power = power
        self.results = {}

    def paired_ttest(self, model1_scores: np.ndarray, model2_scores: np.ndarray,
                    model1_name: str = "Model1", model2_name: str = "Model2") -> Dict:
        """
        ìŒì²´ t-ê²€ì • (Paired t-test)

        ë™ì¼í•œ ë°ì´í„°ì—ì„œ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµ
        ê°€ì •: ì •ê·œë¶„í¬, ìŒì²´ ê´€ì¸¡

        Args:
            model1_scores: ëª¨ë¸1ì˜ ì„±ëŠ¥ ì ìˆ˜ë“¤
            model2_scores: ëª¨ë¸2ì˜ ì„±ëŠ¥ ì ìˆ˜ë“¤
            model1_name: ëª¨ë¸1 ì´ë¦„
            model2_name: ëª¨ë¸2 ì´ë¦„

        Returns:
            ê²€ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ì…ë ¥ ê²€ì¦
        if len(model1_scores) != len(model2_scores):
            raise ValueError("ë‘ ëª¨ë¸ì˜ ì ìˆ˜ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

        if len(model1_scores) < 3:
            raise ValueError("ìœ ì˜í•œ ê²€ì •ì„ ìœ„í•´ ìµœì†Œ 3ê°œ ì´ìƒì˜ ê´€ì¸¡ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤")

        # ì •ê·œì„± ê²€ì • (Shapiro-Wilk)
        diff_scores = model1_scores - model2_scores
        shapiro_stat, shapiro_p = stats.shapiro(diff_scores)
        is_normal = shapiro_p > 0.05

        # t-ê²€ì • ìˆ˜í–‰
        t_stat, p_value = ttest_rel(model1_scores, model2_scores)

        # íš¨ê³¼ í¬ê¸° (Cohen's d for paired samples)
        mean_diff = np.mean(diff_scores)
        std_diff = np.std(diff_scores, ddof=1)
        cohens_d = mean_diff / std_diff if std_diff != 0 else 0

        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (95%)
        n = len(diff_scores)
        se_diff = std_diff / np.sqrt(n)
        t_critical = stats.t.ppf(1 - self.alpha/2, n-1)
        ci_lower = mean_diff - t_critical * se_diff
        ci_upper = mean_diff + t_critical * se_diff

        # ê²€ì •ë ¥ ê³„ì‚°
        calculated_power = ttest_power(cohens_d, n, self.alpha, alternative='two-sided')

        # ì‹¤ìš©ì  ìœ ì˜ì„± íŒë‹¨
        # Cohen's d ê¸°ì¤€: 0.2(ì‘ìŒ), 0.5(ì¤‘ê°„), 0.8(í¼)
        if abs(cohens_d) < 0.2:
            effect_size_interpretation = "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼"
        elif abs(cohens_d) < 0.5:
            effect_size_interpretation = "ì‘ì€ íš¨ê³¼"
        elif abs(cohens_d) < 0.8:
            effect_size_interpretation = "ì¤‘ê°„ íš¨ê³¼"
        else:
            effect_size_interpretation = "í° íš¨ê³¼"

        result = {
            'test_type': 'Paired t-test',
            'model1_name': model1_name,
            'model2_name': model2_name,
            'n_observations': n,
            'model1_mean': np.mean(model1_scores),
            'model2_mean': np.mean(model2_scores),
            'mean_difference': mean_diff,
            'std_difference': std_diff,
            't_statistic': t_stat,
            'p_value': p_value,
            'is_significant': p_value < self.alpha,
            'alpha': self.alpha,
            'cohens_d': cohens_d,
            'effect_size_interpretation': effect_size_interpretation,
            'confidence_interval_95': (ci_lower, ci_upper),
            'statistical_power': calculated_power,
            'normality_test': {
                'shapiro_statistic': shapiro_stat,
                'shapiro_p_value': shapiro_p,
                'is_normal': is_normal
            },
            'recommendation': self._get_ttest_recommendation(p_value, is_normal, calculated_power)
        }

        return result

    def wilcoxon_signed_rank_test(self, model1_scores: np.ndarray, model2_scores: np.ndarray,
                                 model1_name: str = "Model1", model2_name: str = "Model2") -> Dict:
        """
        ìœŒì½•ìŠ¨ ë¶€í˜¸ ìˆœìœ„ ê²€ì • (Wilcoxon Signed-Rank Test)

        ë¹„ëª¨ìˆ˜ ê²€ì •ìœ¼ë¡œ ì •ê·œì„± ê°€ì •ì´ í•„ìš” ì—†ìŒ
        ìŒì²´ ë°ì´í„°ì˜ ì¤‘ì•™ê°’ ì°¨ì´ ê²€ì •

        Args:
            model1_scores: ëª¨ë¸1ì˜ ì„±ëŠ¥ ì ìˆ˜ë“¤
            model2_scores: ëª¨ë¸2ì˜ ì„±ëŠ¥ ì ìˆ˜ë“¤
            model1_name: ëª¨ë¸1 ì´ë¦„
            model2_name: ëª¨ë¸2 ì´ë¦„

        Returns:
            ê²€ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ì…ë ¥ ê²€ì¦
        if len(model1_scores) != len(model2_scores):
            raise ValueError("ë‘ ëª¨ë¸ì˜ ì ìˆ˜ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

        # ì°¨ì´ ê³„ì‚°
        diff_scores = model1_scores - model2_scores

        # 0ì¸ ì°¨ì´ ì œê±°
        non_zero_diff = diff_scores[diff_scores != 0]

        if len(non_zero_diff) < 5:
            warnings.warn("ìœ ì˜í•œ ìœŒì½•ìŠ¨ ê²€ì •ì„ ìœ„í•´ ìµœì†Œ 5ê°œ ì´ìƒì˜ non-zero ì°¨ì´ê°€ ê¶Œì¥ë©ë‹ˆë‹¤")

        # ìœŒì½•ìŠ¨ ê²€ì • ìˆ˜í–‰
        if len(non_zero_diff) == 0:
            w_stat, p_value = 0, 1.0
        else:
            w_stat, p_value = wilcoxon(non_zero_diff, alternative='two-sided')

        # íš¨ê³¼ í¬ê¸° (r = Z / sqrt(N))
        n = len(non_zero_diff)
        if n > 0:
            z_score = stats.norm.ppf(1 - p_value/2) if p_value > 0 else 0
            effect_size_r = abs(z_score) / np.sqrt(n)
        else:
            effect_size_r = 0

        # íš¨ê³¼ í¬ê¸° í•´ì„ (Cohen's ê¸°ì¤€)
        if effect_size_r < 0.1:
            effect_size_interpretation = "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼"
        elif effect_size_r < 0.3:
            effect_size_interpretation = "ì‘ì€ íš¨ê³¼"
        elif effect_size_r < 0.5:
            effect_size_interpretation = "ì¤‘ê°„ íš¨ê³¼"
        else:
            effect_size_interpretation = "í° íš¨ê³¼"

        result = {
            'test_type': 'Wilcoxon Signed-Rank Test',
            'model1_name': model1_name,
            'model2_name': model2_name,
            'n_observations': len(model1_scores),
            'n_non_zero_differences': n,
            'model1_median': np.median(model1_scores),
            'model2_median': np.median(model2_scores),
            'median_difference': np.median(diff_scores),
            'w_statistic': w_stat,
            'p_value': p_value,
            'is_significant': p_value < self.alpha,
            'alpha': self.alpha,
            'effect_size_r': effect_size_r,
            'effect_size_interpretation': effect_size_interpretation,
            'zero_differences': len(diff_scores) - n,
            'recommendation': self._get_wilcoxon_recommendation(p_value, n)
        }

        return result

    def independent_samples_test(self, model1_scores: np.ndarray, model2_scores: np.ndarray,
                               model1_name: str = "Model1", model2_name: str = "Model2") -> Dict:
        """
        ë…ë¦½í‘œë³¸ ê²€ì • (t-test ë˜ëŠ” Mann-Whitney U test)

        ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµ
        ì •ê·œì„±ì— ë”°ë¼ ì ì ˆí•œ ê²€ì • ìë™ ì„ íƒ

        Args:
            model1_scores: ëª¨ë¸1ì˜ ì„±ëŠ¥ ì ìˆ˜ë“¤
            model2_scores: ëª¨ë¸2ì˜ ì„±ëŠ¥ ì ìˆ˜ë“¤
            model1_name: ëª¨ë¸1 ì´ë¦„
            model2_name: ëª¨ë¸2 ì´ë¦„

        Returns:
            ê²€ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ì •ê·œì„± ê²€ì •
        if len(model1_scores) >= 3:
            _, p1 = stats.shapiro(model1_scores)
        else:
            p1 = 0

        if len(model2_scores) >= 3:
            _, p2 = stats.shapiro(model2_scores)
        else:
            p2 = 0

        is_normal = (p1 > 0.05) and (p2 > 0.05)

        if is_normal and len(model1_scores) >= 3 and len(model2_scores) >= 3:
            # ë…ë¦½í‘œë³¸ t-ê²€ì •
            t_stat, p_value = stats.ttest_ind(model1_scores, model2_scores)
            test_used = "Independent t-test"

            # Cohen's d ê³„ì‚°
            pooled_std = np.sqrt(((len(model1_scores)-1)*np.var(model1_scores, ddof=1) +
                                 (len(model2_scores)-1)*np.var(model2_scores, ddof=1)) /
                                (len(model1_scores) + len(model2_scores) - 2))
            cohens_d = (np.mean(model1_scores) - np.mean(model2_scores)) / pooled_std if pooled_std != 0 else 0

        else:
            # Mann-Whitney U ê²€ì • (ë¹„ëª¨ìˆ˜)
            u_stat, p_value = mannwhitneyu(model1_scores, model2_scores, alternative='two-sided')
            test_used = "Mann-Whitney U test"
            t_stat = u_stat
            cohens_d = None

        # íš¨ê³¼ í¬ê¸° í•´ì„
        if cohens_d is not None:
            if abs(cohens_d) < 0.2:
                effect_interpretation = "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼"
            elif abs(cohens_d) < 0.5:
                effect_interpretation = "ì‘ì€ íš¨ê³¼"
            elif abs(cohens_d) < 0.8:
                effect_interpretation = "ì¤‘ê°„ íš¨ê³¼"
            else:
                effect_interpretation = "í° íš¨ê³¼"
        else:
            effect_interpretation = "ë¹„ëª¨ìˆ˜ ê²€ì •ìœ¼ë¡œ Cohen's d ê³„ì‚° ë¶ˆê°€"

        result = {
            'test_type': test_used,
            'model1_name': model1_name,
            'model2_name': model2_name,
            'n_observations_model1': len(model1_scores),
            'n_observations_model2': len(model2_scores),
            'model1_mean': np.mean(model1_scores),
            'model2_mean': np.mean(model2_scores),
            'model1_std': np.std(model1_scores, ddof=1),
            'model2_std': np.std(model2_scores, ddof=1),
            'test_statistic': t_stat,
            'p_value': p_value,
            'is_significant': p_value < self.alpha,
            'alpha': self.alpha,
            'cohens_d': cohens_d,
            'effect_size_interpretation': effect_interpretation,
            'normality_model1_p': p1,
            'normality_model2_p': p2,
            'both_normal': is_normal
        }

        return result

    def multiple_model_comparison(self, model_scores: Dict[str, np.ndarray],
                                paired: bool = True) -> Dict:
        """
        ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ ë¶„ì„

        ì—¬ëŸ¬ ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ì„ ìŒë³„ ë¹„êµí•˜ê³  ë‹¤ì¤‘ ë¹„êµ ë³´ì • ì ìš©

        Args:
            model_scores: ëª¨ë¸ëª…ì„ í‚¤ë¡œ í•˜ê³  ì ìˆ˜ ë°°ì—´ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            paired: ìŒì²´ ë°ì´í„° ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

        Returns:
            ì „ì²´ ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        model_names = list(model_scores.keys())
        n_models = len(model_names)
        n_comparisons = n_models * (n_models - 1) // 2

        if n_models < 2:
            raise ValueError("ë¹„êµë¥¼ ìœ„í•´ ìµœì†Œ 2ê°œ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤")

        # ëª¨ë“  ìŒë³„ ë¹„êµ ìˆ˜í–‰
        pairwise_results = []
        p_values = []
        comparison_names = []

        for i in range(n_models):
            for j in range(i+1, n_models):
                model1_name = model_names[i]
                model2_name = model_names[j]
                scores1 = model_scores[model1_name]
                scores2 = model_scores[model2_name]

                if paired:
                    # ìŒì²´ ê²€ì • (ì •ê·œì„±ì— ë”°ë¼ t-test ë˜ëŠ” Wilcoxon ì„ íƒ)
                    diff_scores = scores1 - scores2
                    if len(diff_scores) >= 3:
                        _, shapiro_p = stats.shapiro(diff_scores)
                        is_normal = shapiro_p > 0.05
                    else:
                        is_normal = False

                    if is_normal:
                        result = self.paired_ttest(scores1, scores2, model1_name, model2_name)
                    else:
                        result = self.wilcoxon_signed_rank_test(scores1, scores2, model1_name, model2_name)
                else:
                    # ë…ë¦½í‘œë³¸ ê²€ì •
                    result = self.independent_samples_test(scores1, scores2, model1_name, model2_name)

                pairwise_results.append(result)
                p_values.append(result['p_value'])
                comparison_names.append(f"{model1_name} vs {model2_name}")

        # ë‹¤ì¤‘ ë¹„êµ ë³´ì •
        # Bonferroni ë³´ì •
        bonferroni_corrected = [p * n_comparisons for p in p_values]
        bonferroni_significant = [p < self.alpha for p in bonferroni_corrected]

        # FDR (False Discovery Rate) ë³´ì • - Benjamini-Hochberg
        reject_fdr, p_corrected_fdr, _, _ = multipletests(p_values, alpha=self.alpha, method='fdr_bh')

        # ì „ì²´ ë¶„ì„ ê²°ê³¼
        result = {
            'n_models': n_models,
            'n_comparisons': n_comparisons,
            'model_names': model_names,
            'pairwise_results': pairwise_results,
            'multiple_comparison_correction': {
                'original_p_values': p_values,
                'comparison_names': comparison_names,
                'bonferroni': {
                    'corrected_p_values': bonferroni_corrected,
                    'significant': bonferroni_significant,
                    'n_significant': sum(bonferroni_significant)
                },
                'fdr_bh': {
                    'corrected_p_values': p_corrected_fdr.tolist(),
                    'significant': reject_fdr.tolist(),
                    'n_significant': sum(reject_fdr)
                }
            },
            'alpha': self.alpha,
            'recommendation': self._get_multiple_comparison_recommendation(n_comparisons, p_values)
        }

        return result

    def _get_ttest_recommendation(self, p_value: float, is_normal: bool, power: float) -> str:
        """t-ê²€ì • ê²°ê³¼ì— ëŒ€í•œ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        if not is_normal:
            return "ì •ê·œì„± ê°€ì •ì´ ìœ„ë°°ë˜ì–´ Wilcoxon signed-rank test ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        elif power < 0.8:
            return f"ê²€ì •ë ¥ì´ ë‚®ìŠµë‹ˆë‹¤ (power={power:.3f}). ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif p_value < self.alpha:
            return "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. íš¨ê³¼ í¬ê¸°ë„ í•¨ê»˜ ê³ ë ¤í•˜ì„¸ìš”."
        else:
            return "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤."

    def _get_wilcoxon_recommendation(self, p_value: float, n_non_zero: int) -> str:
        """ìœŒì½•ìŠ¨ ê²€ì • ê²°ê³¼ì— ëŒ€í•œ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        if n_non_zero < 5:
            return "non-zero ì°¨ì´ê°€ ë„ˆë¬´ ì ì–´ ê²°ê³¼ê°€ ì‹ ë¢°í•  ìˆ˜ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif p_value < self.alpha:
            return "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤ (ë¹„ëª¨ìˆ˜ ê²€ì •)."
        else:
            return "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤ (ë¹„ëª¨ìˆ˜ ê²€ì •)."

    def _get_multiple_comparison_recommendation(self, n_comparisons: int, p_values: List[float]) -> str:
        """ë‹¤ì¤‘ ë¹„êµ ê²°ê³¼ì— ëŒ€í•œ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        min_p = min(p_values)
        uncorrected_significant = sum([p < self.alpha for p in p_values])

        if n_comparisons > 10:
            return f"ë‹¤ì¤‘ ë¹„êµê°€ ë§ìŠµë‹ˆë‹¤ ({n_comparisons}ê°œ). FDR ë³´ì • ê²°ê³¼ë¥¼ ìš°ì„  ê³ ë ¤í•˜ì„¸ìš”."
        elif uncorrected_significant == 0:
            return "ì–´ë–¤ ëª¨ë¸ ìŒì—ì„œë„ ìœ ì˜í•œ ì°¨ì´ë¥¼ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        else:
            return f"ë³´ì • ì „ {uncorrected_significant}ê°œ ë¹„êµì—ì„œ ìœ ì˜í•œ ì°¨ì´ ë°œê²¬. ë‹¤ì¤‘ ë¹„êµ ë³´ì • ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”."

def main():
    """í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸ“Š í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)

    # ëª¨ë¸ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜ (ì •í™•ë„ ì ìˆ˜)
    model_a_scores = np.random.normal(0.75, 0.05, 30)  # í‰ê·  75%, í‘œì¤€í¸ì°¨ 5%
    model_b_scores = np.random.normal(0.73, 0.05, 30)  # í‰ê·  73%, í‘œì¤€í¸ì°¨ 5%
    model_c_scores = np.random.normal(0.77, 0.04, 30)  # í‰ê·  77%, í‘œì¤€í¸ì°¨ 4%

    tester = StatisticalSignificanceTester(alpha=0.05)

    # 1. ìŒì²´ t-ê²€ì •
    print("\n1. ìŒì²´ t-ê²€ì • (Model A vs Model B)")
    print("-" * 40)
    ttest_result = tester.paired_ttest(model_a_scores, model_b_scores, "Model A", "Model B")
    print(f"p-value: {ttest_result['p_value']:.6f}")
    print(f"Cohen's d: {ttest_result['cohens_d']:.3f}")
    print(f"ìœ ì˜í•œ ì°¨ì´: {ttest_result['is_significant']}")
    print(f"ê¶Œê³ ì‚¬í•­: {ttest_result['recommendation']}")

    # 2. ìœŒì½•ìŠ¨ ê²€ì •
    print("\n2. ìœŒì½•ìŠ¨ ë¶€í˜¸ ìˆœìœ„ ê²€ì • (Model A vs Model C)")
    print("-" * 40)
    wilcoxon_result = tester.wilcoxon_signed_rank_test(model_a_scores, model_c_scores, "Model A", "Model C")
    print(f"p-value: {wilcoxon_result['p_value']:.6f}")
    print(f"Effect size r: {wilcoxon_result['effect_size_r']:.3f}")
    print(f"ìœ ì˜í•œ ì°¨ì´: {wilcoxon_result['is_significant']}")

    # 3. ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ
    print("\n3. ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ (Bonferroni & FDR ë³´ì •)")
    print("-" * 40)
    model_scores = {
        'Model A': model_a_scores,
        'Model B': model_b_scores,
        'Model C': model_c_scores
    }

    multi_result = tester.multiple_model_comparison(model_scores, paired=True)
    print(f"ì´ ë¹„êµ ìˆ˜: {multi_result['n_comparisons']}")

    bonf_results = multi_result['multiple_comparison_correction']['bonferroni']
    fdr_results = multi_result['multiple_comparison_correction']['fdr_bh']

    print(f"Bonferroni ë³´ì • í›„ ìœ ì˜í•œ ë¹„êµ: {bonf_results['n_significant']}")
    print(f"FDR ë³´ì • í›„ ìœ ì˜í•œ ë¹„êµ: {fdr_results['n_significant']}")

    print(f"\nê¶Œê³ ì‚¬í•­: {multi_result['recommendation']}")

if __name__ == "__main__":
    main()