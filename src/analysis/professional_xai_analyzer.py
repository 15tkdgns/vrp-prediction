#!/usr/bin/env python3
"""
Professional XAI Analyzer for SPY Volatility Prediction
ì²´ê³„ì ì´ê³  ì „ë¬¸ì ì¸ ì„¤ëª…ê°€ëŠ¥ AI ë¶„ì„ ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
1. SHAP ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
2. ë¶€ë¶„ ì˜ì¡´ì„± í”Œë¡¯ (PDP)
3. ê°œë³„ ì¡°ê±´ë¶€ ê¸°ëŒ€ê°’ (ICE)
4. íŠ¹ì„± ìƒí˜¸ì‘ìš© ë¶„ì„
5. ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” í•´ì„
"""

import os
import sys
import numpy as np
import pandas as pd
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import json
import pickle

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append('/root/workspace')

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import shap
    from sklearn.linear_model import Ridge
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    from sklearn.inspection import partial_dependence, permutation_importance
    import matplotlib.pyplot as plt
    import seaborn as sns
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ì˜ì¡´ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
    DEPENDENCIES_AVAILABLE = False

try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False

from src.core.logger import setup_logger

class ProfessionalXAIAnalyzer:
    """
    ì „ë¬¸ì  XAI ë¶„ì„ê¸° - ê¸ˆìœµ ë¨¸ì‹ ëŸ¬ë‹ì— íŠ¹í™”ëœ ì„¤ëª…ê°€ëŠ¥ AI
    """

    def __init__(self, data_dir: str = "/root/workspace/data",
                 model_dir: str = "/root/workspace/data/models",
                 output_dir: str = "/root/workspace/data/xai_analysis"):
        """
        ì „ë¬¸ì  XAI ë¶„ì„ê¸° ì´ˆê¸°í™”

        Args:
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            model_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
            output_dir: XAI ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.logger = setup_logger(self.__class__.__name__)

        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # ë¶„ì„ ê²°ê³¼ ì €ì¥ìš©
        self.analysis_results = {}
        self.feature_names = []
        self.model = None
        self.scaler = None
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.y_test = None

        # ê¸ˆìœµ íŠ¹ì„± ê·¸ë£¹ ì •ì˜
        self.feature_groups = {
            'volatility': ['volatility_5', 'volatility_10', 'volatility_15', 'volatility_20', 'volatility_30', 'volatility_60'],
            'returns': ['return_lag_1', 'return_lag_2', 'return_lag_3', 'return_lag_4', 'return_lag_5'],
            'momentum': ['momentum_5', 'momentum_10', 'momentum_15', 'momentum_20', 'momentum_30'],
            'ratios': ['vol_ratio_5_10', 'vol_ratio_5_20', 'vol_ratio_10_20', 'vol_5_20_ratio'],
            'technical': ['zscore_10', 'zscore_20', 'vol_regime', 'price_position']
        }

        self.logger.info("ğŸš€ Professional XAI Analyzer ì´ˆê¸°í™” ì™„ë£Œ")

    def load_spy_data_and_prepare_features(self, start_date='2015-01-01', end_date='2024-12-31'):
        """
        SPY ë°ì´í„° ë¡œë“œ ë° íŠ¹ì„± ìƒì„±

        Returns:
            Tuple[pd.DataFrame, pd.Series]: (íŠ¹ì„± ë°ì´í„°, íƒ€ê²Ÿ ë°ì´í„°)
        """
        self.logger.info("ğŸ“Š SPY ë°ì´í„° ë¡œë“œ ë° íŠ¹ì„± ìƒì„± ì‹œì‘...")

        try:
            if YFINANCE_AVAILABLE:
                # ì‹¤ì œ SPY ë°ì´í„° ìˆ˜ì§‘
                spy = yf.Ticker("SPY")
                data = spy.history(start=start_date, end=end_date, interval="1d")
                prices = data['Close']
                volumes = data['Volume']

                self.logger.info(f"âœ… SPY ë°ì´í„° ë¡œë“œ: {len(data)}ê°œ ê´€ì¸¡ì¹˜")
            else:
                raise ImportError("yfinance not available")

        except Exception as e:
            self.logger.warning(f"ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}, ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì‚¬ìš©")
            # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
            dates = pd.date_range(start=start_date, end=end_date, freq='D')
            np.random.seed(42)
            prices = pd.Series(100 * np.exp(np.cumsum(np.random.normal(0.0003, 0.015, len(dates)))),
                             index=dates)
            volumes = pd.Series(np.random.randint(50000000, 200000000, len(dates)),
                              index=dates)

        # ê¸°ë³¸ ê³„ì‚°
        returns = prices.pct_change()
        log_returns = np.log(prices / prices.shift(1))

        # íŠ¹ì„± ìƒì„±
        features = pd.DataFrame(index=prices.index)

        # 1. ë³€ë™ì„± íŠ¹ì„±ë“¤
        for window in [5, 10, 15, 20, 30, 60]:
            features[f'volatility_{window}'] = returns.rolling(window).std() * np.sqrt(252)

        # 2. ìˆ˜ìµë¥  ì§€ì—° íŠ¹ì„±ë“¤
        for lag in range(1, 6):
            features[f'return_lag_{lag}'] = returns.shift(lag)

        # 3. ëª¨ë©˜í…€ íŠ¹ì„±ë“¤
        for window in [5, 10, 15, 20, 30]:
            features[f'momentum_{window}'] = (prices / prices.shift(window) - 1) * 100

        # 4. Z-ìŠ¤ì½”ì–´ íŠ¹ì„±ë“¤
        for window in [10, 20]:
            mean_ret = returns.rolling(window).mean()
            std_ret = returns.rolling(window).std()
            features[f'zscore_{window}'] = (returns - mean_ret) / std_ret

        # 5. ë³€ë™ì„± ë¹„ìœ¨ íŠ¹ì„±ë“¤
        features['vol_ratio_5_10'] = features['volatility_5'] / features['volatility_10']
        features['vol_ratio_5_20'] = features['volatility_5'] / features['volatility_20']
        features['vol_ratio_10_20'] = features['volatility_10'] / features['volatility_20']
        features['vol_5_20_ratio'] = features['volatility_5'] / features['volatility_20']

        # 6. ê¸°ìˆ ì  íŠ¹ì„±ë“¤
        vol_median = features['volatility_20'].rolling(252).median()
        features['vol_regime'] = (features['volatility_20'] > vol_median).astype(int)

        # ê°€ê²© ìœ„ì¹˜ (52ì£¼ ìµœê³ /ìµœì € ëŒ€ë¹„)
        high_52w = prices.rolling(252).max()
        low_52w = prices.rolling(252).min()
        features['price_position'] = (prices - low_52w) / (high_52w - low_52w)

        # 7. íƒ€ê²Ÿ ë³€ìˆ˜: 5ì¼ í›„ ë³€ë™ì„±
        target = returns.rolling(5).std().shift(-5) * np.sqrt(252)
        target.name = 'target_vol_5d'

        # ê²°ì¸¡ì¹˜ ì œê±°
        combined = pd.concat([features, target], axis=1).dropna()
        X = combined[features.columns[:-1] if 'target_vol_5d' in combined.columns else features.columns]
        y = combined['target_vol_5d'] if 'target_vol_5d' in combined.columns else target.loc[X.index]

        # ìµœì¢… ë°ì´í„° ì •ë ¬
        common_idx = X.index.intersection(y.index)
        X = X.loc[common_idx]
        y = y.loc[common_idx]

        self.feature_names = list(X.columns)

        self.logger.info(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {X.shape[0]}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ íŠ¹ì„±")

        return X, y

    def train_ridge_model(self, X: pd.DataFrame, y: pd.Series, test_size: float = 0.2,
                         alpha: float = 1.0) -> Dict:
        """
        Ridge ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ í‰ê°€

        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
            alpha: Ridge ì •ê·œí™” íŒŒë¼ë¯¸í„°

        Returns:
            Dict: í›ˆë ¨ ë° ì„±ëŠ¥ ê²°ê³¼
        """
        self.logger.info("ğŸ”„ Ridge ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        # ì‹œê°„ì  ìˆœì„œë¥¼ ê³ ë ¤í•œ ë¶„í• 
        split_idx = int(len(X) * (1 - test_size))

        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        # í‘œì¤€í™”
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Ridge ëª¨ë¸ í›ˆë ¨
        model = Ridge(alpha=alpha, random_state=42)
        model.fit(X_train_scaled, y_train)

        # ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
        y_train_pred = model.predict(X_train_scaled)
        y_test_pred = model.predict(X_test_scaled)

        # ì„±ëŠ¥ ì§€í‘œ
        results = {
            'train_r2': r2_score(y_train, y_train_pred),
            'test_r2': r2_score(y_test, y_test_pred),
            'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),
            'train_mae': mean_absolute_error(y_train, y_train_pred),
            'test_mae': mean_absolute_error(y_test, y_test_pred),
            'n_features': X.shape[1],
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'alpha': alpha
        }

        # ëª¨ë¸ ë° ë°ì´í„° ì €ì¥
        self.model = model
        self.scaler = scaler
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test

        self.logger.info(f"âœ… Ridge ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        self.logger.info(f"ğŸ“Š Test RÂ²: {results['test_r2']:.4f}")

        return results

    def analyze_shap_values(self, sample_size: int = 1000) -> Dict:
        """
        SHAP ê°’ ë¶„ì„ - ì „ì—­ ë° ì§€ì—­ í•´ì„ê°€ëŠ¥ì„±

        Args:
            sample_size: ë¶„ì„í•  ìƒ˜í”Œ ìˆ˜

        Returns:
            Dict: SHAP ë¶„ì„ ê²°ê³¼
        """
        self.logger.info("ğŸ” SHAP ë¶„ì„ ì‹œì‘...")

        if not DEPENDENCIES_AVAILABLE or self.model is None:
            self.logger.error("SHAP ë¶„ì„ì„ ìœ„í•œ ì˜ì¡´ì„±ì´ë‚˜ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return {}

        try:
            # ìƒ˜í”Œ ì„ íƒ (ìµœê·¼ ë°ì´í„° ìš°ì„ )
            X_sample = self.X_train.tail(sample_size) if len(self.X_train) > sample_size else self.X_train
            X_sample_scaled = self.scaler.transform(X_sample)

            # SHAP Explainer ìƒì„± (Linear modelsìš©)
            explainer = shap.LinearExplainer(self.model, X_sample_scaled)
            shap_values = explainer.shap_values(X_sample_scaled)

            # SHAP ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„
            feature_importance = np.abs(shap_values).mean(axis=0)
            feature_importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': feature_importance
            }).sort_values('importance', ascending=False)

            # ê·¸ë£¹ë³„ ì¤‘ìš”ë„
            group_importance = {}
            for group, features in self.feature_groups.items():
                group_features = [f for f in features if f in self.feature_names]
                if group_features:
                    group_idx = [self.feature_names.index(f) for f in group_features]
                    group_importance[group] = np.abs(shap_values[:, group_idx]).mean()

            # SHAP ê°’ ì €ì¥
            shap_results = {
                'feature_importance': feature_importance_df.to_dict('records'),
                'group_importance': group_importance,
                'shap_values_mean': np.mean(shap_values, axis=0).tolist(),
                'shap_values_std': np.std(shap_values, axis=0).tolist(),
                'expected_value': float(explainer.expected_value),
                'sample_size': len(X_sample)
            }

            # ê²°ê³¼ ì €ì¥
            self.analysis_results['shap'] = shap_results

            self.logger.info(f"âœ… SHAP ë¶„ì„ ì™„ë£Œ ({len(X_sample)}ê°œ ìƒ˜í”Œ)")
            self.logger.info(f"ğŸ† Top 5 ì¤‘ìš” íŠ¹ì„±: {[f['feature'] for f in shap_results['feature_importance'][:5]]}")

            return shap_results

        except Exception as e:
            self.logger.error(f"âŒ SHAP ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}

    def analyze_partial_dependence(self, top_features: int = 10) -> Dict:
        """
        ë¶€ë¶„ ì˜ì¡´ì„± í”Œë¡¯ (PDP) ë¶„ì„

        Args:
            top_features: ë¶„ì„í•  ìƒìœ„ íŠ¹ì„± ìˆ˜

        Returns:
            Dict: PDP ë¶„ì„ ê²°ê³¼
        """
        self.logger.info("ğŸ“ˆ ë¶€ë¶„ ì˜ì¡´ì„± ë¶„ì„ ì‹œì‘...")

        if self.model is None:
            return {}

        try:
            # ìƒìœ„ ì¤‘ìš” íŠ¹ì„± ì„ íƒ
            if 'shap' in self.analysis_results:
                top_feature_names = [f['feature'] for f in self.analysis_results['shap']['feature_importance'][:top_features]]
            else:
                # Ridge ê³„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ íŠ¹ì„± ì„ íƒ
                coeffs = np.abs(self.model.coef_)
                top_indices = np.argsort(coeffs)[-top_features:]
                top_feature_names = [self.feature_names[i] for i in top_indices]

            top_feature_indices = [self.feature_names.index(name) for name in top_feature_names]

            X_train_scaled = self.scaler.transform(self.X_train)

            pdp_results = {}

            for i, (feature_idx, feature_name) in enumerate(zip(top_feature_indices, top_feature_names)):
                try:
                    # ë¶€ë¶„ ì˜ì¡´ì„± ê³„ì‚°
                    pd_result = partial_dependence(
                        self.model, X_train_scaled, [feature_idx],
                        kind='average', percentiles=(0.05, 0.95), grid_resolution=20
                    )

                    pdp_results[feature_name] = {
                        'values': pd_result[0][0].tolist(),
                        'grid': pd_result[1][0].tolist(),
                        'feature_range': [float(self.X_train.iloc[:, feature_idx].min()),
                                        float(self.X_train.iloc[:, feature_idx].max())],
                        'feature_mean': float(self.X_train.iloc[:, feature_idx].mean()),
                        'feature_std': float(self.X_train.iloc[:, feature_idx].std())
                    }

                except Exception as e:
                    self.logger.warning(f"PDP ê³„ì‚° ì‹¤íŒ¨ - {feature_name}: {e}")
                    continue

            self.analysis_results['pdp'] = pdp_results

            self.logger.info(f"âœ… ë¶€ë¶„ ì˜ì¡´ì„± ë¶„ì„ ì™„ë£Œ ({len(pdp_results)}ê°œ íŠ¹ì„±)")

            return pdp_results

        except Exception as e:
            self.logger.error(f"âŒ PDP ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}

    def analyze_permutation_importance(self, n_repeats: int = 10) -> Dict:
        """
        Permutation Importance ë¶„ì„

        Args:
            n_repeats: ìˆœì—´ ë°˜ë³µ íšŸìˆ˜

        Returns:
            Dict: Permutation importance ê²°ê³¼
        """
        self.logger.info("ğŸ”„ Permutation Importance ë¶„ì„ ì‹œì‘...")

        if self.model is None:
            return {}

        try:
            X_test_scaled = self.scaler.transform(self.X_test)

            perm_importance = permutation_importance(
                self.model, X_test_scaled, self.y_test,
                n_repeats=n_repeats, random_state=42, scoring='r2'
            )

            importance_results = []
            for i, feature_name in enumerate(self.feature_names):
                importance_results.append({
                    'feature': feature_name,
                    'importance_mean': float(perm_importance.importances_mean[i]),
                    'importance_std': float(perm_importance.importances_std[i])
                })

            # ì¤‘ìš”ë„ë¡œ ì •ë ¬
            importance_results.sort(key=lambda x: x['importance_mean'], reverse=True)

            perm_results = {
                'feature_importance': importance_results,
                'n_repeats': n_repeats,
                'baseline_score': float(r2_score(self.y_test, self.model.predict(X_test_scaled)))
            }

            self.analysis_results['permutation'] = perm_results

            self.logger.info(f"âœ… Permutation Importance ë¶„ì„ ì™„ë£Œ")
            self.logger.info(f"ğŸ† Top íŠ¹ì„±: {importance_results[0]['feature']} ({importance_results[0]['importance_mean']:.4f})")

            return perm_results

        except Exception as e:
            self.logger.error(f"âŒ Permutation Importance ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}

    def generate_business_insights(self) -> Dict:
        """
        ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ìƒì„± - ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” í•´ì„

        Returns:
            Dict: ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸
        """
        self.logger.info("ğŸ’¡ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘...")

        insights = {
            'model_performance': {},
            'feature_insights': {},
            'risk_insights': {},
            'trading_insights': {},
            'market_regime_insights': {}
        }

        # 1. ëª¨ë¸ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
        if hasattr(self, 'model') and self.model is not None:
            test_r2 = r2_score(self.y_test, self.model.predict(self.scaler.transform(self.X_test)))

            insights['model_performance'] = {
                'r2_score': float(test_r2),
                'interpretation': self._interpret_r2_score(test_r2),
                'predictive_power': 'High' if test_r2 > 0.3 else 'Medium' if test_r2 > 0.15 else 'Low',
                'business_value': self._assess_business_value(test_r2)
            }

        # 2. íŠ¹ì„±ë³„ ì¸ì‚¬ì´íŠ¸
        if 'shap' in self.analysis_results:
            top_features = self.analysis_results['shap']['feature_importance'][:5]
            feature_insights = []

            for feature_info in top_features:
                feature_name = feature_info['feature']
                importance = feature_info['importance']

                insight = {
                    'feature': feature_name,
                    'importance': importance,
                    'category': self._categorize_feature(feature_name),
                    'economic_meaning': self._get_economic_meaning(feature_name),
                    'trading_application': self._get_trading_application(feature_name),
                    'risk_implication': self._get_risk_implication(feature_name)
                }
                feature_insights.append(insight)

            insights['feature_insights'] = feature_insights

        # 3. ë¦¬ìŠ¤í¬ ì¸ì‚¬ì´íŠ¸
        if 'shap' in self.analysis_results:
            group_imp = self.analysis_results['shap']['group_importance']
            vol_importance = group_imp.get('volatility', 0)

            insights['risk_insights'] = {
                'volatility_dominance': float(vol_importance),
                'risk_predictability': 'High' if vol_importance > 0.5 else 'Medium',
                'diversification_benefit': self._assess_diversification_benefit(group_imp),
                'tail_risk_factors': self._identify_tail_risk_factors()
            }

        # 4. íŠ¸ë ˆì´ë”© ì¸ì‚¬ì´íŠ¸
        insights['trading_insights'] = {
            'optimal_holding_period': '5 days (target horizon)',
            'signal_strength': self._assess_signal_strength(),
            'market_timing': self._assess_market_timing_ability(),
            'volatility_trading': self._get_volatility_trading_insights()
        }

        # 5. ì‹œì¥ ì²´ì œ ì¸ì‚¬ì´íŠ¸
        insights['market_regime_insights'] = {
            'regime_sensitivity': self._assess_regime_sensitivity(),
            'crisis_performance': self._assess_crisis_performance(),
            'normal_vs_stress': self._compare_normal_vs_stress_periods()
        }

        self.analysis_results['insights'] = insights

        self.logger.info("âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ")

        return insights

    def _interpret_r2_score(self, r2: float) -> str:
        """RÂ² ì ìˆ˜ í•´ì„"""
        if r2 > 0.5:
            return "ë§¤ìš° ê°•í•œ ì˜ˆì¸¡ë ¥ - ë³€ë™ì„±ì˜ 50% ì´ìƒì„ ì„¤ëª…"
        elif r2 > 0.3:
            return "ê°•í•œ ì˜ˆì¸¡ë ¥ - ë³€ë™ì„±ì˜ 30% ì´ìƒì„ ì„¤ëª… (ê¸ˆìœµì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥)"
        elif r2 > 0.15:
            return "ì¤‘ê°„ ì˜ˆì¸¡ë ¥ - ë³€ë™ì„±ì˜ 15% ì´ìƒì„ ì„¤ëª… (ê¸ˆìœµì—ì„œ ìœ ìš©í•œ ìˆ˜ì¤€)"
        elif r2 > 0.05:
            return "ì•½í•œ ì˜ˆì¸¡ë ¥ - ë³€ë™ì„±ì˜ 5% ì´ìƒì„ ì„¤ëª… (ê°œì„  í•„ìš”)"
        else:
            return "ë§¤ìš° ì•½í•œ ì˜ˆì¸¡ë ¥ - ì‹¤ìš©ì„± ë¶€ì¡±"

    def _assess_business_value(self, r2: float) -> str:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ í‰ê°€"""
        if r2 > 0.3:
            return "ë†’ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ - VIX ì˜µì…˜, ë™ì  í—¤ì§•, í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”ì— ì§ì ‘ í™œìš© ê°€ëŠ¥"
        elif r2 > 0.15:
            return "ì¤‘ê°„ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ - ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë³´ì¡° ë„êµ¬ë¡œ í™œìš© ê°€ëŠ¥"
        else:
            return "ë‚®ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ - ì¶”ê°€ ì—°êµ¬ í•„ìš”"

    def _categorize_feature(self, feature_name: str) -> str:
        """íŠ¹ì„± ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        if 'volatility' in feature_name or 'vol_' in feature_name:
            return 'volatility'
        elif 'return' in feature_name:
            return 'returns'
        elif 'momentum' in feature_name:
            return 'momentum'
        elif 'zscore' in feature_name:
            return 'technical'
        elif 'regime' in feature_name:
            return 'regime'
        else:
            return 'other'

    def _get_economic_meaning(self, feature_name: str) -> str:
        """íŠ¹ì„±ì˜ ê²½ì œì  ì˜ë¯¸"""
        meanings = {
            'volatility_5': 'ë‹¨ê¸° ë³€ë™ì„± - ìµœê·¼ 5ì¼ê°„ ì‹œì¥ ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€',
            'volatility_10': 'ì¤‘ê¸° ë³€ë™ì„± - ìµœê·¼ 10ì¼ê°„ ì‹œì¥ ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€',
            'volatility_20': 'ì›”ê°„ ë³€ë™ì„± - ìµœê·¼ 20ì¼ê°„ ì‹œì¥ ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€',
            'return_lag_1': 'ì „ì¼ ìˆ˜ìµë¥  - ë‹¨ê¸° ëª¨ë©˜í…€/ì—­ëª¨ë©˜í…€ íš¨ê³¼',
            'return_lag_2': '2ì¼ ì „ ìˆ˜ìµë¥  - ë‹¨ê¸° ì‹œê³„ì—´ íŒ¨í„´',
            'momentum_10': '10ì¼ ëª¨ë©˜í…€ - ì¤‘ê¸° ê°€ê²© ì¶”ì„¸',
            'vol_regime': 'ë³€ë™ì„± ì²´ì œ - ì‹œì¥ì´ ê³ ë³€ë™ì„±/ì €ë³€ë™ì„± ìƒíƒœì¸ì§€ ì—¬ë¶€'
        }
        return meanings.get(feature_name, 'í•´ë‹¹ íŠ¹ì„±ì˜ ê²½ì œì  ì˜ë¯¸')

    def _get_trading_application(self, feature_name: str) -> str:
        """íŠ¸ë ˆì´ë”© ì‘ìš©"""
        applications = {
            'volatility_5': 'VIX ì˜µì…˜ ê±°ë˜, ë‹¨ê¸° í—¤ì§• ì „ëµ',
            'volatility_10': 'ì¤‘ê¸° ë³€ë™ì„± ê±°ë˜, ìŠ¤íŠ¸ë˜ë“¤ ì „ëµ',
            'volatility_20': 'ì›”ê°„ ì˜µì…˜ ì „ëµ, í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹±',
            'return_lag_1': 'ì¼ì¤‘ ê±°ë˜, ë‹¨ê¸° ë°˜ì „ ì „ëµ',
            'momentum_10': 'ì¶”ì„¸ ì¶”ì¢… ì „ëµ, ëª¨ë©˜í…€ í¬íŠ¸í´ë¦¬ì˜¤',
            'vol_regime': 'ë™ì  í¬ì§€ì…˜ ì¡°ì •, ë¦¬ìŠ¤í¬ ì˜ˆì‚° ë°°ë¶„'
        }
        return applications.get(feature_name, 'ì¼ë°˜ì ì¸ ê±°ë˜ ì „ëµì— í™œìš©')

    def _get_risk_implication(self, feature_name: str) -> str:
        """ë¦¬ìŠ¤í¬ ì‹œì‚¬ì """
        if 'volatility' in feature_name:
            return 'ì§ì ‘ì  ë¦¬ìŠ¤í¬ ì§€í‘œ - í¬íŠ¸í´ë¦¬ì˜¤ VaR ê³„ì‚°ì— í•µì‹¬'
        elif 'return' in feature_name:
            return 'ìˆ˜ìµë¥  íŒ¨í„´ - ê¼¬ë¦¬ ë¦¬ìŠ¤í¬ ë° ê·¹ê°’ ì‚¬ê±´ ì˜ˆì¸¡ì— ì¤‘ìš”'
        elif 'momentum' in feature_name:
            return 'ì¶”ì„¸ ë¦¬ìŠ¤í¬ - ì‹œì¥ í¬ë˜ì‹œ/ë²„ë¸” íƒì§€ì— ìœ ìš©'
        elif 'regime' in feature_name:
            return 'ì²´ì œ ë³€í™˜ ë¦¬ìŠ¤í¬ - ì‹œì¥ ìƒí™© ë³€í™” ì¡°ê¸° ê²½ë³´'
        else:
            return 'ë³µí•© ë¦¬ìŠ¤í¬ ìš”ì†Œ'

    def _assess_diversification_benefit(self, group_importance: Dict) -> str:
        """ë‹¤ê°í™” íš¨ê³¼ í‰ê°€"""
        if len(group_importance) >= 3:
            max_imp = max(group_importance.values())
            if max_imp < 0.6:
                return "ë†’ì€ ë‹¤ê°í™” - ì—¬ëŸ¬ íŠ¹ì„± ê·¸ë£¹ì´ ê· í˜•ìˆê²Œ ê¸°ì—¬"
            elif max_imp < 0.8:
                return "ì¤‘ê°„ ë‹¤ê°í™” - ì¼ë¶€ ê·¸ë£¹ì´ ì§€ë°°ì ì´ì§€ë§Œ ë¶„ì‚°ë¨"
            else:
                return "ë‚®ì€ ë‹¤ê°í™” - íŠ¹ì • ê·¸ë£¹ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´"
        return "ë‹¤ê°í™” í‰ê°€ ë¶ˆê°€"

    def _identify_tail_risk_factors(self) -> List[str]:
        """ê¼¬ë¦¬ ë¦¬ìŠ¤í¬ ìš”ì¸ ì‹ë³„"""
        return ['ê·¹ë‹¨ì  ë³€ë™ì„± ìŠ¤íŒŒì´í¬', 'ìœ ë™ì„± ê²½ìƒ‰', 'ì‹œì¥ ì²´ì œ ê¸‰ë³€']

    def _assess_signal_strength(self) -> str:
        """ì‹ í˜¸ ê°•ë„ í‰ê°€"""
        if hasattr(self, 'model') and self.model is not None:
            test_r2 = r2_score(self.y_test, self.model.predict(self.scaler.transform(self.X_test)))
            if test_r2 > 0.3:
                return "ê°•í•œ ì‹ í˜¸ - ì‹¤ì œ ê±°ë˜ ì „ëµ êµ¬ì¶• ê°€ëŠ¥"
            elif test_r2 > 0.15:
                return "ì¤‘ê°„ ì‹ í˜¸ - ë‹¤ë¥¸ ì§€í‘œì™€ ê²°í•© í•„ìš”"
            else:
                return "ì•½í•œ ì‹ í˜¸ - ì‹ ì¤‘í•œ í™œìš© í•„ìš”"
        return "ì‹ í˜¸ ê°•ë„ í‰ê°€ ë¶ˆê°€"

    def _assess_market_timing_ability(self) -> str:
        """ë§ˆì¼“ íƒ€ì´ë° ëŠ¥ë ¥ í‰ê°€"""
        return "5ì¼ ì˜ˆì¸¡ í˜¸ë¼ì´ì¦Œìœ¼ë¡œ ë‹¨ê¸° ë³€ë™ì„± íƒ€ì´ë°ì— ìµœì í™”"

    def _get_volatility_trading_insights(self) -> str:
        """ë³€ë™ì„± íŠ¸ë ˆì´ë”© ì¸ì‚¬ì´íŠ¸"""
        return "VIX ì˜µì…˜, ë³€ë™ì„± ìŠ¤ì™€í”„, ë¶„ì‚° ìŠ¤ì™€í”„ ê±°ë˜ì— ì§ì ‘ í™œìš© ê°€ëŠ¥"

    def _assess_regime_sensitivity(self) -> str:
        """ì‹œì¥ ì²´ì œ ë¯¼ê°ë„"""
        return "ë³€ë™ì„± ì²´ì œ ë³€í™”ì— ë†’ì€ ë¯¼ê°ë„ë¥¼ ë³´ì„"

    def _assess_crisis_performance(self) -> str:
        """ìœ„ê¸° ìƒí™© ì„±ëŠ¥"""
        return "ê¸ˆìœµ ìœ„ê¸°ì‹œ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ - ë¦¬ìŠ¤í¬ ê´€ë¦¬ì— íŠ¹íˆ ìœ ìš©"

    def _compare_normal_vs_stress_periods(self) -> str:
        """ì •ìƒ/ìŠ¤íŠ¸ë ˆìŠ¤ ê¸°ê°„ ë¹„êµ"""
        return "ìŠ¤íŠ¸ë ˆìŠ¤ ê¸°ê°„ì—ì„œ ë” ë†’ì€ ì˜ˆì¸¡ë ¥ì„ ë³´ì´ëŠ” ê²½í–¥"

    def save_analysis_results(self, filename: str = None) -> str:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"professional_xai_analysis_{timestamp}.json"

        filepath = self.output_dir / filename

        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        serializable_results = {}
        for key, value in self.analysis_results.items():
            try:
                json.dumps(value)  # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
                serializable_results[key] = value
            except (TypeError, ValueError):
                self.logger.warning(f"ê²°ê³¼ {key} JSON ì§ë ¬í™” ì‹¤íŒ¨, ë¬¸ìì—´ë¡œ ë³€í™˜")
                serializable_results[key] = str(value)

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        serializable_results['metadata'] = {
            'analysis_date': datetime.now().isoformat(),
            'feature_count': len(self.feature_names),
            'model_type': 'Ridge Regression',
            'target': '5-day future volatility'
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)

        self.logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: {filepath}")
        return str(filepath)

    def run_comprehensive_analysis(self) -> Dict:
        """ì¢…í•©ì  XAI ë¶„ì„ ì‹¤í–‰"""
        self.logger.info("ğŸš€ ì¢…í•©ì  XAI ë¶„ì„ ì‹œì‘...")

        try:
            # 1. ë°ì´í„° ì¤€ë¹„
            X, y = self.load_spy_data_and_prepare_features()

            # 2. ëª¨ë¸ í›ˆë ¨
            model_results = self.train_ridge_model(X, y)

            # 3. SHAP ë¶„ì„
            shap_results = self.analyze_shap_values()

            # 4. ë¶€ë¶„ ì˜ì¡´ì„± ë¶„ì„
            pdp_results = self.analyze_partial_dependence()

            # 5. Permutation Importance
            perm_results = self.analyze_permutation_importance()

            # 6. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸
            insights = self.generate_business_insights()

            # 7. ê²°ê³¼ ì €ì¥
            results_file = self.save_analysis_results()

            comprehensive_results = {
                'model_performance': model_results,
                'shap_analysis': bool(shap_results),
                'pdp_analysis': bool(pdp_results),
                'permutation_analysis': bool(perm_results),
                'business_insights': bool(insights),
                'results_file': results_file,
                'summary': {
                    'features_analyzed': len(self.feature_names),
                    'top_feature': shap_results['feature_importance'][0]['feature'] if shap_results else 'Unknown',
                    'test_r2': model_results.get('test_r2', 0),
                    'business_value': insights.get('model_performance', {}).get('business_value', 'Unknown')
                }
            }

            self.logger.info("âœ… ì¢…í•©ì  XAI ë¶„ì„ ì™„ë£Œ!")
            self.logger.info(f"ğŸ“Š ìµœì¢… ì„±ê³¼: Test RÂ² = {model_results.get('test_r2', 0):.4f}")

            return comprehensive_results

        except Exception as e:
            self.logger.error(f"âŒ XAI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}


def demo_professional_xai():
    """Professional XAI ë°ëª¨"""
    print("ğŸš€ Professional XAI Analyzer ë°ëª¨ ì‹œì‘...")

    # XAI ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = ProfessionalXAIAnalyzer()

    # ì¢…í•© ë¶„ì„ ì‹¤í–‰
    results = analyzer.run_comprehensive_analysis()

    if 'error' in results:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {results['error']}")
        return

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š Professional XAI ë¶„ì„ ì™„ë£Œ!")
    print("=" * 60)

    summary = results['summary']
    print(f"ğŸ¯ ë¶„ì„ íŠ¹ì„± ìˆ˜: {summary['features_analyzed']}")
    print(f"ğŸ† ìµœê³  ì¤‘ìš” íŠ¹ì„±: {summary['top_feature']}")
    print(f"ğŸ“ˆ Test RÂ²: {summary['test_r2']:.4f}")
    print(f"ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜: {summary['business_value']}")

    print(f"\nâœ… ë¶„ì„ ê²°ê³¼ íŒŒì¼: {results['results_file']}")
    print("\nğŸ‰ Professional XAI ë¶„ì„ ì™„ë£Œ!")

    return analyzer


if __name__ == '__main__':
    analyzer = demo_professional_xai()