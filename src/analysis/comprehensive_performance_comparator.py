#!/usr/bin/env python3
"""
ğŸ† ì¢…í•© ì„±ëŠ¥ ë¹„êµ ë° ìµœì  ëª¨ë¸ ì„ ì • ì‹œìŠ¤í…œ

ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³  ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìµœì  ëª¨ë¸ ì„ ì •
"""

import sys
sys.path.append('/root/workspace/src')

import pandas as pd
import numpy as np
import json
import glob
from datetime import datetime
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class ComprehensivePerformanceComparator:
    """ì¢…í•© ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.results_dir = '/root/workspace/data/results'
        self.experiments_summary = {}
        self.safety_criteria = {
            'max_r2': 0.25,           # RÂ² ìƒí•œì„ 
            'max_direction_acc': 70.0, # ë°©í–¥ì •í™•ë„ ìƒí•œì„  (%)
            'max_correlation': 0.3     # íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ìƒí•œì„ 
        }

        print(f"ğŸ† ì¢…í•© ì„±ëŠ¥ ë¹„êµ ë° ìµœì  ëª¨ë¸ ì„ ì • ì‹œìŠ¤í…œ")
        print(f"   ğŸ“Š ì•ˆì „ ê¸°ì¤€: RÂ²<{self.safety_criteria['max_r2']}, ë°©í–¥ì •í™•ë„<{self.safety_criteria['max_direction_acc']}%")

    def load_experiment_results(self):
        """ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ ë¡œë”©"""
        print("ğŸ“‚ ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ ë¡œë”©...")

        experiment_files = []

        # JSON ê²°ê³¼ íŒŒì¼ë“¤ ì°¾ê¸°
        json_patterns = [
            'ultra_safe_no_leak_results_*.json',
            'ultra_safe_hyperparameter_tuning_*.json',
            'safe_neural_networks_*.json',
            'safe_ensemble_methods_*.json',
            'advanced_leak_free_regression_*.json',
            'comprehensive_regression_results_*.json'
        ]

        for pattern in json_patterns:
            files = glob.glob(os.path.join(self.results_dir, pattern))
            experiment_files.extend(files)

        print(f"   ë°œê²¬ëœ ì‹¤í—˜ íŒŒì¼: {len(experiment_files)}ê°œ")

        # íŒŒì¼ë³„ë¡œ ë¡œë”©
        all_results = {}

        for file_path in experiment_files:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)

                    # íŒŒì¼ëª…ì—ì„œ ì‹¤í—˜ íƒ€ì… ì¶”ì¶œ
                    file_name = os.path.basename(file_path)
                    experiment_type = data.get('experiment_type', file_name.split('_')[0])

                    all_results[file_name] = {
                        'experiment_type': experiment_type,
                        'timestamp': data.get('timestamp', 'unknown'),
                        'results': data.get('results', {}),
                        'file_path': file_path
                    }

                print(f"   âœ… {file_name}: {len(data.get('results', {}))}ê°œ ëª¨ë¸")

            except Exception as e:
                print(f"   âŒ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}")

        # ê¸°ì¡´ ì„±ëŠ¥ ë°ì´í„°ë„ ë¡œë”©
        self._load_existing_performance_data(all_results)

        return all_results

    def _load_existing_performance_data(self, all_results):
        """ê¸°ì¡´ ì„±ëŠ¥ ë°ì´í„° ë¡œë”©"""

        # model_performance.json ë¡œë”©
        performance_file = '/root/workspace/data/raw/model_performance.json'
        if os.path.exists(performance_file):
            try:
                with open(performance_file, 'r') as f:
                    data = json.load(f)
                    all_results['model_performance.json'] = {
                        'experiment_type': 'existing_models',
                        'timestamp': data.get('timestamp', 'existing'),
                        'results': data.get('models', {}),
                        'file_path': performance_file
                    }
                print(f"   âœ… ê¸°ì¡´ ì„±ëŠ¥ ë°ì´í„°: {len(data.get('models', {}))}ê°œ ëª¨ë¸")
            except Exception as e:
                print(f"   âŒ ê¸°ì¡´ ì„±ëŠ¥ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")

        # validation_report.json ë¡œë”©
        validation_file = '/root/workspace/data/raw/validation_report.json'
        if os.path.exists(validation_file):
            try:
                with open(validation_file, 'r') as f:
                    data = json.load(f)
                    # validation_reportì˜ êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
                    validation_results = {}
                    if 'results' in data:
                        for model_name, metrics in data['results'].items():
                            validation_results[f"{model_name}_validated"] = metrics

                    all_results['validation_report.json'] = {
                        'experiment_type': 'validation_results',
                        'timestamp': data.get('timestamp', 'validation'),
                        'results': validation_results,
                        'file_path': validation_file
                    }
                print(f"   âœ… ê²€ì¦ ë°ì´í„°: {len(validation_results)}ê°œ ëª¨ë¸")
            except Exception as e:
                print(f"   âŒ ê²€ì¦ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")

    def normalize_model_metrics(self, results_dict):
        """ëª¨ë¸ ë©”íŠ¸ë¦­ ì •ê·œí™”"""
        print("ğŸ”§ ëª¨ë¸ ë©”íŠ¸ë¦­ ì •ê·œí™”...")

        normalized_models = {}

        for file_name, file_data in results_dict.items():
            experiment_type = file_data['experiment_type']
            timestamp = file_data['timestamp']

            for model_name, metrics in file_data['results'].items():

                # ì •ê·œí™”ëœ ëª¨ë¸ëª… ìƒì„±
                normalized_name = f"{experiment_type}_{model_name}"

                # ë©”íŠ¸ë¦­ ì •ê·œí™”
                normalized_metrics = {
                    'experiment_type': experiment_type,
                    'timestamp': timestamp,
                    'file_source': file_name,
                    'direction_accuracy': None,
                    'mae': None,
                    'r2': None,
                    'mse': None,
                    'fold_accuracies': None,
                    'safety_status': 'unknown'
                }

                # ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ í˜•ì‹ ì²˜ë¦¬
                if isinstance(metrics, dict):
                    # ë°©í–¥ ì •í™•ë„ ì¶”ì¶œ
                    for key in ['direction_accuracy', 'avg_direction_accuracy', 'accuracy', 'best_direction_accuracy']:
                        if key in metrics and metrics[key] is not None:
                            normalized_metrics['direction_accuracy'] = float(metrics[key])
                            if normalized_metrics['direction_accuracy'] > 1:  # ë°±ë¶„ìœ¨ì„ ì†Œìˆ˜ë¡œ ë³€í™˜
                                normalized_metrics['direction_accuracy'] /= 100
                            break

                    # MAE ì¶”ì¶œ
                    for key in ['mae', 'avg_mae', 'mean_absolute_error']:
                        if key in metrics and metrics[key] is not None:
                            normalized_metrics['mae'] = float(metrics[key])
                            break

                    # RÂ² ì¶”ì¶œ
                    for key in ['r2', 'avg_r2', 'r2_score', 'r_squared']:
                        if key in metrics and metrics[key] is not None:
                            normalized_metrics['r2'] = float(metrics[key])
                            break

                    # MSE ì¶”ì¶œ
                    for key in ['mse', 'avg_mse', 'mean_squared_error']:
                        if key in metrics and metrics[key] is not None:
                            normalized_metrics['mse'] = float(metrics[key])
                            break

                    # Fold accuracies ì¶”ì¶œ
                    for key in ['fold_accuracies', 'cv_accuracies']:
                        if key in metrics and metrics[key] is not None:
                            normalized_metrics['fold_accuracies'] = metrics[key]
                            break

                    # ì•ˆì „ì„± ìƒíƒœ í‰ê°€
                    normalized_metrics['safety_status'] = self._evaluate_safety(normalized_metrics)

                normalized_models[normalized_name] = normalized_metrics

        print(f"   âœ… ì •ê·œí™”ëœ ëª¨ë¸: {len(normalized_models)}ê°œ")
        return normalized_models

    def _evaluate_safety(self, metrics):
        """ëª¨ë¸ ì•ˆì „ì„± í‰ê°€"""
        direction_acc = metrics.get('direction_accuracy')
        r2 = metrics.get('r2')

        if direction_acc is None and r2 is None:
            return 'insufficient_data'

        unsafe_reasons = []

        # ë°©í–¥ ì •í™•ë„ ê²€ì‚¬
        if direction_acc is not None:
            if direction_acc > 0.95:
                unsafe_reasons.append('extreme_accuracy')
            elif direction_acc > (self.safety_criteria['max_direction_acc'] / 100):
                unsafe_reasons.append('high_accuracy')

        # RÂ² ê²€ì‚¬
        if r2 is not None:
            if r2 > 0.9:
                unsafe_reasons.append('extreme_r2')
            elif r2 > self.safety_criteria['max_r2']:
                unsafe_reasons.append('high_r2')

        if unsafe_reasons:
            return f"unsafe_{','.join(unsafe_reasons)}"
        else:
            return 'safe'

    def create_comprehensive_ranking(self, normalized_models):
        """ì¢…í•© ìˆœìœ„ ìƒì„±"""
        print("ğŸ† ì¢…í•© ìˆœìœ„ ìƒì„±...")

        # ì•ˆì „í•œ ëª¨ë¸ë“¤ë§Œ í•„í„°ë§
        safe_models = {k: v for k, v in normalized_models.items()
                      if v['safety_status'] == 'safe'}

        print(f"   ì•ˆì „í•œ ëª¨ë¸: {len(safe_models)}ê°œ")
        print(f"   ìœ„í—˜í•œ ëª¨ë¸: {len(normalized_models) - len(safe_models)}ê°œ")

        if not safe_models:
            print("   âš ï¸ ì•ˆì „í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return []

        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        ranked_models = []

        for model_name, metrics in safe_models.items():
            score = self._calculate_composite_score(metrics)

            ranked_models.append({
                'model_name': model_name,
                'composite_score': score,
                **metrics
            })

        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        ranked_models.sort(key=lambda x: x['composite_score'], reverse=True)

        return ranked_models

    def _calculate_composite_score(self, metrics):
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        weight_sum = 0.0

        # ë°©í–¥ ì •í™•ë„ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 0.5)
        direction_acc = metrics.get('direction_accuracy')
        if direction_acc is not None:
            # 50%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚° (50% = 0ì , 70% = 100ì )
            direction_score = max(0, (direction_acc - 0.5) / 0.2 * 100)
            score += direction_score * 0.5
            weight_sum += 0.5

        # RÂ² ì ìˆ˜ (ê°€ì¤‘ì¹˜: 0.3)
        r2 = metrics.get('r2')
        if r2 is not None:
            # RÂ²: ìŒìˆ˜ë©´ 0ì , 0.25ë©´ 100ì 
            r2_score = max(0, min(100, (r2 + 0.1) / 0.35 * 100))
            score += r2_score * 0.3
            weight_sum += 0.3

        # MAE ì ìˆ˜ (ê°€ì¤‘ì¹˜: 0.2) - ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        mae = metrics.get('mae')
        if mae is not None:
            # MAE: 1.0ì´ë©´ 0ì , 0.3ì´ë©´ 100ì 
            mae_score = max(0, min(100, (1.0 - mae) / 0.7 * 100))
            score += mae_score * 0.2
            weight_sum += 0.2

        # ì •ê·œí™”
        if weight_sum > 0:
            score /= weight_sum

        return score

    def generate_comprehensive_report(self, ranked_models, all_results):
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“‹ ì¢…í•© ë³´ê³ ì„œ ìƒì„±...")

        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_models_analyzed': len(all_results),
                'safe_models': len([m for m in ranked_models if m['safety_status'] == 'safe']),
                'unsafe_models': len(all_results) - len([m for m in ranked_models if m['safety_status'] == 'safe']),
                'best_model': ranked_models[0]['model_name'] if ranked_models else None,
                'best_score': ranked_models[0]['composite_score'] if ranked_models else None
            },
            'safety_criteria': self.safety_criteria,
            'ranking': ranked_models[:10],  # ìƒìœ„ 10ê°œ ëª¨ë¸
            'experiment_summary': self._create_experiment_summary(ranked_models)
        }

        return report

    def _create_experiment_summary(self, ranked_models):
        """ì‹¤í—˜ë³„ ìš”ì•½ ìƒì„±"""
        experiment_summary = {}

        for model in ranked_models:
            exp_type = model['experiment_type']
            if exp_type not in experiment_summary:
                experiment_summary[exp_type] = {
                    'total_models': 0,
                    'best_model': None,
                    'best_score': 0,
                    'avg_direction_accuracy': 0,
                    'avg_r2': 0
                }

            experiment_summary[exp_type]['total_models'] += 1

            if model['composite_score'] > experiment_summary[exp_type]['best_score']:
                experiment_summary[exp_type]['best_model'] = model['model_name']
                experiment_summary[exp_type]['best_score'] = model['composite_score']

        # í‰ê·  ê³„ì‚°
        for exp_type in experiment_summary:
            exp_models = [m for m in ranked_models if m['experiment_type'] == exp_type]

            direction_accs = [m['direction_accuracy'] for m in exp_models if m['direction_accuracy'] is not None]
            if direction_accs:
                experiment_summary[exp_type]['avg_direction_accuracy'] = np.mean(direction_accs)

            r2s = [m['r2'] for m in exp_models if m['r2'] is not None]
            if r2s:
                experiment_summary[exp_type]['avg_r2'] = np.mean(r2s)

        return experiment_summary

    def run_comprehensive_analysis(self):
        """ì¢…í•© ë¶„ì„ ì‹¤í–‰"""
        print("ğŸ” ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ì‹œì‘")
        print("="*70)

        try:
            # 1. ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ë¡œë”©
            all_results = self.load_experiment_results()

            if not all_results:
                print("âŒ ë¶„ì„í•  ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return None

            # 2. ë©”íŠ¸ë¦­ ì •ê·œí™”
            normalized_models = self.normalize_model_metrics(all_results)

            # 3. ì¢…í•© ìˆœìœ„ ìƒì„±
            ranked_models = self.create_comprehensive_ranking(normalized_models)

            # 4. ë³´ê³ ì„œ ìƒì„±
            report = self.generate_comprehensive_report(ranked_models, normalized_models)

            # 5. ê²°ê³¼ ì¶œë ¥
            self._print_analysis_results(report)

            # 6. ê²°ê³¼ ì €ì¥
            self._save_analysis_results(report)

            return report

        except Exception as e:
            print(f"âŒ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def _print_analysis_results(self, report):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ† ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼")
        print("="*70)

        summary = report['summary']
        print(f"ğŸ“Š ë¶„ì„ ìš”ì•½:")
        print(f"   ì´ ë¶„ì„ ëª¨ë¸: {summary['total_models_analyzed']}ê°œ")
        print(f"   ì•ˆì „í•œ ëª¨ë¸: {summary['safe_models']}ê°œ")
        print(f"   ìœ„í—˜í•œ ëª¨ë¸: {summary['unsafe_models']}ê°œ")

        if summary['best_model']:
            print(f"   ğŸ¥‡ ìµœê³  ëª¨ë¸: {summary['best_model']}")
            print(f"   ğŸ¯ ìµœê³  ì ìˆ˜: {summary['best_score']:.2f}")

        print(f"\nğŸ… ìƒìœ„ ëª¨ë¸ ìˆœìœ„:")
        print("-" * 70)

        for i, model in enumerate(report['ranking'][:5], 1):
            direction_acc = model.get('direction_accuracy', 0)
            r2 = model.get('r2', 0)
            mae = model.get('mae', 0)

            print(f"{i:2d}. {model['model_name'][:40]:<40}")
            print(f"     ì¢…í•©ì ìˆ˜: {model['composite_score']:.2f} | "
                  f"ë°©í–¥ì •í™•ë„: {direction_acc:.1%} | "
                  f"RÂ²: {r2:.4f} | "
                  f"MAE: {mae:.4f}")

        print(f"\nğŸ“ˆ ì‹¤í—˜ë³„ ìš”ì•½:")
        print("-" * 70)

        for exp_type, summary in report['experiment_summary'].items():
            print(f"{exp_type}:")
            print(f"   ëª¨ë¸ ìˆ˜: {summary['total_models']}ê°œ")
            print(f"   ìµœê³  ëª¨ë¸: {summary['best_model']}")
            print(f"   í‰ê·  ë°©í–¥ì •í™•ë„: {summary['avg_direction_accuracy']:.1%}")
            print(f"   í‰ê·  RÂ²: {summary['avg_r2']:.4f}")

    def _save_analysis_results(self, report):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output_path = f"/root/workspace/data/results/comprehensive_performance_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        try:
            with open(output_path, 'w') as f:
                json.dump(report, f, indent=2, default=str)
            print(f"\nğŸ’¾ ì¢…í•© ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œë„ ìƒì„±
        self._generate_markdown_report(report)

    def _generate_markdown_report(self, report):
        """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±"""
        output_path = f"/root/workspace/comprehensive_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("# ğŸ† ì¢…í•© ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ\n\n")

                f.write("## ğŸ“Š ë¶„ì„ ìš”ì•½\n\n")
                summary = report['summary']
                f.write(f"- **ì´ ë¶„ì„ ëª¨ë¸**: {summary['total_models_analyzed']}ê°œ\n")
                f.write(f"- **ì•ˆì „í•œ ëª¨ë¸**: {summary['safe_models']}ê°œ\n")
                f.write(f"- **ìœ„í—˜í•œ ëª¨ë¸**: {summary['unsafe_models']}ê°œ\n")

                if summary['best_model']:
                    f.write(f"- **ğŸ¥‡ ìµœê³  ëª¨ë¸**: {summary['best_model']}\n")
                    f.write(f"- **ğŸ¯ ìµœê³  ì ìˆ˜**: {summary['best_score']:.2f}\n")

                f.write("\n## ğŸ… ìƒìœ„ ëª¨ë¸ ìˆœìœ„\n\n")
                f.write("| ìˆœìœ„ | ëª¨ë¸ëª… | ì¢…í•©ì ìˆ˜ | ë°©í–¥ì •í™•ë„ | RÂ² | MAE | ì‹¤í—˜íƒ€ì… |\n")
                f.write("|------|--------|----------|-----------|----|----|----------|\n")

                for i, model in enumerate(report['ranking'][:10], 1):
                    direction_acc = model.get('direction_accuracy', 0)
                    r2 = model.get('r2', 0)
                    mae = model.get('mae', 0)
                    exp_type = model.get('experiment_type', 'unknown')

                    f.write(f"| {i} | {model['model_name'][:30]} | {model['composite_score']:.2f} | "
                           f"{direction_acc:.1%} | {r2:.4f} | {mae:.4f} | {exp_type} |\n")

                f.write("\n## ğŸ“ˆ ì‹¤í—˜ë³„ ìš”ì•½\n\n")

                for exp_type, exp_summary in report['experiment_summary'].items():
                    f.write(f"### {exp_type}\n\n")
                    f.write(f"- **ëª¨ë¸ ìˆ˜**: {exp_summary['total_models']}ê°œ\n")
                    f.write(f"- **ìµœê³  ëª¨ë¸**: {exp_summary['best_model']}\n")
                    f.write(f"- **í‰ê·  ë°©í–¥ì •í™•ë„**: {exp_summary['avg_direction_accuracy']:.1%}\n")
                    f.write(f"- **í‰ê·  RÂ²**: {exp_summary['avg_r2']:.4f}\n\n")

                f.write(f"\n## ğŸ“‹ ë¶„ì„ ì‹œê°„\n\n")
                f.write(f"**ìƒì„± ì‹œê°„**: {report['timestamp']}\n")

            print(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥: {output_path}")

        except Exception as e:
            print(f"âŒ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    comparator = ComprehensivePerformanceComparator()
    report = comparator.run_comprehensive_analysis()

    if report:
        print("\nğŸ‰ ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ!")
        print("âœ… ëª¨ë“  ëª¨ë¸ì´ ì•ˆì „ì„± ê²€ì¦ì„ ê±°ì³ ìˆœìœ„ê°€ ë§¤ê²¨ì¡ŒìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨!")

    return report

if __name__ == "__main__":
    main()