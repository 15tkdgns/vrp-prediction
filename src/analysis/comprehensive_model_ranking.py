#!/usr/bin/env python3
"""
ì¢…í•©ì ì¸ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ í‰ê°€ ì‹œìŠ¤í…œ
ë‹¤ì°¨ì› ì§€í‘œ ê¸°ë°˜ ìƒì„¸ ë¶„ì„
"""

import sys
sys.path.append('/root/workspace')

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

from src.optimization.fast_safe_gridsearch import FastSafeGridSearch

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveModelRanking:
    """ì¢…í•©ì ì¸ ëª¨ë¸ ìˆœìœ„ í‰ê°€"""

    def __init__(self):
        self.optimizer = FastSafeGridSearch()

        # í‰ê°€ ê°€ì¤‘ì¹˜
        self.weights = {
            'r2': 0.25,           # RÂ² ê°€ì¤‘ì¹˜
            'mae': 0.25,          # MAE ê°€ì¤‘ì¹˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            'direction_acc': 0.30, # ë°©í–¥ ì •í™•ë„ ê°€ì¤‘ì¹˜
            'stability': 0.20     # ì•ˆì •ì„± ê°€ì¤‘ì¹˜
        }

        logger.info("ğŸ“Š ì¢…í•© ëª¨ë¸ ìˆœìœ„ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")

    def calculate_advanced_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
        """ê³ ê¸‰ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""

        # ê¸°ë³¸ ì§€í‘œ
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)

        # ë°©í–¥ ì •í™•ë„
        direction_actual = (y_true > 0).astype(int)
        direction_pred = (y_pred > 0).astype(int)
        direction_accuracy = (direction_actual == direction_pred).mean() * 100

        # ìƒìŠ¹/í•˜ë½ ë³„ ì •í™•ë„
        up_mask = y_true > 0
        down_mask = y_true <= 0

        up_accuracy = 0
        down_accuracy = 0

        if np.sum(up_mask) > 0:
            up_accuracy = (direction_actual[up_mask] == direction_pred[up_mask]).mean() * 100

        if np.sum(down_mask) > 0:
            down_accuracy = (direction_actual[down_mask] == direction_pred[down_mask]).mean() * 100

        # ì˜ˆì¸¡ í¸í–¥ì„±
        prediction_bias = np.mean(y_pred - y_true)

        # ì˜ˆì¸¡ ë¶„ì‚°
        prediction_variance = np.var(y_pred - y_true)

        # ìƒê´€ê´€ê³„
        correlation = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else 0

        return {
            'r2': r2,
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'direction_accuracy': direction_accuracy,
            'up_accuracy': up_accuracy,
            'down_accuracy': down_accuracy,
            'prediction_bias': prediction_bias,
            'prediction_variance': prediction_variance,
            'correlation': correlation
        }

    def evaluate_model_stability(self, model, X: np.ndarray, y: np.ndarray) -> Dict:
        """ëª¨ë¸ ì•ˆì •ì„± í‰ê°€ (ì—¬ëŸ¬ fold ì„±ëŠ¥ ë¶„ì‚°)"""

        tscv = TimeSeriesSplit(n_splits=3, test_size=80, gap=2)

        fold_r2_scores = []
        fold_mae_scores = []
        fold_direction_scores = []

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            # ë°ì´í„° ë¶„í• 
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # ìŠ¤ì¼€ì¼ë§
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            # ëª¨ë¸ ë³µì‚¬ ë° í›ˆë ¨
            from sklearn.base import clone
            fold_model = clone(model)
            fold_model.fit(X_train_scaled, y_train)
            y_pred = fold_model.predict(X_val_scaled)

            # ì„±ëŠ¥ ê³„ì‚°
            metrics = self.calculate_advanced_metrics(y_val, y_pred)

            fold_r2_scores.append(metrics['r2'])
            fold_mae_scores.append(metrics['mae'])
            fold_direction_scores.append(metrics['direction_accuracy'])

        # ì•ˆì •ì„± ì§€í‘œ (ë‚®ì€ ë¶„ì‚°ì´ ì¢‹ìŒ)
        r2_stability = 1 / (1 + np.std(fold_r2_scores))
        mae_stability = 1 / (1 + np.std(fold_mae_scores))
        direction_stability = 1 / (1 + np.std(fold_direction_scores))

        overall_stability = (r2_stability + mae_stability + direction_stability) / 3

        return {
            'r2_stability': r2_stability,
            'mae_stability': mae_stability,
            'direction_stability': direction_stability,
            'overall_stability': overall_stability,
            'fold_r2_scores': fold_r2_scores,
            'fold_mae_scores': fold_mae_scores,
            'fold_direction_scores': fold_direction_scores
        }

    def calculate_comprehensive_score(self, metrics: Dict, stability: Dict) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""

        # RÂ² ì •ê·œí™” (0 ê¸°ì¤€ìœ¼ë¡œ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, ìµœëŒ€ 0.15)
        r2_normalized = max(0, metrics['r2']) / 0.15

        # MAE ì •ê·œí™” (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, ì—­ìˆ˜ ì·¨í•¨)
        mae_normalized = 1 / (1 + metrics['mae'] * 100)  # 100ë°°ë¡œ ìŠ¤ì¼€ì¼ë§

        # ë°©í–¥ ì •í™•ë„ ì •ê·œí™” (ìµœëŒ€ 65%)
        direction_normalized = metrics['direction_accuracy'] / 65.0

        # ì•ˆì •ì„± ì •ê·œí™” (ì´ë¯¸ 0~1 ë²”ìœ„)
        stability_normalized = stability['overall_stability']

        # ê°€ì¤‘ í‰ê· 
        comprehensive_score = (
            self.weights['r2'] * r2_normalized +
            self.weights['mae'] * mae_normalized +
            self.weights['direction_acc'] * direction_normalized +
            self.weights['stability'] * stability_normalized
        )

        return comprehensive_score

    def run_comprehensive_ranking(self, data_path: str) -> Dict:
        """ì¢…í•©ì ì¸ ëª¨ë¸ ìˆœìœ„ í‰ê°€ ì‹¤í–‰"""

        logger.info("ğŸ† ì¢…í•©ì ì¸ ëª¨ë¸ ìˆœìœ„ í‰ê°€ ì‹œì‘")

        # 1. ê¸°ë³¸ ìµœì í™” ì‹¤í–‰
        optimization_results = self.optimizer.run_fast_optimization(data_path)

        # 2. ë°ì´í„° ì¤€ë¹„
        data_dict = self.optimizer.data_processor.prepare_ultra_safe_data(data_path)
        X, y = data_dict['X'], data_dict['y']

        # 3. ê° ëª¨ë¸ë³„ ìƒì„¸ í‰ê°€
        detailed_rankings = []

        for model_name, result in optimization_results['detailed_results'].items():
            if result['status'] == 'success' and result['cv_results']['status'] == 'safe':
                logger.info(f"\nğŸ“Š {model_name} ìƒì„¸ í‰ê°€")

                # ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬
                model = result['model']
                scaler = result['scaler']

                # ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… í‰ê°€
                X_scaled = scaler.transform(X)

                # Hold-out í…ŒìŠ¤íŠ¸ (ë§ˆì§€ë§‰ 20% ë°ì´í„°)
                split_point = int(len(X) * 0.8)
                X_train, X_test = X_scaled[:split_point], X_scaled[split_point:]
                y_train, y_test = y[:split_point], y[split_point:]

                # ìµœì¢… ëª¨ë¸ í›ˆë ¨
                from sklearn.base import clone
                final_model = clone(model)
                final_model.fit(X_train, y_train)
                y_pred = final_model.predict(X_test)

                # ê³ ê¸‰ ì§€í‘œ ê³„ì‚°
                advanced_metrics = self.calculate_advanced_metrics(y_test, y_pred)

                # ì•ˆì •ì„± í‰ê°€
                stability_metrics = self.evaluate_model_stability(model, X, y)

                # ì¢…í•© ì ìˆ˜
                comprehensive_score = self.calculate_comprehensive_score(
                    advanced_metrics, stability_metrics
                )

                detailed_rankings.append({
                    'model_name': model_name,
                    'best_params': result['best_params'],
                    'advanced_metrics': advanced_metrics,
                    'stability_metrics': stability_metrics,
                    'comprehensive_score': comprehensive_score
                })

                logger.info(f"   ì¢…í•© ì ìˆ˜: {comprehensive_score:.3f}")
                logger.info(f"   RÂ²: {advanced_metrics['r2']:.4f}")
                logger.info(f"   MAE: {advanced_metrics['mae']:.4f}")
                logger.info(f"   ë°©í–¥ì •í™•ë„: {advanced_metrics['direction_accuracy']:.1f}%")
                logger.info(f"   ì•ˆì •ì„±: {stability_metrics['overall_stability']:.3f}")

        # 4. ì¢…í•© ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        detailed_rankings.sort(key=lambda x: x['comprehensive_score'], reverse=True)

        # 5. ìµœì¢… ìˆœìœ„ ì¶œë ¥
        logger.info(f"\n{'='*100}")
        logger.info(f"ğŸ† ìµœì¢… ì¢…í•© ìˆœìœ„ (ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨)")
        logger.info(f"{'='*100}")

        for rank, result in enumerate(detailed_rankings, 1):
            model_name = result['model_name']
            metrics = result['advanced_metrics']
            stability = result['stability_metrics']
            score = result['comprehensive_score']

            logger.info(f"\n{rank}ìœ„. {model_name} (ì¢…í•©ì ìˆ˜: {score:.3f})")
            logger.info(f"   ğŸ¯ ì„±ëŠ¥ ì§€í‘œ:")
            logger.info(f"      RÂ²: {metrics['r2']:.4f}")
            logger.info(f"      MAE: {metrics['mae']:.4f}")
            logger.info(f"      RMSE: {metrics['rmse']:.4f}")
            logger.info(f"      ë°©í–¥ì •í™•ë„: {metrics['direction_accuracy']:.1f}%")
            logger.info(f"      ìƒìŠ¹ì •í™•ë„: {metrics['up_accuracy']:.1f}%")
            logger.info(f"      í•˜ë½ì •í™•ë„: {metrics['down_accuracy']:.1f}%")
            logger.info(f"      ìƒê´€ê´€ê³„: {metrics['correlation']:.3f}")
            logger.info(f"   ğŸ›¡ï¸ ì•ˆì •ì„± ì§€í‘œ:")
            logger.info(f"      ì „ì²´ì•ˆì •ì„±: {stability['overall_stability']:.3f}")
            logger.info(f"      RÂ²ì•ˆì •ì„±: {stability['r2_stability']:.3f}")
            logger.info(f"      MAEì•ˆì •ì„±: {stability['mae_stability']:.3f}")
            logger.info(f"      ë°©í–¥ì•ˆì •ì„±: {stability['direction_stability']:.3f}")
            logger.info(f"   âš™ï¸ ìµœì  íŒŒë¼ë¯¸í„°: {result['best_params']}")

        return {
            'detailed_rankings': detailed_rankings,
            'total_safe_models': len(detailed_rankings),
            'evaluation_weights': self.weights,
            'winner': detailed_rankings[0] if detailed_rankings else None
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    evaluator = ComprehensiveModelRanking()

    try:
        results = evaluator.run_comprehensive_ranking(
            '/root/workspace/data/training/sp500_2020_2024_enhanced.csv'
        )

        print(f"\n{'='*100}")
        print(f"ğŸ“Š ì¢…í•© ëª¨ë¸ ìˆœìœ„ í‰ê°€ ì™„ë£Œ")
        print(f"{'='*100}")

        if results['winner']:
            winner = results['winner']
            print(f"\nğŸ† ìµœìš°ìˆ˜ ëª¨ë¸: {winner['model_name']}")
            print(f"   ì¢…í•© ì ìˆ˜: {winner['comprehensive_score']:.3f}")
            print(f"   RÂ²: {winner['advanced_metrics']['r2']:.4f}")
            print(f"   MAE: {winner['advanced_metrics']['mae']:.4f}")
            print(f"   ë°©í–¥ì •í™•ë„: {winner['advanced_metrics']['direction_accuracy']:.1f}%")
            print(f"   ì•ˆì •ì„±: {winner['stability_metrics']['overall_stability']:.3f}")
            print(f"   ìµœì  íŒŒë¼ë¯¸í„°: {winner['best_params']}")

        print(f"\nğŸ“ˆ í‰ê°€ ì™„ë£Œ ëª¨ë¸: {results['total_safe_models']}ê°œ")
        print(f"ğŸ›¡ï¸ ëª¨ë“  ëª¨ë¸ì´ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ ê¸°ì¤€ í†µê³¼")

        return results

    except Exception as e:
        logger.error(f"ì¢…í•© ìˆœìœ„ í‰ê°€ ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    result = main()