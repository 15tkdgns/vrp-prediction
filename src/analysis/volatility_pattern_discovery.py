#!/usr/bin/env python3
"""
ë³€ë™ì„± íŒ¨í„´ ë°œê²¬ ë° ë¶„ì„
ëª©í‘œ: ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ì°¾ì•„ RÂ² 0.33 â†’ 0.40+ ë‹¬ì„±
"""

import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.signal import find_peaks
import warnings
warnings.filterwarnings('ignore')

class VolatilityPatternDiscovery:
    """ë³€ë™ì„± íŒ¨í„´ ì‹¬ì¸µ ë¶„ì„"""

    def __init__(self, ticker="SPY", start_date="2015-01-01", end_date="2024-12-31"):
        self.ticker = ticker
        self.start_date = start_date
        self.end_date = end_date
        self.data = None
        self.patterns = {}

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë³€ë™ì„± ê³„ì‚°"""
        print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")

        spy = yf.Ticker(self.ticker)
        df = spy.history(start=self.start_date, end=self.end_date)
        df.index = pd.to_datetime(df.index).tz_localize(None)

        # ê¸°ë³¸ ê³„ì‚°
        df['returns'] = np.log(df['Close'] / df['Close'].shift(1))
        df['volatility'] = df['returns'].rolling(20).std()
        df['volatility_5d'] = df['returns'].rolling(5).std()
        df['volatility_60d'] = df['returns'].rolling(60).std()

        # íƒ€ê²Ÿ
        df['target_vol'] = df['volatility_5d'].shift(-5)

        df = df.dropna()
        self.data = df

        print(f"âœ… ë°ì´í„°: {len(df)} ìƒ˜í”Œ")
        return True

    def analyze_autocorrelation(self):
        """ìê¸°ìƒê´€ íŒ¨í„´ ë¶„ì„"""
        print("\nğŸ” íŒ¨í„´ 1: ë³€ë™ì„± ìê¸°ìƒê´€ ë¶„ì„...")

        vol = self.data['volatility'].values

        # ë‹¤ì–‘í•œ lagì— ëŒ€í•œ ìê¸°ìƒê´€
        lags = range(1, 61)
        autocorrs = [pd.Series(vol).autocorr(lag=lag) for lag in lags]

        # ê°•í•œ ìê¸°ìƒê´€ lag ì°¾ê¸°
        strong_lags = [(lag, corr) for lag, corr in zip(lags, autocorrs) if abs(corr) > 0.3]

        print(f"   ê°•í•œ ìê¸°ìƒê´€ lag: {len(strong_lags)}ê°œ")
        for lag, corr in strong_lags[:10]:
            print(f"     Lag {lag}: {corr:.4f}")

        self.patterns['strong_autocorr_lags'] = strong_lags

        # ë°œê²¬: íŠ¹ì • lagì—ì„œ ê°•í•œ ìƒê´€ê´€ê³„ â†’ í•´ë‹¹ lag íŠ¹ì„± ì¶”ê°€ ê°€ëŠ¥
        return autocorrs

    def analyze_volatility_cycles(self):
        """ë³€ë™ì„± ì‚¬ì´í´ íŒ¨í„´ ë¶„ì„"""
        print("\nğŸ” íŒ¨í„´ 2: ë³€ë™ì„± ì‚¬ì´í´ ë¶„ì„...")

        vol = self.data['volatility'].values

        # Peak detection (ê³ ë³€ë™ ì‚¬ì´í´)
        peaks, peak_props = find_peaks(vol, distance=20, prominence=0.001)

        # Trough detection (ì €ë³€ë™ ì‚¬ì´í´)
        troughs, trough_props = find_peaks(-vol, distance=20, prominence=0.001)

        print(f"   ê³ ë³€ë™ í”¼í¬: {len(peaks)}ê°œ")
        print(f"   ì €ë³€ë™ ì €ì : {len(troughs)}ê°œ")

        # ì‚¬ì´í´ ê¸¸ì´ ê³„ì‚°
        if len(peaks) > 1:
            cycle_lengths = np.diff(peaks)
            avg_cycle = np.mean(cycle_lengths)
            print(f"   í‰ê·  ì‚¬ì´í´ ê¸¸ì´: {avg_cycle:.1f}ì¼")

            self.patterns['avg_cycle_length'] = avg_cycle

        # ë°œê²¬: ê³ ë³€ë™ â†’ ì €ë³€ë™ ì „í™˜ íŒ¨í„´ ì¡´ì¬
        return peaks, troughs

    def analyze_volatility_persistence(self):
        """ë³€ë™ì„± ì§€ì†ì„± íŒ¨í„´"""
        print("\nğŸ” íŒ¨í„´ 3: ë³€ë™ì„± ì§€ì†ì„± ë¶„ì„...")

        df = self.data.copy()

        # í˜„ì¬ ë³€ë™ì„± vs ë¯¸ë˜ ë³€ë™ì„±
        df['vol_change'] = df['target_vol'] - df['volatility_5d']

        # ë³€ë™ì„±ì´ ë†’ì„ ë•Œ vs ë‚®ì„ ë•Œ ì§€ì†ì„± ì°¨ì´
        high_vol_mask = df['volatility_5d'] > df['volatility_5d'].median()

        persistence_high = df[high_vol_mask]['vol_change'].mean()
        persistence_low = df[~high_vol_mask]['vol_change'].mean()

        print(f"   ê³ ë³€ë™ ì‹œ ë³€í™”ëŸ‰: {persistence_high:.6f}")
        print(f"   ì €ë³€ë™ ì‹œ ë³€í™”ëŸ‰: {persistence_low:.6f}")
        print(f"   ë¹„ëŒ€ì¹­ì„±: {abs(persistence_high/persistence_low):.2f}ë°°")

        self.patterns['persistence_asymmetry'] = abs(persistence_high/persistence_low)

        # ë°œê²¬: ê³ ë³€ë™ì€ ë¹ ë¥´ê²Œ í•˜ë½, ì €ë³€ë™ì€ ì²œì²œíˆ ìƒìŠ¹
        return persistence_high, persistence_low

    def analyze_return_volatility_relation(self):
        """ìˆ˜ìµë¥ -ë³€ë™ì„± ê´€ê³„ íŒ¨í„´"""
        print("\nğŸ” íŒ¨í„´ 4: ìˆ˜ìµë¥ -ë³€ë™ì„± ë¹„ì„ í˜• ê´€ê³„...")

        df = self.data.copy()

        # ëˆ„ì  ìˆ˜ìµë¥  êµ¬ê°„ë³„ ë¯¸ë˜ ë³€ë™ì„±
        df['cumulative_returns_5d'] = df['returns'].rolling(5).sum()

        # ìˆ˜ìµë¥ ì„ 10ê°œ êµ¬ê°„ìœ¼ë¡œ ë¶„í• 
        df['return_decile'] = pd.qcut(df['cumulative_returns_5d'], q=10, labels=False, duplicates='drop')

        # ê° êµ¬ê°„ë³„ í‰ê·  ë¯¸ë˜ ë³€ë™ì„±
        decile_vol = df.groupby('return_decile')['target_vol'].mean()

        print(f"   ìˆ˜ìµë¥  êµ¬ê°„ë³„ ë¯¸ë˜ ë³€ë™ì„±:")
        for decile, vol in decile_vol.items():
            print(f"     Decile {decile}: {vol:.6f}")

        # U-shape í™•ì¸ (ê·¹ë‹¨ ìˆ˜ìµë¥  â†’ ë†’ì€ ë³€ë™ì„±)
        edge_vol = (decile_vol.iloc[0] + decile_vol.iloc[-1]) / 2
        center_vol = decile_vol.iloc[4:6].mean()
        u_shape_ratio = edge_vol / center_vol

        print(f"   U-shape ë¹„ìœ¨: {u_shape_ratio:.2f}")

        self.patterns['u_shape_ratio'] = u_shape_ratio

        # ë°œê²¬: ê·¹ë‹¨ ìˆ˜ìµë¥ (ìƒìŠ¹/í•˜ë½) â†’ ë†’ì€ ë¯¸ë˜ ë³€ë™ì„±
        return decile_vol

    def analyze_volume_volatility_relation(self):
        """ê±°ë˜ëŸ‰-ë³€ë™ì„± ê´€ê³„"""
        print("\nğŸ” íŒ¨í„´ 5: ê±°ë˜ëŸ‰-ë³€ë™ì„± ê´€ê³„...")

        df = self.data.copy()

        # ê±°ë˜ëŸ‰ ì´ìƒì¹˜ (í‰ê·  ëŒ€ë¹„)
        df['volume_ratio'] = df['Volume'] / df['Volume'].rolling(20).mean()
        df['volume_spike'] = (df['volume_ratio'] > 1.5).astype(int)

        # ê±°ë˜ëŸ‰ ê¸‰ì¦ í›„ ë³€ë™ì„± ë³€í™”
        spike_data = df[df['volume_spike'] == 1]
        normal_data = df[df['volume_spike'] == 0]

        spike_future_vol = spike_data['target_vol'].mean()
        normal_future_vol = normal_data['target_vol'].mean()

        print(f"   ê±°ë˜ëŸ‰ ê¸‰ì¦ í›„ ë³€ë™ì„±: {spike_future_vol:.6f}")
        print(f"   ì •ìƒ ê±°ë˜ëŸ‰ í›„ ë³€ë™ì„±: {normal_future_vol:.6f}")
        print(f"   ë¹„ìœ¨: {spike_future_vol/normal_future_vol:.2f}ë°°")

        self.patterns['volume_spike_effect'] = spike_future_vol / normal_future_vol

        # ë°œê²¬: ê±°ë˜ëŸ‰ ê¸‰ì¦ â†’ ë¯¸ë˜ ë³€ë™ì„± ì¦ê°€
        return spike_future_vol, normal_future_vol

    def analyze_intraday_patterns(self):
        """ì¼ì¤‘ íŒ¨í„´ (High-Low spread)"""
        print("\nğŸ” íŒ¨í„´ 6: ì¼ì¤‘ ê°€ê²© ë²”ìœ„ íŒ¨í„´...")

        df = self.data.copy()

        # True Range (Wilder)
        df['high_low'] = df['High'] - df['Low']
        df['high_close'] = abs(df['High'] - df['Close'].shift(1))
        df['low_close'] = abs(df['Low'] - df['Close'].shift(1))
        df['true_range'] = df[['high_low', 'high_close', 'low_close']].max(axis=1)

        # ATR (Average True Range)
        df['atr_14'] = df['true_range'].rolling(14).mean()
        df['atr_ratio'] = df['atr_14'] / df['Close']

        # ATR vs ë¯¸ë˜ ë³€ë™ì„± ìƒê´€ê´€ê³„
        corr = df[['atr_ratio', 'target_vol']].corr().iloc[0, 1]

        print(f"   ATR vs ë¯¸ë˜ ë³€ë™ì„± ìƒê´€: {corr:.4f}")

        self.patterns['atr_correlation'] = corr

        # ë°œê²¬: ATRì€ ë¯¸ë˜ ë³€ë™ì„±ì˜ ê°•ë ¥í•œ ì„ í–‰ì§€í‘œ
        return corr

    def analyze_gap_patterns(self):
        """ê°­ íŒ¨í„´ (overnight ë³€ë™)"""
        print("\nğŸ” íŒ¨í„´ 7: Overnight Gap íŒ¨í„´...")

        df = self.data.copy()

        # Gap = Open - Previous Close
        df['gap'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1)
        df['gap_size'] = df['gap'].abs()

        # í° ê°­ ë°œìƒ í›„ ë³€ë™ì„±
        large_gap_mask = df['gap_size'] > df['gap_size'].quantile(0.9)

        large_gap_vol = df[large_gap_mask]['target_vol'].mean()
        normal_gap_vol = df[~large_gap_mask]['target_vol'].mean()

        print(f"   í° ê°­ í›„ ë³€ë™ì„±: {large_gap_vol:.6f}")
        print(f"   ì •ìƒ ê°­ í›„ ë³€ë™ì„±: {normal_gap_vol:.6f}")
        print(f"   ë¹„ìœ¨: {large_gap_vol/normal_gap_vol:.2f}ë°°")

        self.patterns['gap_effect'] = large_gap_vol / normal_gap_vol

        return large_gap_vol, normal_gap_vol

    def analyze_skewness_kurtosis(self):
        """ìˆ˜ìµë¥  ë¶„í¬ ì™œë„/ì²¨ë„ íŒ¨í„´"""
        print("\nğŸ” íŒ¨í„´ 8: ìˆ˜ìµë¥  ë¶„í¬ í˜•íƒœ íŒ¨í„´...")

        df = self.data.copy()

        # ë¡¤ë§ ì™œë„ (skewness)
        df['rolling_skew'] = df['returns'].rolling(20).skew()

        # ë¡¤ë§ ì²¨ë„ (kurtosis)
        df['rolling_kurt'] = df['returns'].rolling(20).kurt()

        # ê³ ì²¨ë„ (fat tail) â†’ ë¯¸ë˜ ë³€ë™ì„±
        high_kurt_mask = df['rolling_kurt'] > df['rolling_kurt'].quantile(0.75)

        high_kurt_vol = df[high_kurt_mask]['target_vol'].mean()
        normal_kurt_vol = df[~high_kurt_mask]['target_vol'].mean()

        print(f"   ê³ ì²¨ë„ í›„ ë³€ë™ì„±: {high_kurt_vol:.6f}")
        print(f"   ì •ìƒ ì²¨ë„ í›„ ë³€ë™ì„±: {normal_kurt_vol:.6f}")
        print(f"   ë¹„ìœ¨: {high_kurt_vol/normal_kurt_vol:.2f}ë°°")

        self.patterns['kurtosis_effect'] = high_kurt_vol / normal_kurt_vol

        # ë°œê²¬: Fat tail (ê·¹ë‹¨ ì´ë²¤íŠ¸) â†’ ë†’ì€ ë¯¸ë˜ ë³€ë™ì„±
        return high_kurt_vol, normal_kurt_vol

    def analyze_volatility_of_volatility(self):
        """ë³€ë™ì„±ì˜ ë³€ë™ì„± (vol-of-vol)"""
        print("\nğŸ” íŒ¨í„´ 9: ë³€ë™ì„±ì˜ ë³€ë™ì„±...")

        df = self.data.copy()

        # ë³€ë™ì„±ì˜ ë³€ë™ì„±
        df['vol_of_vol'] = df['volatility_5d'].rolling(20).std()

        # Vol-of-vol vs ë¯¸ë˜ ë³€ë™ì„±
        corr = df[['vol_of_vol', 'target_vol']].corr().iloc[0, 1]

        print(f"   Vol-of-vol vs ë¯¸ë˜ ë³€ë™ì„± ìƒê´€: {corr:.4f}")

        # ë†’ì€ vol-of-vol â†’ ë¶ˆì•ˆì • ì‹œì¥
        high_vov_mask = df['vol_of_vol'] > df['vol_of_vol'].median()

        high_vov_vol = df[high_vov_mask]['target_vol'].mean()
        low_vov_vol = df[~high_vov_mask]['target_vol'].mean()

        print(f"   ë†’ì€ vol-of-vol í›„: {high_vov_vol:.6f}")
        print(f"   ë‚®ì€ vol-of-vol í›„: {low_vov_vol:.6f}")
        print(f"   ë¹„ìœ¨: {high_vov_vol/low_vov_vol:.2f}ë°°")

        self.patterns['vol_of_vol_effect'] = high_vov_vol / low_vov_vol

        return corr

    def analyze_momentum_volatility(self):
        """ëª¨ë©˜í…€-ë³€ë™ì„± ê´€ê³„"""
        print("\nğŸ” íŒ¨í„´ 10: ê°€ê²© ëª¨ë©˜í…€ê³¼ ë³€ë™ì„±...")

        df = self.data.copy()

        # ê°€ê²© ëª¨ë©˜í…€ (20ì¼)
        df['momentum_20'] = (df['Close'] / df['Close'].shift(20) - 1)

        # ê°•í•œ ëª¨ë©˜í…€ vs ì•½í•œ ëª¨ë©˜í…€
        strong_momentum_mask = df['momentum_20'].abs() > df['momentum_20'].abs().quantile(0.75)

        strong_mom_vol = df[strong_momentum_mask]['target_vol'].mean()
        weak_mom_vol = df[~strong_momentum_mask]['target_vol'].mean()

        print(f"   ê°•í•œ ëª¨ë©˜í…€ í›„ ë³€ë™ì„±: {strong_mom_vol:.6f}")
        print(f"   ì•½í•œ ëª¨ë©˜í…€ í›„ ë³€ë™ì„±: {weak_mom_vol:.6f}")
        print(f"   ë¹„ìœ¨: {strong_mom_vol/weak_mom_vol:.2f}ë°°")

        self.patterns['momentum_effect'] = strong_mom_vol / weak_mom_vol

        return strong_mom_vol, weak_mom_vol

    def summarize_patterns(self):
        """ë°œê²¬ëœ íŒ¨í„´ ìš”ì•½"""
        print("\n" + "="*60)
        print("ğŸ“Š ë°œê²¬ëœ í•µì‹¬ íŒ¨í„´ ìš”ì•½")
        print("="*60)

        # íš¨ê³¼ í¬ê¸° ìˆœ ì •ë ¬
        effects = {
            'Volume Spike Effect': self.patterns.get('volume_spike_effect', 1.0),
            'Gap Effect': self.patterns.get('gap_effect', 1.0),
            'Kurtosis Effect': self.patterns.get('kurtosis_effect', 1.0),
            'Vol-of-Vol Effect': self.patterns.get('vol_of_vol_effect', 1.0),
            'Momentum Effect': self.patterns.get('momentum_effect', 1.0),
            'U-Shape Ratio': self.patterns.get('u_shape_ratio', 1.0),
            'Persistence Asymmetry': self.patterns.get('persistence_asymmetry', 1.0),
        }

        sorted_effects = sorted(effects.items(), key=lambda x: abs(x[1]-1.0), reverse=True)

        print("\níš¨ê³¼ í¬ê¸° ìˆœìœ„:")
        for i, (name, effect) in enumerate(sorted_effects, 1):
            deviation = abs(effect - 1.0) * 100
            print(f"{i}. {name:25s}: {effect:.2f}ë°° (í¸ì°¨ {deviation:.1f}%)")

        # ìƒˆë¡œìš´ íŠ¹ì„± ì œì•ˆ
        print("\n" + "="*60)
        print("ğŸ’¡ ìƒˆë¡œìš´ íŠ¹ì„± ì œì•ˆ")
        print("="*60)

        new_features = [
            "1. ATR (Average True Range) ê¸°ë°˜ íŠ¹ì„±",
            "2. Volume spike ì§€í‘œ",
            "3. Overnight gap size",
            "4. Rolling skewness/kurtosis",
            "5. Volatility-of-volatility",
            "6. ìˆ˜ìµë¥  ê·¹ë‹¨ê°’ ì¹´ìš´íŠ¸",
            "7. ê°€ê²© ëª¨ë©˜í…€ ê°•ë„",
            "8. íŠ¹ì • lag ìê¸°ìƒê´€ íŠ¹ì„±",
            "9. ë³€ë™ì„± ì‚¬ì´í´ ìœ„ì¹˜",
            "10. U-shape ìˆ˜ìµë¥  ë³€í™˜"
        ]

        for feature in new_features:
            print(f"   {feature}")

        return sorted_effects

    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("="*60)
        print("ğŸ”¬ ë³€ë™ì„± íŒ¨í„´ ì‹¬ì¸µ ë¶„ì„")
        print("="*60)

        self.load_data()

        # 10ê°€ì§€ íŒ¨í„´ ë¶„ì„
        self.analyze_autocorrelation()
        self.analyze_volatility_cycles()
        self.analyze_volatility_persistence()
        self.analyze_return_volatility_relation()
        self.analyze_volume_volatility_relation()
        self.analyze_intraday_patterns()
        self.analyze_gap_patterns()
        self.analyze_skewness_kurtosis()
        self.analyze_volatility_of_volatility()
        self.analyze_momentum_volatility()

        # ìš”ì•½
        sorted_effects = self.summarize_patterns()

        print("\n" + "="*60)
        print("âœ… íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
        print("="*60)

        return sorted_effects

if __name__ == "__main__":
    analyzer = VolatilityPatternDiscovery()
    analyzer.run_analysis()
