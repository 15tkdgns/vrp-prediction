#!/usr/bin/env python3
"""
SHAP (SHapley Additive exPlanations) ë¶„ì„ ëª¨ë“ˆ
ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê°œë³„ íŠ¹ì„± ê¸°ì—¬ë„ë¡œ ë¶„í•´í•˜ì—¬ í•´ì„ì„± í–¥ìƒ
"""

import pandas as pd
import numpy as np
import shap
import json
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import logging
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SHAPAnalyzer:
    """SHAPë¥¼ ì´ìš©í•œ ëª¨ë¸ í•´ì„ì„± ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, data_file: str = "data/raw/integrated_spy_news_data.csv", 
                 results_dir: str = "results"):
        self.data_file = data_file
        self.results_dir = results_dir
        self.data = None
        self.models = {}
        self.shap_values = {}
        self.explainers = {}
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(results_dir, exist_ok=True)
        
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        logger.info(f"Loading data from {self.data_file}")
        self.data = pd.read_csv(self.data_file)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
        if 'date' in self.data.columns:
            self.data['date'] = pd.to_datetime(self.data['date'])
            self.data = self.data.sort_values('date')
            
        # NaN ê°’ ì œê±°
        self.data = self.data.dropna()
        
        logger.info(f"Data loaded: {len(self.data)} samples, {self.data.shape[1]} features")
        
    def load_trained_models(self, model_file: str = "data/models/model_comparison_models.pkl"):
        """í›ˆë ¨ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        try:
            with open(model_file, 'rb') as f:
                saved_data = pickle.load(f)
                self.models = saved_data.get('models', {})
                logger.info(f"Loaded {len(self.models)} trained models")
        except FileNotFoundError:
            logger.warning(f"Model file not found: {model_file}")
            logger.info("Will train models from scratch if needed")
            
    def create_explainers(self, model_name: str, model_data: Dict):
        """ëª¨ë¸ë³„ SHAP Explainer ìƒì„±"""
        model = model_data['model']
        features = model_data['features']
        X = self.data[features]
        
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ Explainer ì„ íƒ
        if 'RandomForest' in model_name or 'GradientBoosting' in model_name or 'XGBoost' in model_name:
            explainer = shap.TreeExplainer(model)
            logger.info(f"Created TreeExplainer for {model_name}")
        elif 'LogisticRegression' in model_name:
            # ë°°ê²½ ë°ì´í„° ìƒ˜í”Œë§ (ë„ˆë¬´ í¬ë©´ ë©”ëª¨ë¦¬ ë¬¸ì œ)
            background = shap.sample(X, min(100, len(X)))
            explainer = shap.LinearExplainer(model, background)
            logger.info(f"Created LinearExplainer for {model_name}")
        else:
            # ì¼ë°˜ì ì¸ ê²½ìš° KernelExplainer ì‚¬ìš© (ëŠë¦¬ì§€ë§Œ ë²”ìš©)
            background = shap.sample(X, min(50, len(X)))
            explainer = shap.KernelExplainer(model.predict_proba, background)
            logger.info(f"Created KernelExplainer for {model_name}")
            
        return explainer, X
        
    def calculate_shap_values(self, model_name: str, sample_size: Optional[int] = None):
        """íŠ¹ì • ëª¨ë¸ì˜ SHAP ê°’ ê³„ì‚°"""
        if model_name not in self.models:
            logger.error(f"Model {model_name} not found")
            return None
            
        logger.info(f"Calculating SHAP values for {model_name}")
        
        model_data = self.models[model_name]
        explainer, X = self.create_explainers(model_name, model_data)
        
        # ìƒ˜í”Œ í¬ê¸° ì¡°ì • (ê³„ì‚° ì‹œê°„ ë‹¨ì¶•)
        if sample_size and len(X) > sample_size:
            X_sample = X.sample(n=sample_size, random_state=42)
            logger.info(f"Using sample size: {sample_size}")
        else:
            X_sample = X
            
        # SHAP ê°’ ê³„ì‚°
        shap_values = explainer.shap_values(X_sample)
        
        # ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš° ì–‘ì„± í´ë˜ìŠ¤ì˜ SHAP ê°’ë§Œ ì‚¬ìš©
        if isinstance(shap_values, list) and len(shap_values) == 2:
            shap_values = shap_values[1]  # ìƒìŠ¹(1) í´ë˜ìŠ¤
            
        self.shap_values[model_name] = {
            'values': shap_values,
            'data': X_sample,
            'features': model_data['features'],
            'explainer': explainer
        }
        
        logger.info(f"SHAP values calculated for {model_name}: {shap_values.shape}")
        return shap_values
        
    def generate_summary_plot(self, model_name: str, max_display: int = 15):
        """SHAP Summary Plot ìƒì„±"""
        if model_name not in self.shap_values:
            logger.error(f"SHAP values not found for {model_name}")
            return None
            
        shap_data = self.shap_values[model_name]
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(
            shap_data['values'], 
            shap_data['data'], 
            feature_names=shap_data['features'],
            max_display=max_display,
            show=False
        )
        plt.title(f'SHAP Summary Plot - {model_name}', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # í•œê¸€ íŠ¹ì„±ëª…ìœ¼ë¡œ ë³€í™˜
        feature_names_kr = self._translate_feature_names(shap_data['features'])
        
        # íŒŒì¼ ì €ì¥
        plot_file = os.path.join(self.results_dir, f'shap_summary_{model_name.lower()}.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"SHAP summary plot saved: {plot_file}")
        return plot_file
        
    def generate_waterfall_plot(self, model_name: str, sample_idx: int = 0):
        """íŠ¹ì • ìƒ˜í”Œì˜ SHAP Waterfall Plot ìƒì„±"""
        if model_name not in self.shap_values:
            logger.error(f"SHAP values not found for {model_name}")
            return None
            
        shap_data = self.shap_values[model_name]
        
        # SHAP Explanation ê°ì²´ ìƒì„±
        explanation = shap.Explanation(
            values=shap_data['values'][sample_idx],
            base_values=shap_data['explainer'].expected_value if hasattr(shap_data['explainer'], 'expected_value') else 0,
            data=shap_data['data'].iloc[sample_idx].values,
            feature_names=shap_data['features']
        )
        
        plt.figure(figsize=(12, 8))
        shap.waterfall_plot(explanation, max_display=15, show=False)
        plt.title(f'SHAP Waterfall Plot - {model_name} (Sample {sample_idx})', 
                  fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        plot_file = os.path.join(self.results_dir, f'shap_waterfall_{model_name.lower()}_sample_{sample_idx}.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"SHAP waterfall plot saved: {plot_file}")
        return plot_file
        
    def calculate_feature_importance(self, model_name: str) -> Dict:
        """SHAP ê°’ ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°"""
        if model_name not in self.shap_values:
            logger.error(f"SHAP values not found for {model_name}")
            return {}
            
        shap_data = self.shap_values[model_name]
        shap_values = shap_data['values']
        features = shap_data['features']
        
        # ì ˆëŒ“ê°’ í‰ê· ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
        importance_scores = np.abs(shap_values).mean(0)
        
        # íŠ¹ì„±ëª…ê³¼ ì¤‘ìš”ë„ë¥¼ ìŒìœ¼ë¡œ ë§Œë“¤ì–´ ì •ë ¬
        feature_importance = list(zip(features, importance_scores))
        feature_importance.sort(key=lambda x: x[1], reverse=True)
        
        # ë‰´ìŠ¤ vs ê¸°ìˆ ì  ì§€í‘œ ë¶„ë¥˜
        news_keywords = ['sentiment', 'news', 'article', 'impact']
        news_features = []
        technical_features = []
        
        for feature, importance in feature_importance:
            feature_data = {
                'feature': feature,
                'feature_kr': self._translate_feature_name(feature),
                'importance': float(importance),
                'importance_pct': float(importance / sum(importance_scores) * 100)
            }
            
            if any(keyword in feature.lower() for keyword in news_keywords):
                news_features.append(feature_data)
            else:
                technical_features.append(feature_data)
                
        return {
            'total_features': len(features),
            'top_features': [
                {
                    'feature': f,
                    'feature_kr': self._translate_feature_name(f),
                    'importance': float(imp),
                    'importance_pct': float(imp / sum(importance_scores) * 100)
                }
                for f, imp in feature_importance[:10]
            ],
            'news_features': news_features,
            'technical_features': technical_features,
            'news_contribution_pct': sum([f['importance'] for f in news_features]) / sum(importance_scores) * 100,
            'technical_contribution_pct': sum([f['importance'] for f in technical_features]) / sum(importance_scores) * 100
        }
        
    def analyze_feature_interactions(self, model_name: str, feature1: str, feature2: str):
        """ë‘ íŠ¹ì„± ê°„ì˜ SHAP ìƒí˜¸ì‘ìš© ë¶„ì„"""
        if model_name not in self.shap_values:
            logger.error(f"SHAP values not found for {model_name}")
            return None
            
        shap_data = self.shap_values[model_name]
        
        if feature1 not in shap_data['features'] or feature2 not in shap_data['features']:
            logger.error(f"Features not found: {feature1}, {feature2}")
            return None
            
        plt.figure(figsize=(10, 6))
        shap.dependence_plot(
            feature1, 
            shap_data['values'], 
            shap_data['data'],
            feature_names=shap_data['features'],
            interaction_index=feature2,
            show=False
        )
        plt.title(f'SHAP Dependence Plot: {feature1} vs {feature2}', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        plot_file = os.path.join(self.results_dir, f'shap_interaction_{model_name.lower()}_{feature1}_{feature2}.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"SHAP interaction plot saved: {plot_file}")
        return plot_file
        
    def generate_daily_explanations(self, model_name: str, start_date: str = None, end_date: str = None) -> List[Dict]:
        """ì¼ë³„ ì˜ˆì¸¡ ì„¤ëª… ìƒì„±"""
        if model_name not in self.shap_values:
            logger.error(f"SHAP values not found for {model_name}")
            return []
            
        shap_data = self.shap_values[model_name]
        data = shap_data['data'].copy()
        
        # ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° í•„í„°ë§
        if 'date' in self.data.columns and start_date:
            date_filter = (self.data['date'] >= start_date)
            if end_date:
                date_filter = date_filter & (self.data['date'] <= end_date)
            
            filtered_indices = self.data[date_filter].index
            # SHAP ë°ì´í„°ì—ì„œ í•´ë‹¹ ì¸ë±ìŠ¤ë§Œ ì„ íƒ
            common_indices = data.index.intersection(filtered_indices)
            data = data.loc[common_indices]
            
        explanations = []
        
        for idx in range(min(20, len(data))):  # ìµœëŒ€ 20ì¼ë§Œ ë¶„ì„
            row_idx = data.index[idx]
            shap_row = shap_data['values'][idx] if idx < len(shap_data['values']) else None
            
            if shap_row is None:
                continue
                
            # ìƒìœ„ ê¸°ì—¬ë„ íŠ¹ì„±ë“¤
            abs_shap = np.abs(shap_row)
            top_indices = np.argsort(abs_shap)[-5:][::-1]  # ìƒìœ„ 5ê°œ
            
            top_features = []
            for i in top_indices:
                feature = shap_data['features'][i]
                contribution = float(shap_row[i])
                value = float(data.iloc[idx, i])
                
                top_features.append({
                    'feature': feature,
                    'feature_kr': self._translate_feature_name(feature),
                    'contribution': contribution,
                    'value': value,
                    'impact': 'positive' if contribution > 0 else 'negative'
                })
                
            date_str = self.data.loc[row_idx, 'date'].strftime('%Y-%m-%d') if 'date' in self.data.columns else f"Sample_{idx}"
            
            explanations.append({
                'date': date_str,
                'sample_idx': idx,
                'total_shap': float(np.sum(shap_row)),
                'top_features': top_features
            })
            
        return explanations
        
    def _translate_feature_name(self, feature: str) -> str:
        """íŠ¹ì„±ëª…ì„ í•œê¸€ë¡œ ë²ˆì—­"""
        translations = {
            'sentiment_change': 'ê°ì • ë³€í™”ìœ¨',
            'sentiment_ma_7': '7ì¼ í‰ê·  ê°ì •',
            'news_count_change': 'ë‰´ìŠ¤ ìˆ˜ ë³€í™”',
            'sentiment_abs': 'ê°ì • ê°•ë„',
            'sentiment_volatility': 'ê°ì • ë³€ë™ì„±',
            'price_to_ma20': '20ì¼ì„  ëŒ€ë¹„ ê°€ê²©',
            'ma_10': '10ì¼ ì´ë™í‰ê· ',
            'volatility_20': '20ì¼ ë³€ë™ì„±',
            'price_change_abs': 'ì ˆëŒ€ ê°€ê²© ë³€í™”',
            'price_to_ma5': '5ì¼ì„  ëŒ€ë¹„ ê°€ê²©',
            'ma_5': '5ì¼ ì´ë™í‰ê· ',
            'ma_20': '20ì¼ ì´ë™í‰ê· ',
            'rsi': 'RSI ì§€í‘œ',
            'macd': 'MACD',
            'volume_change': 'ê±°ë˜ëŸ‰ ë³€í™”',
            'unusual_volume': 'ë¹„ì •ìƒ ê±°ë˜ëŸ‰',
            'price_change': 'ê°€ê²© ë³€í™”ìœ¨',
            'volatility_5': '5ì¼ ë³€ë™ì„±'
        }
        return translations.get(feature, feature)
        
    def _translate_feature_names(self, features: List[str]) -> List[str]:
        """íŠ¹ì„±ëª… ë¦¬ìŠ¤íŠ¸ë¥¼ í•œê¸€ë¡œ ë²ˆì—­"""
        return [self._translate_feature_name(f) for f in features]
        
    def save_results(self, model_name: str, output_file: str = None):
        """SHAP ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        if model_name not in self.shap_values:
            logger.error(f"SHAP values not found for {model_name}")
            return None
            
        if not output_file:
            output_file = os.path.join(self.results_dir, f'shap_analysis_{model_name.lower()}.json')
            
        # íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
        feature_importance = self.calculate_feature_importance(model_name)
        
        # ì¼ë³„ ì„¤ëª… ìƒì„±
        daily_explanations = self.generate_daily_explanations(model_name)
        
        # ê²°ê³¼ í†µí•©
        results = {
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'feature_importance': feature_importance,
            'daily_explanations': daily_explanations[:10],  # ìƒìœ„ 10ì¼ë§Œ
            'summary': {
                'total_samples': len(self.shap_values[model_name]['data']),
                'total_features': len(self.shap_values[model_name]['features']),
                'news_features_count': len(feature_importance['news_features']),
                'technical_features_count': len(feature_importance['technical_features'])
            }
        }
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        results = self._convert_to_json_serializable(results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        logger.info(f"SHAP analysis results saved: {output_file}")
        return output_file
        
    def _convert_to_json_serializable(self, obj):
        """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(v) for v in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32, np.int_)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32, np.float_)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj
            
    def run_complete_analysis(self, model_names: List[str] = None, sample_size: int = 200):
        """ì „ì²´ SHAP ë¶„ì„ ì‹¤í–‰"""
        logger.info("Starting complete SHAP analysis")
        
        # ë°ì´í„° ë¡œë“œ
        self.load_data()
        
        # ëª¨ë¸ ë¡œë“œ (ì—†ìœ¼ë©´ ìƒˆë¡œ í•™ìŠµ)
        self.load_trained_models()
        
        if not self.models:
            logger.info("No pre-trained models found, training new models...")
            self._train_models_for_analysis()
            
        # ë¶„ì„í•  ëª¨ë¸ ì„ íƒ
        if not model_names:
            model_names = list(self.models.keys())
            
        logger.info(f"Available models: {list(self.models.keys())}")
        logger.info(f"Target models: {model_names}")
            
        results_files = []
        
        for model_name in model_names:
            try:
                logger.info(f"Analyzing model: {model_name}")
                
                # SHAP ê°’ ê³„ì‚°
                self.calculate_shap_values(model_name, sample_size=sample_size)
                
                # ì‹œê°í™” ìƒì„±
                self.generate_summary_plot(model_name)
                self.generate_waterfall_plot(model_name, sample_idx=0)
                
                # ê²°ê³¼ ì €ì¥
                result_file = self.save_results(model_name)
                results_files.append(result_file)
                
                logger.info(f"Analysis completed for {model_name}")
                
            except Exception as e:
                logger.error(f"Error analyzing {model_name}: {str(e)}")
                continue
                
        logger.info(f"SHAP analysis completed. Results saved: {results_files}")
        return results_files
        
    def _train_models_for_analysis(self):
        """ë¶„ì„ìš© ëª¨ë¸ í•™ìŠµ (ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°)"""
        logger.info("Training models for SHAP analysis...")
        
        # ëª¨ë¸ ë¹„êµ ëª¨ë“ˆ ì„í¬íŠ¸
        try:
            import sys
            sys.path.append('/root/workspace/src/utils')
            from model_comparison import ModelComparison
            
            # ëª¨ë¸ í•™ìŠµ
            comparator = ModelComparison(self.data_file)
            comparator.load_data()
            X_train, X_test, y_train, y_test = comparator.prepare_data()
            comparator.train_models()  # ëª¨ë¸ í›ˆë ¨ (ë°˜í™˜ê°’ ì—†ìŒ)
            
            # Enhanced ëª¨ë¸ë“¤ë§Œ ì‚¬ìš©
            self.models = comparator.enhanced_models
            
            logger.info(f"Trained {len(self.models)} models for analysis")
            
        except ImportError as e:
            logger.error(f"Could not import ModelComparison: {e}")
            logger.error("Please run model comparison first")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = SHAPAnalyzer()
    
    # ì£¼ìš” ëª¨ë¸ë“¤ì— ëŒ€í•´ SHAP ë¶„ì„ ì‹¤í–‰ (ModelComparisonì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤ëª…)
    target_models = [
        'GradientBoosting',
        'RandomForest', 
        'LogisticRegression'
    ]
    
    results = analyzer.run_complete_analysis(
        model_names=target_models,
        sample_size=150  # ê³„ì‚° ì‹œê°„ ë‹¨ì¶•
    )
    
    print(f"âœ… SHAP ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {results}")
    

if __name__ == "__main__":
    main()