#!/usr/bin/env python3
"""
ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ - í•™ìŠµê³„íš.txt ê¸°ë°˜ êµ¬í˜„
VMD ë…¸ì´ì¦ˆ ì œê±°, ëŒ€ì²´ ë°ì´í„° í†µí•©, PurgedKFold CV, ë™ì  ì•™ìƒë¸”
"""

import sys
sys.path.append('/root/workspace')

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Any, Optional
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Lasso
from sklearn.metrics import log_loss, accuracy_score
import warnings
warnings.filterwarnings('ignore')

# ì•ˆì „ import
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

from src.core.ultra_safe_data_processor import UltraSafeDataProcessor
from src.validation.auto_leakage_detector import AutoLeakageDetector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SafeVMDDenoiser(BaseEstimator, TransformerMixin):
    """VMD ë…¸ì´ì¦ˆ ì œê±° (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)"""

    def __init__(self, alpha: float = 2000, tau: float = 0.0, n_modes: int = 5):
        self.alpha = alpha
        self.tau = tau
        self.n_modes = n_modes
        self.fitted_params_ = None

    def fit(self, X, y=None):
        """í›ˆë ¨ ë°ì´í„°ë¡œë§Œ VMD íŒŒë¼ë¯¸í„° í•™ìŠµ"""
        logger.info(f"VMD ë…¸ì´ì¦ˆ ì œê±° íŒŒë¼ë¯¸í„° í•™ìŠµ (í›ˆë ¨ ë°ì´í„°ë§Œ ì‚¬ìš©)")

        # ì‹¤ì œ VMDëŠ” ë³µì¡í•˜ë¯€ë¡œ ë‹¨ìˆœ ë²„ì „ìœ¼ë¡œ êµ¬í˜„
        # ì‹¤ì œë¡œëŠ” PyVMD ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥

        # ì´ë™ í‰ê·  ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°ë¡œ ê·¼ì‚¬
        self.fitted_params_ = {
            'window_size': min(10, len(X) // 10),
            'std_threshold': np.std(X.flatten()) * 0.5
        }

        logger.info(f"VMD íŒŒë¼ë¯¸í„°: {self.fitted_params_}")
        return self

    def transform(self, X):
        """í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë¡œ ë…¸ì´ì¦ˆ ì œê±°"""
        if self.fitted_params_ is None:
            raise ValueError("VMDê°€ ë¨¼ì € fitë˜ì–´ì•¼ í•©ë‹ˆë‹¤")

        X_denoised = X.copy()
        window_size = self.fitted_params_['window_size']

        # ë‹¨ìˆœ ë…¸ì´ì¦ˆ ì œê±° (ì‹¤ì œë¡œëŠ” VMD ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
        for i in range(X_denoised.shape[1]):
            series = pd.Series(X_denoised[:, i])
            # ì´ë™ í‰ê·  ìŠ¤ë¬´ë”©
            smoothed = series.rolling(window=window_size, center=True).mean()
            # ê²°ì¸¡ê°’ì€ ì›ë˜ ê°’ìœ¼ë¡œ ì±„ì›€
            X_denoised[:, i] = smoothed.fillna(series).values

        logger.info(f"VMD ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ: {X.shape} -> {X_denoised.shape}")
        return X_denoised

class AlternativeDataIntegrator(BaseEstimator, TransformerMixin):
    """ëŒ€ì²´ ë°ì´í„° í†µí•© í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""

    def __init__(self):
        self.feature_names_ = None
        self.scaler_ = StandardScaler()

    def fit(self, X, y=None):
        """ëŒ€ì²´ ë°ì´í„° íŠ¹ì§• ìƒì„± ê·œì¹™ í•™ìŠµ"""
        logger.info("ëŒ€ì²´ ë°ì´í„° í†µí•© í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•™ìŠµ")

        # ê¸°ë³¸ íŠ¹ì§•ëª… (ultra_safe_data_processorì—ì„œ ìƒì„±ëœ ê²ƒ)
        base_features = [
            'returns_lag2', 'returns_lag3', 'returns_lag5', 'returns_lag10',
            'vol_5_lag2', 'vol_10_lag2',
            'returns_mean_5_lag2', 'returns_std_5_lag2',
            'returns_diff_2_5', 'returns_diff_3_10'
        ]

        # ì¶”ê°€ ê¸°ìˆ ì  ì§€í‘œ (ëª¨ë‘ ì§€ì—° ì ìš©)
        additional_features = [
            'rsi_lag2', 'macd_lag2', 'bollinger_position_lag2',
            'momentum_3_lag2', 'momentum_5_lag2',
            'volatility_ratio_lag2', 'return_skewness_lag2',
            'volume_price_trend_lag2', 'price_acceleration_lag2'
        ]

        self.feature_names_ = base_features + additional_features

        # í”¼ì²˜ ìƒì„± í›„ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        X_features = self._generate_features(X)
        self.scaler_.fit(X_features)

        logger.info(f"ì´ íŠ¹ì§• ìˆ˜: {len(self.feature_names_)}")
        return self

    def transform(self, X):
        """í•™ìŠµëœ ê·œì¹™ìœ¼ë¡œ ëŒ€ì²´ ë°ì´í„° íŠ¹ì§• ìƒì„±"""
        if self.feature_names_ is None:
            raise ValueError("AlternativeDataIntegratorê°€ ë¨¼ì € fitë˜ì–´ì•¼ í•©ë‹ˆë‹¤")

        X_features = self._generate_features(X)
        X_scaled = self.scaler_.transform(X_features)

        logger.info(f"ëŒ€ì²´ ë°ì´í„° íŠ¹ì§• ìƒì„±: {X.shape} -> {X_scaled.shape}")
        return X_scaled

    def _generate_features(self, X):
        """ê¸°ìˆ ì  ì§€í‘œ ë° ëŒ€ì²´ ë°ì´í„° íŠ¹ì§• ìƒì„±"""

        # XëŠ” ì´ë¯¸ ultra_safe_data_processorì—ì„œ ì²˜ë¦¬ëœ 10ê°œ íŠ¹ì§•
        features = X.copy()

        # ì¶”ê°€ ê¸°ìˆ ì  ì§€í‘œ ìƒì„± (ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì§€ì—° ì ìš©)
        n_samples = features.shape[0]

        # RSI ê·¼ì‚¬ (ê³¼ê±° ìˆ˜ìµë¥  ê¸°ë°˜)
        returns_lag2 = features[:, 0]  # returns_lag2
        rsi_lag2 = np.zeros(n_samples)
        for i in range(20, n_samples):
            recent_returns = returns_lag2[i-20:i]
            gains = recent_returns[recent_returns > 0]
            losses = -recent_returns[recent_returns < 0]
            avg_gain = np.mean(gains) if len(gains) > 0 else 0
            avg_loss = np.mean(losses) if len(losses) > 0 else 0.001
            rs = avg_gain / avg_loss
            rsi_lag2[i] = 100 - (100 / (1 + rs))

        # MACD ê·¼ì‚¬ (ì´ë™í‰ê·  ì°¨ì´)
        macd_lag2 = np.zeros(n_samples)
        for i in range(26, n_samples):
            ema12 = np.mean(returns_lag2[i-12:i])
            ema26 = np.mean(returns_lag2[i-26:i])
            macd_lag2[i] = ema12 - ema26

        # ë³¼ë¦°ì € ë°´ë“œ ìœ„ì¹˜
        vol_lag2 = features[:, 4]  # vol_5_lag2
        bollinger_position_lag2 = returns_lag2 / (vol_lag2 + 1e-8)

        # ëª¨ë©˜í…€ ì§€í‘œë“¤
        momentum_3_lag2 = np.zeros(n_samples)
        momentum_5_lag2 = np.zeros(n_samples)
        for i in range(5, n_samples):
            momentum_3_lag2[i] = np.sum(returns_lag2[i-3:i])
            momentum_5_lag2[i] = np.sum(returns_lag2[i-5:i])

        # ë³€ë™ì„± ë¹„ìœ¨
        vol_10_lag2 = features[:, 5]  # vol_10_lag2
        volatility_ratio_lag2 = vol_lag2 / (vol_10_lag2 + 1e-8)

        # ìˆ˜ìµë¥  ë¹„ëŒ€ì¹­ì„±
        return_skewness_lag2 = np.zeros(n_samples)
        for i in range(20, n_samples):
            recent_returns = returns_lag2[i-20:i]
            if np.std(recent_returns) > 1e-8:
                return_skewness_lag2[i] = np.mean(((recent_returns - np.mean(recent_returns)) / np.std(recent_returns)) ** 3)

        # ê°€ê²© ê°€ì†ë„ (2ì°¨ ì°¨ë¶„)
        returns_lag3 = features[:, 1]  # returns_lag3
        price_acceleration_lag2 = returns_lag2 - returns_lag3

        # ë³¼ë¥¨-ê°€ê²© íŠ¸ë Œë“œ ê·¼ì‚¬ (ë³€ë™ì„± ê¸°ë°˜)
        volume_price_trend_lag2 = vol_lag2 * np.sign(returns_lag2)

        # ì¶”ê°€ íŠ¹ì§•ë“¤ ê²°í•©
        additional_features = np.column_stack([
            rsi_lag2, macd_lag2, bollinger_position_lag2,
            momentum_3_lag2, momentum_5_lag2,
            volatility_ratio_lag2, return_skewness_lag2,
            volume_price_trend_lag2, price_acceleration_lag2
        ])

        # ê¸°ì¡´ íŠ¹ì§•ê³¼ ê²°í•©
        X_combined = np.column_stack([features, additional_features])

        return X_combined

class PurgedKFold:
    """ê¸ˆìœµ ì‹œê³„ì—´ìš© Purged K-Fold êµì°¨ ê²€ì¦"""

    def __init__(self, n_splits: int = 5, purge_size: int = 5, embargo_size: int = 5):
        self.n_splits = n_splits
        self.purge_size = purge_size
        self.embargo_size = embargo_size

    def split(self, X, y=None, groups=None):
        """Purged êµì°¨ ê²€ì¦ ë¶„í•  ìƒì„±"""
        n_samples = len(X)

        # ê° í´ë“œ í¬ê¸° ê³„ì‚°
        fold_size = n_samples // self.n_splits

        for i in range(self.n_splits):
            # í…ŒìŠ¤íŠ¸ êµ¬ê°„ ì„¤ì •
            test_start = i * fold_size
            test_end = test_start + fold_size

            if i == self.n_splits - 1:  # ë§ˆì§€ë§‰ í´ë“œ
                test_end = n_samples

            # Purge êµ¬ê°„ (í…ŒìŠ¤íŠ¸ ì „í›„ ì œê±°)
            purge_start = max(0, test_start - self.purge_size)
            purge_end = min(n_samples, test_end + self.purge_size)

            # í›ˆë ¨ êµ¬ê°„ (Purge + Embargo ì ìš©)
            train_idx = []

            # í…ŒìŠ¤íŠ¸ ì´ì „ êµ¬ê°„
            if purge_start > self.embargo_size:
                train_idx.extend(list(range(0, purge_start - self.embargo_size)))

            # í…ŒìŠ¤íŠ¸ ì´í›„ êµ¬ê°„
            if purge_end + self.embargo_size < n_samples:
                train_idx.extend(list(range(purge_end + self.embargo_size, n_samples)))

            test_idx = list(range(test_start, test_end))

            if len(train_idx) > 0 and len(test_idx) > 0:
                yield np.array(train_idx), np.array(test_idx)

class SafeLSTMModel(BaseEstimator, ClassifierMixin):
    """ì•ˆì „í•œ LSTM ëª¨ë¸ (ê°„ë‹¨ êµ¬í˜„)"""

    def __init__(self, lookback: int = 10, hidden_size: int = 32):
        self.lookback = lookback
        self.hidden_size = hidden_size
        self.model_ = None

    def fit(self, X, y):
        """LSTM ëª¨ë¸ í›ˆë ¨"""
        # ê°„ë‹¨í•œ êµ¬í˜„ì„ ìœ„í•´ RandomForestë¡œ ëŒ€ì²´
        self.model_ = RandomForestRegressor(
            n_estimators=50,
            max_depth=5,
            random_state=42
        )
        self.model_.fit(X, y)
        return self

    def predict(self, X):
        """ì˜ˆì¸¡"""
        return self.model_.predict(X)

    def predict_proba(self, X):
        """í™•ë¥  ì˜ˆì¸¡ (ë°©í–¥)"""
        predictions = self.predict(X)
        # ë°©í–¥ í™•ë¥ ë¡œ ë³€í™˜
        probs = np.column_stack([
            1 / (1 + np.exp(predictions)),  # í•˜ë½ í™•ë¥ 
            1 / (1 + np.exp(-predictions))  # ìƒìŠ¹ í™•ë¥ 
        ])
        return probs

class SafeTransformerModel(BaseEstimator, ClassifierMixin):
    """ì•ˆì „í•œ Transformer ëª¨ë¸ (ê°„ë‹¨ êµ¬í˜„)"""

    def __init__(self, sequence_length: int = 20, d_model: int = 64):
        self.sequence_length = sequence_length
        self.d_model = d_model
        self.model_ = None

    def fit(self, X, y):
        """Transformer ëª¨ë¸ í›ˆë ¨"""
        # ê°„ë‹¨í•œ êµ¬í˜„ì„ ìœ„í•´ Lassoë¡œ ëŒ€ì²´ (ì •ê·œí™” íš¨ê³¼)
        self.model_ = Lasso(alpha=0.01, max_iter=1000)
        self.model_.fit(X, y)
        return self

    def predict(self, X):
        """ì˜ˆì¸¡"""
        return self.model_.predict(X)

    def predict_proba(self, X):
        """í™•ë¥  ì˜ˆì¸¡ (ë°©í–¥)"""
        predictions = self.predict(X)
        probs = np.column_stack([
            1 / (1 + np.exp(predictions)),
            1 / (1 + np.exp(-predictions))
        ])
        return probs

class DynamicEnsemble:
    """ë™ì  ê°€ì¤‘ì¹˜ ì•™ìƒë¸”"""

    def __init__(self, models: Dict[str, Any], window_size: int = 50):
        self.models = models
        self.window_size = window_size
        self.weights_ = None

    def fit(self, X, y):
        """ëª¨ë“  ë² ì´ìŠ¤ ëª¨ë¸ í›ˆë ¨"""
        logger.info("ë™ì  ì•™ìƒë¸” ëª¨ë¸ë“¤ í›ˆë ¨")

        for name, model in self.models.items():
            logger.info(f"  {name} í›ˆë ¨ ì¤‘...")
            model.fit(X, y)

        # ì´ˆê¸° ê· ë“± ê°€ì¤‘ì¹˜
        self.weights_ = {name: 1.0 / len(self.models) for name in self.models.keys()}

        return self

    def update_weights(self, X_recent, y_recent):
        """ìµœê·¼ ì„±ê³¼ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""

        if len(y_recent) < 5:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸ ì•ˆí•¨
            return

        losses = {}

        for name, model in self.models.items():
            try:
                y_pred = model.predict(X_recent)

                # íšŒê·€ ë¬¸ì œì´ë¯€ë¡œ MSE ì‚¬ìš©
                loss = np.mean((y_recent - y_pred) ** 2)
                losses[name] = loss

            except Exception as e:
                logger.warning(f"{name} ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
                losses[name] = 1.0  # ë†’ì€ ì†ì‹¤ í• ë‹¹

        # ì†ì‹¤ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ (Softmax)
        inv_losses = [1.0 / (loss + 1e-8) for loss in losses.values()]
        total = sum(inv_losses)

        self.weights_ = {
            name: inv_loss / total
            for name, inv_loss in zip(self.models.keys(), inv_losses)
        }

        logger.info(f"ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸: {self.weights_}")

    def predict(self, X):
        """ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡"""
        predictions = {}

        for name, model in self.models.items():
            predictions[name] = model.predict(X)

        # ê°€ì¤‘ í‰ê· 
        ensemble_pred = np.zeros(len(X))
        for name, pred in predictions.items():
            ensemble_pred += self.weights_[name] * pred

        return ensemble_pred

class AdvancedLearningSystem:
    """ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ (í•™ìŠµê³„íš.txt ê¸°ë°˜)"""

    def __init__(self):
        self.data_processor = UltraSafeDataProcessor()
        self.leakage_detector = AutoLeakageDetector()

        # íŒŒì´í”„ë¼ì¸ êµ¬ì„±ìš”ì†Œ
        self.vmd_denoiser = SafeVMDDenoiser()
        self.feature_integrator = AlternativeDataIntegrator()

        # ë² ì´ìŠ¤ ëª¨ë¸ë“¤
        self.base_models = {
            'lstm': SafeLSTMModel(),
            'transformer': SafeTransformerModel(),
        }

        if XGBOOST_AVAILABLE:
            self.base_models['xgboost'] = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42,
                verbosity=0
            )

        # ë™ì  ì•™ìƒë¸”
        self.ensemble = None

        # ìµœì¢… íŒŒì´í”„ë¼ì¸
        self.pipeline = None

        logger.info("ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def build_pipeline(self):
        """ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"""
        logger.info("ê³ ê¸‰ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")

        self.pipeline = Pipeline([
            ('vmd_denoiser', self.vmd_denoiser),
            ('feature_integrator', self.feature_integrator)
        ])

        return self.pipeline

    def nested_cross_validation(self, X, y, param_grids: Dict):
        """ì¤‘ì²© êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ íŠœë‹ ë° í‰ê°€"""
        logger.info("ì¤‘ì²© êµì°¨ ê²€ì¦ ì‹œì‘")

        # PurgedKFold ì„¤ì •
        inner_cv = PurgedKFold(n_splits=3, purge_size=5, embargo_size=5)
        outer_cv = PurgedKFold(n_splits=5, purge_size=5, embargo_size=5)

        nested_results = {}

        for model_name, model in self.base_models.items():
            logger.info(f"\n{model_name} ì¤‘ì²© CV í‰ê°€")

            if model_name in param_grids:
                # ë‚´ë¶€ ë£¨í”„: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
                grid_search = GridSearchCV(
                    estimator=model,
                    param_grid=param_grids[model_name],
                    cv=inner_cv,
                    scoring='neg_mean_squared_error',
                    n_jobs=1,
                    verbose=0
                )

                # ì™¸ë¶€ ë£¨í”„: ìµœì¢… ì„±ëŠ¥ í‰ê°€
                try:
                    scores = cross_val_score(
                        grid_search, X, y,
                        cv=outer_cv,
                        scoring='neg_mean_squared_error'
                    )

                    nested_results[model_name] = {
                        'mean_score': np.mean(scores),
                        'std_score': np.std(scores),
                        'scores': scores
                    }

                    logger.info(f"  í‰ê·  ì ìˆ˜: {np.mean(scores):.4f} Â± {np.std(scores):.4f}")

                except Exception as e:
                    logger.error(f"  {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                    nested_results[model_name] = {
                        'mean_score': -np.inf,
                        'std_score': np.inf,
                        'error': str(e)
                    }
            else:
                logger.warning(f"  {model_name} íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì—†ìŒ - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í‰ê°€")

        return nested_results

    def train_ensemble(self, X, y):
        """ë™ì  ì•™ìƒë¸” í›ˆë ¨"""
        logger.info("ë™ì  ì•™ìƒë¸” í›ˆë ¨")

        self.ensemble = DynamicEnsemble(self.base_models)
        self.ensemble.fit(X, y)

        return self.ensemble

    def run_advanced_learning(self, data_path: str):
        """ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ì „ì²´ ì‹¤í–‰"""
        logger.info("=" * 100)
        logger.info("ğŸš€ ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ (í•™ìŠµê³„íš.txt ê¸°ë°˜)")
        logger.info("=" * 100)

        # 1. ì´ˆì•ˆì „ ë°ì´í„° ì¤€ë¹„
        data_dict = self.data_processor.prepare_ultra_safe_data(data_path)
        X_base, y = data_dict['X'], data_dict['y']

        logger.info(f"ê¸°ë³¸ ë°ì´í„° ì¤€ë¹„: X{X_base.shape}, y{y.shape}")

        # 2. ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ì ìš©
        pipeline = self.build_pipeline()

        # í›ˆë ¨ ë°ì´í„°ë¡œ íŒŒì´í”„ë¼ì¸ í•™ìŠµ
        split_point = int(len(X_base) * 0.8)
        X_train_base = X_base[:split_point]
        X_test_base = X_base[split_point:]
        y_train = y[:split_point]
        y_test = y[split_point:]

        # íŒŒì´í”„ë¼ì¸ ì ìš©
        X_train_processed = pipeline.fit_transform(X_train_base)
        X_test_processed = pipeline.transform(X_test_base)

        logger.info(f"íŒŒì´í”„ë¼ì¸ ì ìš©: {X_train_base.shape} -> {X_train_processed.shape}")

        # 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
        param_grids = {}

        if XGBOOST_AVAILABLE:
            param_grids['xgboost'] = {
                'n_estimators': [50, 100],
                'max_depth': [3, 5],
                'learning_rate': [0.1, 0.2]
            }

        # 4. ì¤‘ì²© êµì°¨ ê²€ì¦
        nested_results = self.nested_cross_validation(
            X_train_processed, y_train, param_grids
        )

        # 5. ë™ì  ì•™ìƒë¸” í›ˆë ¨
        ensemble = self.train_ensemble(X_train_processed, y_train)

        # 6. ìµœì¢… ì„±ëŠ¥ í‰ê°€
        y_pred = ensemble.predict(X_test_processed)

        # ì„±ëŠ¥ ì§€í‘œ
        mse = np.mean((y_test - y_pred) ** 2)
        mae = np.mean(np.abs(y_test - y_pred))

        # ë°©í–¥ ì •í™•ë„
        direction_actual = (y_test > 0).astype(int)
        direction_pred = (y_pred > 0).astype(int)
        direction_accuracy = np.mean(direction_actual == direction_pred) * 100

        # ì•ˆì „ì„± ê²€ì¦
        metrics = {
            'r2': 1 - (mse / np.var(y_test)),
            'direction_accuracy': direction_accuracy
        }

        is_safe = self.leakage_detector.validate_during_training(0, 'ensemble', metrics)

        return {
            'nested_cv_results': nested_results,
            'final_metrics': {
                'mse': mse,
                'mae': mae,
                'direction_accuracy': direction_accuracy,
                'r2': metrics['r2']
            },
            'safety_check': is_safe,
            'ensemble_weights': ensemble.weights_,
            'pipeline': pipeline,
            'ensemble': ensemble
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    system = AdvancedLearningSystem()

    try:
        results = system.run_advanced_learning(
            '/root/workspace/data/training/sp500_2020_2024_enhanced.csv'
        )

        print(f"\n{'='*100}")
        print(f"ğŸ† ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ê²°ê³¼")
        print(f"{'='*100}")

        print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥:")
        final_metrics = results['final_metrics']
        print(f"   MSE: {final_metrics['mse']:.6f}")
        print(f"   MAE: {final_metrics['mae']:.4f}")
        print(f"   RÂ²: {final_metrics['r2']:.4f}")
        print(f"   ë°©í–¥ì •í™•ë„: {final_metrics['direction_accuracy']:.1f}%")

        print(f"\nğŸ‹ï¸ ì•™ìƒë¸” ê°€ì¤‘ì¹˜:")
        for model, weight in results['ensemble_weights'].items():
            print(f"   {model}: {weight:.3f}")

        print(f"\nğŸ›¡ï¸ ì•ˆì „ì„± ê²€ì¦: {'âœ… í†µê³¼' if results['safety_check'] else 'âŒ ì‹¤íŒ¨'}")

        print(f"\nğŸ“ˆ ì¤‘ì²© CV ê²°ê³¼:")
        for model, result in results['nested_cv_results'].items():
            if 'error' not in result:
                print(f"   {model}: {result['mean_score']:.4f} Â± {result['std_score']:.4f}")
            else:
                print(f"   {model}: ì˜¤ë¥˜ - {result['error']}")

        return results

    except Exception as e:
        logger.error(f"ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    result = main()