#!/usr/bin/env python3
"""
ì•ˆì „í•œ ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ
ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨ íŒŒì´í”„ë¼ì¸
"""

import sys
import os
sys.path.append('/root/workspace/src')

import numpy as np
import pandas as pd
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any

from core.safe_data_processor import SafeDataProcessor
from validation.auto_leakage_detector import AutoLeakageDetector
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SafeLearningPipeline:
    """ì•ˆì „í•œ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._get_default_config()

        # í•µì‹¬ êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”
        self.data_processor = SafeDataProcessor()
        self.leakage_detector = AutoLeakageDetector()

        # ëª¨ë¸ ì„¤ì • (ê°„ë‹¨í•˜ê³  ì•ˆì „í•œ ëª¨ë¸ë§Œ)
        self.safe_models = self._get_safe_models()

        # ê²°ê³¼ ì €ì¥
        self.training_results = {}
        self.validation_reports = []

        logger.info("ì•ˆì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("CLAUDE.md ì¤€ìˆ˜: ë°ì´í„°ëˆ„ì¶œ ë°©ì§€, ì„±ëŠ¥ 95% ë¯¸ë§Œ ê°•ì œ")

    def _get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì •"""
        return {
            'data_path': '/root/workspace/data/training/sp500_2020_2024_enhanced.csv',
            'target_type': 'return',  # 'return' or 'direction'
            'max_features': 20,       # ìµœëŒ€ íŠ¹ì§• ìˆ˜ ì œí•œ
            'cv_splits': 3,           # êµì°¨ê²€ì¦ ë¶„í•  ìˆ˜
            'enable_auto_stop': True, # ìë™ ì¤‘ë‹¨ í™œì„±í™”
            'save_results': True,
            'output_dir': '/root/workspace/data/results/safe_learning'
        }

    def _get_safe_models(self) -> Dict:
        """ì•ˆì „í•œ ëª¨ë¸ ì„¤ì • (ë³µì¡ì„± ì œí•œ)"""
        return {
            'linear_regression': {
                'model_class': LinearRegression,
                'params': {},
                'description': 'ê¸°ë³¸ ì„ í˜• íšŒê·€'
            },
            'ridge_regression': {
                'model_class': Ridge,
                'params': {
                    'alpha': 1.0,
                    'random_state': 42
                },
                'description': 'L2 ì •ê·œí™” íšŒê·€'
            },
            'lasso_regression': {
                'model_class': Lasso,
                'params': {
                    'alpha': 0.01,
                    'random_state': 42,
                    'max_iter': 2000
                },
                'description': 'L1 ì •ê·œí™” íšŒê·€'
            },
            'random_forest_simple': {
                'model_class': RandomForestRegressor,
                'params': {
                    'n_estimators': 50,  # ë³µì¡ì„± ì œí•œ
                    'max_depth': 5,      # ê¹Šì´ ì œí•œ
                    'min_samples_split': 20,  # ê³¼ì í•© ë°©ì§€
                    'min_samples_leaf': 10,
                    'random_state': 42,
                    'n_jobs': -1
                },
                'description': 'ë‹¨ìˆœ ëœë¤ í¬ë ˆìŠ¤íŠ¸'
            }
        }

    def run_safe_pipeline(self) -> Dict:
        """ì•ˆì „í•œ íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤í–‰"""
        logger.info("=" * 80)
        logger.info("ğŸ›¡ï¸ ì•ˆì „í•œ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("ğŸš¨ ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨ ëª¨ë“œ")
        logger.info("=" * 80)

        start_time = datetime.now()

        try:
            # 1. ì•ˆì „í•œ ë°ì´í„° ì¤€ë¹„
            logger.info("1ë‹¨ê³„: ì•ˆì „í•œ ë°ì´í„° ì¤€ë¹„")
            data_dict = self._prepare_safe_data()

            # 2. í›ˆë ¨ ì „ ê²€ì¦
            logger.info("2ë‹¨ê³„: í›ˆë ¨ ì „ ì•ˆì „ì„± ê²€ì¦")
            pre_validation_passed = self._validate_before_training(data_dict)

            if not pre_validation_passed:
                raise ValueError("í›ˆë ¨ ì „ ê²€ì¦ ì‹¤íŒ¨ - ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ ê°ì§€")

            # 3. ì•ˆì „í•œ ëª¨ë¸ í›ˆë ¨
            logger.info("3ë‹¨ê³„: ì•ˆì „í•œ ëª¨ë¸ í›ˆë ¨")
            training_results = self._train_safe_models(data_dict)

            # 4. í›ˆë ¨ í›„ ê²€ì¦
            logger.info("4ë‹¨ê³„: í›ˆë ¨ í›„ ì•ˆì „ì„± ê²€ì¦")
            post_validation_passed = self._validate_after_training(training_results)

            if not post_validation_passed:
                logger.warning("í›ˆë ¨ í›„ ê²€ì¦ì—ì„œ ì¼ë¶€ ëª¨ë¸ ì‹¤íŒ¨")

            # 5. ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥
            logger.info("5ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬")
            pipeline_results = self._compile_results(data_dict, training_results, start_time)

            # 6. ê²€ì¦ ë³´ê³ ì„œ ìƒì„±
            validation_report = self.leakage_detector.generate_validation_report()
            pipeline_results['validation_report'] = validation_report

            # 7. ê²°ê³¼ ì €ì¥
            if self.config['save_results']:
                self._save_results(pipeline_results)

            # 8. ìµœì¢… ìš”ì•½ ì¶œë ¥
            self._print_final_summary(pipeline_results)

            logger.info("âœ… ì•ˆì „í•œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            return pipeline_results

        except Exception as e:
            logger.error(f"âŒ ì•ˆì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ê²€ì¦ ë³´ê³ ì„œëŠ” ìƒì„±
            try:
                validation_report = self.leakage_detector.generate_validation_report()
                self.validation_reports.append(validation_report)
            except:
                pass
            raise

    def _prepare_safe_data(self) -> Dict:
        """ì•ˆì „í•œ ë°ì´í„° ì¤€ë¹„"""
        data_dict = self.data_processor.prepare_safe_ml_data(
            self.config['data_path'],
            self.config['target_type']
        )

        # íŠ¹ì§• ìˆ˜ ì œí•œ (ë³µì¡ì„± ê°ì†Œ)
        if data_dict['X'].shape[1] > self.config['max_features']:
            logger.info(f"íŠ¹ì§• ìˆ˜ ì œí•œ: {data_dict['X'].shape[1]} â†’ {self.config['max_features']}")

            # ê°„ë‹¨í•œ ë¶„ì‚° ê¸°ë°˜ íŠ¹ì§• ì„ íƒ
            feature_vars = np.var(data_dict['X'], axis=0)
            top_indices = np.argsort(feature_vars)[-self.config['max_features']:]

            data_dict['X'] = data_dict['X'][:, top_indices]
            data_dict['feature_names'] = [data_dict['feature_names'][i] for i in top_indices]

        logger.info(f"ìµœì¢… ë°ì´í„°: X{data_dict['X'].shape}, íŠ¹ì§• {len(data_dict['feature_names'])}ê°œ")

        return data_dict

    def _validate_before_training(self, data_dict: Dict) -> bool:
        """í›ˆë ¨ ì „ ê²€ì¦"""
        return self.leakage_detector.validate_data_before_training(
            data_dict['X'],
            data_dict['y'],
            data_dict['feature_names']
        )

    def _train_safe_models(self, data_dict: Dict) -> Dict:
        """ì•ˆì „í•œ ëª¨ë¸ í›ˆë ¨"""
        X = data_dict['X']
        y = data_dict['y']
        splits = data_dict['splits']

        all_results = {}

        for model_name, model_config in self.safe_models.items():
            logger.info(f"=== {model_name} í›ˆë ¨ ì‹œì‘ ===")

            model_results = {
                'fold_r2': [],
                'fold_mse': [],
                'fold_mae': [],
                'fold_direction_acc': [],
                'description': model_config['description'],
                'training_stopped': False
            }

            # í´ë“œë³„ í›ˆë ¨
            for fold, (train_idx, val_idx) in enumerate(splits):
                logger.info(f"Fold {fold + 1}/{len(splits)} í›ˆë ¨")

                # ë°ì´í„° ë¶„í• 
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                # ì•ˆì „í•œ ìŠ¤ì¼€ì¼ë§
                X_train_scaled, X_val_scaled, scaler = self.data_processor.safe_scale_data(X_train, X_val)

                # ëª¨ë¸ í›ˆë ¨
                model = model_config['model_class'](**model_config['params'])
                model.fit(X_train_scaled, y_train)

                # ì˜ˆì¸¡ ë° í‰ê°€
                y_pred = model.predict(X_val_scaled)

                # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                r2 = r2_score(y_val, y_pred)
                mse = mean_squared_error(y_val, y_pred)
                mae = mean_absolute_error(y_val, y_pred)

                # ë°©í–¥ ì •í™•ë„ ê³„ì‚°
                direction_correct = np.sum(np.sign(y_val) == np.sign(y_pred))
                direction_acc = direction_correct / len(y_val) * 100

                # ê²°ê³¼ ì €ì¥
                model_results['fold_r2'].append(r2)
                model_results['fold_mse'].append(mse)
                model_results['fold_mae'].append(mae)
                model_results['fold_direction_acc'].append(direction_acc)

                # ì‹¤ì‹œê°„ ê²€ì¦
                fold_metrics = {
                    'r2': r2,
                    'mse': mse,
                    'mae': mae,
                    'direction_accuracy': direction_acc
                }

                # ìë™ ì¤‘ë‹¨ ê²€ì‚¬
                if self.config['enable_auto_stop']:
                    should_continue = self.leakage_detector.validate_during_training(
                        fold, model_name, fold_metrics
                    )

                    if not should_continue:
                        logger.error(f"ğŸ›‘ {model_name} í›ˆë ¨ ì¤‘ë‹¨ (Fold {fold})")
                        model_results['training_stopped'] = True
                        break

                logger.info(f"Fold {fold} ì„±ëŠ¥: RÂ²={r2:.4f}, ë°©í–¥ì •í™•ë„={direction_acc:.1f}%")

            # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
            if model_results['fold_r2']:
                model_results['mean_r2'] = np.mean(model_results['fold_r2'])
                model_results['std_r2'] = np.std(model_results['fold_r2'])
                model_results['mean_mse'] = np.mean(model_results['fold_mse'])
                model_results['mean_mae'] = np.mean(model_results['fold_mae'])
                model_results['mean_direction_accuracy'] = np.mean(model_results['fold_direction_acc'])
                model_results['std_direction_accuracy'] = np.std(model_results['fold_direction_acc'])

                logger.info(f"{model_name} ìµœì¢… ì„±ëŠ¥:")
                logger.info(f"   RÂ²: {model_results['mean_r2']:.4f} Â± {model_results['std_r2']:.4f}")
                logger.info(f"   ë°©í–¥ì •í™•ë„: {model_results['mean_direction_accuracy']:.1f}% Â± {model_results['std_direction_accuracy']:.1f}%")

            all_results[model_name] = model_results

        return all_results

    def _validate_after_training(self, training_results: Dict) -> bool:
        """í›ˆë ¨ í›„ ê²€ì¦"""
        return self.leakage_detector.validate_after_training(training_results)

    def _compile_results(self, data_dict: Dict, training_results: Dict, start_time: datetime) -> Dict:
        """ê²°ê³¼ ì¢…í•©"""
        return {
            'pipeline_info': {
                'execution_time': str(datetime.now() - start_time),
                'config': self.config,
                'data_shape': data_dict['X'].shape,
                'target_type': data_dict['target_type'],
                'feature_count': len(data_dict['feature_names']),
                'safety_validated': data_dict['safety_validated']
            },
            'training_results': training_results,
            'data_info': {
                'feature_names': data_dict['feature_names'],
                'splits_count': len(data_dict['splits']),
            },
            'safety_summary': self._generate_safety_summary(training_results)
        }

    def _generate_safety_summary(self, training_results: Dict) -> Dict:
        """ì•ˆì „ì„± ìš”ì•½ ìƒì„±"""
        summary = {
            'total_models': len(training_results),
            'completed_models': 0,
            'stopped_models': 0,
            'safe_models': 0,
            'suspicious_models': 0,
            'max_r2': 0,
            'max_direction_acc': 0
        }

        for model_name, results in training_results.items():
            if results.get('training_stopped', False):
                summary['stopped_models'] += 1
            else:
                summary['completed_models'] += 1

            # ìµœëŒ€ ì„±ëŠ¥ ì¶”ì 
            mean_r2 = results.get('mean_r2', 0)
            mean_direction = results.get('mean_direction_accuracy', 0)

            summary['max_r2'] = max(summary['max_r2'], mean_r2)
            summary['max_direction_acc'] = max(summary['max_direction_acc'], mean_direction)

            # ì•ˆì „ì„± ë¶„ë¥˜
            if mean_r2 < 0.10 and mean_direction < 60:
                summary['safe_models'] += 1
            else:
                summary['suspicious_models'] += 1

        # CLAUDE.md ì¤€ìˆ˜ í™•ì¸
        summary['claude_md_compliant'] = (
            summary['max_r2'] < 0.95 and
            summary['max_direction_acc'] < 95.0
        )

        return summary

    def _save_results(self, pipeline_results: Dict) -> None:
        """ê²°ê³¼ ì €ì¥"""
        output_dir = Path(self.config['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = output_dir / f"safe_learning_results_{timestamp}.json"

        # JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
        def convert_for_json(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, pd.Timestamp):
                return obj.isoformat()
            return obj

        def clean_for_json(data):
            if isinstance(data, dict):
                return {k: clean_for_json(v) for k, v in data.items()}
            elif isinstance(data, list):
                return [clean_for_json(item) for item in data]
            else:
                return convert_for_json(data)

        cleaned_results = clean_for_json(pipeline_results)

        with open(results_file, 'w') as f:
            json.dump(cleaned_results, f, indent=2)

        logger.info(f"ì•ˆì „í•œ í•™ìŠµ ê²°ê³¼ ì €ì¥: {results_file}")

    def _print_final_summary(self, pipeline_results: Dict) -> None:
        """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ›¡ï¸ ì•ˆì „í•œ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê²°ê³¼")
        print("=" * 80)

        # íŒŒì´í”„ë¼ì¸ ì •ë³´
        info = pipeline_results['pipeline_info']
        print(f"\nğŸ“‹ ì‹¤í–‰ ì •ë³´:")
        print(f"   ì‹¤í–‰ ì‹œê°„: {info['execution_time']}")
        print(f"   ë°ì´í„° í¬ê¸°: {info['data_shape']}")
        print(f"   íŠ¹ì§• ìˆ˜: {info['feature_count']}")
        print(f"   íƒ€ê²Ÿ íƒ€ì…: {info['target_type']}")

        # ì•ˆì „ì„± ìš”ì•½
        safety = pipeline_results['safety_summary']
        print(f"\nğŸ›¡ï¸ ì•ˆì „ì„± ìš”ì•½:")
        print(f"   ì´ ëª¨ë¸: {safety['total_models']}")
        print(f"   ì™„ë£Œëœ ëª¨ë¸: {safety['completed_models']}")
        print(f"   ì¤‘ë‹¨ëœ ëª¨ë¸: {safety['stopped_models']}")
        print(f"   ì•ˆì „í•œ ëª¨ë¸: {safety['safe_models']}")
        print(f"   ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ëª¨ë¸: {safety['suspicious_models']}")

        # ì„±ëŠ¥ ìš”ì•½
        print(f"\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        print(f"   ìµœëŒ€ RÂ²: {safety['max_r2']:.4f}")
        print(f"   ìµœëŒ€ ë°©í–¥ì •í™•ë„: {safety['max_direction_acc']:.1f}%")

        # CLAUDE.md ì¤€ìˆ˜
        compliance_icon = "âœ…" if safety['claude_md_compliant'] else "âŒ"
        print(f"\nğŸ“‹ CLAUDE.md ì¤€ìˆ˜: {compliance_icon}")

        # ëª¨ë¸ë³„ ê²°ê³¼
        print(f"\nğŸ† ëª¨ë¸ë³„ ì„±ëŠ¥:")
        training_results = pipeline_results['training_results']

        for model_name, results in training_results.items():
            if not results.get('training_stopped', False):
                r2 = results.get('mean_r2', 0)
                direction = results.get('mean_direction_accuracy', 0)
                status = "ğŸ›¡ï¸" if r2 < 0.10 and direction < 60 else "âš ï¸"
                print(f"   {status} {model_name:20s}: RÂ²={r2:6.3f}, ë°©í–¥={direction:5.1f}%")
            else:
                print(f"   ğŸ›‘ {model_name:20s}: í›ˆë ¨ ì¤‘ë‹¨ë¨")

        print("\n" + "=" * 80)

        # ê¶Œì¥ì‚¬í•­
        if safety['claude_md_compliant']:
            print("âœ… ëª¨ë“  ëª¨ë¸ì´ ì•ˆì „ ê¸°ì¤€ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")
            print("ğŸ’¡ í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ë²”ìœ„ ë‚´ì—ì„œ ì•ˆì „í•˜ê²Œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ ì¼ë¶€ ëª¨ë¸ì´ ì•ˆì „ ê¸°ì¤€ì„ ìœ„ë°˜í–ˆìŠµë‹ˆë‹¤.")
            print("ğŸš¨ ë°ì´í„° ëˆ„ì¶œ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë‹ˆ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        print("=" * 80)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    config = {
        'data_path': '/root/workspace/data/training/sp500_2020_2024_enhanced.csv',
        'target_type': 'return',
        'max_features': 10,  # íŠ¹ì§• ìˆ˜ ì œí•œ
        'cv_splits': 3,
        'enable_auto_stop': True,
        'save_results': True,
        'output_dir': '/root/workspace/data/results/safe_learning'
    }

    pipeline = SafeLearningPipeline(config)
    results = pipeline.run_safe_pipeline()

    return results

if __name__ == "__main__":
    results = main()