#!/usr/bin/env python3
"""
ê³ ê¸‰ ì§€í‘œ ê¸°ë°˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸
ë¡œê·¸ ì†ì‹¤ ë° F1-Score ìµœì í™” í†µí•© ì‹œìŠ¤í…œ
"""

import sys
import os
sys.path.append('/root/workspace/src')

import numpy as np
import pandas as pd
import json
import logging
from pathlib import Path
from datetime import datetime

from core.data_processor import DataProcessor
from training.advanced_metric_trainer import AdvancedMetricTrainer
from evaluation.performance_evaluator import PerformanceEvaluator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedMetricPipeline:
    """ê³ ê¸‰ ì§€í‘œ ê¸°ë°˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config=None):
        self.config = config or self._get_default_config()
        self.data_processor = DataProcessor()
        self.model_trainer = AdvancedMetricTrainer(gpu_enabled=self.config['gpu_enabled'])
        self.evaluator = PerformanceEvaluator()

        logger.info("ê³ ê¸‰ ì§€í‘œ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _get_default_config(self):
        """ê¸°ë³¸ ì„¤ì •"""
        return {
            'data_path': '/root/workspace/data/training/sp500_2020_2024_enhanced.csv',
            'target_type': 'direction',
            'sequence_length': 20,
            'cv_splits': 3,
            'gpu_enabled': True,
            'save_models': True,
            'save_results': True,
            'output_dir': '/root/workspace/data/results/advanced_metrics'
        }

    def load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"""
        logger.info("=== ê³ ê¸‰ ì§€í‘œ ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ ===")

        # ë°ì´í„° ë¡œë”©
        df = self.data_processor.load_and_validate_data(self.config['data_path'])
        if df is None:
            raise ValueError("ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")

        # MLìš© ë°ì´í„° ì¤€ë¹„
        X, y, feature_cols = self.data_processor.prepare_ml_data(df, self.config['target_type'])

        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
        X_seq, y_seq, scaler = self.data_processor.prepare_sequence_data(
            df, self.config['sequence_length'], self.config['target_type']
        )

        # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        integrity_check = self.data_processor.validate_data_integrity(X.values, y.values)
        logger.info(f"ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬: {integrity_check}")

        # í•™ìŠµ/ê²€ì¦ ë¶„í• 
        splits = self.data_processor.create_train_val_split(X.values, y.values, self.config['cv_splits'])

        return {
            'flat_data': (X.values, y.values),
            'sequence_data': (X_seq, y_seq),
            'feature_names': feature_cols,
            'splits': splits,
            'scaler': scaler,
            'integrity_check': integrity_check
        }

    def train_advanced_models(self, data_dict):
        """ê³ ê¸‰ ì§€í‘œ ëª¨ë¸ í›ˆë ¨"""
        logger.info("=== ê³ ê¸‰ ì§€í‘œ ëª¨ë¸ í›ˆë ¨ ë‹¨ê³„ ===")

        X, y = data_dict['flat_data']
        X_seq, y_seq = data_dict['sequence_data']
        splits = data_dict['splits']

        all_results = {}

        # ê° í´ë“œë³„ë¡œ í›ˆë ¨ ë° í‰ê°€
        for fold, (train_idx, val_idx) in enumerate(splits):
            logger.info(f"Fold {fold + 1}/{len(splits)} ê³ ê¸‰ ì§€í‘œ í›ˆë ¨ ì‹œì‘")

            # ë°ì´í„° ë¶„í• 
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # ì‹œí€€ìŠ¤ ë°ì´í„° ë¶„í•  (ì¸ë±ìŠ¤ ì¡°ì •)
            seq_train_idx = train_idx[train_idx < len(X_seq)]
            seq_val_idx = val_idx[val_idx < len(X_seq)]

            if len(seq_train_idx) > 0 and len(seq_val_idx) > 0:
                X_seq_train, X_seq_val = X_seq[seq_train_idx], X_seq[seq_val_idx]
                sequence_data = (X_seq_train, X_seq_val)
            else:
                sequence_data = None

            # ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨
            fold_results = self.model_trainer.train_all_advanced_models(
                X_train, X_val, y_train, y_val, sequence_data=sequence_data
            )

            # ê²°ê³¼ ëˆ„ì 
            for model_name, result in fold_results.items():
                if model_name not in all_results:
                    all_results[model_name] = {
                        'fold_accuracies': [],
                        'fold_f1_scores': [],
                        'fold_log_losses': [],
                        'fold_aucs': [],
                        'fold_precisions': [],
                        'fold_recalls': [],
                        'type': result['type']
                    }

                all_results[model_name]['fold_accuracies'].append(result['accuracy'])
                all_results[model_name]['fold_f1_scores'].append(result['f1_score'])
                all_results[model_name]['fold_log_losses'].append(result['log_loss'])
                all_results[model_name]['fold_aucs'].append(result['auc'])
                all_results[model_name]['fold_precisions'].append(result['precision'])
                all_results[model_name]['fold_recalls'].append(result['recall'])

        # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        final_results = {}
        for model_name, data in all_results.items():
            final_results[model_name] = {
                'mean_accuracy': np.mean(data['fold_accuracies']),
                'std_accuracy': np.std(data['fold_accuracies']),
                'mean_f1_score': np.mean(data['fold_f1_scores']),
                'std_f1_score': np.std(data['fold_f1_scores']),
                'mean_log_loss': np.mean([x for x in data['fold_log_losses'] if x != float('inf')]),
                'std_log_loss': np.std([x for x in data['fold_log_losses'] if x != float('inf')]),
                'mean_auc': np.mean(data['fold_aucs']),
                'std_auc': np.std(data['fold_aucs']),
                'mean_precision': np.mean(data['fold_precisions']),
                'std_precision': np.std(data['fold_precisions']),
                'mean_recall': np.mean(data['fold_recalls']),
                'std_recall': np.std(data['fold_recalls']),
                'direction_accuracy': np.mean(data['fold_accuracies']) * 100,
                'fold_results': data,
                'type': data['type']
            }

        return final_results

    def analyze_metric_performance(self, results):
        """ì§€í‘œë³„ ì„±ëŠ¥ ë¶„ì„"""
        logger.info("=== ì§€í‘œë³„ ì„±ëŠ¥ ë¶„ì„ ===")

        analysis = {
            'best_by_accuracy': None,
            'best_by_f1': None,
            'best_by_log_loss': None,
            'best_by_auc': None,
            'metric_rankings': {},
            'cross_metric_analysis': {}
        }

        # ê° ì§€í‘œë³„ ìµœê³  ëª¨ë¸ ì°¾ê¸°
        best_accuracy = 0
        best_f1 = 0
        best_log_loss = float('inf')
        best_auc = 0

        for model_name, result in results.items():
            # ì •í™•ë„ ê¸°ì¤€
            if result['mean_accuracy'] > best_accuracy:
                best_accuracy = result['mean_accuracy']
                analysis['best_by_accuracy'] = {
                    'model': model_name,
                    'score': best_accuracy,
                    'details': result
                }

            # F1 ì ìˆ˜ ê¸°ì¤€
            if result['mean_f1_score'] > best_f1:
                best_f1 = result['mean_f1_score']
                analysis['best_by_f1'] = {
                    'model': model_name,
                    'score': best_f1,
                    'details': result
                }

            # ë¡œê·¸ ì†ì‹¤ ê¸°ì¤€ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            if result['mean_log_loss'] < best_log_loss and result['mean_log_loss'] != float('inf'):
                best_log_loss = result['mean_log_loss']
                analysis['best_by_log_loss'] = {
                    'model': model_name,
                    'score': best_log_loss,
                    'details': result
                }

            # AUC ê¸°ì¤€
            if result['mean_auc'] > best_auc:
                best_auc = result['mean_auc']
                analysis['best_by_auc'] = {
                    'model': model_name,
                    'score': best_auc,
                    'details': result
                }

        # ì§€í‘œë³„ ìˆœìœ„ ë§¤ê¸°ê¸°
        for metric in ['mean_accuracy', 'mean_f1_score', 'mean_auc']:
            ranking = sorted(results.items(),
                           key=lambda x: x[1][metric],
                           reverse=True)
            analysis['metric_rankings'][metric] = ranking

        # ë¡œê·¸ ì†ì‹¤ ìˆœìœ„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        log_loss_ranking = sorted(results.items(),
                                key=lambda x: x[1]['mean_log_loss'] if x[1]['mean_log_loss'] != float('inf') else 999,
                                reverse=False)
        analysis['metric_rankings']['mean_log_loss'] = log_loss_ranking

        return analysis

    def run_advanced_pipeline(self):
        """ì „ì²´ ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("=" * 80)
        logger.info("ğŸš€ ê³ ê¸‰ ì§€í‘œ ê¸°ë°˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("=" * 80)

        start_time = datetime.now()

        try:
            # 1. ë°ì´í„° ì¤€ë¹„
            data_dict = self.load_and_prepare_data()

            # 2. ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨
            model_results = self.train_advanced_models(data_dict)

            # 3. ì§€í‘œë³„ ì„±ëŠ¥ ë¶„ì„
            metric_analysis = self.analyze_metric_performance(model_results)

            # 4. ì¢…í•© ê²°ê³¼
            pipeline_results = {
                'pipeline_config': self.config,
                'execution_time': str(datetime.now() - start_time),
                'data_info': {
                    'feature_count': len(data_dict['feature_names']),
                    'sample_count': len(data_dict['flat_data'][1]),
                    'sequence_length': self.config['sequence_length'],
                    'target_type': self.config['target_type']
                },
                'model_results': model_results,
                'metric_analysis': metric_analysis,
                'data_integrity': data_dict['integrity_check']
            }

            # 5. ëª¨ë¸ ì €ì¥
            if self.config['save_models']:
                self.model_trainer.save_advanced_models()

            # 6. ê²°ê³¼ ì €ì¥
            if self.config['save_results']:
                self._save_results(pipeline_results)

            # 7. ê²°ê³¼ ì¶œë ¥
            self._print_advanced_summary(pipeline_results)

            logger.info("âœ… ê³ ê¸‰ ì§€í‘œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
            return pipeline_results

        except Exception as e:
            logger.error(f"âŒ ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise

    def _save_results(self, results):
        """ê²°ê³¼ ì €ì¥"""
        output_dir = Path(self.config['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)

        # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ë³€í™˜
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            return obj

        def clean_for_json(data):
            if isinstance(data, dict):
                return {k: clean_for_json(v) for k, v in data.items()}
            elif isinstance(data, list):
                return [clean_for_json(item) for item in data]
            else:
                return convert_numpy(data)

        cleaned_results = clean_for_json(results)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = output_dir / f"advanced_metric_results_{timestamp}.json"

        with open(results_file, 'w') as f:
            json.dump(cleaned_results, f, indent=2)

        logger.info(f"ê³ ê¸‰ ì§€í‘œ ê²°ê³¼ ì €ì¥: {results_file}")

    def _print_advanced_summary(self, results):
        """ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ê³ ê¸‰ ì§€í‘œ ê¸°ë°˜ í•™ìŠµ ê²°ê³¼")
        print("=" * 80)

        # ê¸°ë³¸ ì •ë³´
        print(f"\nğŸ“‹ ì‹¤í–‰ ì •ë³´:")
        print(f"   ì‹¤í–‰ ì‹œê°„: {results['execution_time']}")
        print(f"   íŠ¹ì§• ìˆ˜: {results['data_info']['feature_count']}")
        print(f"   ìƒ˜í”Œ ìˆ˜: {results['data_info']['sample_count']}")

        # ì§€í‘œë³„ ìµœê³  ì„±ëŠ¥
        analysis = results['metric_analysis']

        print(f"\nğŸ† ì§€í‘œë³„ ìµœê³  ì„±ëŠ¥:")
        print("-" * 80)

        if analysis['best_by_accuracy']:
            acc_best = analysis['best_by_accuracy']
            print(f"ì •í™•ë„ ìµœê³ :     {acc_best['model']:30s} {acc_best['score']*100:.2f}%")

        if analysis['best_by_f1']:
            f1_best = analysis['best_by_f1']
            print(f"F1 ì ìˆ˜ ìµœê³ :    {f1_best['model']:30s} {f1_best['score']:.4f}")

        if analysis['best_by_log_loss']:
            log_best = analysis['best_by_log_loss']
            print(f"ë¡œê·¸ ì†ì‹¤ ìµœê³ :  {log_best['model']:30s} {log_best['score']:.4f}")

        if analysis['best_by_auc']:
            auc_best = analysis['best_by_auc']
            print(f"AUC ìµœê³ :        {auc_best['model']:30s} {auc_best['score']:.4f}")

        # ì¢…í•© ìˆœìœ„ (ì •í™•ë„ ê¸°ì¤€)
        print(f"\nğŸ“ˆ ì¢…í•© ì„±ëŠ¥ ìˆœìœ„ (ì •í™•ë„ ê¸°ì¤€):")
        print("-" * 80)

        if 'mean_accuracy' in analysis['metric_rankings']:
            for i, (model_name, result) in enumerate(analysis['metric_rankings']['mean_accuracy'][:5], 1):
                acc = result['mean_accuracy'] * 100
                f1 = result['mean_f1_score']
                print(f"{i:2d}. {model_name:30s} | ì •í™•ë„: {acc:6.2f}% | F1: {f1:.4f}")

        print("\n" + "=" * 80)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    config = {
        'data_path': '/root/workspace/data/training/sp500_2020_2024_enhanced.csv',
        'target_type': 'direction',
        'sequence_length': 20,
        'cv_splits': 3,
        'gpu_enabled': True,
        'save_models': True,
        'save_results': True,
        'output_dir': '/root/workspace/data/results/advanced_metrics'
    }

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = AdvancedMetricPipeline(config)
    results = pipeline.run_advanced_pipeline()

    return results

if __name__ == "__main__":
    results = main()