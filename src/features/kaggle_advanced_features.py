#!/usr/bin/env python3
"""
ìºê¸€ ìš°ìŠ¹ì ê¸°ë²• ê¸°ë°˜ ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
Two Sigma, Jane Street, Optiver ë“± ëŒ€íšŒ ìš°ìŠ¹ ì†”ë£¨ì…˜ ì ìš©
"""

import sys
sys.path.append('/root/workspace')

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

from src.core.ultra_safe_data_processor import UltraSafeDataProcessor
from src.validation.auto_leakage_detector import AutoLeakageDetector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KaggleAdvancedFeatureEngineer(BaseEstimator, TransformerMixin):
    """ìºê¸€ ìš°ìŠ¹ì ê¸°ë²• ê¸°ë°˜ ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§"""

    def __init__(self, safety_mode: bool = True):
        self.safety_mode = safety_mode
        self.MAX_CORRELATION = 0.25 if safety_mode else 0.30
        self.scaler_ = StandardScaler()
        self.feature_names_ = None
        self.leakage_detector = AutoLeakageDetector()

        logger.info("ğŸ† ìºê¸€ ìš°ìŠ¹ì ê¸°ë²• ê¸°ë°˜ ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì´ˆê¸°í™”")
        logger.info(f"ì•ˆì „ ëª¨ë“œ: {safety_mode}, ìµœëŒ€ ìƒê´€ê´€ê³„: {self.MAX_CORRELATION}")

    def fit(self, X, y=None):
        """ê³ ê¸‰ íŠ¹ì§• ìƒì„± ê·œì¹™ í•™ìŠµ"""
        logger.info("=== ìºê¸€ ê¸°ë²• ê¸°ë°˜ ê³ ê¸‰ íŠ¹ì§• í•™ìŠµ ===")

        # ê¸°ë³¸ íŠ¹ì§•ëª… (10ê°œ)
        base_features = [
            'returns_lag2', 'returns_lag3', 'returns_lag5', 'returns_lag10',
            'vol_5_lag2', 'vol_10_lag2',
            'returns_mean_5_lag2', 'returns_std_5_lag2',
            'returns_diff_2_5', 'returns_diff_3_10'
        ]

        # ìºê¸€ ìš°ìŠ¹ì ê¸°ë²• íŠ¹ì§•ë“¤
        kaggle_features = [
            # Two Sigma ê¸°ë²•: ì‹œê°„ êµ¬ì¡° íŠ¹ì§•
            'intraday_pattern_lag2', 'volatility_regime_lag2', 'trend_strength_lag2',

            # Jane Street ê¸°ë²•: ê³ ê¸‰ ê¸°ìˆ ì  ì§€í‘œ
            'fractal_dimension_lag2', 'hurst_exponent_lag2', 'entropy_measure_lag2',

            # Optiver ê¸°ë²•: ì‹œì¥ ë¯¸ì‹œêµ¬ì¡°
            'price_acceleration_lag2', 'volatility_of_volatility_lag2', 'momentum_persistence_lag2',

            # ë©”íƒ€ ë¼ë²¨ë§ íŠ¹ì§•
            'prediction_confidence_lag2', 'regime_classification_lag2',

            # ì–´í…ì…˜ ê¸°ë°˜ íŠ¹ì§•
            'temporal_attention_score_lag2', 'feature_importance_score_lag2'
        ]

        self.feature_names_ = base_features + kaggle_features

        # íŠ¹ì§• ìƒì„± í›„ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        X_features = self._generate_kaggle_features(X)
        self.scaler_.fit(X_features)

        logger.info(f"ì´ ê³ ê¸‰ íŠ¹ì§• ìˆ˜: {len(self.feature_names_)}")
        logger.info(f"ì¶”ê°€ëœ ìºê¸€ íŠ¹ì§•: {len(kaggle_features)}ê°œ")

        return self

    def transform(self, X):
        """í•™ìŠµëœ ê·œì¹™ìœ¼ë¡œ ê³ ê¸‰ íŠ¹ì§• ìƒì„±"""
        if self.feature_names_ is None:
            raise ValueError("ë¨¼ì € fití•´ì•¼ í•©ë‹ˆë‹¤")

        X_features = self._generate_kaggle_features(X)
        X_scaled = self.scaler_.transform(X_features)

        logger.info(f"ìºê¸€ ê¸°ë²• íŠ¹ì§• ìƒì„±: {X.shape} -> {X_scaled.shape}")
        return X_scaled

    def _generate_kaggle_features(self, X):
        """ìºê¸€ ìš°ìŠ¹ì ê¸°ë²•ìœ¼ë¡œ ê³ ê¸‰ íŠ¹ì§• ìƒì„±"""

        # ê¸°ì¡´ íŠ¹ì§• ì‚¬ìš©
        features = X.copy()
        n_samples = features.shape[0]

        # ê¸°ë³¸ íŠ¹ì§•ë“¤ ì¶”ì¶œ
        returns_lag2 = features[:, 0]
        returns_lag3 = features[:, 1]
        returns_lag5 = features[:, 2]
        vol_5_lag2 = features[:, 4]
        vol_10_lag2 = features[:, 5]
        returns_mean_5_lag2 = features[:, 6]
        returns_std_5_lag2 = features[:, 7]

        logger.info("1. Two Sigma ê¸°ë²•: ì‹œê°„ êµ¬ì¡° íŠ¹ì§• ìƒì„±")

        # 1. Two Sigma ê¸°ë²•: ì‹œê°„ êµ¬ì¡° íŠ¹ì§•
        # ì¼ì¤‘ íŒ¨í„´ (ê°„ì†Œí™”ëœ ë²„ì „)
        intraday_pattern_lag2 = np.zeros(n_samples)
        for i in range(20, n_samples):
            # ìµœê·¼ 20ì¼ íŒ¨í„´ì˜ ì£¼ê¸°ì„± ì¸¡ì •
            recent_returns = returns_lag2[i-20:i]
            if len(recent_returns) > 0:
                intraday_pattern_lag2[i] = np.std(recent_returns) / (np.abs(np.mean(recent_returns)) + 1e-8)

        # ë³€ë™ì„± ì²´ì œ ë¶„ë¥˜
        volatility_regime_lag2 = np.zeros(n_samples)
        for i in range(30, n_samples):
            recent_vol = vol_5_lag2[i-30:i]
            long_term_vol = vol_10_lag2[i-30:i]
            if len(recent_vol) > 0 and len(long_term_vol) > 0:
                volatility_regime_lag2[i] = np.mean(recent_vol) / (np.mean(long_term_vol) + 1e-8)

        # ì¶”ì„¸ ê°•ë„ ì¸¡ì •
        trend_strength_lag2 = np.zeros(n_samples)
        for i in range(15, n_samples):
            recent_returns = returns_lag2[i-15:i]
            if len(recent_returns) > 0:
                # ì—°ì†ì ì¸ ê°™ì€ ë°©í–¥ ì›€ì§ì„ì˜ ë¹„ìœ¨
                directions = np.sign(recent_returns)
                trend_strength_lag2[i] = np.abs(np.sum(directions)) / len(directions)

        logger.info("2. Jane Street ê¸°ë²•: ê³ ê¸‰ ê¸°ìˆ ì  ì§€í‘œ ìƒì„±")

        # 2. Jane Street ê¸°ë²•: ê³ ê¸‰ ê¸°ìˆ ì  ì§€í‘œ
        # í”„ë™íƒˆ ì°¨ì› (ê°„ì†Œí™”ëœ Higuchi ë°©ë²•)
        fractal_dimension_lag2 = np.zeros(n_samples)
        for i in range(25, n_samples):
            series = returns_lag2[i-25:i]
            if len(series) > 10:
                # ê°„ì†Œí™”ëœ í”„ë™íƒˆ ì°¨ì› ê³„ì‚°
                diffs = np.abs(np.diff(series))
                fractal_dimension_lag2[i] = np.log(np.sum(diffs)) / np.log(len(diffs))

        # í—ˆìŠ¤íŠ¸ ì§€ìˆ˜ (R/S ë¶„ì„ ê°„ì†Œí™”)
        hurst_exponent_lag2 = np.zeros(n_samples)
        for i in range(30, n_samples):
            series = returns_lag2[i-30:i]
            if len(series) > 5:
                # R/S í†µê³„ ê³„ì‚°
                mean_series = np.mean(series)
                deviations = np.cumsum(series - mean_series)
                R = np.max(deviations) - np.min(deviations)
                S = np.std(series)
                if S > 1e-8:
                    hurst_exponent_lag2[i] = np.log(R/S) / np.log(len(series))

        # ì—”íŠ¸ë¡œí”¼ ì¸¡ì • (Shannon ì—”íŠ¸ë¡œí”¼ ê·¼ì‚¬)
        entropy_measure_lag2 = np.zeros(n_samples)
        for i in range(20, n_samples):
            series = returns_lag2[i-20:i]
            if len(series) > 0:
                # ìˆ˜ìµë¥ ì„ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
                hist, _ = np.histogram(series, bins=5, density=True)
                hist = hist[hist > 0]  # 0ì´ ì•„ë‹Œ ê°’ë§Œ
                if len(hist) > 0:
                    entropy_measure_lag2[i] = -np.sum(hist * np.log(hist + 1e-8))

        logger.info("3. Optiver ê¸°ë²•: ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° íŠ¹ì§• ìƒì„±")

        # 3. Optiver ê¸°ë²•: ì‹œì¥ ë¯¸ì‹œêµ¬ì¡°
        # ê°€ê²© ê°€ì†ë„ (2ì°¨ ì°¨ë¶„)
        price_acceleration_lag2 = returns_lag2 - returns_lag3

        # ë³€ë™ì„±ì˜ ë³€ë™ì„±
        volatility_of_volatility_lag2 = np.zeros(n_samples)
        for i in range(15, n_samples):
            recent_vols = vol_5_lag2[i-15:i]
            if len(recent_vols) > 0:
                volatility_of_volatility_lag2[i] = np.std(recent_vols)

        # ëª¨ë©˜í…€ ì§€ì†ì„±
        momentum_persistence_lag2 = np.zeros(n_samples)
        for i in range(10, n_samples):
            recent_returns = returns_lag2[i-10:i]
            if len(recent_returns) > 0:
                # ì—°ì†ì ì¸ ì›€ì§ì„ì˜ ì¼ê´€ì„± ì¸¡ì •
                directions = np.sign(recent_returns)
                momentum_persistence_lag2[i] = np.mean(np.abs(np.diff(directions)) == 0)

        logger.info("4. ë©”íƒ€ ë¼ë²¨ë§ íŠ¹ì§• ìƒì„±")

        # 4. ë©”íƒ€ ë¼ë²¨ë§ íŠ¹ì§•
        # ì˜ˆì¸¡ ì‹ ë¢°ë„ (ë³€ë™ì„± ê¸°ë°˜ ê·¼ì‚¬)
        prediction_confidence_lag2 = 1 / (1 + vol_5_lag2)

        # ì²´ì œ ë¶„ë¥˜ (í‰ê·  íšŒê·€ vs ì¶”ì„¸)
        regime_classification_lag2 = np.zeros(n_samples)
        for i in range(20, n_samples):
            recent_returns = returns_lag2[i-20:i]
            if len(recent_returns) > 0:
                # ìê¸°ìƒê´€ ì¸¡ì •ìœ¼ë¡œ ì²´ì œ ë¶„ë¥˜
                if len(recent_returns) > 1:
                    autocorr = np.corrcoef(recent_returns[:-1], recent_returns[1:])[0, 1]
                    regime_classification_lag2[i] = autocorr if not np.isnan(autocorr) else 0

        logger.info("5. ì–´í…ì…˜ ê¸°ë°˜ íŠ¹ì§• ìƒì„±")

        # 5. ì–´í…ì…˜ ê¸°ë°˜ íŠ¹ì§•
        # ì‹œê°„ ì–´í…ì…˜ ì ìˆ˜ (ìµœê·¼ì„± ê°€ì¤‘ì¹˜)
        temporal_attention_score_lag2 = np.zeros(n_samples)
        for i in range(10, n_samples):
            weights = np.exp(-np.arange(10) / 5)  # ì§€ìˆ˜ ê°ì†Œ ê°€ì¤‘ì¹˜
            recent_returns = returns_lag2[i-10:i]
            if len(recent_returns) > 0:
                temporal_attention_score_lag2[i] = np.average(np.abs(recent_returns), weights=weights)

        # íŠ¹ì§• ì¤‘ìš”ë„ ì ìˆ˜ (ë³€ë™ì„±ê³¼ ìˆ˜ìµë¥ ì˜ ì¡°í•©)
        feature_importance_score_lag2 = np.abs(returns_lag2) * vol_5_lag2

        # ëª¨ë“  ìƒˆë¡œìš´ íŠ¹ì§•ë“¤ ê²°í•©
        kaggle_features = np.column_stack([
            intraday_pattern_lag2, volatility_regime_lag2, trend_strength_lag2,
            fractal_dimension_lag2, hurst_exponent_lag2, entropy_measure_lag2,
            price_acceleration_lag2, volatility_of_volatility_lag2, momentum_persistence_lag2,
            prediction_confidence_lag2, regime_classification_lag2,
            temporal_attention_score_lag2, feature_importance_score_lag2
        ])

        # ê¸°ì¡´ íŠ¹ì§•ê³¼ ê²°í•©
        X_combined = np.column_stack([features, kaggle_features])

        logger.info(f"ìºê¸€ ê¸°ë²• íŠ¹ì§• ìƒì„± ì™„ë£Œ: {features.shape} -> {X_combined.shape}")

        return X_combined

    def validate_feature_safety(self, X_features, y):
        """íŠ¹ì§• ì•ˆì „ì„± ê²€ì¦"""
        logger.info("=== ìºê¸€ íŠ¹ì§• ì•ˆì „ì„± ê²€ì¦ ===")

        if y is None:
            logger.warning("íƒ€ê²Ÿì´ ì—†ì–´ ìƒê´€ê´€ê³„ ê²€ì¦ ìƒëµ")
            return True

        dangerous_features = []
        correlations = []

        for i, feature_name in enumerate(self.feature_names_):
            if i < X_features.shape[1]:
                corr = abs(np.corrcoef(X_features[:, i], y)[0, 1])
                correlations.append(corr)

                if not np.isnan(corr) and corr > self.MAX_CORRELATION:
                    dangerous_features.append((feature_name, corr))

        max_corr = max(correlations) if correlations else 0

        logger.info(f"ìµœëŒ€ ìƒê´€ê´€ê³„: {max_corr:.3f}")
        logger.info(f"ì•ˆì „ ê¸°ì¤€: {self.MAX_CORRELATION}")

        if dangerous_features:
            logger.error(f"ìœ„í—˜í•œ íŠ¹ì§• ë°œê²¬: {len(dangerous_features)}ê°œ")
            for feature, corr in dangerous_features:
                logger.error(f"   {feature}: {corr:.3f}")
            return False

        logger.info("âœ… ëª¨ë“  ìºê¸€ íŠ¹ì§•ì´ ì•ˆì „ ê¸°ì¤€ í†µê³¼")
        return True

class SafeKaggleEnhancedSystem:
    """ì•ˆì „í•œ ìºê¸€ ê¸°ë²• í–¥ìƒ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.data_processor = UltraSafeDataProcessor()
        self.kaggle_engineer = KaggleAdvancedFeatureEngineer(safety_mode=True)
        self.leakage_detector = AutoLeakageDetector()

        # ë” ì—„ê²©í•œ ì•ˆì „ ê¸°ì¤€
        self.MAX_R2 = 0.12
        self.MAX_DIRECTION_ACC = 62.0

        logger.info("ì•ˆì „í•œ ìºê¸€ ê¸°ë²• í–¥ìƒ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")

    def run_kaggle_enhancement(self, data_path: str):
        """ìºê¸€ ê¸°ë²• ê¸°ë°˜ ì•ˆì „í•œ ì„±ëŠ¥ í–¥ìƒ"""
        logger.info("=" * 100)
        logger.info("ğŸ† ìºê¸€ ê¸°ë²• ê¸°ë°˜ ì•ˆì „í•œ ì„±ëŠ¥ í–¥ìƒ ì‹œì‘")
        logger.info("=" * 100)

        # 1. ê¸°ë³¸ ì•ˆì „ ë°ì´í„° ì¤€ë¹„
        data_dict = self.data_processor.prepare_ultra_safe_data(data_path)
        X_base, y = data_dict['X'], data_dict['y']

        # 2. ë°ì´í„° ë¶„í• 
        split_point = int(len(X_base) * 0.8)
        X_train_base = X_base[:split_point]
        X_test_base = X_base[split_point:]
        y_train = y[:split_point]
        y_test = y[split_point:]

        # 3. ìºê¸€ ê¸°ë²• ì ìš©
        logger.info("ìºê¸€ ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì ìš©")
        X_train_enhanced = self.kaggle_engineer.fit_transform(X_train_base)
        X_test_enhanced = self.kaggle_engineer.transform(X_test_base)

        # 4. íŠ¹ì§• ì•ˆì „ì„± ê²€ì¦
        is_safe = self.kaggle_engineer.validate_feature_safety(X_train_enhanced, y_train)

        if not is_safe:
            logger.error("âŒ ìºê¸€ íŠ¹ì§•ì´ ì•ˆì „ ê¸°ì¤€ì„ ì´ˆê³¼í•¨")
            return None

        # 5. ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ì„±ëŠ¥ í‰ê°€
        from sklearn.linear_model import Ridge
        model = Ridge(alpha=50.0)  # ê°•í•œ ì •ê·œí™”ë¡œ ì•ˆì „ì„± í™•ë³´
        model.fit(X_train_enhanced, y_train)
        y_pred = model.predict(X_test_enhanced)

        # 6. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        from sklearn.metrics import mean_squared_error, mean_absolute_error

        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = 1 - (mse / np.var(y_test))

        # ë°©í–¥ ì •í™•ë„
        direction_actual = (y_test > 0).astype(int)
        direction_pred = (y_pred > 0).astype(int)
        direction_accuracy = np.mean(direction_actual == direction_pred) * 100

        # 7. ì•ˆì „ì„± ìµœì¢… ê²€ì¦
        metrics = {
            'r2': r2,
            'direction_accuracy': direction_accuracy
        }

        final_safe = self.leakage_detector.validate_during_training(0, 'kaggle_enhanced', metrics)

        # ì¶”ê°€ ì•ˆì „ ê²€ì¦
        safe_performance = (r2 <= self.MAX_R2 and direction_accuracy <= self.MAX_DIRECTION_ACC)

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ† ìºê¸€ ê¸°ë²• ì„±ëŠ¥ í–¥ìƒ ê²°ê³¼")
        logger.info(f"{'='*60}")
        logger.info(f"RÂ²: {r2:.4f} ({'âœ… ì•ˆì „' if r2 <= self.MAX_R2 else 'âŒ ìœ„í—˜'})")
        logger.info(f"MAE: {mae:.4f}")
        logger.info(f"ë°©í–¥ì •í™•ë„: {direction_accuracy:.1f}% ({'âœ… ì•ˆì „' if direction_accuracy <= self.MAX_DIRECTION_ACC else 'âŒ ìœ„í—˜'})")
        logger.info(f"íŠ¹ì§• ìˆ˜: {X_train_enhanced.shape[1]}ê°œ (ê¸°ì¡´ ëŒ€ë¹„ +{X_train_enhanced.shape[1] - X_train_base.shape[1]})")

        success = final_safe and safe_performance

        logger.info(f"ìµœì¢… ì•ˆì „ì„±: {'âœ… ì„±ê³µ' if success else 'âŒ ì‹¤íŒ¨'}")

        return {
            'enhanced_features': X_train_enhanced.shape[1],
            'performance': {
                'r2': r2,
                'mae': mae,
                'direction_accuracy': direction_accuracy
            },
            'safety_check': success,
            'feature_names': self.kaggle_engineer.feature_names_,
            'model': model,
            'kaggle_engineer': self.kaggle_engineer
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    system = SafeKaggleEnhancedSystem()

    try:
        results = system.run_kaggle_enhancement(
            '/root/workspace/data/training/sp500_2020_2024_enhanced.csv'
        )

        if results is None:
            print("âŒ ìºê¸€ ê¸°ë²• ì ìš© ì‹¤íŒ¨ - ì•ˆì „ ê¸°ì¤€ ìœ„ë°˜")
            return None

        print(f"\n{'='*80}")
        print(f"ğŸ† ìºê¸€ ê¸°ë²• ê¸°ë°˜ ì„±ëŠ¥ í–¥ìƒ ì™„ë£Œ")
        print(f"{'='*80}")

        perf = results['performance']
        print(f"\nğŸ“Š í–¥ìƒëœ ì„±ëŠ¥:")
        print(f"   RÂ²: {perf['r2']:.4f}")
        print(f"   MAE: {perf['mae']:.4f}")
        print(f"   ë°©í–¥ì •í™•ë„: {perf['direction_accuracy']:.1f}%")

        print(f"\nğŸ”§ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§:")
        print(f"   ì´ íŠ¹ì§• ìˆ˜: {results['enhanced_features']}ê°œ")
        print(f"   ìºê¸€ ê¸°ë²• ì ìš©: 13ê°œ ì¶”ê°€")

        print(f"\nğŸ›¡ï¸ ì•ˆì „ì„±: {'âœ… í†µê³¼' if results['safety_check'] else 'âŒ ì‹¤íŒ¨'}")

        if results['safety_check']:
            print(f"\nğŸ‰ ì„±ê³µ: ìºê¸€ ìš°ìŠ¹ì ê¸°ë²•ìœ¼ë¡œ ì•ˆì „í•œ ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±!")
        else:
            print(f"\nâš ï¸ ì£¼ì˜: ì•ˆì „ ê¸°ì¤€ ì´ˆê³¼ - ì¶”ê°€ ë³´ì • í•„ìš”")

        return results

    except Exception as e:
        logger.error(f"ìºê¸€ ê¸°ë²• í–¥ìƒ ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    result = main()