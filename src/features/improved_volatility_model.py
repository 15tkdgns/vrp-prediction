#!/usr/bin/env python3
"""
ê°œì„ ëœ ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸
ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ê³¼ ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•œ ì„±ëŠ¥ ê°œì„ 
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
import warnings
import os
import json
from datetime import datetime

warnings.filterwarnings('ignore')

def load_enhanced_spy_data():
    """í™•ì¥ëœ SPY ë°ì´í„° ë¡œë“œ"""
    print("ğŸ“Š í™•ì¥ëœ SPY ë°ì´í„° ë¡œë“œ ì¤‘...")

    # SPY ë°ì´í„°
    spy = yf.download('SPY', start='2015-01-01', end='2024-12-31', progress=False)
    spy['returns'] = spy['Close'].pct_change()

    # VIX ë°ì´í„° (ë³€ë™ì„± ì§€ìˆ˜)
    try:
        vix = yf.download('^VIX', start='2015-01-01', end='2024-12-31', progress=False)
        vix_close = vix['Close'].reindex(spy.index, method='ffill')
        spy['vix'] = vix_close
        print("âœ… VIX ë°ì´í„° ì¶”ê°€")
    except:
        spy['vix'] = 20.0
        print("âš ï¸ VIX ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")

    spy = spy.dropna()
    print(f"âœ… í™•ì¥ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(spy)} ê´€ì¸¡ì¹˜")
    return spy

def create_comprehensive_features(data):
    """í¬ê´„ì ì¸ íŠ¹ì„± ìƒì„±"""
    print("ğŸ”§ í¬ê´„ì ì¸ íŠ¹ì„± ìƒì„± ì¤‘...")

    features = pd.DataFrame(index=data.index)
    returns = data['returns']
    prices = data['Close']
    high = data['High']
    low = data['Low']
    volume = data['Volume']

    # 1. ê¸°ë³¸ ë³€ë™ì„± íŠ¹ì„±
    for window in [5, 10, 20, 50]:
        features[f'volatility_{window}'] = returns.rolling(window).std()
        features[f'realized_vol_{window}'] = features[f'volatility_{window}'] * np.sqrt(252)

    # 2. ì§€ìˆ˜ ê°€ì¤‘ ë³€ë™ì„± (GARCH ìŠ¤íƒ€ì¼)
    for span in [5, 10, 20]:
        features[f'ewm_vol_{span}'] = returns.ewm(span=span).std()

    # 3. ë˜ê·¸ íŠ¹ì„±
    for lag in [1, 2, 3, 5, 10]:
        features[f'return_lag_{lag}'] = returns.shift(lag)
        features[f'vol_lag_{lag}'] = features['volatility_5'].shift(lag)

    # 4. ê³ -ì € ê¸°ë°˜ ë³€ë™ì„± (Garman-Klass)
    for window in [5, 10, 20]:
        gk_vol = np.log(high / low) ** 2
        features[f'garman_klass_{window}'] = gk_vol.rolling(window).mean()

    # 5. ì¼ì¤‘ ë³€ë™ì„±
    for window in [5, 10, 20]:
        intraday_range = (high - low) / prices
        features[f'intraday_vol_{window}'] = intraday_range.rolling(window).mean()

    # 6. VIX ê¸°ë°˜ íŠ¹ì„±
    if 'vix' in data.columns:
        vix = data['vix']
        features['vix_level'] = vix
        features['vix_change'] = vix.pct_change()
        for window in [5, 10, 20]:
            features[f'vix_ma_{window}'] = vix.rolling(window).mean()
            features[f'vix_std_{window}'] = vix.rolling(window).std()

    # 7. ë³¼ë¥¨ ê¸°ë°˜ íŠ¹ì„±
    volume_ma_5 = volume.rolling(5).mean()
    volume_ma_10 = volume.rolling(10).mean()
    volume_ma_20 = volume.rolling(20).mean()

    features['volume_ma_5'] = volume_ma_5
    features['volume_ma_10'] = volume_ma_10
    features['volume_ma_20'] = volume_ma_20
    features['volume_ratio_5'] = volume / (volume_ma_5 + 1e-8)
    features['volume_ratio_20'] = volume / (volume_ma_20 + 1e-8)

    # 8. ìˆ˜ìµë¥  í†µê³„
    for window in [5, 10, 20]:
        features[f'return_mean_{window}'] = returns.rolling(window).mean()
        features[f'return_skew_{window}'] = returns.rolling(window).skew()
        features[f'return_kurt_{window}'] = returns.rolling(window).kurt()

    # 9. ë³€ë™ì„± ë¹„ìœ¨
    features['vol_ratio_5_20'] = features['volatility_5'] / (features['volatility_20'] + 1e-8)
    features['vol_ratio_10_50'] = features['volatility_10'] / (features['volatility_50'] + 1e-8)

    # 10. Z-score íŠ¹ì„±
    for window in [20, 50]:
        mean_ret = returns.rolling(window).mean()
        std_ret = returns.rolling(window).std()
        features[f'return_zscore_{window}'] = (returns - mean_ret) / (std_ret + 1e-8)

        mean_vol = features['volatility_5'].rolling(window).mean()
        std_vol = features['volatility_5'].rolling(window).std()
        features[f'vol_zscore_{window}'] = (features['volatility_5'] - mean_vol) / (std_vol + 1e-8)

    # 11. ëª¨ë©˜í…€ íŠ¹ì„±
    for window in [5, 10, 20]:
        features[f'momentum_{window}'] = returns.rolling(window).sum()
        features[f'price_momentum_{window}'] = prices / prices.shift(window) - 1

    # 12. ë³€ë™ì„± ì§€ì†ì„±
    vol_5 = features['volatility_5']
    for lag in [1, 2, 3, 5]:
        # ë³€ë™ì„± ìê¸°ìƒê´€
        features[f'vol_autocorr_{lag}'] = vol_5.rolling(20).corr(vol_5.shift(lag))

    print(f"âœ… í¬ê´„ì ì¸ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
    return features

def create_interaction_features(base_features, n_top=8):
    """ìƒìœ„ íŠ¹ì„±ë“¤ì˜ ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±"""
    print(f"ğŸ”§ ìƒìœ„ {n_top}ê°œ íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš© ìƒì„± ì¤‘...")

    # ìƒìœ„ íŠ¹ì„±ë§Œ ì„ íƒ
    selected_features = base_features.iloc[:, :n_top]
    interactions = pd.DataFrame(index=base_features.index)

    # ê³±ì…ˆ ìƒí˜¸ì‘ìš©
    for i, col1 in enumerate(selected_features.columns):
        for j, col2 in enumerate(selected_features.columns[i+1:], i+1):
            if j < len(selected_features.columns):
                interactions[f'{col1}_x_{col2}'] = selected_features.iloc[:, i] * selected_features.iloc[:, j]

    # ë¹„ìœ¨ ìƒí˜¸ì‘ìš© (ìƒìœ„ 5ê°œë§Œ)
    for i in range(min(5, len(selected_features.columns))):
        for j in range(i+1, min(6, len(selected_features.columns))):
            col1 = selected_features.columns[i]
            col2 = selected_features.columns[j]
            interactions[f'{col1}_div_{col2}'] = selected_features.iloc[:, i] / (selected_features.iloc[:, j] + 1e-8)

    print(f"âœ… ìƒí˜¸ì‘ìš© íŠ¹ì„± {len(interactions.columns)}ê°œ ìƒì„±")
    return interactions

def create_future_volatility_targets(data):
    """ë¯¸ë˜ ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„±"""
    print("ğŸ¯ ë¯¸ë˜ ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„± ì¤‘...")

    targets = pd.DataFrame(index=data.index)
    returns = data['returns']

    # ë‹¤ì–‘í•œ ê¸°ê°„ì˜ ë¯¸ë˜ ë³€ë™ì„±
    for window in [1, 3, 5, 10, 20]:
        vol_values = []
        for i in range(len(returns)):
            if i + window < len(returns):
                future_window = returns.iloc[i+1:i+1+window]
                vol_values.append(future_window.std())
            else:
                vol_values.append(np.nan)
        targets[f'target_vol_{window}d'] = vol_values

    print(f"âœ… íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ: {len(targets.columns)}ê°œ")
    return targets

def test_improved_models(X, y, model_name='5ì¼ ë³€ë™ì„±'):
    """ê°œì„ ëœ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ¤– {model_name} ì˜ˆì¸¡ - ê°œì„ ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # ì™„ì „í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
    combined_data = pd.concat([X, y], axis=1).dropna()
    print(f"ìœ íš¨ ìƒ˜í”Œ ìˆ˜: {len(combined_data)}")

    if len(combined_data) < 200:
        print("âš ï¸ ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±")
        return {}

    X_clean = combined_data[X.columns]
    y_clean = combined_data[y.name]

    # ì‹œê°„ ìˆœì„œ êµì°¨ ê²€ì¦
    tscv = TimeSeriesSplit(n_splits=3)
    results = {}

    models = {
        'Ridge (Î±=0.01)': Ridge(alpha=0.01),
        'Ridge (Î±=1.0)': Ridge(alpha=1.0),
        'Ridge (Î±=100.0)': Ridge(alpha=100.0),
        'Lasso (Î±=0.001)': Lasso(alpha=0.001, max_iter=3000),
        'Lasso (Î±=0.01)': Lasso(alpha=0.01, max_iter=3000),
        'ElasticNet (Î±=0.01)': ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=3000),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=8),
        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5)
    }

    for name, model in models.items():
        scores = []
        mae_scores = []

        for train_idx, test_idx in tscv.split(X_clean):
            X_train, X_test = X_clean.iloc[train_idx], X_clean.iloc[test_idx]
            y_train, y_test = y_clean.iloc[train_idx], y_clean.iloc[test_idx]

            # ìŠ¤ì¼€ì¼ë§ (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ ì œì™¸)
            if 'Forest' not in name and 'Boosting' not in name:
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
            else:
                X_train_scaled = X_train.values
                X_test_scaled = X_test.values

            try:
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)

                score = r2_score(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)

                scores.append(score)
                mae_scores.append(mae)
            except Exception as e:
                print(f"  ëª¨ë¸ {name} ì˜¤ë¥˜: {e}")
                scores.append(-999)
                mae_scores.append(999)

        avg_score = np.mean(scores)
        std_score = np.std(scores)
        avg_mae = np.mean(mae_scores)

        results[name] = {
            'mean_r2': avg_score,
            'std_r2': std_score,
            'mean_mae': avg_mae
        }

        print(f"{name:20}: RÂ² = {avg_score:7.4f} Â± {std_score:.4f}, MAE = {avg_mae:.6f}")

    return results

def main():
    """ë©”ì¸ ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("ğŸš€ ê°œì„ ëœ ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print("=" * 60)

    # 1. í™•ì¥ëœ ë°ì´í„° ë¡œë“œ
    spy_data = load_enhanced_spy_data()

    # 2. í¬ê´„ì ì¸ íŠ¹ì„± ìƒì„±
    comprehensive_features = create_comprehensive_features(spy_data)

    # 3. íƒ€ê²Ÿ ìƒì„±
    targets = create_future_volatility_targets(spy_data)

    # 4. íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„ (ìƒìœ„ íŠ¹ì„± ì„ ë³„)
    if 'target_vol_5d' in targets.columns:
        combined_for_selection = pd.concat([comprehensive_features, targets[['target_vol_5d']]], axis=1).dropna()

        if len(combined_for_selection) > 100:
            print(f"\nğŸ“Š íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„ (ìƒ˜í”Œ ìˆ˜: {len(combined_for_selection)})")

            correlations = combined_for_selection[comprehensive_features.columns].corrwith(
                combined_for_selection['target_vol_5d']
            ).abs().sort_values(ascending=False)

            print("ìƒìœ„ 15ê°œ íŠ¹ì„±:")
            for i, (feature, corr) in enumerate(correlations.head(15).items()):
                print(f"  {i+1:2d}. {feature:25}: {corr:.4f}")

            # 5. ìƒìœ„ íŠ¹ì„± ì„ ë³„ ë° ìƒí˜¸ì‘ìš© ìƒì„±
            top_15_features = correlations.head(15).index
            top_features_df = comprehensive_features[top_15_features]

            # ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± (ìƒìœ„ 8ê°œë¡œ ì œí•œ)
            interaction_features = create_interaction_features(top_features_df, n_top=8)

            # ìµœì¢… íŠ¹ì„± ì„¸íŠ¸
            final_features = pd.concat([top_features_df, interaction_features], axis=1)
            print(f"\nğŸ“Š ìµœì¢… íŠ¹ì„± ìˆ˜: {len(final_features.columns)}")

            # 6. ëª¨ë¸ í…ŒìŠ¤íŠ¸
            results = test_improved_models(final_features, targets['target_vol_5d'])

            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
            if results:
                valid_results = {k: v for k, v in results.items() if v['mean_r2'] > -900}
                if valid_results:
                    best_model = max(valid_results.items(), key=lambda x: x[1]['mean_r2'])
                    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model[0]}")
                    print(f"   RÂ² = {best_model[1]['mean_r2']:.4f} Â± {best_model[1]['std_r2']:.4f}")
                    print(f"   MAE = {best_model[1]['mean_mae']:.6f}")

            # 7. ê²°ê³¼ ì €ì¥
            os.makedirs('results', exist_ok=True)

            improved_results = {
                'timestamp': datetime.now().isoformat(),
                'data_source': 'Enhanced SPY + VIX (2015-2024)',
                'feature_counts': {
                    'comprehensive': len(comprehensive_features.columns),
                    'top_selected': len(top_features_df.columns),
                    'interactions': len(interaction_features.columns),
                    'final_total': len(final_features.columns)
                },
                'model_results': results,
                'top_features': top_15_features.tolist(),
                'best_model': {
                    'name': best_model[0] if 'best_model' in locals() else None,
                    'performance': best_model[1] if 'best_model' in locals() else None
                }
            }

            with open('results/improved_volatility_model.json', 'w') as f:
                json.dump(improved_results, f, indent=2, default=str)

            print(f"\nğŸ’¾ ê°œì„ ëœ ëª¨ë¸ ê²°ê³¼ ì €ì¥: results/improved_volatility_model.json")

    print("=" * 60)

if __name__ == "__main__":
    main()