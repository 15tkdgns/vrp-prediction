#!/usr/bin/env python3
"""
ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ì‹œìŠ¤í…œ
CLAUDE.md 3ëŒ€ ê¸ˆê¸°ì‚¬í•­ ì¤€ìˆ˜: ë°ì´í„°ëˆ„ì¶œë¡œ ì¸í•œ ì„±ëŠ¥ 95%ì´ìƒ ë°©ì§€
"""

import numpy as np
import pandas as pd
import logging
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataLeakageChecker:
    """ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.leakage_tests = []
        self.max_safe_r2 = 0.95  # CLAUDE.md ê¸ˆê¸°ì‚¬í•­: 95% ì´ìƒ ê¸ˆì§€

    def load_data(self, data_path):
        """ë°ì´í„° ë¡œë”©"""
        logger.info("=== ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ì‹œì‘ ===")
        df = pd.read_csv(data_path)
        logger.info(f"ì›ë³¸ ë°ì´í„°: {df.shape}")
        return df

    def check_future_information_leakage(self, df):
        """ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ê²€ì‚¬"""
        logger.info("1. ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ê²€ì‚¬")

        leakage_found = False
        issues = []

        # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸
        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]
        if date_cols:
            logger.info(f"   ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {date_cols}")

            # ë‚ ì§œ ìˆœì„œ í™•ì¸
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date'])
                is_sorted = df['Date'].is_monotonic_increasing
                if not is_sorted:
                    leakage_found = True
                    issues.append("ë‚ ì§œê°€ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì§€ ì•ŠìŒ")
                    logger.warning("   âš ï¸ ë‚ ì§œê°€ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì§€ ì•ŠìŒ")
                else:
                    logger.info("   âœ… ë‚ ì§œê°€ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ë¨")

        # íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ì‹œê°„ì  ì¼ê´€ì„± í™•ì¸
        if 'Close' in df.columns:
            returns = df['Close'].pct_change()

            # ê·¹ë‹¨ì ì¸ ìˆ˜ìµë¥  íŒ¨í„´ ê²€ì‚¬
            extreme_returns = np.abs(returns) > 0.1  # 10% ì´ìƒ ë³€ë™
            if extreme_returns.sum() > len(df) * 0.05:  # 5% ì´ìƒì´ ê·¹ë‹¨ì 
                logger.warning(f"   âš ï¸ ê·¹ë‹¨ì  ìˆ˜ìµë¥  íŒ¨í„´ ê°ì§€: {extreme_returns.sum()}ê±´")

        self.leakage_tests.append({
            'test': 'future_information',
            'passed': not leakage_found,
            'issues': issues
        })

        return not leakage_found

    def check_target_variable_leakage(self, df):
        """íƒ€ê²Ÿ ë³€ìˆ˜ ëˆ„ì¶œ ê²€ì‚¬"""
        logger.info("2. íƒ€ê²Ÿ ë³€ìˆ˜ ëˆ„ì¶œ ê²€ì‚¬")

        leakage_found = False
        issues = []

        # ê°€ê²© ì •ë³´ì™€ ìˆ˜ìµë¥ ì˜ ê´€ê³„ í™•ì¸
        if 'Close' in df.columns:
            # ë‹¤ìŒë‚  ì¢…ê°€ê°€ íŠ¹ì§•ìœ¼ë¡œ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            feature_cols = [col for col in df.columns if col not in ['Date', 'Close', 'Open', 'High', 'Low', 'Volume']]

            for col in feature_cols:
                if 'close' in col.lower() and 'prev' not in col.lower() and 'lag' not in col.lower():
                    # í˜„ì¬ ì¢…ê°€ ê¸°ë°˜ì´ ì•„ë‹Œ ë¯¸ë˜ ì¢…ê°€ ê¸°ë°˜ì¸ì§€ í™•ì¸
                    corr = df[col].corr(df['Close'])
                    if abs(corr) > 0.99:  # ê±°ì˜ ì™„ë²½í•œ ìƒê´€ê´€ê³„
                        leakage_found = True
                        issues.append(f"ì»¬ëŸ¼ {col}ì´ í˜„ì¬ ì¢…ê°€ì™€ ê³¼ë„í•œ ìƒê´€ê´€ê³„: {corr:.4f}")
                        logger.warning(f"   âš ï¸ {col} - ì¢…ê°€ ìƒê´€ê´€ê³„: {corr:.4f}")

        # ìˆ˜ìµë¥  ê³„ì‚°ì˜ ì‹œê°„ì  ì¼ê´€ì„± í™•ì¸
        if 'Close' in df.columns and len(df) > 1:
            manual_returns = df['Close'].pct_change()

            # ìˆ˜ìµë¥  íŠ¹ì§•ì´ ìˆë‹¤ë©´ ê²€ì¦
            return_cols = [col for col in df.columns if 'return' in col.lower() or 'pct' in col.lower()]
            for col in return_cols:
                if col in df.columns:
                    corr = df[col].corr(manual_returns)
                    if abs(corr) > 0.98:  # ê±°ì˜ ë™ì¼í•œ íŒ¨í„´
                        logger.info(f"   âœ… {col} ì˜¬ë°”ë¥¸ ìˆ˜ìµë¥  ê³„ì‚°: ìƒê´€ê´€ê³„ {corr:.4f}")
                    else:
                        logger.warning(f"   âš ï¸ {col} ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìˆ˜ìµë¥  íŒ¨í„´: ìƒê´€ê´€ê³„ {corr:.4f}")

        self.leakage_tests.append({
            'test': 'target_variable',
            'passed': not leakage_found,
            'issues': issues
        })

        return not leakage_found

    def check_feature_independence(self, X, y):
        """íŠ¹ì§• ë…ë¦½ì„± ê²€ì‚¬"""
        logger.info("3. íŠ¹ì§•-íƒ€ê²Ÿ ë…ë¦½ì„± ê²€ì‚¬")

        leakage_found = False
        issues = []
        max_correlation = 0
        suspicious_features = []

        # ê° íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ê°„ì˜ ìƒê´€ê´€ê³„ í™•ì¸
        for i, feature_name in enumerate(range(X.shape[1])):
            corr = np.corrcoef(X[:, i], y)[0, 1]
            if not np.isnan(corr):
                max_correlation = max(max_correlation, abs(corr))

                # ê³¼ë„í•œ ìƒê´€ê´€ê³„ ê²€ì‚¬
                if abs(corr) > 0.8:  # 80% ì´ìƒ ìƒê´€ê´€ê³„
                    leakage_found = True
                    suspicious_features.append((i, corr))
                    issues.append(f"íŠ¹ì§• {i}ì™€ íƒ€ê²Ÿ ê°„ ê³¼ë„í•œ ìƒê´€ê´€ê³„: {corr:.4f}")

        logger.info(f"   ìµœëŒ€ ìƒê´€ê´€ê³„: {max_correlation:.4f}")
        if suspicious_features:
            logger.warning(f"   âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŠ¹ì§•ë“¤: {len(suspicious_features)}ê°œ")
            for feat_idx, corr in suspicious_features[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                logger.warning(f"      íŠ¹ì§• {feat_idx}: {corr:.4f}")
        else:
            logger.info("   âœ… ëª¨ë“  íŠ¹ì§•ì´ ì•ˆì „í•œ ìƒê´€ê´€ê³„ ë²”ìœ„ ë‚´")

        self.leakage_tests.append({
            'test': 'feature_independence',
            'passed': not leakage_found,
            'max_correlation': max_correlation,
            'suspicious_count': len(suspicious_features),
            'issues': issues
        })

        return not leakage_found, max_correlation

    def check_temporal_validation(self, X, y, test_size=0.2):
        """ì‹œê°„ì  ê²€ì¦ ë¬´ê²°ì„± í™•ì¸"""
        logger.info("4. ì‹œê°„ì  ê²€ì¦ ë¬´ê²°ì„± í™•ì¸")

        issues = []

        # TimeSeriesSplit ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=3)
        r2_scores = []

        scaler = StandardScaler()

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            # ì¸ë±ìŠ¤ ë¬´ê²°ì„± í™•ì¸
            if train_idx.max() >= val_idx.min():
                issues.append(f"Fold {fold}: í›ˆë ¨ ë°ì´í„°ê°€ ê²€ì¦ ë°ì´í„° ì´í›„ ì‹œì  í¬í•¨")
                logger.warning(f"   âš ï¸ Fold {fold}: ì‹œê°„ì  ìˆœì„œ ìœ„ë°˜")
                continue

            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # ìŠ¤ì¼€ì¼ë§ (ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ë°©ì§€)
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            # ë‹¨ìˆœ ì„ í˜• íšŒê·€ë¡œ ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •
            model = LinearRegression()
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_val_scaled)

            r2 = r2_score(y_val, y_pred)
            r2_scores.append(r2)

            logger.info(f"   Fold {fold}: RÂ² = {r2:.4f}")

            # ê³¼ë„í•œ ì„±ëŠ¥ ê²€ì‚¬ (CLAUDE.md ê¸ˆê¸°ì‚¬í•­)
            if r2 > self.max_safe_r2:
                issues.append(f"Fold {fold}: RÂ² {r2:.4f} > {self.max_safe_r2} (ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê³ ì„±ëŠ¥)")
                logger.error(f"   âŒ Fold {fold}: ê¸ˆê¸°ì‚¬í•­ ìœ„ë°˜ - RÂ² {r2:.4f} > {self.max_safe_r2}")

        avg_r2 = np.mean(r2_scores) if r2_scores else 0
        logger.info(f"   í‰ê·  RÂ² (ê¸°ì¤€ ëª¨ë¸): {avg_r2:.4f}")

        # ìµœì¢… ì•ˆì „ì„± í‰ê°€
        is_safe = avg_r2 < self.max_safe_r2 and len(issues) == 0

        self.leakage_tests.append({
            'test': 'temporal_validation',
            'passed': is_safe,
            'avg_r2_baseline': avg_r2,
            'max_r2': max(r2_scores) if r2_scores else 0,
            'issues': issues
        })

        return is_safe, avg_r2

    def check_data_preprocessing_leakage(self, df):
        """ë°ì´í„° ì „ì²˜ë¦¬ ëˆ„ì¶œ ê²€ì‚¬"""
        logger.info("5. ë°ì´í„° ì „ì²˜ë¦¬ ëˆ„ì¶œ ê²€ì‚¬")

        issues = []

        # ê²°ì¸¡ê°’ ì²˜ë¦¬ ë°©ë²• í™•ì¸
        missing_info = df.isnull().sum()
        if missing_info.sum() > 0:
            logger.info(f"   ê²°ì¸¡ê°’ ë°œê²¬: {missing_info.sum()}ê°œ")

            # ë¯¸ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•œ ê²°ì¸¡ê°’ ì²˜ë¦¬ ê²€ì‚¬
            for col in df.columns:
                if missing_info[col] > 0:
                    # Forward fillì´ ì•„ë‹Œ ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš© ì‹œ ê²½ê³ 
                    logger.info(f"   {col}: {missing_info[col]}ê°œ ê²°ì¸¡ê°’")

        # ì´ìƒì¹˜ ì²˜ë¦¬ ê²€ì‚¬
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col != 'Date':
                q99 = df[col].quantile(0.99)
                q01 = df[col].quantile(0.01)
                outliers = ((df[col] > q99) | (df[col] < q01)).sum()
                if outliers == 0:
                    # ì´ìƒì¹˜ê°€ ì „í˜€ ì—†ë‹¤ë©´ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì „ì²˜ë¦¬
                    logger.info(f"   {col}: ì´ìƒì¹˜ ì—†ìŒ (ê³¼ë„í•œ ì „ì²˜ë¦¬ ê°€ëŠ¥ì„±)")

        self.leakage_tests.append({
            'test': 'preprocessing',
            'passed': True,  # ì „ì²˜ë¦¬ëŠ” ê²½ê³ ë§Œ
            'issues': issues
        })

        return True

    def run_comprehensive_check(self, data_path):
        """ì¢…í•© ë°ì´í„° ëˆ„ì¶œ ê²€ì¦"""
        logger.info("=" * 80)
        logger.info("ğŸ” ì¢…í•© ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ì‹œì‘")
        logger.info("=" * 80)

        # ë°ì´í„° ë¡œë”©
        df = self.load_data(data_path)

        # 1. ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ê²€ì‚¬
        future_check = self.check_future_information_leakage(df)

        # 2. íƒ€ê²Ÿ ë³€ìˆ˜ ëˆ„ì¶œ ê²€ì‚¬
        target_check = self.check_target_variable_leakage(df)

        # 3. ë°ì´í„° ì „ì²˜ë¦¬ ëˆ„ì¶œ ê²€ì‚¬
        preprocessing_check = self.check_data_preprocessing_leakage(df)

        # ML ë°ì´í„° ì¤€ë¹„ (ê°„ë‹¨í•œ íŠ¹ì§• ìƒì„±)
        logger.info("\níŠ¹ì§• ìƒì„± ë° íƒ€ê²Ÿ ì„¤ì •...")

        # ê¸°ë³¸ íŠ¹ì§• ìƒì„± (ê³¼ê±° ì •ë³´ë§Œ ì‚¬ìš©)
        df['returns'] = df['Close'].pct_change()
        df['returns_lag1'] = df['returns'].shift(1)
        df['returns_lag2'] = df['returns'].shift(2)
        df['volatility'] = df['returns'].rolling(5).std()
        df['ma_5'] = df['Close'].rolling(5).mean()
        df['ma_20'] = df['Close'].rolling(20).mean()

        # ê²°ì¸¡ê°’ ì œê±°
        df = df.dropna()

        # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_cols = ['returns_lag1', 'returns_lag2', 'volatility']
        X = df[feature_cols].values
        y = df['returns'].values

        # 4. íŠ¹ì§• ë…ë¦½ì„± ê²€ì‚¬
        independence_check, max_corr = self.check_feature_independence(X, y)

        # 5. ì‹œê°„ì  ê²€ì¦ ë¬´ê²°ì„± í™•ì¸
        temporal_check, baseline_r2 = self.check_temporal_validation(X, y)

        # ì¢…í•© ê²°ê³¼
        self._print_comprehensive_results(baseline_r2, max_corr)

        return self._get_final_assessment()

    def _print_comprehensive_results(self, baseline_r2, max_corr):
        """ì¢…í•© ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ê²°ê³¼")
        print("=" * 80)

        print(f"\nğŸ” ê²€ì¦ ìš”ì•½:")
        for test in self.leakage_tests:
            status = "âœ… í†µê³¼" if test['passed'] else "âŒ ì‹¤íŒ¨"
            print(f"   {test['test']:20s}: {status}")
            if test['issues']:
                for issue in test['issues'][:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                    print(f"     âš ï¸ {issue}")

        print(f"\nğŸ“Š í•µì‹¬ ì§€í‘œ:")
        print(f"   ê¸°ì¤€ ëª¨ë¸ RÂ²: {baseline_r2:.4f}")
        print(f"   ìµœëŒ€ ìƒê´€ê´€ê³„: {max_corr:.4f}")
        print(f"   ì•ˆì „ ê¸°ì¤€ RÂ²: < {self.max_safe_r2}")

        # CLAUDE.md ê¸ˆê¸°ì‚¬í•­ í™•ì¸
        if baseline_r2 >= self.max_safe_r2:
            print(f"\nâŒ CLAUDE.md ê¸ˆê¸°ì‚¬í•­ ìœ„ë°˜:")
            print(f"   ë°ì´í„°ëˆ„ì¶œë¡œ ì¸í•œ ì„±ëŠ¥ {baseline_r2*100:.1f}% â‰¥ 95%")
        else:
            print(f"\nâœ… CLAUDE.md ê¸ˆê¸°ì‚¬í•­ ì¤€ìˆ˜:")
            print(f"   ì„±ëŠ¥ {baseline_r2*100:.1f}% < 95% (ì•ˆì „)")

        print("\n" + "=" * 80)

    def _get_final_assessment(self):
        """ìµœì¢… í‰ê°€"""
        passed_tests = sum(1 for test in self.leakage_tests if test['passed'])
        total_tests = len(self.leakage_tests)

        assessment = {
            'overall_passed': passed_tests == total_tests,
            'passed_tests': passed_tests,
            'total_tests': total_tests,
            'test_results': self.leakage_tests,
            'claude_md_compliant': all(
                test.get('avg_r2_baseline', 0) < self.max_safe_r2
                for test in self.leakage_tests
                if 'avg_r2_baseline' in test
            )
        }

        return assessment

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    checker = DataLeakageChecker()
    data_path = '/root/workspace/data/training/sp500_2020_2024_enhanced.csv'

    results = checker.run_comprehensive_check(data_path)

    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    print(f"   ì „ì²´ í†µê³¼: {'âœ… ì˜ˆ' if results['overall_passed'] else 'âŒ ì•„ë‹ˆì˜¤'}")
    print(f"   CLAUDE.md ì¤€ìˆ˜: {'âœ… ì˜ˆ' if results['claude_md_compliant'] else 'âŒ ì•„ë‹ˆì˜¤'}")
    print(f"   í†µê³¼ìœ¨: {results['passed_tests']}/{results['total_tests']}")

    return results

if __name__ == "__main__":
    results = main()