#!/usr/bin/env python3
"""
ğŸ” ê³ ê¸‰ ë°ì´í„° ìœ ì¶œ íƒì§€ ì‹œìŠ¤í…œ
ë¯¸ë¬˜í•œ look-ahead biasì™€ ì •ë³´ ìœ ì¶œ íƒì§€
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

class AdvancedLeakageDetector:
    """
    ê³ ê¸‰ ë°ì´í„° ìœ ì¶œ íƒì§€ê¸°
    - ë¯¸ë¬˜í•œ look-ahead bias íƒì§€
    - íŠ¹ì§• ê°„ ì‹œê°„ì  ì¼ê´€ì„± ê²€ì¦
    - ì •ë³´ ìœ ì¶œ íŒ¨í„´ ë¶„ì„
    """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.leakage_issues = []
        self.suspicious_patterns = []

    def analyze_feature_timing(self, df):
        """íŠ¹ì§•ë³„ ì‹œê°„ì  ì •ë³´ ì‚¬ìš© ë¶„ì„"""
        print("ğŸ” íŠ¹ì§•ë³„ ì‹œê°„ì  ì •ë³´ ì‚¬ìš© ë¶„ì„...")

        issues = []

        # 1. ì´ë™í‰ê·  ê²€ì¦ - ë” ì—„ê²©í•œ ê¸°ì¤€
        ma_features = [col for col in df.columns if col.startswith('MA_')]
        for ma_col in ma_features:
            window = int(ma_col.split('_')[1])

            # ì²« ë²ˆì§¸ ìœ íš¨ê°’ ìœ„ì¹˜ í™•ì¸
            first_valid = df[ma_col].first_valid_index()
            expected_first_valid = window - 1  # 0-indexed

            if first_valid < expected_first_valid:
                issues.append(f"âŒ {ma_col}: {first_valid}ë²ˆì§¸ë¶€í„° ê°’ (ìµœì†Œ {expected_first_valid}ë²ˆì§¸ë¶€í„° ìˆì–´ì•¼ í•¨)")

                # ì‹¤ì œ ê³„ì‚° ê²€ì¦
                manual_ma = df['Close'].rolling(window=window, min_periods=window).mean()
                if not df[ma_col].equals(manual_ma):
                    issues.append(f"âŒ {ma_col}: ê³„ì‚° ë°©ì‹ì´ ì•ˆì „í•˜ì§€ ì•ŠìŒ")

        # 2. RSI ê³„ì‚° ê²€ì¦
        if 'RSI' in df.columns:
            rsi_first_valid = df['RSI'].first_valid_index()
            # RSIëŠ” diff(1) + rolling(14) = 14ê°œ í–‰ì´ NaNì´ì–´ì•¼ í•¨
            expected_rsi_start = 14

            if rsi_first_valid < expected_rsi_start:
                issues.append(f"âŒ RSI: {rsi_first_valid}ë²ˆì§¸ë¶€í„° ê°’ (ìµœì†Œ {expected_rsi_start}ë²ˆì§¸ë¶€í„° ìˆì–´ì•¼ í•¨)")

        # 3. Lag íŠ¹ì§• ê²€ì¦ - ë§¤ìš° ì—„ê²©
        lag_features = [col for col in df.columns if '_lag_' in col]
        for lag_col in lag_features:
            try:
                lag_num = int(lag_col.split('_')[-1])
                first_valid = df[lag_col].first_valid_index()

                if first_valid < lag_num:
                    issues.append(f"âŒ {lag_col}: {first_valid}ë²ˆì§¸ë¶€í„° ê°’ (ìµœì†Œ {lag_num}ë²ˆì§¸ë¶€í„° ìˆì–´ì•¼ í•¨)")

                # ì›ë³¸ íŠ¹ì§•ê³¼ì˜ ì¼ê´€ì„± ê²€ì¦
                base_feature = '_'.join(lag_col.split('_')[:-2])
                if base_feature in df.columns:
                    # ìˆ˜ë™ìœ¼ë¡œ lag ê³„ì‚°
                    manual_lag = df[base_feature].shift(lag_num)
                    if not df[lag_col].fillna(-999).equals(manual_lag.fillna(-999)):
                        issues.append(f"âŒ {lag_col}: lag ê³„ì‚°ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ")

            except ValueError:
                continue

        # 4. ë³¼ë¦°ì €ë°´ë“œ ê²€ì¦
        bb_features = [col for col in df.columns if col.startswith('BB_')]
        for bb_col in bb_features:
            if bb_col.endswith('position'):
                first_valid = df[bb_col].first_valid_index()
                expected_start = 19  # 20ì¼ ìœˆë„ìš° - 1

                if first_valid < expected_start:
                    issues.append(f"âŒ {bb_col}: {first_valid}ë²ˆì§¸ë¶€í„° ê°’ (ìµœì†Œ {expected_start}ë²ˆì§¸ë¶€í„° ìˆì–´ì•¼ í•¨)")

        # 5. ATR ê²€ì¦
        if 'ATR' in df.columns:
            atr_first_valid = df['ATR'].first_valid_index()
            expected_atr_start = 14  # 14ì¼ ìœˆë„ìš°

            if atr_first_valid < expected_atr_start:
                issues.append(f"âŒ ATR: {atr_first_valid}ë²ˆì§¸ë¶€í„° ê°’ (ìµœì†Œ {expected_atr_start}ë²ˆì§¸ë¶€í„° ìˆì–´ì•¼ í•¨)")

        return issues

    def detect_future_information_usage(self, df):
        """ë¯¸ë˜ ì •ë³´ ì‚¬ìš© íƒì§€"""
        print("ğŸ” ë¯¸ë˜ ì •ë³´ ì‚¬ìš© íŒ¨í„´ íƒì§€...")

        issues = []

        # 1. Returnsì™€ íŠ¹ì§• ê°„ì˜ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ìƒê´€ê´€ê³„ ê²€ì¦
        if 'Returns' in df.columns:
            for col in df.columns:
                if col != 'Returns' and col != 'Date' and col != 'Close':
                    # í˜„ì¬ ìˆ˜ìµë¥ ê³¼ í˜„ì¬ íŠ¹ì§•ì˜ ìƒê´€ê´€ê³„ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ ì˜ì‹¬
                    corr = df['Returns'].corr(df[col])
                    if abs(corr) > 0.7:  # 70% ì´ìƒ ìƒê´€ê´€ê³„ëŠ” ì˜ì‹¬ìŠ¤ëŸ¬ì›€
                        issues.append(f"âš ï¸ {col}: Returnsì™€ ê³¼ë„í•œ ìƒê´€ê´€ê³„ {corr:.3f}")

        # 2. íŠ¹ì§•ê°’ì˜ ê¸‰ê²©í•œ ë³€í™” íŒ¨í„´ ê²€ì¦
        for col in df.columns:
            if col not in ['Date', 'Returns', 'Close', 'Volume']:
                if df[col].dtype in ['float64', 'int64']:
                    # ì²« ë²ˆì§¸ ìœ íš¨ê°’ì´ 0ì´ê±°ë‚˜ ë¹„í˜„ì‹¤ì ìœ¼ë¡œ í° ê°’ì¸ì§€ í™•ì¸
                    first_valid_idx = df[col].first_valid_index()
                    if first_valid_idx is not None:
                        first_val = df[col].iloc[first_valid_idx]
                        if abs(first_val) > 1000 or first_val == 0:
                            issues.append(f"âš ï¸ {col}: ì²« ë²ˆì§¸ ê°’ì´ ì˜ì‹¬ìŠ¤ëŸ¬ì›€ {first_val}")

        # 3. ì‹œê°„ì  ì¼ê´€ì„± ê²€ì¦
        # ê° íŠ¹ì§•ì´ ì‹œê°„ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ”ì§€ í™•ì¸
        for col in df.columns:
            if col.startswith('MA_') or col == 'RSI' or col.startswith('Volatility_'):
                if df[col].dtype in ['float64', 'int64']:
                    # ê¸‰ê²©í•œ ì í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                    diff = df[col].diff().abs()
                    if len(diff.dropna()) > 0:
                        extreme_changes = diff > diff.quantile(0.99) * 5
                        if extreme_changes.sum() > len(df) * 0.01:  # 1% ì´ìƒì´ ê·¹ê°’ì´ë©´ ì˜ì‹¬
                            issues.append(f"âš ï¸ {col}: ë¹„ì •ìƒì ì¸ ê¸‰ê²©í•œ ë³€í™” íŒ¨í„´")

        return issues

    def analyze_target_leakage(self, df):
        """íƒ€ê²Ÿ ë³€ìˆ˜ ê´€ë ¨ ë°ì´í„° ìœ ì¶œ ê²€ì¦"""
        print("ğŸ” íƒ€ê²Ÿ ë³€ìˆ˜ ê´€ë ¨ ë°ì´í„° ìœ ì¶œ ë¶„ì„...")

        issues = []

        if 'Returns' in df.columns and 'Close' in df.columns:
            # 1. Returns ê³„ì‚° ë°©ì‹ ê²€ì¦
            manual_returns = df['Close'].pct_change()

            # ì²« ë²ˆì§¸ Returns ê°’ì€ NaNì´ì–´ì•¼ í•¨
            if not pd.isna(df['Returns'].iloc[0]):
                issues.append("âŒ Returns[0]ì´ NaNì´ ì•„ë‹˜ - ì´ì „ ê°€ê²© ì •ë³´ ì‚¬ìš© ì˜ì‹¬")

            # ê³„ì‚° ì¼ê´€ì„± ê²€ì¦
            diff_count = (df['Returns'].fillna(-999) != manual_returns.fillna(-999)).sum()
            if diff_count > 0:
                issues.append(f"âŒ Returns ê³„ì‚°ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ - {diff_count}ê°œ ë¶ˆì¼ì¹˜")

        # 2. ë¯¸ë˜ ê°€ê²© ì •ë³´ ìœ ì¶œ ê²€ì¦
        # Price momentum íŠ¹ì§•ë“¤ ê²€ì¦
        momentum_cols = [col for col in df.columns if 'momentum' in col.lower()]
        for mom_col in momentum_cols:
            if '_5' in mom_col:
                expected_start = 5
            elif '_10' in mom_col:
                expected_start = 10
            else:
                continue

            first_valid = df[mom_col].first_valid_index()
            if first_valid < expected_start:
                issues.append(f"âŒ {mom_col}: {first_valid}ë²ˆì§¸ë¶€í„° ê°’ (ìµœì†Œ {expected_start}ë²ˆì§¸ë¶€í„° ìˆì–´ì•¼ í•¨)")

        return issues

    def comprehensive_leakage_audit(self, data_file):
        """í¬ê´„ì  ë°ì´í„° ìœ ì¶œ ê°ì‚¬"""
        print("ğŸ”’ í¬ê´„ì  ë°ì´í„° ìœ ì¶œ ê°ì‚¬ ì‹œì‘")
        print(f"ğŸ“ íŒŒì¼: {data_file}")

        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_file)
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date').reset_index(drop=True)

        print(f"ğŸ“Š ë°ì´í„°: {df.shape}")

        all_issues = []

        # 1. íŠ¹ì§•ë³„ ì‹œê°„ì  ì •ë³´ ì‚¬ìš© ë¶„ì„
        timing_issues = self.analyze_feature_timing(df)
        all_issues.extend(timing_issues)

        # 2. ë¯¸ë˜ ì •ë³´ ì‚¬ìš© íƒì§€
        future_info_issues = self.detect_future_information_usage(df)
        all_issues.extend(future_info_issues)

        # 3. íƒ€ê²Ÿ ë³€ìˆ˜ ê´€ë ¨ ìœ ì¶œ ê²€ì¦
        target_issues = self.analyze_target_leakage(df)
        all_issues.extend(target_issues)

        # ê²°ê³¼ ì •ë¦¬
        print(f"\\nğŸ“‹ ê³ ê¸‰ ë°ì´í„° ìœ ì¶œ ê°ì‚¬ ê²°ê³¼:")
        print(f"   ë°œê²¬ëœ ì´ìŠˆ: {len(all_issues)}ê°œ")

        if len(all_issues) == 0:
            print("   âœ… ì¶”ê°€ ë°ì´í„° ìœ ì¶œ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
        else:
            print("   âŒ ì¶”ê°€ ë°ì´í„° ìœ ì¶œ ë°œê²¬:")
            for i, issue in enumerate(all_issues, 1):
                print(f"     {i}. {issue}")

        # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
        critical_issues = [issue for issue in all_issues if issue.startswith('âŒ')]
        warning_issues = [issue for issue in all_issues if issue.startswith('âš ï¸')]

        print(f"\\nğŸ“Š ì‹¬ê°ë„ë³„ ë¶„ë¥˜:")
        print(f"   ğŸš¨ Critical (ìˆ˜ì • í•„ìš”): {len(critical_issues)}ê°œ")
        print(f"   âš ï¸ Warning (ê²€í†  í•„ìš”): {len(warning_issues)}ê°œ")

        # ê¶Œê³ ì‚¬í•­
        if len(critical_issues) > 0:
            print(f"\\nğŸ”§ ê¶Œê³ ì‚¬í•­:")
            print(f"   1. Critical ì´ìŠˆ ìš°ì„  ìˆ˜ì • í•„ìš”")
            print(f"   2. íŠ¹ì§• ìƒì„± ë¡œì§ ì¬ê²€í† ")
            print(f"   3. ë” ë³´ìˆ˜ì ì¸ ìœˆë„ìš°/Lag ì ìš©")
        elif len(warning_issues) > 0:
            print(f"\\nğŸ’¡ ê¶Œê³ ì‚¬í•­:")
            print(f"   1. Warning ì´ìŠˆ ê²€í†  ê¶Œì¥")
            print(f"   2. í˜„ì¬ ì„±ëŠ¥ì´ ê³¼ì í•©ì¼ ê°€ëŠ¥ì„± ê²€í† ")
        else:
            print(f"\\nâœ… ë°ì´í„° ë¬´ê²°ì„± ìš°ìˆ˜")

        return {
            'total_issues': len(all_issues),
            'critical_issues': len(critical_issues),
            'warning_issues': len(warning_issues),
            'all_issues': all_issues,
            'timing_issues': timing_issues,
            'future_info_issues': future_info_issues,
            'target_issues': target_issues
        }


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    detector = AdvancedLeakageDetector()
    data_file = "/root/workspace/data/training/sp500_ultra_leak_free.csv"

    results = detector.comprehensive_leakage_audit(data_file)
    return results


if __name__ == "__main__":
    results = main()