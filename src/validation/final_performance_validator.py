#!/usr/bin/env python3
"""
ğŸ† ìµœì¢… ì„±ëŠ¥ ê²€ì¦ ë° ë¹„êµ ì‹œìŠ¤í…œ

ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì„±ëŠ¥ í‰ê°€ ë° ê²€ì¦
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import glob

class FinalPerformanceValidator:
    """ìµœì¢… ì„±ëŠ¥ ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.results_dir = Path("/root/workspace/data/results")
        self.baseline_accuracy = 0.8482  # ê²€ì¦ëœ ê¸°ì¤€ì„ 
        self.target_min = 0.87
        self.target_max = 0.90

    def collect_all_results(self):
        """ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘"""
        print("ğŸ“Š ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...")

        all_results = []

        # ë‹¤ì–‘í•œ ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ë“¤ ì°¾ê¸°
        result_patterns = [
            "advanced_metric_results_*.json",
            "fast_ensemble_results_*.json",
            "improved_lstm_results_*.json",
            "enhanced_performance_results_*.json",
            "conservative_tuning_results_*.json",
            "fast_test_results_*.json"
        ]

        for pattern in result_patterns:
            files = glob.glob(str(self.results_dir / pattern))
            for file_path in files:
                try:
                    with open(file_path, 'r') as f:
                        data = json.load(f)

                    # ê²°ê³¼ ì •ê·œí™”
                    result = self._normalize_result(data, file_path)
                    if result:
                        all_results.append(result)

                except Exception as e:
                    print(f"   âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")

        print(f"   âœ… ì´ {len(all_results)}ê°œ ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘")
        return all_results

    def _normalize_result(self, data, file_path):
        """ì‹¤í—˜ ê²°ê³¼ ì •ê·œí™”"""
        try:
            # íŒŒì¼ëª…ì—ì„œ ì‹¤í—˜ íƒ€ì… ì¶”ì¶œ
            filename = Path(file_path).name
            if "advanced_metric" in filename:
                return self._normalize_advanced_metric(data)
            elif "fast_ensemble" in filename:
                return self._normalize_ensemble(data)
            elif "improved_lstm" in filename:
                return self._normalize_lstm(data)
            elif "enhanced_performance" in filename:
                return self._normalize_enhanced(data)
            elif "conservative_tuning" in filename:
                return self._normalize_tuning(data)
            elif "fast_test" in filename:
                return self._normalize_fast_test(data)

        except Exception as e:
            print(f"   âš ï¸ ì •ê·œí™” ì˜¤ë¥˜ {file_path}: {e}")
            return None

    def _normalize_advanced_metric(self, data):
        """Advanced Metric ê²°ê³¼ ì •ê·œí™”"""
        if 'model_results' not in data:
            return None

        best_acc = 0
        best_model = None

        for model_name, metrics in data['model_results'].items():
            if 'direction_accuracy' in metrics:
                acc = metrics['direction_accuracy'] / 100.0
                if acc > best_acc:
                    best_acc = acc
                    best_model = model_name

        return {
            'experiment_type': 'advanced_metric_pipeline',
            'timestamp': data.get('timestamp', ''),
            'accuracy': best_acc,
            'best_model': best_model,
            'sample_count': data.get('data_info', {}).get('sample_count', 0),
            'feature_count': data.get('data_info', {}).get('feature_count', 0),
            'status': 'completed'
        }

    def _normalize_ensemble(self, data):
        """ì•™ìƒë¸” ê²°ê³¼ ì •ê·œí™”"""
        return {
            'experiment_type': 'fast_ensemble',
            'timestamp': data.get('timestamp', ''),
            'accuracy': data.get('achieved_accuracy', 0),
            'improvement': data.get('improvement', 0),
            'sample_count': data.get('sample_count', 0),
            'feature_count': data.get('feature_count', 0),
            'status': data.get('status', 'completed')
        }

    def _normalize_lstm(self, data):
        """LSTM ê²°ê³¼ ì •ê·œí™”"""
        return {
            'experiment_type': 'improved_lstm',
            'timestamp': data.get('timestamp', ''),
            'accuracy': data.get('achieved_accuracy', 0),
            'improvement': data.get('improvement', 0),
            'sample_count': data.get('sample_count', 0),
            'feature_count': data.get('feature_count', 0),
            'status': data.get('status', 'completed')
        }

    def _normalize_enhanced(self, data):
        """Enhanced ê²°ê³¼ ì •ê·œí™”"""
        return {
            'experiment_type': 'enhanced_performance',
            'timestamp': data.get('timestamp', ''),
            'accuracy': data.get('achieved_accuracy', 0),
            'improvement': data.get('improvement', 0),
            'target_achieved': data.get('target_achieved', False),
            'sample_count': data.get('sample_count', 0),
            'feature_count': data.get('feature_count', 0),
            'status': data.get('status', 'completed')
        }

    def _normalize_tuning(self, data):
        """Tuning ê²°ê³¼ ì •ê·œí™”"""
        return {
            'experiment_type': 'conservative_tuning',
            'timestamp': data.get('timestamp', ''),
            'accuracy': data.get('achieved_accuracy', 0),
            'improvement': data.get('improvement', 0),
            'best_configuration': data.get('best_configuration', ''),
            'target_achieved': data.get('target_achieved', False),
            'status': data.get('status', 'completed')
        }

    def _normalize_fast_test(self, data):
        """Fast Test ê²°ê³¼ ì •ê·œí™”"""
        return {
            'experiment_type': 'fast_test_10min',
            'timestamp': data.get('timestamp', ''),
            'accuracy': data.get('best_accuracy', 0),
            'improvement': data.get('improvement', 0),
            'total_time': data.get('total_time_seconds', 0),
            'completed_tests': data.get('completed_tests', 0),
            'status': 'completed' if data.get('completed_tests', 0) > 0 else 'failed'
        }

    def analyze_performance(self, all_results):
        """ì„±ëŠ¥ ë¶„ì„"""
        print("\nğŸ” ì„±ëŠ¥ ë¶„ì„ ìˆ˜í–‰...")

        if not all_results:
            print("   âŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ì„±ëŠ¥ ìˆœìœ„
        valid_results = [r for r in all_results if r['accuracy'] > 0]
        if not valid_results:
            print("   âŒ ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        sorted_results = sorted(valid_results, key=lambda x: x['accuracy'], reverse=True)

        print(f"\nğŸ“Š ì‹¤í—˜ ì„±ëŠ¥ ìˆœìœ„:")
        print("-" * 80)

        for i, result in enumerate(sorted_results[:10], 1):
            improvement = (result['accuracy'] - self.baseline_accuracy) * 100
            status_icon = "ğŸ¯" if result['accuracy'] >= self.target_min else "ğŸ“Š"

            print(f"{i:2d}. {status_icon} {result['experiment_type']:25s} "
                  f"{result['accuracy']:.4f} ({improvement:+.2f}%p)")

        # í†µê³„ ë¶„ì„
        accuracies = [r['accuracy'] for r in valid_results]
        improvements = [(r['accuracy'] - self.baseline_accuracy) * 100 for r in valid_results]

        stats = {
            'total_experiments': len(all_results),
            'valid_experiments': len(valid_results),
            'best_accuracy': max(accuracies),
            'mean_accuracy': np.mean(accuracies),
            'std_accuracy': np.std(accuracies),
            'best_improvement': max(improvements),
            'mean_improvement': np.mean(improvements),
            'target_achieved_count': sum(1 for acc in accuracies if acc >= self.target_min),
            'target_achievement_rate': sum(1 for acc in accuracies if acc >= self.target_min) / len(accuracies) * 100
        }

        print(f"\nğŸ“ˆ í†µê³„ ë¶„ì„:")
        print(f"   ì´ ì‹¤í—˜ ìˆ˜: {stats['total_experiments']}ê°œ")
        print(f"   ìœ íš¨ ì‹¤í—˜ ìˆ˜: {stats['valid_experiments']}ê°œ")
        print(f"   ìµœê³  ì„±ëŠ¥: {stats['best_accuracy']:.4f}")
        print(f"   í‰ê·  ì„±ëŠ¥: {stats['mean_accuracy']:.4f} Â± {stats['std_accuracy']:.4f}")
        print(f"   ìµœê³  ê°œì„ : {stats['best_improvement']:+.2f}%p")
        print(f"   í‰ê·  ê°œì„ : {stats['mean_improvement']:+.2f}%p")
        print(f"   ëª©í‘œ ë‹¬ì„±: {stats['target_achieved_count']}/{stats['valid_experiments']}ê°œ "
              f"({stats['target_achievement_rate']:.1f}%)")

        return {
            'results_ranking': sorted_results,
            'statistics': stats,
            'analysis_timestamp': datetime.now().isoformat()
        }

    def validate_integrity(self, all_results):
        """ê²°ê³¼ ë¬´ê²°ì„± ê²€ì¦"""
        print("\nğŸ” ê²°ê³¼ ë¬´ê²°ì„± ê²€ì¦...")

        integrity_issues = []

        for result in all_results:
            # ë¹„í˜„ì‹¤ì  ì„±ëŠ¥ ì²´í¬
            if result['accuracy'] > 0.98:
                integrity_issues.append(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê³ ì„±ëŠ¥: {result['experiment_type']} = {result['accuracy']:.4f}")

            # ìŒìˆ˜ ê°œì„  ì²´í¬
            if 'improvement' in result and result['improvement'] < -50:
                integrity_issues.append(f"ê³¼ë„í•œ ì„±ëŠ¥ ì €í•˜: {result['experiment_type']} = {result['improvement']:.2f}%p")

            # ë°ì´í„° ì¼ê´€ì„± ì²´í¬
            if result.get('sample_count', 0) > 0 and result['sample_count'] < 1000:
                integrity_issues.append(f"ì ì€ ìƒ˜í”Œ ìˆ˜: {result['experiment_type']} = {result['sample_count']}ê°œ")

        if integrity_issues:
            print(f"   âš ï¸ ë°œê²¬ëœ ë¬¸ì œ:")
            for issue in integrity_issues:
                print(f"      - {issue}")
        else:
            print(f"   âœ… ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")

        return len(integrity_issues) == 0

    def generate_final_report(self):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ† ìµœì¢… ì„±ëŠ¥ ê²€ì¦ ë° ë³´ê³ ì„œ ìƒì„±")
        print("="*70)

        # ê²°ê³¼ ìˆ˜ì§‘
        all_results = self.collect_all_results()

        # ì„±ëŠ¥ ë¶„ì„
        analysis = self.analyze_performance(all_results)

        # ë¬´ê²°ì„± ê²€ì¦
        integrity_ok = self.validate_integrity(all_results)

        # ìµœì¢… í‰ê°€
        if analysis:
            best_accuracy = analysis['statistics']['best_accuracy']
            target_achieved = best_accuracy >= self.target_min

            print(f"\nğŸ¯ ìµœì¢… í‰ê°€:")
            print(f"   ê¸°ì¤€ì„ : {self.baseline_accuracy:.4f}")
            print(f"   ëª©í‘œ ë²”ìœ„: {self.target_min:.2f}-{self.target_max:.2f}")
            print(f"   ë‹¬ì„± ì„±ëŠ¥: {best_accuracy:.4f}")

            if target_achieved:
                if best_accuracy <= self.target_max:
                    print(f"   ğŸ‰ ëª©í‘œ ë‹¬ì„±! (ë²”ìœ„ ë‚´)")
                else:
                    print(f"   ğŸš€ ëª©í‘œ ì´ˆê³¼ ë‹¬ì„±!")
            else:
                needed = (self.target_min - best_accuracy) * 100
                print(f"   ğŸ“Š ëª©í‘œ ë¯¸ë‹¬ (ì¶”ê°€ {needed:.2f}%p í•„ìš”)")

        # ë³´ê³ ì„œ ì €ì¥
        final_report = {
            'validation_timestamp': datetime.now().isoformat(),
            'baseline_accuracy': self.baseline_accuracy,
            'target_range': [self.target_min, self.target_max],
            'all_results': all_results,
            'analysis': analysis,
            'integrity_passed': integrity_ok,
            'final_assessment': {
                'target_achieved': target_achieved if analysis else False,
                'best_accuracy': analysis['statistics']['best_accuracy'] if analysis else 0,
                'recommendation': self._generate_recommendation(analysis, integrity_ok)
            }
        }

        output_path = f"/root/workspace/data/final_performance_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_path, 'w') as f:
            json.dump(final_report, f, indent=2)

        print(f"\nğŸ’¾ ìµœì¢… ë³´ê³ ì„œ ì €ì¥: {output_path}")

        return final_report

    def _generate_recommendation(self, analysis, integrity_ok):
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        if not analysis or not integrity_ok:
            return "ì¶”ê°€ ê²€ì¦ í•„ìš” - ê²°ê³¼ ë¬´ê²°ì„± ë¬¸ì œ"

        best_acc = analysis['statistics']['best_accuracy']

        if best_acc >= 0.90:
            return "ì‹¤ì „ ì ìš© ê¶Œì¥ - ëª©í‘œ ì´ˆê³¼ ë‹¬ì„±"
        elif best_acc >= 0.87:
            return "ì‹¤ì „ ì ìš© ê°€ëŠ¥ - ëª©í‘œ ë‹¬ì„±"
        elif best_acc >= 0.86:
            return "ì¶”ê°€ ìµœì í™” í›„ ì ìš© - ëª©í‘œ ê·¼ì ‘"
        else:
            return "ì¶”ê°€ ì—°êµ¬ í•„ìš” - ëª©í‘œ ë¯¸ë‹¬"

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    validator = FinalPerformanceValidator()
    report = validator.generate_final_report()

    return report

if __name__ == "__main__":
    main()