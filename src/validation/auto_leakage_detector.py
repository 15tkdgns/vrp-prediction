#!/usr/bin/env python3
"""
ìë™ ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ íŒŒì´í”„ë¼ì¸
ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì¦‰ì‹œ ì°¨ë‹¨ ì‹œìŠ¤í…œ
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional, Any
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, accuracy_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AutoLeakageDetector:
    """ìë™ ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self):
        # CLAUDE.md ì¤€ìˆ˜ ê¸°ì¤€
        self.CRITICAL_R2_THRESHOLD = 0.95     # 95% ì´ìƒ ì ˆëŒ€ ê¸ˆì§€
        self.SUSPICIOUS_R2_THRESHOLD = 0.15   # 15% ì´ìƒ ì˜ì‹¬
        self.WARNING_R2_THRESHOLD = 0.10      # 10% ì´ìƒ ê²½ê³ 

        self.CRITICAL_ACC_THRESHOLD = 95.0    # 95% ì´ìƒ ì ˆëŒ€ ê¸ˆì§€
        self.SUSPICIOUS_ACC_THRESHOLD = 65.0  # 65% ì´ìƒ ì˜ì‹¬
        self.WARNING_ACC_THRESHOLD = 60.0     # 60% ì´ìƒ ê²½ê³ 

        self.MAX_CORRELATION = 0.30           # ìµœëŒ€ í—ˆìš© ìƒê´€ê´€ê³„
        self.MAX_IC = 0.08                   # ìµœëŒ€ ì •ë³´ ê³„ìˆ˜

        self.validation_history = []
        self.alert_count = 0

        logger.info("ìë™ ëˆ„ì¶œ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info(f"CLAUDE.md ê¸°ì¤€ - ê¸ˆì§€: >95%, ì˜ì‹¬: >15% RÂ² / >65% ì •í™•ë„")

    def validate_data_before_training(self, X: np.ndarray, y: np.ndarray, feature_names: List[str]) -> bool:
        """í›ˆë ¨ ì „ ë°ì´í„° ê²€ì¦"""
        logger.info("=== í›ˆë ¨ ì „ ë°ì´í„° ê²€ì¦ ===")

        validation_result = {
            'stage': 'pre_training',
            'timestamp': pd.Timestamp.now(),
            'passed': True,
            'issues': []
        }

        # 1. ê¸°ë³¸ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        issues = self._check_data_quality(X, y, feature_names)
        validation_result['issues'].extend(issues)

        # 2. íŠ¹ì§•-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ê²€ì‚¬
        correlation_issues = self._check_feature_target_correlation(X, y, feature_names)
        validation_result['issues'].extend(correlation_issues)

        # 3. ì •ë³´ ê³„ìˆ˜ ê²€ì‚¬
        ic_issues = self._check_information_coefficient(X, y)
        validation_result['issues'].extend(ic_issues)

        # 4. ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì •
        baseline_metrics = self._measure_baseline_performance(X, y)
        validation_result['baseline_metrics'] = baseline_metrics

        # 5. ê²€ì¦ ê²°ê³¼ í‰ê°€
        validation_result['passed'] = len(validation_result['issues']) == 0

        # ê¸°ì¤€ì„  ì„±ëŠ¥ ê²€ì‚¬
        if baseline_metrics.get('r2', 0) > self.SUSPICIOUS_R2_THRESHOLD:
            validation_result['issues'].append(f"ê¸°ì¤€ì„  RÂ² ê³¼ë„: {baseline_metrics['r2']:.3f}")
            validation_result['passed'] = False

        if baseline_metrics.get('direction_acc', 0) > self.SUSPICIOUS_ACC_THRESHOLD:
            validation_result['issues'].append(f"ê¸°ì¤€ì„  ì •í™•ë„ ê³¼ë„: {baseline_metrics['direction_acc']:.1f}%")
            validation_result['passed'] = False

        # ê²°ê³¼ ê¸°ë¡
        self.validation_history.append(validation_result)

        # ê²°ê³¼ ì¶œë ¥
        if validation_result['passed']:
            logger.info("âœ… í›ˆë ¨ ì „ ê²€ì¦ í†µê³¼")
        else:
            logger.error("âŒ í›ˆë ¨ ì „ ê²€ì¦ ì‹¤íŒ¨")
            for issue in validation_result['issues']:
                logger.error(f"   - {issue}")

        return validation_result['passed']

    def validate_during_training(self, fold: int, model_name: str, metrics: Dict[str, float]) -> bool:
        """í›ˆë ¨ ì¤‘ ì‹¤ì‹œê°„ ê²€ì¦"""
        logger.info(f"=== í›ˆë ¨ ì¤‘ ê²€ì¦ (Fold {fold}, {model_name}) ===")

        validation_result = {
            'stage': 'during_training',
            'fold': fold,
            'model_name': model_name,
            'timestamp': pd.Timestamp.now(),
            'metrics': metrics,
            'status': 'PASS',
            'issues': []
        }

        # 1. RÂ² ê²€ì‚¬ (íšŒê·€ ëª¨ë¸)
        r2 = metrics.get('r2', 0)
        if r2 > self.CRITICAL_R2_THRESHOLD:
            validation_result['status'] = 'CRITICAL_STOP'
            validation_result['issues'].append(f"CLAUDE.md ìœ„ë°˜: RÂ² {r2:.3f} > {self.CRITICAL_R2_THRESHOLD}")
            self.alert_count += 1

        elif r2 > self.SUSPICIOUS_R2_THRESHOLD:
            validation_result['status'] = 'SUSPICIOUS'
            validation_result['issues'].append(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ RÂ²: {r2:.3f} > {self.SUSPICIOUS_R2_THRESHOLD}")

        elif r2 > self.WARNING_R2_THRESHOLD:
            validation_result['status'] = 'WARNING'
            validation_result['issues'].append(f"ê²½ê³  RÂ²: {r2:.3f} > {self.WARNING_R2_THRESHOLD}")

        # 2. ì •í™•ë„ ê²€ì‚¬ (ë¶„ë¥˜ ëª¨ë¸)
        accuracy = metrics.get('accuracy', 0) * 100  # ë°±ë¶„ìœ¨ ë³€í™˜
        direction_acc = metrics.get('direction_accuracy', 0)

        max_acc = max(accuracy, direction_acc)

        if max_acc > self.CRITICAL_ACC_THRESHOLD:
            validation_result['status'] = 'CRITICAL_STOP'
            validation_result['issues'].append(f"CLAUDE.md ìœ„ë°˜: ì •í™•ë„ {max_acc:.1f}% > {self.CRITICAL_ACC_THRESHOLD}%")
            self.alert_count += 1

        elif max_acc > self.SUSPICIOUS_ACC_THRESHOLD:
            validation_result['status'] = 'SUSPICIOUS'
            validation_result['issues'].append(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì •í™•ë„: {max_acc:.1f}% > {self.SUSPICIOUS_ACC_THRESHOLD}%")

        elif max_acc > self.WARNING_ACC_THRESHOLD:
            validation_result['status'] = 'WARNING'
            validation_result['issues'].append(f"ê²½ê³  ì •í™•ë„: {max_acc:.1f}% > {self.WARNING_ACC_THRESHOLD}%")

        # 3. ê²°ê³¼ ê¸°ë¡
        self.validation_history.append(validation_result)

        # 4. ê²°ê³¼ ì¶œë ¥
        status_icon = {
            'PASS': 'âœ…',
            'WARNING': 'âš ï¸',
            'SUSPICIOUS': 'ğŸš¨',
            'CRITICAL_STOP': 'âŒ'
        }

        logger.info(f"{status_icon[validation_result['status']]} {validation_result['status']}")

        if validation_result['issues']:
            for issue in validation_result['issues']:
                logger.warning(f"   - {issue}")

        # 5. ì¤‘ë‹¨ ì—¬ë¶€ ê²°ì •
        should_stop = validation_result['status'] == 'CRITICAL_STOP'

        if should_stop:
            logger.error(f"ğŸ›‘ í›ˆë ¨ ì¤‘ë‹¨ í•„ìš”: {model_name} (Fold {fold})")

        return not should_stop  # Falseë©´ ì¤‘ë‹¨

    def validate_after_training(self, all_results: Dict[str, Dict]) -> bool:
        """í›ˆë ¨ í›„ ì¢…í•© ê²€ì¦"""
        logger.info("=== í›ˆë ¨ í›„ ì¢…í•© ê²€ì¦ ===")

        validation_result = {
            'stage': 'post_training',
            'timestamp': pd.Timestamp.now(),
            'model_count': len(all_results),
            'passed_models': [],
            'failed_models': [],
            'overall_status': 'PASS'
        }

        for model_name, results in all_results.items():
            model_status = self._validate_single_model_results(model_name, results)

            if model_status['passed']:
                validation_result['passed_models'].append(model_name)
            else:
                validation_result['failed_models'].append({
                    'name': model_name,
                    'issues': model_status['issues']
                })

        # ì „ì²´ í‰ê°€
        if validation_result['failed_models']:
            validation_result['overall_status'] = 'FAILED'

        # ê²°ê³¼ ê¸°ë¡
        self.validation_history.append(validation_result)

        # ê²°ê³¼ ì¶œë ¥
        logger.info(f"í†µê³¼ ëª¨ë¸: {len(validation_result['passed_models'])}ê°œ")
        logger.info(f"ì‹¤íŒ¨ ëª¨ë¸: {len(validation_result['failed_models'])}ê°œ")

        if validation_result['failed_models']:
            logger.error("ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:")
            for failed in validation_result['failed_models']:
                logger.error(f"   - {failed['name']}: {failed['issues']}")

        return validation_result['overall_status'] == 'PASS'

    def _check_data_quality(self, X: np.ndarray, y: np.ndarray, feature_names: List[str]) -> List[str]:
        """ê¸°ë³¸ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
        issues = []

        # 1. ë¬´í•œê°’/NaN ê²€ì‚¬
        if np.any(np.isinf(X)) or np.any(np.isnan(X)):
            issues.append("íŠ¹ì§•ì— ë¬´í•œê°’ ë˜ëŠ” NaN í¬í•¨")

        if np.any(np.isinf(y)) or np.any(np.isnan(y)):
            issues.append("íƒ€ê²Ÿì— ë¬´í•œê°’ ë˜ëŠ” NaN í¬í•¨")

        # 2. ë°ì´í„° í¬ê¸° ê²€ì‚¬
        if len(X) < 200:
            issues.append(f"ë°ì´í„° í¬ê¸° ë¶€ì¡±: {len(X)}ê°œ (ìµœì†Œ 200ê°œ í•„ìš”)")

        # 3. íŠ¹ì§• ê°œìˆ˜ ê²€ì‚¬
        if X.shape[1] > 100:
            issues.append(f"íŠ¹ì§• ê°œìˆ˜ ê³¼ë‹¤: {X.shape[1]}ê°œ (ê¶Œì¥ ìµœëŒ€ 50ê°œ)")

        # 4. íƒ€ê²Ÿ ë¶„ì‚° ê²€ì‚¬
        if np.std(y) < 1e-6:
            issues.append("íƒ€ê²Ÿ ë¶„ì‚°ì´ ê±°ì˜ 0 (ìƒìˆ˜ íƒ€ê²Ÿ ì˜ì‹¬)")

        return issues

    def _check_feature_target_correlation(self, X: np.ndarray, y: np.ndarray, feature_names: List[str]) -> List[str]:
        """íŠ¹ì§•-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ê²€ì‚¬"""
        issues = []

        max_corr = 0
        suspicious_features = []

        for i in range(X.shape[1]):
            corr = abs(np.corrcoef(X[:, i], y)[0, 1])

            if not np.isnan(corr):
                max_corr = max(max_corr, corr)

                if corr > self.MAX_CORRELATION:
                    feature_name = feature_names[i] if i < len(feature_names) else f"feature_{i}"
                    suspicious_features.append((feature_name, corr))

        if suspicious_features:
            issues.append(f"ê³¼ë„í•œ ìƒê´€ê´€ê³„ íŠ¹ì§•: {len(suspicious_features)}ê°œ")
            for name, corr in suspicious_features[:5]:
                issues.append(f"   {name}: {corr:.3f}")

        logger.info(f"ìµœëŒ€ íŠ¹ì§•-íƒ€ê²Ÿ ìƒê´€ê´€ê³„: {max_corr:.3f}")

        return issues

    def _check_information_coefficient(self, X: np.ndarray, y: np.ndarray) -> List[str]:
        """ì •ë³´ ê³„ìˆ˜ ê²€ì‚¬"""
        issues = []

        from scipy.stats import spearmanr

        ic_values = []
        for i in range(X.shape[1]):
            try:
                ic, _ = spearmanr(X[:, i], y)
                if not np.isnan(ic):
                    ic_values.append(abs(ic))
            except:
                continue

        if ic_values:
            max_ic = max(ic_values)
            avg_ic = np.mean(ic_values)

            logger.info(f"ì •ë³´ ê³„ìˆ˜ - ìµœëŒ€: {max_ic:.3f}, í‰ê· : {avg_ic:.3f}")

            if max_ic > self.MAX_IC:
                issues.append(f"ê³¼ë„í•œ ì •ë³´ ê³„ìˆ˜: {max_ic:.3f} > {self.MAX_IC}")

        return issues

    def _measure_baseline_performance(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì •"""
        logger.info("ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì •")

        # ê°„ë‹¨í•œ ì‹œê°„ ë¶„í• 
        split_point = int(len(X) * 0.8)
        X_train, X_test = X[:split_point], X[split_point:]
        y_train, y_test = y[:split_point], y[split_point:]

        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ì„ í˜• íšŒê·€ ê¸°ì¤€ì„ 
        model = LinearRegression()
        model.fit(X_train_scaled, y_train)

        y_pred = model.predict(X_test_scaled)

        # íšŒê·€ ì§€í‘œ
        r2 = r2_score(y_test, y_pred)

        # ë°©í–¥ ì •í™•ë„ (íšŒê·€ì—ì„œë„ ì¸¡ì •)
        direction_correct = np.sum(np.sign(y_test) == np.sign(y_pred))
        direction_acc = direction_correct / len(y_test) * 100

        metrics = {
            'r2': r2,
            'direction_acc': direction_acc,
            'samples': len(y_test)
        }

        logger.info(f"ê¸°ì¤€ì„  ì„±ëŠ¥ - RÂ²: {r2:.3f}, ë°©í–¥ì •í™•ë„: {direction_acc:.1f}%")

        return metrics

    def _validate_single_model_results(self, model_name: str, results: Dict) -> Dict:
        """ê°œë³„ ëª¨ë¸ ê²°ê³¼ ê²€ì¦"""
        validation = {
            'model_name': model_name,
            'passed': True,
            'issues': []
        }

        # í‰ê·  ì„±ëŠ¥ ì¶”ì¶œ
        avg_r2 = results.get('mean_r2', 0)
        avg_acc = results.get('mean_accuracy', 0) * 100
        avg_direction = results.get('mean_direction_accuracy', 0)

        # RÂ² ê²€ì¦
        if avg_r2 > self.CRITICAL_R2_THRESHOLD:
            validation['passed'] = False
            validation['issues'].append(f"CLAUDE.md ìœ„ë°˜ RÂ²: {avg_r2:.3f}")

        elif avg_r2 > self.SUSPICIOUS_R2_THRESHOLD:
            validation['passed'] = False
            validation['issues'].append(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ RÂ²: {avg_r2:.3f}")

        # ì •í™•ë„ ê²€ì¦
        max_acc = max(avg_acc, avg_direction)
        if max_acc > self.CRITICAL_ACC_THRESHOLD:
            validation['passed'] = False
            validation['issues'].append(f"CLAUDE.md ìœ„ë°˜ ì •í™•ë„: {max_acc:.1f}%")

        elif max_acc > self.SUSPICIOUS_ACC_THRESHOLD:
            validation['passed'] = False
            validation['issues'].append(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì •í™•ë„: {max_acc:.1f}%")

        return validation

    def generate_validation_report(self) -> Dict:
        """ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        logger.info("=== ê²€ì¦ ë³´ê³ ì„œ ìƒì„± ===")

        report = {
            'summary': {
                'total_validations': len(self.validation_history),
                'alert_count': self.alert_count,
                'last_validation': self.validation_history[-1]['timestamp'] if self.validation_history else None
            },
            'stage_summary': {},
            'detailed_history': self.validation_history
        }

        # ë‹¨ê³„ë³„ ìš”ì•½
        stages = ['pre_training', 'during_training', 'post_training']
        for stage in stages:
            stage_validations = [v for v in self.validation_history if v.get('stage') == stage]
            report['stage_summary'][stage] = {
                'count': len(stage_validations),
                'passed': sum(1 for v in stage_validations if v.get('passed', True) or v.get('status') in ['PASS', 'WARNING']),
                'failed': sum(1 for v in stage_validations if not v.get('passed', True) or v.get('status') in ['SUSPICIOUS', 'CRITICAL_STOP'])
            }

        # ë³´ê³ ì„œ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ğŸ“‹ ìë™ ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ë³´ê³ ì„œ")
        print("=" * 80)

        print(f"\nğŸ“Š ê²€ì¦ ìš”ì•½:")
        print(f"   ì´ ê²€ì¦ íšŸìˆ˜: {report['summary']['total_validations']}")
        print(f"   ê²½ê³  ë°œìƒ: {report['summary']['alert_count']}")

        for stage, summary in report['stage_summary'].items():
            print(f"\n{stage.replace('_', ' ').title()}:")
            print(f"   ê²€ì¦ ìˆ˜í–‰: {summary['count']}")
            print(f"   í†µê³¼: {summary['passed']}")
            print(f"   ì‹¤íŒ¨: {summary['failed']}")

        # ìµœê·¼ ì´ìŠˆë“¤
        recent_issues = []
        for validation in self.validation_history[-5:]:  # ìµœê·¼ 5ê°œ
            if validation.get('issues'):
                recent_issues.extend(validation['issues'])

        if recent_issues:
            print(f"\nâš ï¸ ìµœê·¼ ë°œê²¬ëœ ì´ìŠˆë“¤:")
            for issue in recent_issues[:10]:  # ìµœëŒ€ 10ê°œ
                print(f"   - {issue}")

        print("\n" + "=" * 80)

        return report

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    detector = AutoLeakageDetector()

    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    X = np.random.randn(1000, 10)
    y = np.random.randn(1000)
    feature_names = [f"feature_{i}" for i in range(10)]

    # 1. í›ˆë ¨ ì „ ê²€ì¦
    pre_valid = detector.validate_data_before_training(X, y, feature_names)
    print(f"í›ˆë ¨ ì „ ê²€ì¦: {'í†µê³¼' if pre_valid else 'ì‹¤íŒ¨'}")

    # 2. í›ˆë ¨ ì¤‘ ê²€ì¦ ì‹œë®¬ë ˆì´ì…˜
    test_metrics = [
        {'r2': 0.05, 'accuracy': 0.55, 'direction_accuracy': 55.0},  # ì •ìƒ
        {'r2': 0.12, 'accuracy': 0.62, 'direction_accuracy': 62.0},  # ê²½ê³ 
        {'r2': 0.20, 'accuracy': 0.70, 'direction_accuracy': 70.0},  # ì˜ì‹¬
        {'r2': 0.96, 'accuracy': 0.96, 'direction_accuracy': 96.0},  # ì¤‘ë‹¨
    ]

    for i, metrics in enumerate(test_metrics):
        result = detector.validate_during_training(i, f"test_model_{i}", metrics)
        print(f"ëª¨ë¸ {i} ê²€ì¦: {'ê³„ì†' if result else 'ì¤‘ë‹¨'}")

    # 3. ê²€ì¦ ë³´ê³ ì„œ
    report = detector.generate_validation_report()

    return detector

if __name__ == "__main__":
    detector = main()