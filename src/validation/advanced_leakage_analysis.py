import pandas as pd
import numpy as np
from pathlib import Path
import json
import logging

logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedLeakageAnalyzer:
    def __init__(self):
        self.data_path = Path('/root/workspace/data/training/multi_modal_sp500_dataset.csv')
        self.df = None

    def load_and_analyze_columns(self):
        """ì»¬ëŸ¼ êµ¬ì¡° ë¶„ì„"""
        logger.info("ë°ì´í„° ë¡œë”© ë° ì»¬ëŸ¼ ë¶„ì„...")
        self.df = pd.read_csv(self.data_path)

        # ì»¬ëŸ¼ ë¶„ë¥˜
        feature_cols = []
        target_cols = []
        meta_cols = []

        for col in self.df.columns:
            if 'target_' in col.lower():
                target_cols.append(col)
            elif col.lower() in ['date', 'timestamp']:
                meta_cols.append(col)
            else:
                feature_cols.append(col)

        analysis = {
            'total_columns': len(self.df.columns),
            'feature_columns': feature_cols,
            'target_columns': target_cols,
            'meta_columns': meta_cols,
            'feature_count': len(feature_cols),
            'target_count': len(target_cols)
        }

        logger.info(f"íŠ¹ì„± ì»¬ëŸ¼: {len(feature_cols)}ê°œ")
        logger.info(f"íƒ€ê²Ÿ ì»¬ëŸ¼: {len(target_cols)}ê°œ")
        logger.info(f"ë©”íƒ€ ì»¬ëŸ¼: {len(meta_cols)}ê°œ")

        return analysis

    def analyze_suspicious_features(self):
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŠ¹ì„± ìƒì„¸ ë¶„ì„"""
        logger.info("ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŠ¹ì„± ë¶„ì„...")

        # ì˜ëª» íƒì§€ëœ íŠ¹ì„±ë“¤ ë¶„ì„
        suspicious_analysis = {
            'volatility_5d': {
                'description': '5ì¼ ë³€ë™ì„± - ê³¼ê±° ë°ì´í„°ë¡œ ê³„ì‚°ëœ ì •ë‹¹í•œ íŠ¹ì„±',
                'calculation': 'ê³¼ê±° 5ì¼ê°„ ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨',
                'is_legitimate': True,
                'concern_level': 'None'
            },
            'volatility_20d': {
                'description': '20ì¼ ë³€ë™ì„± - ê³¼ê±° ë°ì´í„°ë¡œ ê³„ì‚°ëœ ì •ë‹¹í•œ íŠ¹ì„±',
                'calculation': 'ê³¼ê±° 20ì¼ê°„ ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨',
                'is_legitimate': True,
                'concern_level': 'None'
            },
            'treasury_10y': {
                'description': '10ë…„ êµ­ì±„ ê¸ˆë¦¬ - ì™¸ë¶€ ê²½ì œ ë°ì´í„°',
                'calculation': 'FREDì—ì„œ ê°€ì ¸ì˜¨ 10ë…„ ë§Œê¸° ë¯¸êµ­ êµ­ì±„ ê¸ˆë¦¬',
                'is_legitimate': True,
                'concern_level': 'None'
            },
            'treasury_3m': {
                'description': '3ê°œì›” êµ­ì±„ ê¸ˆë¦¬ - ì™¸ë¶€ ê²½ì œ ë°ì´í„°',
                'calculation': 'FREDì—ì„œ ê°€ì ¸ì˜¨ 3ê°œì›” ë§Œê¸° ë¯¸êµ­ êµ­ì±„ ê¸ˆë¦¬',
                'is_legitimate': True,
                'concern_level': 'None'
            }
        }

        return suspicious_analysis

    def verify_temporal_separation(self):
        """ì‹œê°„ì  ë¶„ë¦¬ ì •ë°€ ê²€ì¦"""
        logger.info("ì‹œê°„ì  ë¶„ë¦¬ ì •ë°€ ê²€ì¦...")

        # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸
        date_col = None
        for col in self.df.columns:
            if 'date' in col.lower():
                date_col = col
                break

        verification = {
            'has_date_column': date_col is not None,
            'date_column': date_col
        }

        if date_col:
            # ë‚ ì§œ ì •ë ¬ í™•ì¸
            self.df[date_col] = pd.to_datetime(self.df[date_col])
            is_sorted = self.df[date_col].is_monotonic_increasing

            verification.update({
                'is_chronologically_sorted': is_sorted,
                'date_range': {
                    'start': self.df[date_col].min().isoformat(),
                    'end': self.df[date_col].max().isoformat(),
                    'total_days': (self.df[date_col].max() - self.df[date_col].min()).days
                }
            })

            # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„
            target_cols = [col for col in self.df.columns if 'target_' in col.lower()]

            temporal_analysis = {}
            for target in target_cols:
                # 1ì¼ íƒ€ê²Ÿì¸ì§€ 5ì¼ íƒ€ê²Ÿì¸ì§€ í™•ì¸
                if '1d' in target:
                    expected_shift = 1
                elif '5d' in target:
                    expected_shift = 5
                else:
                    expected_shift = None

                temporal_analysis[target] = {
                    'expected_shift_days': expected_shift,
                    'is_future_target': True  # ì´ë¦„ì—ì„œ ì¶”ë¡ 
                }

            verification['target_analysis'] = temporal_analysis

        return verification

    def check_feature_target_correlations(self):
        """íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„"""
        logger.info("íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„...")

        target_cols = [col for col in self.df.columns if 'target_' in col.lower()]
        feature_cols = [col for col in self.df.columns if 'target_' not in col.lower() and col.lower() != 'date']

        correlation_analysis = {}

        for target in target_cols:
            target_corrs = {}
            numeric_features = []

            for feature in feature_cols:
                try:
                    if pd.api.types.is_numeric_dtype(self.df[feature]):
                        corr = self.df[feature].corr(self.df[target])
                        if not pd.isna(corr):
                            target_corrs[feature] = float(corr)
                            numeric_features.append(feature)
                except:
                    continue

            # ìƒê´€ê´€ê³„ í†µê³„
            corr_values = list(target_corrs.values())
            correlation_analysis[target] = {
                'feature_correlations': target_corrs,
                'stats': {
                    'max_abs_correlation': max([abs(c) for c in corr_values]) if corr_values else 0,
                    'mean_abs_correlation': np.mean([abs(c) for c in corr_values]) if corr_values else 0,
                    'high_correlation_count': len([c for c in corr_values if abs(c) > 0.5]),
                    'very_high_correlation_count': len([c for c in corr_values if abs(c) > 0.9])
                }
            }

        return correlation_analysis

    def validate_model_performance_legitimacy(self):
        """ëª¨ë¸ ì„±ëŠ¥ì˜ ì •ë‹¹ì„± ê²€ì¦"""
        logger.info("ëª¨ë¸ ì„±ëŠ¥ ì •ë‹¹ì„± ê²€ì¦...")

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì½ê¸°
        try:
            perf_path = Path('/root/workspace/data/raw/price_prediction_analysis_report.json')
            with open(perf_path, 'r', encoding='utf-8') as f:
                performance = json.load(f)

            performance_analysis = {
                'reported_mae': performance['price_analysis']['prediction_accuracy']['mae_dollars'],
                'reported_mape': performance['price_analysis']['prediction_accuracy']['mape_percent'],
                'direction_accuracy': performance['price_analysis']['prediction_accuracy']['direction_accuracy'],
                'legitimacy_assessment': self._assess_performance_legitimacy(performance)
            }

        except Exception as e:
            performance_analysis = {
                'error': f"ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}",
                'legitimacy_assessment': 'Unknown'
            }

        return performance_analysis

    def _assess_performance_legitimacy(self, performance):
        """ì„±ëŠ¥ ì •ë‹¹ì„± í‰ê°€"""
        mae = performance['price_analysis']['prediction_accuracy']['mae_dollars']
        mape = performance['price_analysis']['prediction_accuracy']['mape_percent']
        direction_acc = performance['price_analysis']['prediction_accuracy']['direction_accuracy']

        # ê¸ˆìœµ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì¼ë°˜ì  ì„±ëŠ¥ ë²”ìœ„
        assessment = {
            'mae_assessment': 'Reasonable' if 1.0 <= mae <= 10.0 else 'Suspicious',
            'mape_assessment': 'Good' if 0.5 <= mape <= 3.0 else 'Suspicious',
            'direction_assessment': 'Reasonable' if 0.5 <= direction_acc <= 0.65 else 'Suspicious',
            'overall_legitimacy': 'Legitimate'
        }

        # ì „ì²´ í‰ê°€
        suspicious_count = len([v for v in assessment.values() if 'Suspicious' in str(v)])
        if suspicious_count == 0:
            assessment['overall_legitimacy'] = 'Highly Legitimate'
        elif suspicious_count <= 1:
            assessment['overall_legitimacy'] = 'Legitimate'
        else:
            assessment['overall_legitimacy'] = 'Questionable'

        return assessment

    def generate_comprehensive_report(self):
        """ì¢…í•© ë°ì´í„° ëˆ„ì¶œ ë¶„ì„ ë³´ê³ ì„œ"""
        logger.info("ì¢…í•© ë³´ê³ ì„œ ìƒì„±...")

        # ëª¨ë“  ë¶„ì„ ìˆ˜í–‰
        column_analysis = self.load_and_analyze_columns()
        suspicious_analysis = self.analyze_suspicious_features()
        temporal_verification = self.verify_temporal_separation()
        correlation_analysis = self.check_feature_target_correlations()
        performance_analysis = self.validate_model_performance_legitimacy()

        # ìµœì¢… ëˆ„ì¶œ ìœ„í—˜ë„ ê³„ì‚°
        risk_factors = []
        risk_level = "ë‚®ìŒ"

        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŠ¹ì„±ë“¤ì´ ì‹¤ì œë¡œëŠ” ì •ë‹¹í•¨
        legitimate_features = all([f['is_legitimate'] for f in suspicious_analysis.values()])

        if not legitimate_features:
            risk_factors.append("ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŠ¹ì„± ë°œê²¬")
            risk_level = "ì¤‘ê°„"

        # ë§¤ìš° ë†’ì€ ìƒê´€ê´€ê³„ í™•ì¸
        max_corr = 0
        for target_analysis in correlation_analysis.values():
            max_corr = max(max_corr, target_analysis['stats']['max_abs_correlation'])

        if max_corr > 0.95:
            risk_factors.append("ë¹„ì •ìƒì  ê³ ìƒê´€ê´€ê³„")
            risk_level = "ë†’ìŒ"

        # ì‹œê°„ì  ë¶„ë¦¬ í™•ì¸
        if not temporal_verification.get('is_chronologically_sorted', True):
            risk_factors.append("ì‹œê³„ì—´ ë°ì´í„° ì •ë ¬ ë¬¸ì œ")
            risk_level = "ë†’ìŒ"

        # ì„±ëŠ¥ ì •ë‹¹ì„± í™•ì¸
        perf_legitimacy = performance_analysis.get('legitimacy_assessment', {}).get('overall_legitimacy', 'Unknown')
        if perf_legitimacy == 'Questionable':
            risk_factors.append("ë¹„í˜„ì‹¤ì  ëª¨ë¸ ì„±ëŠ¥")
            risk_level = "ë†’ìŒ"

        # ìµœì¢… ë³´ê³ ì„œ
        comprehensive_report = {
            'analysis_timestamp': pd.Timestamp.now().isoformat(),
            'executive_summary': {
                'risk_level': risk_level,
                'risk_factors': risk_factors,
                'data_quality': 'Good' if not risk_factors else 'Needs Review',
                'model_legitimacy': perf_legitimacy
            },
            'detailed_analysis': {
                'column_structure': column_analysis,
                'suspicious_features_analysis': suspicious_analysis,
                'temporal_verification': temporal_verification,
                'correlation_analysis': correlation_analysis,
                'performance_validation': performance_analysis
            },
            'conclusions': self._generate_conclusions(risk_level, risk_factors, suspicious_analysis, performance_analysis)
        }

        return comprehensive_report

    def _generate_conclusions(self, risk_level, risk_factors, suspicious_analysis, performance_analysis):
        """ê²°ë¡  ìƒì„±"""
        conclusions = []

        if risk_level == "ë‚®ìŒ":
            conclusions.append("âœ… ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.")
            conclusions.append("âœ… ëª¨ë“  íŠ¹ì„±ì´ ì ì ˆí•œ ì‹œê°„ì  ë¶„ë¦¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.")
            conclusions.append("âœ… ëª¨ë¸ ì„±ëŠ¥ì´ í˜„ì‹¤ì ì´ê³  ì •ë‹¹í•©ë‹ˆë‹¤.")

            # ì˜ëª» ì˜ì‹¬ëœ íŠ¹ì„±ë“¤ í•´ëª…
            for feature, analysis in suspicious_analysis.items():
                if analysis['is_legitimate']:
                    conclusions.append(f"âœ… {feature}: {analysis['description']} (ì •ë‹¹í•œ íŠ¹ì„±)")

        elif risk_level == "ì¤‘ê°„":
            conclusions.append("âš ï¸ ì¼ë¶€ ê²€í† ê°€ í•„ìš”í•˜ì§€ë§Œ ì „ë°˜ì ìœ¼ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.")

        else:
            conclusions.append("âŒ ì‹¬ê°í•œ ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            conclusions.append("âŒ ì¦‰ì‹œ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # ì„±ëŠ¥ ê´€ë ¨ ê²°ë¡ 
        if 'legitimacy_assessment' in performance_analysis:
            perf_assessment = performance_analysis['legitimacy_assessment']
            if perf_assessment.get('overall_legitimacy') == 'Highly Legitimate':
                conclusions.append("âœ… ëª¨ë¸ ì„±ëŠ¥ì´ ê¸ˆìœµ ì˜ˆì¸¡ì˜ í˜„ì‹¤ì  ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.")
            elif perf_assessment.get('overall_legitimacy') == 'Legitimate':
                conclusions.append("âœ… ëª¨ë¸ ì„±ëŠ¥ì´ í•©ë¦¬ì ì…ë‹ˆë‹¤.")

        return conclusions

if __name__ == "__main__":
    analyzer = AdvancedLeakageAnalyzer()
    report = analyzer.generate_comprehensive_report()

    print("\n" + "="*70)
    print("S&P 500 ì˜ˆì¸¡ ëª¨ë¸ ê³ ë„ ë°ì´í„° ëˆ„ì¶œ ë¶„ì„ ë³´ê³ ì„œ")
    print("="*70)

    # ìš”ì•½
    summary = report['executive_summary']
    print(f"\nğŸ“Š ì¢…í•© í‰ê°€:")
    print(f"   ìœ„í—˜ ìˆ˜ì¤€: {summary['risk_level']}")
    print(f"   ë°ì´í„° í’ˆì§ˆ: {summary['data_quality']}")
    print(f"   ëª¨ë¸ ì •ë‹¹ì„±: {summary['model_legitimacy']}")

    if summary['risk_factors']:
        print(f"\nâš ï¸ ìœ„í—˜ ìš”ì†Œ:")
        for factor in summary['risk_factors']:
            print(f"   â€¢ {factor}")
    else:
        print(f"\nâœ… ìœ„í—˜ ìš”ì†Œ: ì—†ìŒ")

    # ìƒì„¸ ë¶„ì„ ê²°ê³¼
    print(f"\nğŸ“‹ ìƒì„¸ ë¶„ì„:")

    # ì»¬ëŸ¼ êµ¬ì¡°
    col_analysis = report['detailed_analysis']['column_structure']
    print(f"   ë°ì´í„° êµ¬ì¡°: {col_analysis['total_columns']}ì»¬ëŸ¼ (íŠ¹ì„±: {col_analysis['feature_count']}, íƒ€ê²Ÿ: {col_analysis['target_count']})")

    # ì‹œê°„ì  ê²€ì¦
    temporal = report['detailed_analysis']['temporal_verification']
    if temporal.get('has_date_column'):
        print(f"   ì‹œê°„ì  ì •ë ¬: {'âœ… OK' if temporal.get('is_chronologically_sorted') else 'âŒ FAIL'}")
        print(f"   ë°ì´í„° ê¸°ê°„: {temporal['date_range']['start'][:10]} ~ {temporal['date_range']['end'][:10]} ({temporal['date_range']['total_days']}ì¼)")

    # ìƒê´€ê´€ê³„ ë¶„ì„
    corr_analysis = report['detailed_analysis']['correlation_analysis']
    if corr_analysis:
        max_corrs = [analysis['stats']['max_abs_correlation'] for analysis in corr_analysis.values()]
        print(f"   ìµœëŒ€ ìƒê´€ê´€ê³„: {max(max_corrs):.3f} {'âœ… OK' if max(max_corrs) < 0.95 else 'âŒ ë†’ìŒ'}")

    # ì„±ëŠ¥ ê²€ì¦
    perf = report['detailed_analysis']['performance_validation']
    if 'reported_mae' in perf:
        print(f"   MAE: ${perf['reported_mae']:.2f}")
        print(f"   MAPE: {perf['reported_mape']:.2f}%")
        print(f"   ë°©í–¥ ì •í™•ë„: {perf['direction_accuracy']*100:.1f}%")

    # ì˜ì‹¬ íŠ¹ì„± í•´ëª…
    print(f"\nğŸ” íŠ¹ì„± ë¶„ì„:")
    suspicious = report['detailed_analysis']['suspicious_features_analysis']
    for feature, analysis in suspicious.items():
        status = "âœ… ì •ë‹¹" if analysis['is_legitimate'] else "âŒ ì˜ì‹¬"
        print(f"   {feature}: {status} - {analysis['description']}")

    # ê²°ë¡ 
    print(f"\nğŸ“ ìµœì¢… ê²°ë¡ :")
    for conclusion in report['conclusions']:
        print(f"   {conclusion}")

    print("\n" + "="*70)

    # JSON ë³´ê³ ì„œ ì €ì¥
    output_path = Path('/root/workspace/comprehensive_leakage_analysis_report.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)

    print(f"ìƒì„¸ ë³´ê³ ì„œ ì €ì¥ë¨: {output_path}")