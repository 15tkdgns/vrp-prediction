"""
ìµœì  ì•™ìƒë¸” ëª¨ë¸ ì¢…í•© ê²€ì¦ ì‹œìŠ¤í…œ
V1(70%) + V2(30%) ì•™ìƒë¸”ì˜ ì‹ ë¢°ì„± ì™„ì „ ê²€ì¦

ê²€ì¦ í•­ëª©:
1. ê³¼ì í•© ì¬ê²€ì¦ (í›ˆë ¨ vs ê²€ì¦ ì„±ëŠ¥)
2. Walk-Forward Validation (ì‹œê°„ì  ì•ˆì •ì„±)
3. ë°ì´í„° ëˆ„ì¶œ ì¬í™•ì¸
4. ê²½ì œì  ë°±í…ŒìŠ¤íŒ… (ì‹¤ì œ ê±°ë˜ ì‹œë®¬ë ˆì´ì…˜)
5. ë‹¤ì–‘í•œ ì‹œì¥ ì¡°ê±´ì—ì„œì˜ ì„±ëŠ¥
6. ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ ì„±ëŠ¥
7. ì•ˆì •ì„± ë° ì‹ ë¢°ì„± ì§€í‘œ
8. Monte Carlo ì‹œë®¬ë ˆì´ì…˜
"""

import numpy as np
import pandas as pd
import yfinance as yf
import json
import logging
from datetime import datetime, timedelta
import warnings
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
from scipy import stats
import time

warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/workspace/data/raw/optimal_ensemble_validation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PurgedKFoldSklearn:
    """sklearn í˜¸í™˜ Purged K-Fold Cross-Validation"""

    def __init__(self, n_splits=5, purge_length=5, embargo_length=5):
        self.n_splits = n_splits
        self.purge_length = purge_length
        self.embargo_length = embargo_length

    def split(self, X, y=None, groups=None):
        n_samples = len(X)
        indices = np.arange(n_samples)
        test_size = n_samples // self.n_splits
        splits = []

        for i in range(self.n_splits):
            test_start = i * test_size
            test_end = min((i + 1) * test_size, n_samples)
            test_indices = indices[test_start:test_end]

            purge_start = test_end
            purge_end = min(test_end + self.purge_length, n_samples)
            embargo_end = min(purge_end + self.embargo_length, n_samples)

            train_indices = np.concatenate([
                indices[:test_start],
                indices[embargo_end:]
            ])

            if len(train_indices) > 0 and len(test_indices) > 0:
                splits.append((train_indices, test_indices))

        return splits

class OptimalEnsembleValidator:
    """ìµœì  ì•™ìƒë¸” ëª¨ë¸ ì¢…í•© ê²€ì¦ê¸°"""

    def __init__(self):
        self.cv = PurgedKFoldSklearn()
        self.scaler_v1 = StandardScaler()
        self.scaler_v2 = StandardScaler()

        # ìµœì  ëª¨ë¸ ì„¤ì •
        self.optimal_weights = [0.7, 0.3]  # V1: 70%, V2: 30%
        self.v1_alpha = 1.8523
        self.v2_alpha = 19.5029

        self.validation_results = {}

    def load_spy_data(self):
        """SPY ë°ì´í„° ë¡œë“œ"""
        logger.info("ğŸ“Š SPY ë°ì´í„° ë¡œë”©...")

        spy = yf.Ticker("SPY")
        data = spy.history(start="2015-01-01", end="2024-12-31")
        data['returns'] = np.log(data['Close'] / data['Close'].shift(1))
        data = data.dropna()

        logger.info(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(data)}ê°œ ê´€ì¸¡ì¹˜")
        logger.info(f"   ê¸°ê°„: {data.index[0].date()} ~ {data.index[-1].date()}")
        return data

    def create_v1_features(self, data):
        """V1 íŠ¹ì„± ìƒì„± (12ê°œ)"""
        returns = data['returns']
        features = pd.DataFrame(index=data.index)

        # ê¸°ë³¸ ë³€ë™ì„± (3ê°œ)
        features['vol_5'] = returns.rolling(5).std()
        features['vol_10'] = returns.rolling(10).std()
        features['vol_20'] = returns.rolling(20).std()

        # ê¸°ë³¸ ë˜ê·¸ (3ê°œ)
        features['return_lag_1'] = returns.shift(1)
        features['return_lag_2'] = returns.shift(2)
        features['return_lag_3'] = returns.shift(3)

        # ê¸°ë³¸ í†µê³„ (4ê°œ)
        for window in [10, 20]:
            ma = returns.rolling(window).mean()
            std = returns.rolling(window).std()
            features[f'zscore_{window}'] = (returns - ma) / (std + 1e-8)
            features[f'momentum_{window}'] = returns.rolling(window).sum()

        # ê¸°ë³¸ ë¹„ìœ¨ (2ê°œ)
        features['vol_5_20_ratio'] = features['vol_5'] / (features['vol_20'] + 1e-8)
        features['vol_regime'] = (features['vol_5'] > features['vol_10']).astype(float)

        return self.finalize_features(features, returns)

    def create_v2_features(self, data):
        """V2 íŠ¹ì„± ìƒì„± (30ê°œ)"""
        returns = data['returns']
        features = pd.DataFrame(index=data.index)

        # ë³€ë™ì„± (6ê°œ)
        for window in [3, 5, 10, 15, 20, 30]:
            features[f'vol_{window}'] = returns.rolling(window).std()

        # í†µê³„ì  ëª¨ë©˜íŠ¸ (6ê°œ)
        for window in [5, 10, 20]:
            features[f'skew_{window}'] = returns.rolling(window).skew()
            features[f'kurt_{window}'] = returns.rolling(window).kurt()

        # ë˜ê·¸ (6ê°œ)
        for lag in [1, 2, 3]:
            features[f'return_lag_{lag}'] = returns.shift(lag)
            features[f'vol_lag_{lag}'] = features['vol_5'].shift(lag)

        # ë³€ë™ì„± ì²´ì œ (4ê°œ)
        short_vol = features['vol_5']
        medium_vol = features['vol_20']
        long_vol = features['vol_30']

        features['vol_regime_short'] = (short_vol > medium_vol).astype(float)
        features['vol_regime_medium'] = (medium_vol > long_vol).astype(float)
        features['vol_expansion'] = short_vol / (long_vol + 1e-8)
        features['vol_contraction'] = long_vol / (short_vol + 1e-8)

        # í†µê³„ ì§€í‘œ (5ê°œ)
        for window in [10, 20, 30]:
            ma = returns.rolling(window).mean()
            std = returns.rolling(window).std()
            features[f'zscore_{window}'] = (returns - ma) / (std + 1e-8)

        for window in [10, 20]:
            ma = returns.rolling(window).mean()
            std = returns.rolling(window).std()
            features[f'sharpe_{window}'] = (ma * np.sqrt(252)) / (std + 1e-8)

        # ìƒí˜¸ì‘ìš© (3ê°œ)
        features['vol_5_20_ratio'] = features['vol_5'] / (features['vol_20'] + 1e-8)
        features['vol_10_30_ratio'] = features['vol_10'] / (features['vol_30'] + 1e-8)
        features['vol_price_interaction'] = features['vol_20'] * returns

        return self.finalize_features(features, returns)

    def finalize_features(self, features, returns):
        """íŠ¹ì„± ë§ˆë¬´ë¦¬ ì²˜ë¦¬"""
        # íƒ€ê²Ÿ: 5ì¼ í›„ ë³€ë™ì„±
        target = []
        for i in range(len(returns)):
            if i + 5 < len(returns):
                future_vol = returns.iloc[i+1:i+6].std()
                target.append(future_vol)
            else:
                target.append(np.nan)

        features['target_vol_5d'] = target
        features = features.dropna()

        X = features.drop('target_vol_5d', axis=1)
        y = features['target_vol_5d']

        return X, y

    def test_1_overfitting_recheck(self):
        """1. ê³¼ì í•© ì¬ê²€ì¦"""
        logger.info("ğŸ” 1. ê³¼ì í•© ì¬ê²€ì¦...")

        data = self.load_spy_data()
        X_v1, y_v1 = self.create_v1_features(data)
        X_v2, y_v2 = self.create_v2_features(data)

        # ê³µí†µ ìƒ˜í”Œ
        common_indices = sorted(list(set(y_v1.index) & set(y_v2.index)))
        v1_mask = y_v1.index.isin(common_indices)
        v2_mask = y_v2.index.isin(common_indices)

        X_v1_scaled = self.scaler_v1.fit_transform(X_v1[v1_mask])
        X_v2_scaled = self.scaler_v2.fit_transform(X_v2[v2_mask])
        y_common = y_v1[v1_mask]

        logger.info(f"   ê³µí†µ ìƒ˜í”Œ: {len(y_common)}ê°œ")

        # í›ˆë ¨ vs ê²€ì¦ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        train_scores = []
        val_scores = []

        splits = list(self.cv.split(X_v1_scaled))

        for train_idx, val_idx in splits:
            # V1 ëª¨ë¸
            v1_model = Ridge(alpha=self.v1_alpha)
            v1_model.fit(X_v1_scaled[train_idx], y_common.iloc[train_idx])
            v1_train_pred = v1_model.predict(X_v1_scaled[train_idx])
            v1_val_pred = v1_model.predict(X_v1_scaled[val_idx])

            # V2 ëª¨ë¸
            v2_model = Ridge(alpha=self.v2_alpha)
            v2_model.fit(X_v2_scaled[train_idx], y_common.iloc[train_idx])
            v2_train_pred = v2_model.predict(X_v2_scaled[train_idx])
            v2_val_pred = v2_model.predict(X_v2_scaled[val_idx])

            # ì•™ìƒë¸” ì˜ˆì¸¡
            ensemble_train_pred = (self.optimal_weights[0] * v1_train_pred +
                                 self.optimal_weights[1] * v2_train_pred)
            ensemble_val_pred = (self.optimal_weights[0] * v1_val_pred +
                                self.optimal_weights[1] * v2_val_pred)

            # ì„±ëŠ¥ ê³„ì‚°
            train_r2 = r2_score(y_common.iloc[train_idx], ensemble_train_pred)
            val_r2 = r2_score(y_common.iloc[val_idx], ensemble_val_pred)

            train_scores.append(train_r2)
            val_scores.append(val_r2)

        train_mean = np.mean(train_scores)
        val_mean = np.mean(val_scores)
        performance_gap = train_mean - val_mean

        result = {
            'train_r2_mean': float(train_mean),
            'train_r2_std': float(np.std(train_scores)),
            'val_r2_mean': float(val_mean),
            'val_r2_std': float(np.std(val_scores)),
            'performance_gap': float(performance_gap),
            'overfitting_risk': 'HIGH' if performance_gap > 0.15 else 'MEDIUM' if performance_gap > 0.08 else 'LOW'
        }

        logger.info(f"   í›ˆë ¨ RÂ²: {train_mean:.4f} Â± {np.std(train_scores):.4f}")
        logger.info(f"   ê²€ì¦ RÂ²: {val_mean:.4f} Â± {np.std(val_scores):.4f}")
        logger.info(f"   ì„±ëŠ¥ ê²©ì°¨: {performance_gap:.4f}")
        logger.info(f"   ê³¼ì í•© ìœ„í—˜: {result['overfitting_risk']}")

        self.validation_results['overfitting_check'] = result
        return result

    def test_2_walk_forward_validation(self):
        """2. Walk-Forward Validation (ì‹œê°„ì  ì•ˆì •ì„±)"""
        logger.info("ğŸ” 2. Walk-Forward Validation...")

        data = self.load_spy_data()
        X_v1, y_v1 = self.create_v1_features(data)
        X_v2, y_v2 = self.create_v2_features(data)

        # ê³µí†µ ìƒ˜í”Œ
        common_indices = sorted(list(set(y_v1.index) & set(y_v2.index)))
        v1_mask = y_v1.index.isin(common_indices)
        v2_mask = y_v2.index.isin(common_indices)

        X_v1_scaled = self.scaler_v1.fit_transform(X_v1[v1_mask])
        X_v2_scaled = self.scaler_v2.fit_transform(X_v2[v2_mask])
        y_common = y_v1[v1_mask]

        # Walk-Forward í…ŒìŠ¤íŠ¸
        n_samples = len(X_v1_scaled)
        train_size = n_samples // 2  # 50% í›ˆë ¨
        test_size = 250  # ê³ ì • í…ŒìŠ¤íŠ¸ í¬ê¸°
        step_size = test_size // 2  # 50% ê²¹ì¹¨

        wf_scores = []
        time_periods = []

        for start_idx in range(train_size, n_samples - test_size, step_size):
            end_idx = min(start_idx + test_size, n_samples)

            # í›ˆë ¨: ì²˜ìŒë¶€í„° start_idxê¹Œì§€
            X_v1_train = X_v1_scaled[:start_idx]
            X_v2_train = X_v2_scaled[:start_idx]
            y_train = y_common.iloc[:start_idx]

            # í…ŒìŠ¤íŠ¸: start_idxë¶€í„° end_idxê¹Œì§€
            X_v1_test = X_v1_scaled[start_idx:end_idx]
            X_v2_test = X_v2_scaled[start_idx:end_idx]
            y_test = y_common.iloc[start_idx:end_idx]

            # ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
            v1_model = Ridge(alpha=self.v1_alpha)
            v2_model = Ridge(alpha=self.v2_alpha)

            v1_model.fit(X_v1_train, y_train)
            v2_model.fit(X_v2_train, y_train)

            v1_pred = v1_model.predict(X_v1_test)
            v2_pred = v2_model.predict(X_v2_test)

            # ì•™ìƒë¸” ì˜ˆì¸¡
            ensemble_pred = (self.optimal_weights[0] * v1_pred +
                           self.optimal_weights[1] * v2_pred)

            # ì„±ëŠ¥ ê³„ì‚°
            wf_r2 = r2_score(y_test, ensemble_pred)
            wf_scores.append(wf_r2)
            time_periods.append(f"{start_idx}-{end_idx}")

            logger.info(f"   Period {start_idx:4d}-{end_idx:4d}: RÂ²={wf_r2:.4f}")

        # ì‹œê°„ì  ì•ˆì •ì„± ë¶„ì„
        wf_mean = np.mean(wf_scores)
        wf_std = np.std(wf_scores)
        time_trend = np.corrcoef(range(len(wf_scores)), wf_scores)[0, 1] if len(wf_scores) > 1 else 0

        result = {
            'wf_scores': [float(s) for s in wf_scores],
            'time_periods': time_periods,
            'wf_mean': float(wf_mean),
            'wf_std': float(wf_std),
            'time_trend': float(time_trend),
            'temporal_stability': 'STABLE' if abs(time_trend) < 0.3 and wf_std < 0.15 else 'UNSTABLE'
        }

        logger.info(f"   Walk-Forward RÂ²: {wf_mean:.4f} Â± {wf_std:.4f}")
        logger.info(f"   ì‹œê°„ íŠ¸ë Œë“œ: {time_trend:.4f}")
        logger.info(f"   ì‹œê°„ì  ì•ˆì •ì„±: {result['temporal_stability']}")

        self.validation_results['walk_forward'] = result
        return result

    def test_3_economic_backtest(self):
        """3. ê²½ì œì  ë°±í…ŒìŠ¤íŒ… (ì‹¤ì œ ê±°ë˜ ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info("ğŸ” 3. ê²½ì œì  ë°±í…ŒìŠ¤íŒ…...")

        data = self.load_spy_data()
        X_v1, y_v1 = self.create_v1_features(data)
        X_v2, y_v2 = self.create_v2_features(data)

        # ê³µí†µ ìƒ˜í”Œ
        common_indices = sorted(list(set(y_v1.index) & set(y_v2.index)))
        v1_mask = y_v1.index.isin(common_indices)
        v2_mask = y_v2.index.isin(common_indices)

        X_v1_scaled = self.scaler_v1.fit_transform(X_v1[v1_mask])
        X_v2_scaled = self.scaler_v2.fit_transform(X_v2[v2_mask])
        y_common = y_v1[v1_mask]

        # ë°±í…ŒìŠ¤íŒ… ì‹œë®¬ë ˆì´ì…˜ (ë§ˆì§€ë§‰ 1ë…„)
        train_size = len(X_v1_scaled) - 252  # ë§ˆì§€ë§‰ 1ë…„ ì œì™¸

        # í›ˆë ¨
        v1_model = Ridge(alpha=self.v1_alpha)
        v2_model = Ridge(alpha=self.v2_alpha)

        v1_model.fit(X_v1_scaled[:train_size], y_common.iloc[:train_size])
        v2_model.fit(X_v2_scaled[:train_size], y_common.iloc[:train_size])

        # í…ŒìŠ¤íŠ¸ (ë§ˆì§€ë§‰ 1ë…„)
        test_indices = range(train_size, len(X_v1_scaled))

        predictions = []
        actuals = []
        returns_data = []

        spy_prices = data['Close'].reindex(y_common.index).iloc[train_size:]

        for i, idx in enumerate(test_indices):
            if idx + 5 < len(X_v1_scaled):  # 5ì¼ í›„ ë°ì´í„° í™•ì¸ ê°€ëŠ¥í•œ ê²½ìš°ë§Œ
                # ì˜ˆì¸¡
                v1_pred = v1_model.predict(X_v1_scaled[idx:idx+1])[0]
                v2_pred = v2_model.predict(X_v2_scaled[idx:idx+1])[0]
                ensemble_pred = (self.optimal_weights[0] * v1_pred +
                               self.optimal_weights[1] * v2_pred)

                # ì‹¤ì œê°’
                actual = y_common.iloc[idx]

                predictions.append(ensemble_pred)
                actuals.append(actual)

                # ê±°ë˜ ì‹ í˜¸ ìƒì„± (ë³€ë™ì„± ì˜ˆì¸¡ ê¸°ë°˜)
                if i > 0:  # ì´ì „ ì˜ˆì¸¡ê³¼ ë¹„êµ
                    prev_pred = predictions[i-1]
                    vol_change = ensemble_pred - prev_pred

                    # ë‹¨ìˆœ ì „ëµ: ë³€ë™ì„± ì¦ê°€ ì˜ˆìƒ ì‹œ ë§¤ë„, ê°ì†Œ ì‹œ ë§¤ìˆ˜
                    signal = -1 if vol_change > 0.001 else 1  # 0.1% ì„ê³„ê°’

                    # ìˆ˜ìµë¥  ê³„ì‚° (5ì¼ ë³´ìœ )
                    if idx + 5 < len(spy_prices):
                        period_return = (spy_prices.iloc[idx + 5] - spy_prices.iloc[idx]) / spy_prices.iloc[idx]
                        strategy_return = signal * period_return
                        returns_data.append({
                            'date': y_common.index[idx],
                            'signal': signal,
                            'period_return': period_return,
                            'strategy_return': strategy_return,
                            'predicted_vol': ensemble_pred,
                            'actual_vol': actual
                        })

        # ì„±ê³¼ ë¶„ì„
        if len(returns_data) > 0:
            returns_df = pd.DataFrame(returns_data)

            total_return = (1 + returns_df['strategy_return']).prod() - 1
            benchmark_return = (1 + returns_df['period_return']).prod() - 1

            strategy_vol = returns_df['strategy_return'].std() * np.sqrt(252/5)  # ì—°ìœ¨í™”
            benchmark_vol = returns_df['period_return'].std() * np.sqrt(252/5)

            sharpe_ratio = (returns_df['strategy_return'].mean() * 252/5) / strategy_vol if strategy_vol > 0 else 0

            win_rate = (returns_df['strategy_return'] > 0).mean()

        else:
            total_return = benchmark_return = strategy_vol = benchmark_vol = sharpe_ratio = win_rate = 0

        # ì˜ˆì¸¡ ì •í™•ë„
        pred_r2 = r2_score(actuals, predictions) if len(predictions) > 0 else 0
        pred_corr = np.corrcoef(predictions, actuals)[0, 1] if len(predictions) > 1 else 0

        result = {
            'prediction_performance': {
                'r2_score': float(pred_r2),
                'correlation': float(pred_corr),
                'n_predictions': len(predictions)
            },
            'economic_performance': {
                'strategy_return': float(total_return),
                'benchmark_return': float(benchmark_return),
                'excess_return': float(total_return - benchmark_return),
                'strategy_volatility': float(strategy_vol),
                'benchmark_volatility': float(benchmark_vol),
                'sharpe_ratio': float(sharpe_ratio),
                'win_rate': float(win_rate),
                'n_trades': len(returns_data)
            }
        }

        logger.info(f"   ì˜ˆì¸¡ RÂ²: {pred_r2:.4f}")
        logger.info(f"   ì „ëµ ìˆ˜ìµë¥ : {total_return:.2%}")
        logger.info(f"   ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥ : {benchmark_return:.2%}")
        logger.info(f"   ì´ˆê³¼ ìˆ˜ìµë¥ : {total_return - benchmark_return:+.2%}")
        logger.info(f"   ìƒ¤í”„ ë¹„ìœ¨: {sharpe_ratio:.4f}")
        logger.info(f"   ìŠ¹ë¥ : {win_rate:.1%}")

        self.validation_results['economic_backtest'] = result
        return result

    def test_4_benchmark_comparison(self):
        """4. ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ ì„±ëŠ¥"""
        logger.info("ğŸ” 4. ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ ì„±ëŠ¥...")

        data = self.load_spy_data()
        X_v1, y_v1 = self.create_v1_features(data)
        X_v2, y_v2 = self.create_v2_features(data)

        # ê³µí†µ ìƒ˜í”Œ
        common_indices = sorted(list(set(y_v1.index) & set(y_v2.index)))
        v1_mask = y_v1.index.isin(common_indices)
        v2_mask = y_v2.index.isin(common_indices)

        X_v1_scaled = self.scaler_v1.fit_transform(X_v1[v1_mask])
        X_v2_scaled = self.scaler_v2.fit_transform(X_v2[v2_mask])
        y_common = y_v1[v1_mask]

        # ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ë“¤
        benchmarks = {
            'V1_Only': Ridge(alpha=self.v1_alpha),
            'V2_Only': Ridge(alpha=self.v2_alpha),
            'Simple_Average': 'ensemble',  # V1ê³¼ V2ì˜ ë‹¨ìˆœ í‰ê· 
            'HAR_Model': Ridge(alpha=1.0),  # ê°„ë‹¨í•œ HAR ëª¨í˜• ê·¼ì‚¬
            'Naive_Persistence': 'naive'  # ë‹¨ìˆœ ì§€ì†ì„± ëª¨ë¸
        }

        benchmark_results = {}
        splits = list(self.cv.split(X_v1_scaled))

        for name, model_spec in benchmarks.items():
            scores = []

            for train_idx, val_idx in splits:
                y_train = y_common.iloc[train_idx]
                y_val = y_common.iloc[val_idx]

                if name == 'V1_Only':
                    model_spec.fit(X_v1_scaled[train_idx], y_train)
                    pred = model_spec.predict(X_v1_scaled[val_idx])
                elif name == 'V2_Only':
                    model_spec.fit(X_v2_scaled[train_idx], y_train)
                    pred = model_spec.predict(X_v2_scaled[val_idx])
                elif name == 'Simple_Average':
                    v1_model = Ridge(alpha=self.v1_alpha)
                    v2_model = Ridge(alpha=self.v2_alpha)
                    v1_model.fit(X_v1_scaled[train_idx], y_train)
                    v2_model.fit(X_v2_scaled[train_idx], y_train)
                    v1_pred = v1_model.predict(X_v1_scaled[val_idx])
                    v2_pred = v2_model.predict(X_v2_scaled[val_idx])
                    pred = (v1_pred + v2_pred) / 2  # ë‹¨ìˆœ í‰ê· 
                elif name == 'HAR_Model':
                    # ê°„ë‹¨í•œ HAR ëª¨ë¸ (ë³€ë™ì„± ë˜ê·¸ ì‚¬ìš©)
                    har_features = np.column_stack([
                        X_v1_scaled[train_idx, 0],  # vol_5
                        X_v1_scaled[train_idx, 1],  # vol_10
                        X_v1_scaled[train_idx, 2]   # vol_20
                    ])
                    model_spec.fit(har_features, y_train)
                    har_test = np.column_stack([
                        X_v1_scaled[val_idx, 0],
                        X_v1_scaled[val_idx, 1],
                        X_v1_scaled[val_idx, 2]
                    ])
                    pred = model_spec.predict(har_test)
                elif name == 'Naive_Persistence':
                    # ë‹¨ìˆœ ì§€ì†ì„±: í˜„ì¬ ë³€ë™ì„±ì´ ë¯¸ë˜ ë³€ë™ì„±
                    pred = X_v1_scaled[val_idx, 0]  # vol_5 ì‚¬ìš©

                r2 = r2_score(y_val, pred)
                scores.append(r2)

            benchmark_results[name] = {
                'scores': [float(s) for s in scores],
                'mean_r2': float(np.mean(scores)),
                'std_r2': float(np.std(scores))
            }

        # ìš°ë¦¬ ëª¨ë¸ (ìµœì  ì•™ìƒë¸”)
        optimal_scores = []
        for train_idx, val_idx in splits:
            v1_model = Ridge(alpha=self.v1_alpha)
            v2_model = Ridge(alpha=self.v2_alpha)

            v1_model.fit(X_v1_scaled[train_idx], y_common.iloc[train_idx])
            v2_model.fit(X_v2_scaled[train_idx], y_common.iloc[train_idx])

            v1_pred = v1_model.predict(X_v1_scaled[val_idx])
            v2_pred = v2_model.predict(X_v2_scaled[val_idx])

            optimal_pred = (self.optimal_weights[0] * v1_pred +
                          self.optimal_weights[1] * v2_pred)

            r2 = r2_score(y_common.iloc[val_idx], optimal_pred)
            optimal_scores.append(r2)

        our_performance = {
            'scores': [float(s) for s in optimal_scores],
            'mean_r2': float(np.mean(optimal_scores)),
            'std_r2': float(np.std(optimal_scores))
        }

        # ì„±ëŠ¥ ë¹„êµ
        logger.info("   ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ ì„±ëŠ¥:")
        logger.info(f"   ìµœì  ì•™ìƒë¸”: RÂ² = {our_performance['mean_r2']:.4f} Â± {our_performance['std_r2']:.4f}")

        for name, perf in benchmark_results.items():
            improvement = ((our_performance['mean_r2'] - perf['mean_r2']) / perf['mean_r2'] * 100) if perf['mean_r2'] > 0 else 0
            logger.info(f"   {name}: RÂ² = {perf['mean_r2']:.4f} Â± {perf['std_r2']:.4f} ({improvement:+.1f}%)")

        result = {
            'optimal_ensemble': our_performance,
            'benchmarks': benchmark_results
        }

        self.validation_results['benchmark_comparison'] = result
        return result

    def test_5_stability_analysis(self):
        """5. ì•ˆì •ì„± ë° ì‹ ë¢°ì„± ë¶„ì„"""
        logger.info("ğŸ” 5. ì•ˆì •ì„± ë° ì‹ ë¢°ì„± ë¶„ì„...")

        data = self.load_spy_data()
        X_v1, y_v1 = self.create_v1_features(data)
        X_v2, y_v2 = self.create_v2_features(data)

        # ê³µí†µ ìƒ˜í”Œ
        common_indices = sorted(list(set(y_v1.index) & set(y_v2.index)))
        v1_mask = y_v1.index.isin(common_indices)
        v2_mask = y_v2.index.isin(common_indices)

        X_v1_scaled = self.scaler_v1.fit_transform(X_v1[v1_mask])
        X_v2_scaled = self.scaler_v2.fit_transform(X_v2[v2_mask])
        y_common = y_v1[v1_mask]

        # 1. Bootstrap ì‹ ë¢°êµ¬ê°„
        n_bootstrap = 100
        bootstrap_scores = []

        for i in range(n_bootstrap):
            # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§
            n_samples = len(X_v1_scaled)
            bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)

            X_v1_boot = X_v1_scaled[bootstrap_idx]
            X_v2_boot = X_v2_scaled[bootstrap_idx]
            y_boot = y_common.iloc[bootstrap_idx]

            # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
            split_idx = n_samples // 2

            v1_model = Ridge(alpha=self.v1_alpha)
            v2_model = Ridge(alpha=self.v2_alpha)

            v1_model.fit(X_v1_boot[:split_idx], y_boot.iloc[:split_idx])
            v2_model.fit(X_v2_boot[:split_idx], y_boot.iloc[:split_idx])

            v1_pred = v1_model.predict(X_v1_boot[split_idx:])
            v2_pred = v2_model.predict(X_v2_boot[split_idx:])

            ensemble_pred = (self.optimal_weights[0] * v1_pred +
                           self.optimal_weights[1] * v2_pred)

            r2 = r2_score(y_boot.iloc[split_idx:], ensemble_pred)
            bootstrap_scores.append(r2)

        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
        ci_lower = np.percentile(bootstrap_scores, 2.5)
        ci_upper = np.percentile(bootstrap_scores, 97.5)
        bootstrap_mean = np.mean(bootstrap_scores)
        bootstrap_std = np.std(bootstrap_scores)

        # 2. ë‹¤ì–‘í•œ ì‹œì¥ ì¡°ê±´ì—ì„œì˜ ì„±ëŠ¥
        # ë³€ë™ì„± ì²´ì œë³„ ì„±ëŠ¥
        vol_regimes = {
            'Low_Vol': y_common < np.percentile(y_common, 33),
            'Medium_Vol': (y_common >= np.percentile(y_common, 33)) & (y_common < np.percentile(y_common, 67)),
            'High_Vol': y_common >= np.percentile(y_common, 67)
        }

        regime_performance = {}
        splits = list(self.cv.split(X_v1_scaled))

        for regime_name, regime_mask in vol_regimes.items():
            regime_scores = []

            for train_idx, val_idx in splits:
                # ê²€ì¦ ì„¸íŠ¸ì—ì„œ í•´ë‹¹ ì²´ì œë§Œ ì„ íƒ
                regime_val_idx = val_idx[regime_mask.iloc[val_idx].values]

                if len(regime_val_idx) < 10:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ë³´
                    continue

                v1_model = Ridge(alpha=self.v1_alpha)
                v2_model = Ridge(alpha=self.v2_alpha)

                v1_model.fit(X_v1_scaled[train_idx], y_common.iloc[train_idx])
                v2_model.fit(X_v2_scaled[train_idx], y_common.iloc[train_idx])

                v1_pred = v1_model.predict(X_v1_scaled[regime_val_idx])
                v2_pred = v2_model.predict(X_v2_scaled[regime_val_idx])

                ensemble_pred = (self.optimal_weights[0] * v1_pred +
                               self.optimal_weights[1] * v2_pred)

                r2 = r2_score(y_common.iloc[regime_val_idx], ensemble_pred)
                regime_scores.append(r2)

            if len(regime_scores) > 0:
                regime_performance[regime_name] = {
                    'mean_r2': float(np.mean(regime_scores)),
                    'std_r2': float(np.std(regime_scores)),
                    'n_observations': int(regime_mask.sum())
                }

        result = {
            'bootstrap_analysis': {
                'mean_r2': float(bootstrap_mean),
                'std_r2': float(bootstrap_std),
                'ci_lower': float(ci_lower),
                'ci_upper': float(ci_upper),
                'n_bootstrap': n_bootstrap
            },
            'regime_performance': regime_performance
        }

        logger.info(f"   Bootstrap RÂ²: {bootstrap_mean:.4f} Â± {bootstrap_std:.4f}")
        logger.info(f"   95% ì‹ ë¢°êµ¬ê°„: [{ci_lower:.4f}, {ci_upper:.4f}]")

        for regime, perf in regime_performance.items():
            logger.info(f"   {regime}: RÂ² = {perf['mean_r2']:.4f} Â± {perf['std_r2']:.4f}")

        self.validation_results['stability_analysis'] = result
        return result

    def generate_comprehensive_validation_report(self):
        """ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        logger.info("ğŸ“‹ ìµœì  ì•™ìƒë¸” ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ ìƒì„±...")

        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        start_time = time.time()

        test1 = self.test_1_overfitting_recheck()
        test2 = self.test_2_walk_forward_validation()
        test3 = self.test_3_economic_backtest()
        test4 = self.test_4_benchmark_comparison()
        test5 = self.test_5_stability_analysis()

        total_time = time.time() - start_time

        # ì¢…í•© í‰ê°€
        validation_scores = {
            'overfitting': 1 if test1['overfitting_risk'] == 'LOW' else 0.5 if test1['overfitting_risk'] == 'MEDIUM' else 0,
            'temporal_stability': 1 if test2['temporal_stability'] == 'STABLE' else 0,
            'economic_value': 1 if test3['economic_performance']['excess_return'] > 0 else 0,
            'benchmark_superior': 1 if test4['optimal_ensemble']['mean_r2'] > max([b['mean_r2'] for b in test4['benchmarks'].values()]) else 0,
            'statistical_significance': 1 if test5['bootstrap_analysis']['ci_lower'] > 0.2 else 0
        }

        overall_score = np.mean(list(validation_scores.values()))

        if overall_score >= 0.8:
            final_verdict = "EXCELLENT"
            recommendation = "ê°•ë ¥ ê¶Œì¥ - ì¦‰ì‹œ í”„ë¡œë•ì…˜ ë°°í¬"
        elif overall_score >= 0.6:
            final_verdict = "GOOD"
            recommendation = "ê¶Œì¥ - ì£¼ì˜ ê¹Šì€ ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ë°°í¬"
        elif overall_score >= 0.4:
            final_verdict = "ACCEPTABLE"
            recommendation = "ì¡°ê±´ë¶€ ê¶Œì¥ - ì¶”ê°€ ê°œì„  í›„ ë°°í¬"
        else:
            final_verdict = "POOR"
            recommendation = "ì¬ê°œë°œ í•„ìš”"

        comprehensive_report = {
            'validation_date': datetime.now().isoformat(),
            'model_specification': {
                'ensemble_type': 'V1_V2_Weighted',
                'weights': self.optimal_weights,
                'v1_alpha': self.v1_alpha,
                'v2_alpha': self.v2_alpha
            },
            'validation_results': self.validation_results,
            'validation_scores': validation_scores,
            'overall_assessment': {
                'overall_score': float(overall_score),
                'final_verdict': final_verdict,
                'recommendation': recommendation,
                'validation_time_seconds': float(total_time)
            },
            'key_findings': self.generate_key_findings()
        }

        # ê²°ê³¼ ì €ì¥
        save_path = '/root/workspace/data/raw/optimal_ensemble_comprehensive_validation.json'
        with open(save_path, 'w') as f:
            json.dump(comprehensive_report, f, indent=2)

        logger.info("="*80)
        logger.info("ğŸ¯ ìµœì  ì•™ìƒë¸” ì¢…í•© ê²€ì¦ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {overall_score:.2f}/1.00")
        logger.info(f"ğŸ“Š ìµœì¢… íŒì •: {final_verdict}")
        logger.info(f"ğŸ’¡ ê¶Œì¥ì‚¬í•­: {recommendation}")
        logger.info(f"â±ï¸ ê²€ì¦ ì‹œê°„: {total_time:.1f}ì´ˆ")
        logger.info(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼: {save_path}")
        logger.info("="*80)

        return comprehensive_report

    def generate_key_findings(self):
        """í•µì‹¬ ë°œê²¬ì‚¬í•­ ìƒì„±"""
        findings = []

        # ê³¼ì í•© ê´€ë ¨
        overfitting = self.validation_results.get('overfitting_check', {})
        if overfitting.get('overfitting_risk') == 'LOW':
            findings.append(f"âœ… ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ (ì„±ëŠ¥ ê²©ì°¨: {overfitting.get('performance_gap', 0):.3f})")
        else:
            findings.append(f"âš ï¸ ê³¼ì í•© ìœ„í—˜ ì¡´ì¬ (ì„±ëŠ¥ ê²©ì°¨: {overfitting.get('performance_gap', 0):.3f})")

        # ì‹œê°„ì  ì•ˆì •ì„±
        wf = self.validation_results.get('walk_forward', {})
        if wf.get('temporal_stability') == 'STABLE':
            findings.append(f"âœ… ì‹œê°„ì  ì•ˆì •ì„± ìš°ìˆ˜ (WF RÂ²: {wf.get('wf_mean', 0):.4f})")
        else:
            findings.append(f"âš ï¸ ì‹œê°„ì  ë¶ˆì•ˆì •ì„± (WF RÂ²: {wf.get('wf_mean', 0):.4f})")

        # ê²½ì œì  ê°€ì¹˜
        econ = self.validation_results.get('economic_backtest', {}).get('economic_performance', {})
        excess_return = econ.get('excess_return', 0)
        if excess_return > 0:
            findings.append(f"âœ… ê²½ì œì  ê°€ì¹˜ ì…ì¦ (ì´ˆê³¼ìˆ˜ìµ: {excess_return:.2%})")
        else:
            findings.append(f"âŒ ê²½ì œì  ê°€ì¹˜ ë¶€ì¡± (ì´ˆê³¼ìˆ˜ìµ: {excess_return:.2%})")

        # ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„
        bench = self.validation_results.get('benchmark_comparison', {})
        our_r2 = bench.get('optimal_ensemble', {}).get('mean_r2', 0)
        findings.append(f"ğŸ“Š ìµœì¢… ê²€ì¦ ì„±ëŠ¥: RÂ² = {our_r2:.4f}")

        # ì‹ ë¢°ì„±
        bootstrap = self.validation_results.get('stability_analysis', {}).get('bootstrap_analysis', {})
        ci_lower = bootstrap.get('ci_lower', 0)
        ci_upper = bootstrap.get('ci_upper', 0)
        findings.append(f"ğŸ“ˆ 95% ì‹ ë¢°êµ¬ê°„: [{ci_lower:.4f}, {ci_upper:.4f}]")

        return findings

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logger.info("ğŸ¯ ìµœì  ì•™ìƒë¸” ëª¨ë¸ ì¢…í•© ê²€ì¦ ì‹œì‘")
    logger.info("ğŸ† ê²€ì¦ ëŒ€ìƒ: V1(70%) + V2(30%) ì•™ìƒë¸”")

    validator = OptimalEnsembleValidator()

    try:
        results = validator.generate_comprehensive_validation_report()

        logger.info("ğŸ’¡ í•µì‹¬ ë°œê²¬ì‚¬í•­:")
        for finding in results['key_findings']:
            logger.info(f"   {finding}")

        return results

    except Exception as e:
        logger.error(f"âŒ ì¢…í•© ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        raise

if __name__ == "__main__":
    main()