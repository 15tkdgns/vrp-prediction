"""
Volatility prediction validation system integrating CPCV and DSR.

This module integrates the advanced statistical validation methods (CPCV and DSR)
with the volatility prediction system to provide robust statistical evidence
for model performance claims.
"""

from __future__ import annotations

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
import warnings
from pathlib import Path
import json
import sys

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.validation.advanced_statistical_validation import (
    CombinatorialPurgedCV,
    DeflatedSharpeRatio,
    AdvancedStatisticalValidator
)
from src.validation.statistical_significance_framework import (
    StatisticalSignificanceFramework,
    BootstrapAnalysis,
    PermutationTest
)
from src.volatility.predictors.elasticnet_predictor import ElasticNetVolatilityPredictor
from src.volatility.features import VolatilityFeatureEngineer
from src.core.exceptions import DataValidationError


@dataclass
class VolatilityValidationResults:
    """Comprehensive validation results for volatility prediction."""

    # Basic performance metrics
    base_r2: float
    base_mae: float
    base_rmse: float

    # CPCV results
    cpcv_r2_distribution: List[float]
    cpcv_r2_mean: float
    cpcv_r2_std: float
    cpcv_r2_ci_lower: float
    cpcv_r2_ci_upper: float
    cpcv_paths_count: int

    # DSR results
    dsr_value: float
    dsr_pvalue: float
    dsr_is_significant: bool
    selection_bias_trials: int

    # Statistical significance
    bootstrap_r2_distribution: List[float]
    bootstrap_ci_lower: float
    bootstrap_ci_upper: float
    permutation_pvalue: float
    statistical_significance: bool

    # Multiple testing corrections
    bonferroni_pvalue: float
    benjamini_hochberg_pvalue: float
    holm_bonferroni_pvalue: float

    # Summary metrics
    validation_score: float  # Overall validation confidence score
    recommendation: str     # Practical recommendation

    def to_dict(self) -> Dict[str, Any]:
        """Convert results to dictionary format."""
        result = asdict(self)
        # Convert numpy types to Python native types for JSON serialization
        for key, value in result.items():
            if hasattr(value, 'item'):  # numpy scalar
                result[key] = value.item()
            elif isinstance(value, np.bool_):
                result[key] = bool(value)
            elif isinstance(value, list) and len(value) > 0 and hasattr(value[0], 'item'):
                result[key] = [v.item() if hasattr(v, 'item') else v for v in value]
        return result

    def summary_report(self) -> str:
        """Generate human-readable summary report."""
        return f"""
=== ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ í†µê³„ì  ê²€ì¦ ê²°ê³¼ ===

ğŸ¯ ê¸°ë³¸ ì„±ëŠ¥:
   RÂ² Score: {self.base_r2:.4f} ({self.base_r2*100:.2f}%)
   MAE: {self.base_mae:.6f}
   RMSE: {self.base_rmse:.6f}

ğŸ“Š CPCV ë¶„ì„ ({self.cpcv_paths_count}ê°œ ê²½ë¡œ):
   RÂ² í‰ê· : {self.cpcv_r2_mean:.4f} Â± {self.cpcv_r2_std:.4f}
   95% ì‹ ë¢°êµ¬ê°„: [{self.cpcv_r2_ci_lower:.4f}, {self.cpcv_r2_ci_upper:.4f}]
   ë¶„í¬ ì•ˆì •ì„±: {'ìš°ìˆ˜' if self.cpcv_r2_std < 0.05 else 'ë³´í†µ' if self.cpcv_r2_std < 0.1 else 'ë¶ˆì•ˆì •'}

ğŸ” DSR ê²€ì¦:
   DSR ê°’: {self.dsr_value:.4f}
   p-value: {self.dsr_pvalue:.6f}
   ì„ íƒí¸í–¥ ë³´ì •: {'í†µê³¼' if self.dsr_is_significant else 'ì‹¤íŒ¨'}
   í…ŒìŠ¤íŠ¸ ì‹œí–‰ìˆ˜: {self.selection_bias_trials}

ğŸ“ˆ í†µê³„ì  ìœ ì˜ì„±:
   Bootstrap 95% CI: [{self.bootstrap_ci_lower:.4f}, {self.bootstrap_ci_upper:.4f}]
   Permutation p-value: {self.permutation_pvalue:.6f}
   ìœ ì˜ì„±: {'í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•¨' if self.statistical_significance else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}

ğŸ›¡ï¸ ë‹¤ì¤‘ê²€ì • ë³´ì •:
   Bonferroni p-value: {self.bonferroni_pvalue:.6f}
   Benjamini-Hochberg p-value: {self.benjamini_hochberg_pvalue:.6f}
   Holm-Bonferroni p-value: {self.holm_bonferroni_pvalue:.6f}

âœ… ìµœì¢… í‰ê°€:
   ê²€ì¦ ì ìˆ˜: {self.validation_score:.0f}/100
   ê¶Œì¥ì‚¬í•­: {self.recommendation}
"""


class VolatilityValidationSystem:
    """
    Comprehensive validation system for volatility prediction models.

    This class integrates all advanced statistical validation methods to provide
    robust evidence for volatility prediction model performance claims.
    """

    def __init__(self,
                 cpcv_n_combinations: int = 500,
                 bootstrap_n_samples: int = 1000,
                 permutation_n_tests: int = 1000,
                 confidence_level: float = 0.95):
        """
        Initialize validation system.

        Args:
            cpcv_n_combinations: Number of CPCV combinations to generate
            bootstrap_n_samples: Number of bootstrap samples
            permutation_n_tests: Number of permutation tests
            confidence_level: Confidence level for intervals
        """
        self.cpcv_n_combinations = cpcv_n_combinations
        self.bootstrap_n_samples = bootstrap_n_samples
        self.permutation_n_tests = permutation_n_tests
        self.confidence_level = confidence_level

        # Initialize validation components
        self.cpcv = CombinatorialPurgedCV(
            n_splits=5,
            min_train_size=252,
            purge_gap=5,
            embargo_pct=0.01,
            max_combinations=self.cpcv_n_combinations
        )

        self.dsr = DeflatedSharpeRatio()
        self.significance_framework = StatisticalSignificanceFramework()

    def validate_volatility_model(self,
                                 data: pd.DataFrame,
                                 model_params: Optional[Dict[str, Any]] = None,
                                 selection_bias_trials: int = 100) -> VolatilityValidationResults:
        """
        Perform comprehensive validation of volatility prediction model.

        Args:
            data: Historical price data with OHLCV columns
            model_params: Model parameters for ElasticNet
            selection_bias_trials: Number of trials for DSR calculation

        Returns:
            Comprehensive validation results
        """
        print("ğŸ”¬ ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ ì¢…í•© ê²€ì¦ ì‹œì‘...")
        print(f"   ğŸ“Š ë°ì´í„°: {len(data)} ì¼ê°„")
        print(f"   ğŸ”€ CPCV ì¡°í•©: {self.cpcv_n_combinations}ê°œ")
        print(f"   ğŸ² Bootstrap ìƒ˜í”Œ: {self.bootstrap_n_samples}ê°œ")
        print(f"   ğŸ§ª ìˆœì—´ê²€ì •: {self.permutation_n_tests}íšŒ")

        try:
            # 1. ê¸°ë³¸ íŠ¹ì„± ì¤€ë¹„
            print("\nğŸ”§ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...")
            features, target = self._prepare_features(data)

            # 2. ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥
            print("ğŸ¤– ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")
            base_metrics = self._evaluate_base_model(features, target, model_params)

            # 3. CPCV ë¶„ì„
            print("ğŸ“Š CPCV ë¶„ì„ (ìˆ˜ë°± ê°œì˜ ë°±í…ŒìŠ¤íŠ¸ ê²½ë¡œ ìƒì„±)...")
            cpcv_results = self._run_cpcv_analysis(features, target, model_params)

            # 4. DSR ê³„ì‚°
            print("ğŸ” ì„ íƒí¸í–¥ ë³´ì • (DSR) ê³„ì‚°...")
            dsr_results = self._calculate_dsr(features, target, model_params, selection_bias_trials)

            # 5. í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
            print("ğŸ“ˆ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦...")
            significance_results = self._test_statistical_significance(features, target, model_params)

            # 6. ë‹¤ì¤‘ê²€ì • ë³´ì •
            print("ğŸ›¡ï¸ ë‹¤ì¤‘ê²€ì • ë³´ì •...")
            corrected_pvalues = self._apply_multiple_testing_corrections([
                significance_results['permutation_pvalue'],
                dsr_results['dsr_pvalue']
            ])

            # 7. ì¢…í•© í‰ê°€
            validation_score, recommendation = self._calculate_validation_score(
                base_metrics, cpcv_results, dsr_results, significance_results
            )

            # ê²°ê³¼ ì •ë¦¬
            results = VolatilityValidationResults(
                # Basic metrics
                base_r2=base_metrics['r2'],
                base_mae=base_metrics['mae'],
                base_rmse=base_metrics['rmse'],

                # CPCV results
                cpcv_r2_distribution=cpcv_results['r2_distribution'],
                cpcv_r2_mean=cpcv_results['r2_mean'],
                cpcv_r2_std=cpcv_results['r2_std'],
                cpcv_r2_ci_lower=cpcv_results['r2_ci_lower'],
                cpcv_r2_ci_upper=cpcv_results['r2_ci_upper'],
                cpcv_paths_count=len(cpcv_results['r2_distribution']),

                # DSR results
                dsr_value=dsr_results['dsr_value'],
                dsr_pvalue=dsr_results['dsr_pvalue'],
                dsr_is_significant=dsr_results['is_significant'],
                selection_bias_trials=selection_bias_trials,

                # Statistical significance
                bootstrap_r2_distribution=significance_results['bootstrap_distribution'],
                bootstrap_ci_lower=significance_results['bootstrap_ci_lower'],
                bootstrap_ci_upper=significance_results['bootstrap_ci_upper'],
                permutation_pvalue=significance_results['permutation_pvalue'],
                statistical_significance=significance_results['is_significant'],

                # Multiple testing corrections
                bonferroni_pvalue=corrected_pvalues['bonferroni'],
                benjamini_hochberg_pvalue=corrected_pvalues['benjamini_hochberg'],
                holm_bonferroni_pvalue=corrected_pvalues['holm_bonferroni'],

                # Summary
                validation_score=validation_score,
                recommendation=recommendation
            )

            print("\nâœ… ì¢…í•© ê²€ì¦ ì™„ë£Œ!")
            return results

        except Exception as e:
            raise DataValidationError(
                f"Volatility validation failed: {str(e)}"
            ) from e

    def _prepare_features(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """Prepare features for validation."""
        engineer = VolatilityFeatureEngineer(
            volatility_windows=[5, 10, 20],
            ma_windows=[20, 50],
            include_lags=True,
            max_lags=3
        )

        feature_result = engineer.create_features(data)
        features_df = feature_result.features

        # Extract target and features
        target = features_df['next_day_volatility']
        feature_columns = [col for col in features_df.columns if col != 'next_day_volatility']
        features = features_df[feature_columns]

        # Clean data
        mask = ~(features.isnull().any(axis=1) | target.isnull())
        clean_features = features[mask]
        clean_target = target[mask]

        print(f"   âœ… íŠ¹ì„± {len(feature_columns)}ê°œ, ìƒ˜í”Œ {len(clean_features)}ê°œ")
        return clean_features, clean_target

    def _evaluate_base_model(self,
                           features: pd.DataFrame,
                           target: pd.Series,
                           model_params: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
        """Evaluate base model performance."""
        if model_params is None:
            model_params = {'alpha': 0.001, 'l1_ratio': 0.5, 'random_state': 42}

        # Simple train-test split
        split_idx = int(len(features) * 0.8)
        X_train, X_test = features.iloc[:split_idx], features.iloc[split_idx:]
        y_train, y_test = target.iloc[:split_idx], target.iloc[split_idx:]

        # Train model
        model = ElasticNetVolatilityPredictor(**model_params)
        model.fit(X_train, y_train)

        # Predictions
        pred_result = model.predict(X_test)
        predictions = pred_result.predictions

        # Metrics
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

        r2 = r2_score(y_test, predictions)
        mae = mean_absolute_error(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))

        print(f"   âœ… ê¸°ë³¸ ì„±ëŠ¥: RÂ²={r2:.4f}, MAE={mae:.6f}, RMSE={rmse:.6f}")

        return {
            'r2': r2,
            'mae': mae,
            'rmse': rmse
        }

    def _run_cpcv_analysis(self,
                          features: pd.DataFrame,
                          target: pd.Series,
                          model_params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Run CPCV analysis to generate performance distribution."""
        if model_params is None:
            model_params = {'alpha': 0.001, 'l1_ratio': 0.5, 'random_state': 42}

        # Use CPCV's run_cpcv method
        try:
            from sklearn.metrics import r2_score

            def r2_metric_func(y_true, y_pred):
                return r2_score(y_true, y_pred)

            cpcv_result = self.cpcv.run_cpcv(
                model_class=ElasticNetVolatilityPredictor,
                model_params=model_params,
                X=features,
                y=target,
                metric_func=r2_metric_func
            )

            print(f"   âœ… CPCV ì™„ë£Œ: {len(cpcv_result.performance_distribution)}ê°œ ê²½ë¡œ ìƒì„±")
            print(f"   ğŸ“Š RÂ² ë¶„í¬: {cpcv_result.mean_performance:.4f} Â± {cpcv_result.std_performance:.4f}")

            return {
                'r2_distribution': cpcv_result.performance_distribution.tolist(),
                'r2_mean': float(cpcv_result.mean_performance),
                'r2_std': float(cpcv_result.std_performance),
                'r2_ci_lower': float(cpcv_result.confidence_intervals['95%'][0]),
                'r2_ci_upper': float(cpcv_result.confidence_intervals['95%'][1])
            }

        except Exception as e:
            # Fallback to simple time series split
            warnings.warn(f"CPCV failed, using fallback: {e}")

            from sklearn.model_selection import TimeSeriesSplit
            tscv = TimeSeriesSplit(n_splits=min(5, len(features) // 100))

            r2_scores = []
            for train_idx, test_idx in tscv.split(features):
                X_train, X_test = features.iloc[train_idx], features.iloc[test_idx]
                y_train, y_test = target.iloc[train_idx], target.iloc[test_idx]

                try:
                    model = ElasticNetVolatilityPredictor(**model_params)
                    model.fit(X_train, y_train)
                    pred_result = model.predict(X_test)
                    predictions = pred_result.predictions

                    from sklearn.metrics import r2_score
                    r2 = r2_score(y_test, predictions)
                    r2_scores.append(r2)

                except Exception:
                    continue

            if r2_scores:
                r2_array = np.array(r2_scores)
                alpha = 1 - self.confidence_level
                ci_lower = np.percentile(r2_array, 100 * alpha/2)
                ci_upper = np.percentile(r2_array, 100 * (1 - alpha/2))

                print(f"   âœ… Fallback CV ì™„ë£Œ: {len(r2_scores)}ê°œ í´ë“œ")
                print(f"   ğŸ“Š RÂ² ë¶„í¬: {r2_array.mean():.4f} Â± {r2_array.std():.4f}")

                return {
                    'r2_distribution': r2_scores,
                    'r2_mean': float(r2_array.mean()),
                    'r2_std': float(r2_array.std()),
                    'r2_ci_lower': float(ci_lower),
                    'r2_ci_upper': float(ci_upper)
                }
            else:
                raise DataValidationError("CPCV analysis failed completely")

    def _calculate_dsr(self,
                      features: pd.DataFrame,
                      target: pd.Series,
                      model_params: Optional[Dict[str, Any]] = None,
                      n_trials: int = 100) -> Dict[str, Any]:
        """Calculate Deflated Sharpe Ratio for selection bias correction."""
        if model_params is None:
            model_params = {'alpha': 0.001, 'l1_ratio': 0.5, 'random_state': 42}

        # Generate multiple model performances to simulate selection bias
        performances = []

        print(f"   ğŸ” {n_trials}íšŒ ì‹œí–‰ìœ¼ë¡œ ì„ íƒí¸í–¥ ì¶”ì •...")

        for trial in range(n_trials):
            # Add noise to model parameters to simulate different trials
            trial_params = model_params.copy()
            trial_params['alpha'] = model_params['alpha'] * np.random.uniform(0.5, 2.0)
            trial_params['l1_ratio'] = np.clip(
                model_params['l1_ratio'] + np.random.normal(0, 0.1), 0.01, 0.99
            )
            trial_params['random_state'] = trial

            # Simple validation
            split_idx = int(len(features) * 0.8)
            X_train, X_test = features.iloc[:split_idx], features.iloc[split_idx:]
            y_train, y_test = target.iloc[:split_idx], target.iloc[split_idx:]

            try:
                model = ElasticNetVolatilityPredictor(**trial_params)
                model.fit(X_train, y_train)
                pred_result = model.predict(X_test)
                predictions = pred_result.predictions

                from sklearn.metrics import r2_score
                r2 = r2_score(y_test, predictions)
                performances.append(r2)

            except Exception:
                continue

            if trial % 20 == 0:
                print(f"   ì§„í–‰ë¥ : {trial}/{n_trials}")

        if len(performances) == 0:
            raise DataValidationError("DSR calculation failed - no successful trials")

        # Calculate DSR
        performances_array = np.array(performances)
        best_performance = np.max(performances_array)

        # Convert RÂ² to pseudo-returns for DSR calculation
        # Higher RÂ² means better "returns"
        pseudo_returns = performances_array * 100  # Scale for better numerical properties

        dsr_result = self.dsr.calculate_dsr(
            returns=pseudo_returns,
            n_trials=len(performances)
        )

        print(f"   âœ… DSR ê³„ì‚° ì™„ë£Œ: {dsr_result.deflated_sharpe:.4f} (p={dsr_result.probability_skill:.6f})")

        return {
            'dsr_value': dsr_result.deflated_sharpe,
            'dsr_pvalue': dsr_result.probability_skill,
            'is_significant': dsr_result.probability_skill < 0.05,
            'best_performance': best_performance,
            'n_trials_successful': len(performances)
        }

    def _test_statistical_significance(self,
                                     features: pd.DataFrame,
                                     target: pd.Series,
                                     model_params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Test statistical significance using bootstrap and permutation tests."""
        if model_params is None:
            model_params = {'alpha': 0.001, 'l1_ratio': 0.5, 'random_state': 42}

        # Simple train-test split
        split_idx = int(len(features) * 0.8)
        X_train, X_test = features.iloc[:split_idx], features.iloc[split_idx:]
        y_train, y_test = target.iloc[:split_idx], target.iloc[split_idx:]

        def score_func(X_tr, y_tr, X_te, y_te):
            """Scoring function for significance tests."""
            model = ElasticNetVolatilityPredictor(**model_params)
            model.fit(X_tr, y_tr)
            pred_result = model.predict(X_te)
            predictions = pred_result.predictions

            from sklearn.metrics import r2_score
            return r2_score(y_te, predictions)

        # Bootstrap analysis
        print("   ğŸ² Bootstrap ë¶„ì„...")
        bootstrap = BootstrapAnalysis(
            n_bootstrap=self.bootstrap_n_samples,
            random_state=42
        )

        # Calculate base score
        base_score = score_func(X_train, y_train, X_test, y_test)

        # Bootstrap the score
        def bootstrap_statistic(indices):
            X_bootstrap = X_train.iloc[indices]
            y_bootstrap = y_train.iloc[indices]
            return score_func(X_bootstrap, y_bootstrap, X_test, y_test)

        bootstrap_result = bootstrap.basic_bootstrap(
            data=np.arange(len(X_train)),
            statistic_func=bootstrap_statistic,
            confidence_levels=[self.confidence_level]
        )

        # Permutation test
        print("   ğŸ§ª ìˆœì—´ê²€ì •...")
        permutation = PermutationTest(
            n_permutations=self.permutation_n_tests
        )

        # Create simple permutation test
        permutation_scores = []
        for _ in range(self.permutation_n_tests):
            y_permuted = y_train.sample(frac=1).reset_index(drop=True)
            perm_score = score_func(X_train, y_permuted, X_test, y_test)
            permutation_scores.append(perm_score)

        # Calculate p-value
        permutation_pvalue = np.mean(np.array(permutation_scores) >= base_score)

        ci_key = f"{self.confidence_level:.0%}"
        if ci_key in bootstrap_result.confidence_intervals:
            ci_lower, ci_upper = bootstrap_result.confidence_intervals[ci_key]
        else:
            ci_lower, ci_upper = bootstrap_result.confidence_intervals['95%']

        print(f"   âœ… í†µê³„ì  ìœ ì˜ì„±: Bootstrap CI=[{ci_lower:.4f}, {ci_upper:.4f}]")
        print(f"   âœ… ìˆœì—´ê²€ì •: p-value={permutation_pvalue:.6f}")

        return {
            'bootstrap_distribution': bootstrap_result.bootstrap_distribution.tolist(),
            'bootstrap_ci_lower': ci_lower,
            'bootstrap_ci_upper': ci_upper,
            'permutation_pvalue': permutation_pvalue,
            'is_significant': permutation_pvalue < 0.05
        }

    def _apply_multiple_testing_corrections(self, p_values: List[float]) -> Dict[str, float]:
        """Apply multiple testing corrections."""
        from scipy.stats import false_discovery_control

        p_array = np.array(p_values)

        # Bonferroni correction
        bonferroni = np.minimum(p_array * len(p_array), 1.0)

        # Benjamini-Hochberg (FDR)
        benjamini_hochberg = false_discovery_control(p_array, method='bh')

        # Holm-Bonferroni
        sorted_indices = np.argsort(p_array)
        holm_bonferroni = np.zeros_like(p_array)
        for i, idx in enumerate(sorted_indices):
            holm_bonferroni[idx] = min(p_array[idx] * (len(p_array) - i), 1.0)

        return {
            'bonferroni': float(bonferroni[0]),  # Use first p-value for main result
            'benjamini_hochberg': float(benjamini_hochberg[0]),
            'holm_bonferroni': float(holm_bonferroni[0])
        }

    def _calculate_validation_score(self,
                                  base_metrics: Dict[str, float],
                                  cpcv_results: Dict[str, Any],
                                  dsr_results: Dict[str, Any],
                                  significance_results: Dict[str, Any]) -> Tuple[float, str]:
        """Calculate overall validation score and recommendation."""
        score = 0
        criteria = []

        # Base performance (30 points)
        r2 = base_metrics['r2']
        if r2 > 0.20:
            score += 30
            criteria.append("âœ… ê°•ë ¥í•œ ê¸°ë³¸ ì„±ëŠ¥ (RÂ² > 0.20)")
        elif r2 > 0.10:
            score += 20
            criteria.append("âœ… ì–‘í˜¸í•œ ê¸°ë³¸ ì„±ëŠ¥ (RÂ² > 0.10)")
        elif r2 > 0.05:
            score += 10
            criteria.append("âš ï¸ ë³´í†µ ê¸°ë³¸ ì„±ëŠ¥ (RÂ² > 0.05)")
        else:
            criteria.append("âŒ ë‚®ì€ ê¸°ë³¸ ì„±ëŠ¥ (RÂ² â‰¤ 0.05)")

        # CPCV stability (25 points)
        cpcv_std = cpcv_results['r2_std']
        if cpcv_std < 0.05:
            score += 25
            criteria.append("âœ… ë§¤ìš° ì•ˆì •ì ì¸ CPCV ë¶„í¬")
        elif cpcv_std < 0.10:
            score += 15
            criteria.append("âœ… ì•ˆì •ì ì¸ CPCV ë¶„í¬")
        elif cpcv_std < 0.20:
            score += 10
            criteria.append("âš ï¸ ë³´í†µ CPCV ë¶„í¬")
        else:
            criteria.append("âŒ ë¶ˆì•ˆì •í•œ CPCV ë¶„í¬")

        # DSR significance (20 points)
        if dsr_results['is_significant']:
            score += 20
            criteria.append("âœ… DSR ê²€ì¦ í†µê³¼ (ì„ íƒí¸í–¥ ì—†ìŒ)")
        else:
            criteria.append("âŒ DSR ê²€ì¦ ì‹¤íŒ¨ (ì„ íƒí¸í–¥ ìš°ë ¤)")

        # Statistical significance (25 points)
        if significance_results['is_significant']:
            score += 25
            criteria.append("âœ… í†µê³„ì  ìœ ì˜ì„± í™•ì¸")
        else:
            criteria.append("âŒ í†µê³„ì  ìœ ì˜ì„± ì—†ìŒ")

        # Generate recommendation
        if score >= 85:
            recommendation = "ğŸŸ¢ ì‹¤ì „ íˆ¬ì ê¶Œì¥ - ëª¨ë“  ê²€ì¦ ê¸°ì¤€ í†µê³¼"
        elif score >= 70:
            recommendation = "ğŸŸ¡ ì œí•œì  ì‚¬ìš© ê¶Œì¥ - ëŒ€ë¶€ë¶„ ê²€ì¦ ê¸°ì¤€ í†µê³¼"
        elif score >= 50:
            recommendation = "ğŸŸ  ì‹ ì¤‘í•œ ì—°êµ¬ ì‚¬ìš© - ì¼ë¶€ ê²€ì¦ ê¸°ì¤€ í†µê³¼"
        else:
            recommendation = "ğŸ”´ ì‚¬ìš© ë¹„ê¶Œì¥ - ê²€ì¦ ê¸°ì¤€ ë¯¸ë‹¬"

        return score, recommendation


def main():
    """Main function for testing the validation system."""
    import argparse

    parser = argparse.ArgumentParser(description="Volatility Model Validation System")
    parser.add_argument("--demo", action="store_true", help="Run demonstration")
    parser.add_argument("--full", action="store_true", help="Run full validation")
    parser.add_argument("--days", type=int, default=1000, help="Number of days to simulate")

    args = parser.parse_args()

    # Generate sample data
    print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ìƒì„±...")
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=args.days, freq='D')

    # Simulate realistic stock data with volatility clustering
    initial_price = 100.0
    returns = []
    current_vol = 0.02

    for i in range(args.days):
        vol_shock = np.random.normal(0, 0.001)
        current_vol = 0.95 * current_vol + 0.05 * 0.02 + vol_shock
        current_vol = max(0.005, min(0.1, current_vol))

        daily_return = np.random.normal(0.0005, current_vol)
        returns.append(daily_return)

    prices = [initial_price]
    for ret in returns:
        new_price = prices[-1] * (1 + ret)
        prices.append(new_price)
    prices = prices[1:]

    data = pd.DataFrame({
        'open': [p * np.random.uniform(0.995, 1.005) for p in prices],
        'high': [p * np.random.uniform(1.005, 1.025) for p in prices],
        'low': [p * np.random.uniform(0.975, 0.995) for p in prices],
        'close': prices,
        'volume': np.random.lognormal(13, 0.5, args.days).astype(int)
    }, index=dates)

    # Initialize validation system
    if args.demo:
        validator = VolatilityValidationSystem(
            cpcv_n_combinations=50,
            bootstrap_n_samples=100,
            permutation_n_tests=100
        )
    else:
        validator = VolatilityValidationSystem(
            cpcv_n_combinations=500,
            bootstrap_n_samples=1000,
            permutation_n_tests=1000
        )

    # Run validation
    results = validator.validate_volatility_model(
        data=data,
        selection_bias_trials=50 if args.demo else 100
    )

    # Display results
    print(results.summary_report())

    # Save results
    output_path = Path("/root/workspace/data/raw/volatility_validation_results.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results.to_dict(), f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {output_path}")

    return results


if __name__ == "__main__":
    main()