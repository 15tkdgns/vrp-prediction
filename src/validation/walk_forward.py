#!/usr/bin/env python3
"""
Walk-Forward Validation ëª¨ë“ˆ
ì‹œê³„ì—´ ë°ì´í„°ì— ìµœì í™”ëœ ê²€ì¦ìœ¼ë¡œ ë” í˜„ì‹¤ì ì¸ ì„±ëŠ¥ í‰ê°€
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional, Union
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
import logging
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class WalkForwardValidator:
    """Walk-Forward Validation êµ¬í˜„ í´ë˜ìŠ¤"""
    
    def __init__(self, data_file: str = "data/raw/integrated_spy_news_data.csv",
                 results_dir: str = "results"):
        self.data_file = data_file
        self.results_dir = results_dir
        self.data = None
        self.results = {}
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(results_dir, exist_ok=True)
        
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        logger.info(f"Loading data from {self.data_file}")
        self.data = pd.read_csv(self.data_file)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
        if 'date' in self.data.columns:
            self.data['date'] = pd.to_datetime(self.data['date'])
            self.data = self.data.sort_values('date').reset_index(drop=True)
        else:
            logger.warning("No date column found, using index as time order")
            self.data = self.data.reset_index(drop=True)
            
        # NaN ê°’ ì œê±°
        self.data = self.data.dropna()
        
        logger.info(f"Data loaded: {len(self.data)} samples")
        logger.info(f"Date range: {self.data['date'].min()} to {self.data['date'].max()}")
        
    def create_walk_forward_splits(self, train_window: int = 30, test_window: int = 5, 
                                   step_size: int = 1, min_train_size: int = 20) -> List[Dict]:
        """Walk-Forward ë¶„í•  ìƒì„±
        
        Args:
            train_window: í›ˆë ¨ ë°ì´í„° ìœˆë„ìš° í¬ê¸° (ì¼ìˆ˜)
            test_window: í…ŒìŠ¤íŠ¸ ë°ì´í„° ìœˆë„ìš° í¬ê¸° (ì¼ìˆ˜) 
            step_size: ìŠ¬ë¼ì´ë”© ìŠ¤í… í¬ê¸° (ì¼ìˆ˜)
            min_train_size: ìµœì†Œ í›ˆë ¨ ë°ì´í„° í¬ê¸°
            
        Returns:
            ë¶„í•  ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        splits = []
        data_size = len(self.data)
        
        # ì‹œì‘ ìœ„ì¹˜ì—ì„œ ì¶©ë¶„í•œ í›ˆë ¨ ë°ì´í„° í™•ë³´
        start_idx = max(train_window, min_train_size)
        
        current_idx = start_idx
        split_id = 1
        
        while current_idx + test_window <= data_size:
            # í›ˆë ¨ ë°ì´í„° ì¸ë±ìŠ¤ ë²”ìœ„
            train_start = max(0, current_idx - train_window)
            train_end = current_idx
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ë±ìŠ¤ ë²”ìœ„  
            test_start = current_idx
            test_end = min(current_idx + test_window, data_size)
            
            # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ìœˆë„ìš°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì¤‘ë‹¨
            if test_end - test_start < test_window:
                break
                
            # ë‚ ì§œ ì •ë³´ ì¶”ê°€
            split_info = {
                'split_id': split_id,
                'train_indices': list(range(train_start, train_end)),
                'test_indices': list(range(test_start, test_end)),
                'train_size': train_end - train_start,
                'test_size': test_end - test_start
            }
            
            # ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if 'date' in self.data.columns:
                split_info.update({
                    'train_start_date': self.data.loc[train_start, 'date'].strftime('%Y-%m-%d'),
                    'train_end_date': self.data.loc[train_end-1, 'date'].strftime('%Y-%m-%d'),
                    'test_start_date': self.data.loc[test_start, 'date'].strftime('%Y-%m-%d'),
                    'test_end_date': self.data.loc[test_end-1, 'date'].strftime('%Y-%m-%d')
                })
                
            splits.append(split_info)
            
            # ë‹¤ìŒ ìœ„ì¹˜ë¡œ ì´ë™
            current_idx += step_size
            split_id += 1
            
        logger.info(f"Created {len(splits)} walk-forward splits")
        logger.info(f"Average train size: {np.mean([s['train_size'] for s in splits]):.1f}")
        logger.info(f"Average test size: {np.mean([s['test_size'] for s in splits]):.1f}")
        
        return splits
        
    def prepare_features(self, exclude_cols: List[str] = None) -> Tuple[List[str], List[str]]:
        """íŠ¹ì„± ì¤€ë¹„ (ë‰´ìŠ¤ vs ê¸°ìˆ ì  ì§€í‘œ ë¶„ë¥˜)"""
        if exclude_cols is None:
            exclude_cols = ['date', 'target']
            
        # ë‰´ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ
        news_keywords = ['sentiment', 'news', 'article', 'impact']
        
        # ë‰´ìŠ¤ íŠ¹ì„± ë¶„ë¥˜
        news_features = [col for col in self.data.columns 
                        if any(keyword in col.lower() for keyword in news_keywords) 
                        and col not in exclude_cols]
        
        # ê¸°ìˆ ì  ì§€í‘œ ë¶„ë¥˜
        technical_features = [col for col in self.data.columns 
                             if col not in exclude_cols 
                             and col not in news_features]
        
        logger.info(f"Technical features ({len(technical_features)}): {technical_features}")
        logger.info(f"News features ({len(news_features)}): {news_features}")
        
        return technical_features, news_features
        
    def validate_model(self, model_class, model_params: Dict, features: List[str],
                      splits: List[Dict], model_name: str) -> Dict:
        """íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ Walk-Forward Validation ìˆ˜í–‰"""
        logger.info(f"Starting Walk-Forward validation for {model_name}")
        
        results = {
            'model_name': model_name,
            'model_class': model_class.__name__,
            'features': features,
            'feature_count': len(features),
            'splits_count': len(splits),
            'split_results': [],
            'overall_metrics': {},
            'time_series_metrics': []
        }
        
        all_predictions = []
        all_actuals = []
        all_dates = []
        
        # í‘œì¤€í™” ìŠ¤ì¼€ì¼ëŸ¬ (ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê²½ìš°ë§Œ ì‚¬ìš©)
        use_scaler = 'Logistic' in model_name
        
        for split in splits:
            try:
                # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
                train_idx = split['train_indices'] 
                test_idx = split['test_indices']
                
                X_train = self.data.loc[train_idx, features]
                y_train = self.data.loc[train_idx, 'target']
                X_test = self.data.loc[test_idx, features]
                y_test = self.data.loc[test_idx, 'target']
                
                # ìŠ¤ì¼€ì¼ë§ (í•„ìš”í•œ ê²½ìš°)
                if use_scaler:
                    scaler = StandardScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    X_train = pd.DataFrame(X_train_scaled, columns=features, index=X_train.index)
                    X_test = pd.DataFrame(X_test_scaled, columns=features, index=X_test.index)
                
                # ëª¨ë¸ í›ˆë ¨
                model = model_class(**model_params)
                model.fit(X_train, y_train)
                
                # ì˜ˆì¸¡
                y_pred = model.predict(X_test)
                y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
                
                # ì„±ëŠ¥ ê³„ì‚°
                metrics = {
                    'split_id': split['split_id'],
                    'accuracy': accuracy_score(y_test, y_pred),
                    'precision': precision_score(y_test, y_pred, zero_division=0),
                    'recall': recall_score(y_test, y_pred, zero_division=0),
                    'f1_score': f1_score(y_test, y_pred, zero_division=0)
                }
                
                if y_pred_proba is not None:
                    try:
                        metrics['auc'] = roc_auc_score(y_test, y_pred_proba)
                    except:
                        metrics['auc'] = 0.5
                else:
                    metrics['auc'] = 0.5
                    
                # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                if 'test_start_date' in split:
                    metrics['test_date'] = split['test_start_date']
                    
                results['split_results'].append(metrics)
                
                # ì „ì²´ ê²°ê³¼ ëˆ„ì 
                all_predictions.extend(y_pred.tolist())
                all_actuals.extend(y_test.tolist())
                
                if 'date' in self.data.columns:
                    test_dates = self.data.loc[test_idx, 'date'].dt.strftime('%Y-%m-%d').tolist()
                    all_dates.extend(test_dates)
                    
            except Exception as e:
                logger.warning(f"Split {split['split_id']} failed: {str(e)}")
                continue
                
        # ì „ì²´ ì„±ëŠ¥ ê³„ì‚°
        if all_predictions and all_actuals:
            overall_metrics = {
                'accuracy': accuracy_score(all_actuals, all_predictions),
                'precision': precision_score(all_actuals, all_predictions, zero_division=0),
                'recall': recall_score(all_actuals, all_predictions, zero_division=0),
                'f1_score': f1_score(all_actuals, all_predictions, zero_division=0),
                'total_predictions': len(all_predictions)
            }
            
            results['overall_metrics'] = overall_metrics
            
            # ì‹œê³„ì—´ ì„±ëŠ¥ ì¶”ì´
            results['time_series_metrics'] = [
                {
                    'date': all_dates[i] if all_dates else f"sample_{i}",
                    'prediction': int(all_predictions[i]),
                    'actual': int(all_actuals[i]),
                    'correct': int(all_predictions[i] == all_actuals[i])
                }
                for i in range(len(all_predictions))
            ]
            
        logger.info(f"Walk-Forward validation completed for {model_name}")
        logger.info(f"Overall accuracy: {overall_metrics.get('accuracy', 0):.4f}")
        
        return results
        
    def compare_validation_methods(self, models_config: Dict, train_window: int = 30, 
                                   test_window: int = 5) -> Dict:
        """Walk-Forward vs ê¸°ì¡´ ê²€ì¦ ë°©ë²• ë¹„êµ"""
        logger.info("Comparing validation methods...")
        
        # íŠ¹ì„± ì¤€ë¹„
        technical_features, news_features = self.prepare_features()
        all_features = technical_features + news_features
        
        # Walk-Forward ë¶„í•  ìƒì„±
        wf_splits = self.create_walk_forward_splits(
            train_window=train_window, 
            test_window=test_window
        )
        
        comparison_results = {
            'validation_comparison': {
                'walk_forward': {},
                'traditional': {}
            },
            'feature_comparison': {
                'baseline': {},  # ê¸°ìˆ ì  ì§€í‘œë§Œ
                'enhanced': {}   # ê¸°ìˆ ì  + ë‰´ìŠ¤
            }
        }
        
        # ëª¨ë¸ë³„ ê²€ì¦ ìˆ˜í–‰
        for model_name, config in models_config.items():
            model_class = config['class']
            model_params = config['params']
            
            # Baseline (ê¸°ìˆ ì  ì§€í‘œë§Œ) - Walk-Forward
            baseline_results = self.validate_model(
                model_class, model_params, technical_features,
                wf_splits, f"Baseline_{model_name}"
            )
            
            # Enhanced (ì „ì²´ íŠ¹ì„±) - Walk-Forward  
            enhanced_results = self.validate_model(
                model_class, model_params, all_features,
                wf_splits, f"Enhanced_{model_name}"
            )
            
            comparison_results['validation_comparison']['walk_forward'][f"Baseline_{model_name}"] = baseline_results
            comparison_results['validation_comparison']['walk_forward'][f"Enhanced_{model_name}"] = enhanced_results
            
            # ê¸°ì¡´ ë°©ë²•ê³¼ ë¹„êµë¥¼ ìœ„í•œ ë‹¨ì¼ ë¶„í•  í…ŒìŠ¤íŠ¸
            train_size = int(len(self.data) * 0.8)
            train_idx = list(range(train_size))
            test_idx = list(range(train_size, len(self.data)))
            
            single_split = [{
                'split_id': 1,
                'train_indices': train_idx,
                'test_indices': test_idx,
                'train_size': len(train_idx),
                'test_size': len(test_idx)
            }]
            
            # Traditional ê²€ì¦
            baseline_traditional = self.validate_model(
                model_class, model_params, technical_features,
                single_split, f"Traditional_Baseline_{model_name}"
            )
            
            enhanced_traditional = self.validate_model(
                model_class, model_params, all_features,
                single_split, f"Traditional_Enhanced_{model_name}"
            )
            
            comparison_results['validation_comparison']['traditional'][f"Baseline_{model_name}"] = baseline_traditional
            comparison_results['validation_comparison']['traditional'][f"Enhanced_{model_name}"] = enhanced_traditional
            
        return comparison_results
        
    def analyze_temporal_stability(self, results: Dict) -> Dict:
        """ì‹œê°„ëŒ€ë³„ ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„"""
        stability_analysis = {}
        
        for method, method_results in results['validation_comparison'].items():
            stability_analysis[method] = {}
            
            for model_name, model_results in method_results.items():
                if 'split_results' not in model_results:
                    continue
                    
                split_accuracies = [s['accuracy'] for s in model_results['split_results']]
                
                stability_metrics = {
                    'mean_accuracy': np.mean(split_accuracies),
                    'std_accuracy': np.std(split_accuracies),
                    'min_accuracy': np.min(split_accuracies),
                    'max_accuracy': np.max(split_accuracies),
                    'stability_score': 1 - (np.std(split_accuracies) / np.mean(split_accuracies)) if np.mean(split_accuracies) > 0 else 0,
                    'coefficient_of_variation': np.std(split_accuracies) / np.mean(split_accuracies) if np.mean(split_accuracies) > 0 else float('inf')
                }
                
                stability_analysis[method][model_name] = stability_metrics
                
        return stability_analysis
        
    def create_performance_plots(self, results: Dict, model_name: str = None):
        """ì„±ëŠ¥ ì¶”ì´ ì‹œê°í™”"""
        if not model_name:
            # ì²« ë²ˆì§¸ Enhanced ëª¨ë¸ ì„ íƒ
            for name in results['validation_comparison']['walk_forward'].keys():
                if 'Enhanced' in name:
                    model_name = name
                    break
                    
        if not model_name:
            logger.warning("No model found for plotting")
            return None
            
        model_results = results['validation_comparison']['walk_forward'][model_name]
        split_results = model_results['split_results']
        
        if not split_results:
            logger.warning(f"No split results found for {model_name}")
            return None
            
        # ì„±ëŠ¥ ì¶”ì´ ê·¸ë˜í”„
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        splits = [s['split_id'] for s in split_results]
        accuracies = [s['accuracy'] for s in split_results]
        precisions = [s['precision'] for s in split_results]
        recalls = [s['recall'] for s in split_results]
        f1_scores = [s['f1_score'] for s in split_results]
        
        # ì •í™•ë„ ì¶”ì´
        ax1.plot(splits, accuracies, 'b-o', linewidth=2, markersize=4)
        ax1.set_title('ì •í™•ë„ ì‹œê°„ ì¶”ì´', fontsize=12, fontweight='bold')
        ax1.set_xlabel('ë¶„í•  ë²ˆí˜¸')
        ax1.set_ylabel('ì •í™•ë„')
        ax1.grid(True, alpha=0.3)
        ax1.axhline(y=np.mean(accuracies), color='r', linestyle='--', alpha=0.7, label=f'í‰ê· : {np.mean(accuracies):.3f}')
        ax1.legend()
        
        # ì •ë°€ë„ ì¶”ì´
        ax2.plot(splits, precisions, 'g-s', linewidth=2, markersize=4)
        ax2.set_title('ì •ë°€ë„ ì‹œê°„ ì¶”ì´', fontsize=12, fontweight='bold')
        ax2.set_xlabel('ë¶„í•  ë²ˆí˜¸')
        ax2.set_ylabel('ì •ë°€ë„')
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=np.mean(precisions), color='r', linestyle='--', alpha=0.7, label=f'í‰ê· : {np.mean(precisions):.3f}')
        ax2.legend()
        
        # ì¬í˜„ìœ¨ ì¶”ì´
        ax3.plot(splits, recalls, 'm-^', linewidth=2, markersize=4)
        ax3.set_title('ì¬í˜„ìœ¨ ì‹œê°„ ì¶”ì´', fontsize=12, fontweight='bold')
        ax3.set_xlabel('ë¶„í•  ë²ˆí˜¸')
        ax3.set_ylabel('ì¬í˜„ìœ¨')
        ax3.grid(True, alpha=0.3)
        ax3.axhline(y=np.mean(recalls), color='r', linestyle='--', alpha=0.7, label=f'í‰ê· : {np.mean(recalls):.3f}')
        ax3.legend()
        
        # F1 ì ìˆ˜ ì¶”ì´
        ax4.plot(splits, f1_scores, 'c-d', linewidth=2, markersize=4)
        ax4.set_title('F1 ì ìˆ˜ ì‹œê°„ ì¶”ì´', fontsize=12, fontweight='bold')
        ax4.set_xlabel('ë¶„í•  ë²ˆí˜¸')
        ax4.set_ylabel('F1 ì ìˆ˜')
        ax4.grid(True, alpha=0.3)
        ax4.axhline(y=np.mean(f1_scores), color='r', linestyle='--', alpha=0.7, label=f'í‰ê· : {np.mean(f1_scores):.3f}')
        ax4.legend()
        
        plt.suptitle(f'Walk-Forward Validation ì„±ëŠ¥ ì¶”ì´: {model_name}', fontsize=14, fontweight='bold', y=0.95)
        plt.tight_layout()
        
        # íŒŒì¼ ì €ì¥
        plot_file = os.path.join(self.results_dir, f'walk_forward_performance_{model_name.lower()}.png')
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Performance plot saved: {plot_file}")
        return plot_file
        
    def save_results(self, results: Dict, output_file: str = None):
        """ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        if not output_file:
            output_file = os.path.join(self.results_dir, 'walk_forward_validation_results.json')
            
        # ì•ˆì •ì„± ë¶„ì„ ì¶”ê°€
        stability_analysis = self.analyze_temporal_stability(results)
        results['stability_analysis'] = stability_analysis
        
        # ë©”íƒ€ ì •ë³´ ì¶”ê°€
        results['metadata'] = {
            'timestamp': datetime.now().isoformat(),
            'total_samples': len(self.data) if self.data is not None else 0,
            'validation_date_range': {
                'start': self.data['date'].min().isoformat() if 'date' in self.data.columns else None,
                'end': self.data['date'].max().isoformat() if 'date' in self.data.columns else None
            }
        }
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        results_serializable = self._convert_to_json_serializable(results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, indent=2, ensure_ascii=False)
            
        logger.info(f"Walk-Forward validation results saved: {output_file}")
        return output_file
        
    def _convert_to_json_serializable(self, obj):
        """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(v) for v in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32, np.int_)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32, np.float_)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif pd.isna(obj):
            return None
        else:
            return obj
            
    def run_complete_validation(self, train_window: int = 30, test_window: int = 5):
        """ì „ì²´ Walk-Forward Validation ì‹¤í–‰"""
        logger.info("Starting complete Walk-Forward validation")
        
        # ë°ì´í„° ë¡œë“œ
        self.load_data()
        
        # ëª¨ë¸ ì„¤ì •
        models_config = {
            'RandomForest': {
                'class': RandomForestClassifier,
                'params': {'n_estimators': 100, 'random_state': 42, 'n_jobs': -1}
            },
            'GradientBoosting': {
                'class': GradientBoostingClassifier, 
                'params': {'n_estimators': 100, 'random_state': 42, 'learning_rate': 0.1}
            },
            'LogisticRegression': {
                'class': LogisticRegression,
                'params': {'random_state': 42, 'max_iter': 1000}
            }
        }
        
        # ê²€ì¦ ì‹¤í–‰
        results = self.compare_validation_methods(
            models_config, 
            train_window=train_window,
            test_window=test_window
        )
        
        # ì‹œê°í™” ìƒì„±
        self.create_performance_plots(results)
        
        # ê²°ê³¼ ì €ì¥
        output_file = self.save_results(results)
        
        # ìš”ì•½ ì¶œë ¥
        self._print_summary(results)
        
        logger.info("Walk-Forward validation completed")
        return output_file
        
    def _print_summary(self, results: Dict):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š WALK-FORWARD VALIDATION ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        # Walk-Forward vs Traditional ë¹„êµ
        wf_results = results['validation_comparison']['walk_forward']
        trad_results = results['validation_comparison']['traditional']
        
        print("\nğŸ” ê²€ì¦ ë°©ë²• ë¹„êµ:")
        print("-" * 40)
        
        for model_type in ['Enhanced_GradientBoosting', 'Enhanced_RandomForest']:
            if model_type in wf_results and model_type in trad_results:
                wf_acc = wf_results[model_type]['overall_metrics']['accuracy']
                trad_acc = trad_results[model_type]['overall_metrics']['accuracy'] 
                
                print(f"{model_type}:")
                print(f"  Walk-Forward: {wf_acc:.4f}")
                print(f"  Traditional:  {trad_acc:.4f}")
                print(f"  ì°¨ì´: {wf_acc - trad_acc:+.4f}")
                print()
                
        # ì•ˆì •ì„± ë¶„ì„
        if 'stability_analysis' in results:
            print("\nğŸ“ˆ ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„:")
            print("-" * 40)
            
            wf_stability = results['stability_analysis'].get('walk_forward', {})
            for model_name, metrics in wf_stability.items():
                if 'Enhanced' in model_name:
                    print(f"{model_name}:")
                    print(f"  í‰ê·  ì •í™•ë„: {metrics['mean_accuracy']:.4f}")
                    print(f"  í‘œì¤€í¸ì°¨: {metrics['std_accuracy']:.4f}")
                    print(f"  ì•ˆì •ì„± ì ìˆ˜: {metrics['stability_score']:.4f}")
                    print()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    validator = WalkForwardValidator()
    
    # Walk-Forward Validation ì‹¤í–‰
    result_file = validator.run_complete_validation(
        train_window=30,  # 30ì¼ í›ˆë ¨ ìœˆë„ìš°
        test_window=5     # 5ì¼ í…ŒìŠ¤íŠ¸ ìœˆë„ìš°
    )
    
    print(f"\nâœ… Walk-Forward Validation ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {result_file}")


if __name__ == "__main__":
    main()