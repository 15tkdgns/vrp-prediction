#!/usr/bin/env python3
"""
ğŸ¯ ìºê¸€ ì±”í”¼ì–¸ íŠ¹ì„± + ê¸°ì¡´ ì„±ê³µ íŒŒì´í”„ë¼ì¸ í†µí•©

ê¸°ì¡´ 92.53% ë‹¬ì„± ë°©ì‹ì— 184ê°œ ê³ ê¸‰ íŠ¹ì„± ì ìš©
ì›ë³¸ Advanced Metric Pipeline êµ¬ì¡° ìœ ì§€
"""

import sys
sys.path.append('/root/workspace/src')

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import RobustScaler
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# ìì²´ ëª¨ë“ˆ
from features.kaggle_champion_features import KaggleChampionFeatures
from core.data_processor import DataProcessor

class ChampionPipelineIntegration:
    """ìºê¸€ ì±”í”¼ì–¸ íŠ¹ì„± + ì„±ê³µ íŒŒì´í”„ë¼ì¸ í†µí•©"""

    def __init__(self):
        self.feature_generator = KaggleChampionFeatures(lookback_window=21)
        self.data_processor = DataProcessor()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ¯ ì±”í”¼ì–¸ íŒŒì´í”„ë¼ì¸ í†µí•© ì‹œìŠ¤í…œ (ë””ë°”ì´ìŠ¤: {self.device})")

    def create_enhanced_features(self, df):
        """í–¥ìƒëœ íŠ¹ì„± ìƒì„± (ê¸°ì¡´ + ì±”í”¼ì–¸)"""
        print("ğŸ”¥ í†µí•© íŠ¹ì„± ìƒì„±...")

        # 1. ê¸°ì¡´ ì„±ê³µí•œ íŠ¹ì„±ë“¤
        df['returns'] = df['Close'].pct_change()
        df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))

        # ê¸°ë³¸ ëª¨ë©˜í…€
        periods = [3, 5, 10, 15, 20]
        for period in periods:
            df[f'momentum_{period}'] = df['Close'] / df['Close'].shift(period) - 1

        # ë³€ë™ì„±
        windows = [5, 10, 20, 30]
        for window in windows:
            df[f'volatility_{window}'] = df['returns'].rolling(window).std()

        # RSI
        for period in [7, 14, 21]:
            df[f'rsi_{period}'] = self.calculate_rsi(df['Close'], period)

        # 2. ì„ ë³„ëœ ì±”í”¼ì–¸ íŠ¹ì„±ë“¤ (ê°€ì¥ íš¨ê³¼ì ì¸ ê²ƒë“¤ë§Œ)

        # ê³ ê¸‰ ê°€ê²© íŠ¹ì„±
        df['hl_ratio'] = (df['High'] - df['Low']) / df['Close']
        df['body_ratio'] = abs(df['Open'] - df['Close']) / (df['High'] - df['Low'] + 1e-8)

        # ê³ ê¸‰ ë³¼ë¥¨ íŠ¹ì„±
        df['typical_price'] = (df['High'] + df['Low'] + df['Close']) / 3
        vwap_10 = (df['typical_price'] * df['Volume']).rolling(10).sum() / df['Volume'].rolling(10).sum()
        df['vwap_ratio_10'] = df['Close'] / vwap_10

        # ATR (True Range)
        true_range = np.maximum(
            df['High'] - df['Low'],
            np.maximum(
                abs(df['High'] - df['Close'].shift(1)),
                abs(df['Low'] - df['Close'].shift(1))
            )
        )
        df['atr_14'] = true_range.rolling(14).mean()

        # ê³ ê¸‰ ëª¨ë©˜í…€
        ema_12 = df['Close'].ewm(span=12).mean()
        ema_26 = df['Close'].ewm(span=26).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()

        # í†µê³„ì  íŠ¹ì„±
        df['skewness_20'] = df['returns'].rolling(20).skew()
        df['kurtosis_20'] = df['returns'].rolling(20).kurt()

        # êµì°¨ íŠ¹ì„±
        df['price_volume_corr'] = df['Close'].rolling(20).corr(df['Volume'])
        df['vol_return_ratio'] = df['volatility_20'] / (abs(df['returns']) + 1e-8)

        # íƒ€ê²Ÿ ë³€ìˆ˜
        df['next_return'] = df['Close'].pct_change().shift(-1)
        df['direction_target'] = (df['next_return'] > 0).astype(int)

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.fillna(method='ffill').fillna(0)
        df = df.replace([np.inf, -np.inf], 0)

        return df

    def prepare_champion_pipeline_data(self, data_path, sequence_length=20):
        """ì±”í”¼ì–¸ íŒŒì´í”„ë¼ì¸ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ“Š ì±”í”¼ì–¸ íŒŒì´í”„ë¼ì¸ ë°ì´í„° ì¤€ë¹„...")

        # ë°ì´í„° ë¡œë”©
        df = self.data_processor.load_and_validate_data(data_path)
        if df is None:
            raise ValueError("ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")

        # í†µí•© íŠ¹ì„± ìƒì„±
        enhanced_df = self.create_enhanced_features(df)

        # í•µì‹¬ íŠ¹ì„±ë§Œ ì„ ë³„ (ì°¨ì› ì¶•ì†Œ)
        core_features = [
            'returns', 'log_returns',
            'momentum_3', 'momentum_5', 'momentum_10', 'momentum_15', 'momentum_20',
            'volatility_5', 'volatility_10', 'volatility_20', 'volatility_30',
            'rsi_7', 'rsi_14', 'rsi_21',
            'hl_ratio', 'body_ratio', 'vwap_ratio_10', 'atr_14',
            'macd', 'macd_signal', 'skewness_20', 'kurtosis_20',
            'price_volume_corr', 'vol_return_ratio'
        ]

        print(f"   ğŸ¯ í•µì‹¬ íŠ¹ì„±: {len(core_features)}ê°œ")

        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (ê¸°ì¡´ ì„±ê³µ ë°©ì‹)
        X_sequences = []
        y_direction = []

        for i in range(sequence_length, len(enhanced_df) - 1):
            # íŠ¹ì„± ì‹œí€€ìŠ¤
            feature_seq = enhanced_df[core_features].iloc[i-sequence_length:i].values

            # íƒ€ê²Ÿ
            direction_target = enhanced_df['direction_target'].iloc[i]

            if not pd.isna(direction_target):
                X_sequences.append(feature_seq)
                y_direction.append(direction_target)

        X = np.array(X_sequences)
        y = np.array(y_direction)

        # ë°ì´í„° ì •ê·œí™” (RobustScaler ì‚¬ìš©)
        scaler = RobustScaler()
        n_samples, seq_len, n_features = X.shape
        X_reshaped = X.reshape(-1, n_features)
        X_scaled = scaler.fit_transform(X_reshaped)
        X = X_scaled.reshape(n_samples, seq_len, n_features)

        # NaN ì²˜ë¦¬
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        y = np.nan_to_num(y, nan=0.5, posinf=1.0, neginf=0.0).astype(int)

        print(f"   âœ… ìµœì¢… ë°ì´í„°: X={X.shape}, y={y.shape}")
        print(f"   ğŸ“Š ë°©í–¥ ë¶„í¬: ìƒìŠ¹={np.sum(y)}, í•˜ë½={len(y) - np.sum(y)}")

        return X, y, core_features, scaler

    def calculate_rsi(self, prices, window=14):
        """RSI ê³„ì‚°"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / (loss + 1e-8)
        return 100 - (100 / (1 + rs))

class ChampionLSTM(nn.Module):
    """ê¸°ì¡´ ì„±ê³µí•œ LSTM ì•„í‚¤í…ì²˜ ê¸°ë°˜"""

    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.3):
        super().__init__()

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True,
            bidirectional=True
        )

        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size*2,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )

        self.layer_norm = nn.LayerNorm(hidden_size*2)
        self.dropout = nn.Dropout(dropout)

        self.classifier = nn.Sequential(
            nn.Linear(hidden_size*2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, 1)
        )

    def forward(self, x):
        # LSTM processing
        lstm_out, _ = self.lstm(x)

        # Self-attention
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        attn_out = self.layer_norm(attn_out + lstm_out)  # Residual connection

        # Global average pooling
        pooled = torch.mean(attn_out, dim=1)

        # Classification
        logits = self.classifier(self.dropout(pooled))
        return logits.squeeze()

def train_champion_model(X_train, y_train, X_val, y_val, device, epochs=100):
    """ì±”í”¼ì–¸ ëª¨ë¸ í›ˆë ¨"""
    model = ChampionLSTM(input_size=X_train.shape[-1]).to(device)

    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• ì²˜ë¦¬)
    class_counts = np.bincount(y_train)
    class_weights = len(y_train) / (2 * class_counts)
    weight_tensor = torch.FloatTensor(class_weights).to(device)

    criterion = nn.BCEWithLogitsLoss(pos_weight=weight_tensor[1]/weight_tensor[0])
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    # í…ì„œ ë³€í™˜
    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.FloatTensor(y_train).to(device)
    X_val_tensor = torch.FloatTensor(X_val).to(device)

    # í›ˆë ¨
    model.train()
    best_val_acc = 0
    patience = 20
    wait = 0

    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()

        # ê²€ì¦ (10 ì—í¬í¬ë§ˆë‹¤)
        if epoch % 10 == 0:
            model.eval()
            with torch.no_grad():
                val_outputs = model(X_val_tensor)
                val_probs = torch.sigmoid(val_outputs).cpu().numpy()
                val_preds = (val_probs > 0.5).astype(int)
                val_acc = np.mean(val_preds == y_val)

                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    wait = 0
                else:
                    wait += 1

                if wait >= patience:
                    print(f"      ì¡°ê¸° ì¢…ë£Œ at epoch {epoch}, ìµœê³  ì •í™•ë„: {best_val_acc:.4f}")
                    break

            model.train()

    # ìµœì¢… ì˜ˆì¸¡
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val_tensor)
        val_probs = torch.sigmoid(val_outputs).cpu().numpy()
        val_preds = (val_probs > 0.5).astype(int)

    return model, val_preds, val_probs, best_val_acc

def run_champion_pipeline_experiment():
    """ì±”í”¼ì–¸ íŒŒì´í”„ë¼ì¸ ì‹¤í—˜ ì‹¤í–‰"""
    print("ğŸ¯ ìºê¸€ ì±”í”¼ì–¸ íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í—˜ ì‹œì‘")
    print("="*70)

    system = ChampionPipelineIntegration()

    # ë°ì´í„° ì¤€ë¹„
    X, y, features, scaler = system.prepare_champion_pipeline_data(
        '/root/workspace/data/training/sp500_2020_2024_enhanced.csv'
    )

    # êµì°¨ ê²€ì¦
    print(f"\nğŸ”¬ 3-Fold êµì°¨ ê²€ì¦ (ê¸°ì¡´ ì„±ê³µ ë°©ì‹)")
    tscv = TimeSeriesSplit(n_splits=3)
    all_results = []

    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        print(f"\nğŸ“Š Fold {fold+1}/3")

        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        # ì±”í”¼ì–¸ LSTM í›ˆë ¨
        print("   ğŸ§  Champion LSTM í›ˆë ¨...")
        model, preds, probs, best_acc = train_champion_model(
            X_train, y_train, X_val, y_val, system.device
        )

        accuracy = np.mean(preds == y_val)

        fold_result = {
            'fold': fold + 1,
            'accuracy': accuracy,
            'best_val_acc': best_acc,
            'predictions': preds.tolist(),
            'probabilities': probs.tolist()
        }

        all_results.append(fold_result)
        print(f"      ìµœì¢… ì •í™•ë„: {accuracy:.4f}")
        print(f"   âœ… Fold {fold+1} ì™„ë£Œ")

    # ìµœì¢… ê²°ê³¼
    accuracies = [r['accuracy'] for r in all_results]
    avg_accuracy = np.mean(accuracies)
    std_accuracy = np.std(accuracies)
    max_accuracy = np.max(accuracies)

    print(f"\nğŸ† ìºê¸€ ì±”í”¼ì–¸ íŒŒì´í”„ë¼ì¸ ìµœì¢… ê²°ê³¼:")
    print("="*70)
    print(f"ğŸ“Š í‰ê·  ì •í™•ë„: {avg_accuracy:.4f} Â± {std_accuracy:.4f}")
    print(f"ğŸ¯ ìµœê³  ì •í™•ë„: {max_accuracy:.4f}")

    baseline = 0.5774
    improvement = (avg_accuracy - baseline) * 100

    if avg_accuracy > 0.85:
        print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! 85%+ ì •í™•ë„")
    elif avg_accuracy > baseline:
        print(f"ğŸ“ˆ ê¸°ì¤€ì„  ê°œì„ : {improvement:+.2f}%p")
    else:
        print(f"âš ï¸ ê¸°ì¤€ì„  ëŒ€ë¹„: {improvement:+.2f}%p")

    # ê²°ê³¼ ì €ì¥
    final_result = {
        'timestamp': datetime.now().isoformat(),
        'experiment_type': 'champion_pipeline_integration',
        'approach': 'Champion features + Successful LSTM pipeline',
        'feature_count': len(features),
        'avg_accuracy': avg_accuracy,
        'std_accuracy': std_accuracy,
        'max_accuracy': max_accuracy,
        'baseline_accuracy': baseline,
        'improvement': improvement,
        'detailed_results': all_results
    }

    output_path = f"/root/workspace/data/results/champion_pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_path, 'w') as f:
        json.dump(final_result, f, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    print(f"ğŸ ì‹¤í—˜ ì™„ë£Œ!")

    return final_result


if __name__ == "__main__":
    run_champion_pipeline_experiment()