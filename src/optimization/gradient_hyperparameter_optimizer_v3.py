"""
ê²½ì‚¬í•˜ê°•ë²• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œìŠ¤í…œ v3
ê°œì„ ì‚¬í•­: sklearn í˜¸í™˜ ëª©ì í•¨ìˆ˜ ì‚¬ìš©

v1-v2 ë¬¸ì œì :
- PyTorch ê¸°ë°˜ Ridgeì™€ sklearn Ridge êµ¬í˜„ ì°¨ì´
- ëª©ì í•¨ìˆ˜ì˜ ì¼ê´€ì„± ë¶€ì¡±

v3 ê°œì„ ì‚¬í•­:
- sklearn Ridge Regressionì„ ì§ì ‘ ì‚¬ìš©
- ë™ì¼í•œ ê²€ì¦ ì¡°ê±´ ë³´ì¥
- ë” ì•ˆì •ì ì¸ ìˆ˜ì¹˜ ê³„ì‚°
"""

import numpy as np
import pandas as pd
import yfinance as yf
import json
import time
from datetime import datetime
import logging
import warnings
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score
import scipy.optimize as opt
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/workspace/data/raw/gradient_optimization_v3.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PurgedKFoldSklearn:
    """sklearn í˜¸í™˜ Purged K-Fold Cross-Validation"""

    def __init__(self, n_splits=5, purge_length=5, embargo_length=5):
        self.n_splits = n_splits
        self.purge_length = purge_length
        self.embargo_length = embargo_length

    def split(self, X, y=None):
        """ì‹œê³„ì—´ ë°ì´í„°ë¥¼ purged ë°©ì‹ìœ¼ë¡œ ë¶„í• """
        n_samples = len(X)
        indices = np.arange(n_samples)

        test_size = n_samples // self.n_splits
        splits = []

        for i in range(self.n_splits):
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸
            test_start = i * test_size
            test_end = min((i + 1) * test_size, n_samples)
            test_indices = indices[test_start:test_end]

            # Purge: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì§í›„ ë°ì´í„° ì œê±°
            purge_start = test_end
            purge_end = min(test_end + self.purge_length, n_samples)

            # Embargo: ì¶”ê°€ ê°„ê²©
            embargo_end = min(purge_end + self.embargo_length, n_samples)

            # í›ˆë ¨ ì„¸íŠ¸ (í…ŒìŠ¤íŠ¸ ì „ + embargo í›„)
            train_indices = np.concatenate([
                indices[:test_start],
                indices[embargo_end:]
            ])

            if len(train_indices) > 0 and len(test_indices) > 0:
                splits.append((train_indices, test_indices))

        return splits

    def get_n_splits(self, X=None, y=None, groups=None):
        return self.n_splits

class GradientHyperparameterOptimizerV3:
    """sklearn ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ê¸° v3"""

    def __init__(self):
        self.cv = PurgedKFoldSklearn()
        self.scaler = StandardScaler()
        self.history = []

    def load_spy_data(self):
        """SPY ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        logger.info("ğŸ“Š SPY ë°ì´í„° ë¡œë”© ì¤‘...")

        spy = yf.Ticker("SPY")
        data = spy.history(start="2015-01-01", end="2024-12-31")
        data['returns'] = np.log(data['Close'] / data['Close'].shift(1))
        data = data.dropna()

        logger.info(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(data)}ê°œ ê´€ì¸¡ì¹˜")
        return data

    def create_features(self, data):
        """ì™„ì „í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (ê¸°ì¡´ê³¼ ë™ì¼í•œ 31ê°œ íŠ¹ì„±)"""
        logger.info("ğŸ”§ ì™„ì „í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (31ê°œ íŠ¹ì„±)...")

        returns = data['returns']
        features = pd.DataFrame(index=data.index)

        # 1. ê³¼ê±° ë³€ë™ì„± íŠ¹ì„± (â‰¤ t)
        for window in [5, 10, 20, 50]:
            features[f'volatility_{window}'] = returns.rolling(window).std()

        # 2. ê³ ì°¨ ëª¨ë©˜íŠ¸ (â‰¤ t)
        for window in [5, 10, 20]:
            features[f'skew_{window}'] = returns.rolling(window).skew()
            features[f'kurt_{window}'] = returns.rolling(window).kurt()

        # 3. ë˜ê·¸ íŠ¹ì„± (ê³¼ê±°ë§Œ)
        for lag in [1, 2, 3, 5]:
            features[f'return_lag_{lag}'] = returns.shift(lag)
            features[f'vol_lag_{lag}'] = features['volatility_5'].shift(lag)

        # 4. êµì°¨ í†µê³„ (ê³¼ê±°ë§Œ)
        features['vol_ratio_5_20'] = features['volatility_5'] / (features['volatility_20'] + 1e-8)
        features['vol_ratio_10_50'] = features['volatility_10'] / (features['volatility_50'] + 1e-8)

        # 5. Z-score (ê³¼ê±°ë§Œ)
        ma_20 = returns.rolling(20).mean()
        std_20 = returns.rolling(20).std()
        features['zscore_20'] = (returns - ma_20) / (std_20 + 1e-8)

        # 6. ëª¨ë©˜í…€ (ê³¼ê±°ë§Œ)
        for window in [5, 10, 20]:
            features[f'momentum_{window}'] = returns.rolling(window).sum()

        # 7. ë¶„ìœ„ìˆ˜ íŠ¹ì„± (ê³¼ê±°ë§Œ)
        for window in [10, 20]:
            features[f'quantile_25_{window}'] = returns.rolling(window).quantile(0.25)
            features[f'quantile_75_{window}'] = returns.rolling(window).quantile(0.75)

        # 8. ê·¹ê°’ íŠ¹ì„± (ê³¼ê±°ë§Œ)
        features['max_drawdown_5'] = returns.rolling(5).apply(
            lambda x: (x.cumsum() - x.cumsum().cummax()).min()
        )

        # íƒ€ê²Ÿ: 5ì¼ í›„ ë³€ë™ì„± (â‰¥ t+1)
        target = []
        for i in range(len(returns)):
            if i + 5 < len(returns):
                future_vol = returns.iloc[i+1:i+6].std()
                target.append(future_vol)
            else:
                target.append(np.nan)

        features['target_vol_5d'] = target

        # NaN ì œê±°
        features = features.dropna()

        X = features.drop('target_vol_5d', axis=1)
        y = features['target_vol_5d']

        logger.info(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {X.shape[1]}ê°œ íŠ¹ì„±, {len(X)}ê°œ ìƒ˜í”Œ")
        return X, y

    def objective_function(self, log_alpha):
        """sklearn ê¸°ë°˜ ëª©ì í•¨ìˆ˜"""
        alpha = np.exp(log_alpha)

        # Ridge ëª¨ë¸ ìƒì„± (sklearn ì‚¬ìš©)
        model = Ridge(alpha=alpha, random_state=42)

        # Purged K-Fold Cross-Validation
        cv_scores = []
        splits = self.cv.split(self.X_scaled)

        for train_idx, test_idx in splits:
            if len(train_idx) < 10 or len(test_idx) < 5:
                continue

            X_train, X_test = self.X_scaled[train_idx], self.X_scaled[test_idx]
            y_train, y_test = self.y.iloc[train_idx], self.y.iloc[test_idx]

            # ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
            model.fit(X_train, y_train)
            predictions = model.predict(X_test)

            # RÂ² ê³„ì‚°
            r2 = r2_score(y_test, predictions)
            cv_scores.append(r2)

        mean_r2 = np.mean(cv_scores) if cv_scores else -1.0

        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.current_iteration += 1
        self.history.append({
            'iteration': self.current_iteration,
            'alpha': alpha,
            'log_alpha': log_alpha,
            'r2_score': mean_r2,
            'timestamp': datetime.now().isoformat()
        })

        # ë¡œê¹…
        if self.current_iteration % 5 == 0:
            logger.info(
                f"ë°˜ë³µ {self.current_iteration:4d}: Î±={alpha:.6f}, "
                f"RÂ²={mean_r2:.6f}, ëª©í‘œ=0.3113"
            )

        return -mean_r2  # ìµœì†Œí™” ë¬¸ì œë¡œ ë³€í™˜

    def optimize(self):
        """scipy.optimize ê¸°ë°˜ ìµœì í™”"""
        logger.info("ğŸš€ ê²½ì‚¬í•˜ê°•ë²• ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” v3 ì‹œì‘")
        logger.info("ğŸ”§ v3 ê°œì„ ì‚¬í•­: sklearn í˜¸í™˜ ëª©ì í•¨ìˆ˜")

        # ë°ì´í„° ì¤€ë¹„
        data = self.load_spy_data()
        X, y = self.create_features(data)

        # í‘œì¤€í™”
        self.X_scaled = self.scaler.fit_transform(X)
        self.y = y
        self.current_iteration = 0

        logger.info(f"ğŸ“ˆ ìµœì í™” ì‹œì‘: sklearn Ridge ê¸°ë°˜")
        logger.info(f"ğŸ“Š ëª©í‘œ ì„±ëŠ¥: RÂ² > 0.3113 (ê¸°ì¡´ ëª¨ë¸)")

        # scipy.optimizeë¥¼ ì‚¬ìš©í•œ ìµœì í™”
        # L-BFGS-B ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© (ê²½ì‚¬í•˜ê°•ë²• ê³„ì—´)
        result = opt.minimize(
            self.objective_function,
            x0=np.array([0.0]),  # log(alpha) = 0, ì¦‰ alpha = 1.0 ì‹œì‘
            method='L-BFGS-B',
            bounds=[(-4.0, 4.0)],  # alpha ë²”ìœ„: 0.018 ~ 54.6
            options={
                'maxiter': 100,
                'ftol': 1e-6,
                'gtol': 1e-6
            }
        )

        # ê²°ê³¼ ì²˜ë¦¬
        best_log_alpha = result.x[0]
        best_alpha = np.exp(best_log_alpha)
        best_score = -result.fun  # ì›ë˜ RÂ² ì ìˆ˜

        logger.info(f"âœ… v3 ìµœì í™” ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ìµœì  alpha: {best_alpha:.6f}")
        logger.info(f"ğŸ“Š ìµœì  RÂ²: {best_score:.6f}")
        logger.info(f"ğŸ“Š ìµœì í™” ì„±ê³µ: {'ì˜ˆ' if result.success else 'ì•„ë‹ˆì˜¤'}")
        logger.info(f"ğŸ“Š í•¨ìˆ˜ í˜¸ì¶œ íšŸìˆ˜: {result.nfev}")

        return best_alpha, best_score, self.history, result

    def save_results(self, best_alpha, best_score, history, optimization_result):
        """ê²°ê³¼ ì €ì¥"""
        results = {
            'version': 'v3',
            'improvements': [
                'sklearn Ridge Regression ì§ì ‘ ì‚¬ìš©',
                'scipy.optimize L-BFGS-B ì•Œê³ ë¦¬ì¦˜',
                'ë” ì•ˆì •ì ì¸ ìˆ˜ì¹˜ ê³„ì‚°',
                'ëª©ì í•¨ìˆ˜ ì¼ê´€ì„± ë³´ì¥'
            ],
            'optimization_completed': datetime.now().isoformat(),
            'best_hyperparameters': {
                'alpha': best_alpha,
                'log_alpha': float(np.log(best_alpha)),
                'model_type': 'Ridge_sklearn'
            },
            'best_performance': {
                'r2_score': best_score,
                'method': 'Purged_K_Fold_CV_sklearn'
            },
            'baseline_comparison': {
                'baseline_r2': 0.3113,
                'optimized_r2': best_score,
                'improvement': best_score - 0.3113,
                'improvement_percent': ((best_score - 0.3113) / 0.3113) * 100
            },
            'optimization_details': {
                'algorithm': 'L-BFGS-B',
                'success': optimization_result.success,
                'function_evaluations': int(optimization_result.nfev),
                'message': optimization_result.message
            },
            'optimization_history': history
        }

        # ê²°ê³¼ ì €ì¥
        save_path = '/root/workspace/data/raw/gradient_optimization_results_v3.json'
        with open(save_path, 'w') as f:
            json.dump(results, f, indent=2)

        logger.info(f"ğŸ’¾ v3 ê²°ê³¼ ì €ì¥ë¨: {save_path}")
        return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ¯ ê²½ì‚¬í•˜ê°•ë²• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” v3 ì‹œì‘")

    # ìµœì í™”ê¸° ìƒì„±
    optimizer = GradientHyperparameterOptimizerV3()

    try:
        # ìµœì í™” ì‹¤í–‰
        best_alpha, best_score, history, opt_result = optimizer.optimize()

        # ê²°ê³¼ ì €ì¥
        results = optimizer.save_results(best_alpha, best_score, history, opt_result)

        # ì„±ëŠ¥ ë¹„êµ
        baseline_r2 = 0.3113
        improvement = best_score - baseline_r2
        improvement_pct = (improvement / baseline_r2) * 100

        logger.info("ğŸ“ˆ v3 ì„±ëŠ¥ ë¹„êµ:")
        logger.info(f"   ê¸°ì¡´ ëª¨ë¸ (Î±=1.0): RÂ² = {baseline_r2:.4f}")
        logger.info(f"   v3 ëª¨ë¸ (Î±={best_alpha:.4f}): RÂ² = {best_score:.4f}")
        logger.info(f"   ì„±ëŠ¥ ë³€í™”: {improvement:+.4f} ({improvement_pct:+.2f}%)")

        if best_score > baseline_r2:
            logger.info("ğŸ‰ v3 ì„±ê³µ: ê¸°ì¡´ ì„±ëŠ¥ ì´ˆê³¼!")
        else:
            logger.info("âš ï¸ v3 ë¯¸ë‹¬: ì¶”ê°€ ê°œì„  í•„ìš” â†’ v4 ì¤€ë¹„")

        return results

    except Exception as e:
        logger.error(f"âŒ v3 ìµœì í™” ì‹¤íŒ¨: {str(e)}")
        raise

if __name__ == "__main__":
    main()