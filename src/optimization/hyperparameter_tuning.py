#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë° ìµœì¢… ëª¨ë¸ ê²€ì¦
Lasso Î±=0.001 ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì •ë°€ íŠœë‹
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso, ElasticNet, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import TimeSeriesSplit, ParameterGrid
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import warnings
import os
import json
from datetime import datetime

warnings.filterwarnings('ignore')

# ì´ì „ ëª¨ë“ˆì—ì„œ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
import sys
sys.path.append('/root/workspace/src/features')
from improved_volatility_model import load_enhanced_spy_data, create_comprehensive_features, create_future_volatility_targets, create_interaction_features

def optimize_lasso_hyperparameters(X, y):
    """Lasso ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
    print("ğŸ” Lasso í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...")

    # ì™„ì „í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
    combined_data = pd.concat([X, y], axis=1).dropna()
    X_clean = combined_data[X.columns]
    y_clean = combined_data[y.name]

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
    alpha_values = [0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05]
    max_iter_values = [1000, 2000, 3000, 5000]

    param_grid = {
        'alpha': alpha_values,
        'max_iter': max_iter_values
    }

    tscv = TimeSeriesSplit(n_splits=3)
    best_score = -999
    best_params = None
    best_std = 999

    results = []

    print(f"ì´ {len(alpha_values) * len(max_iter_values)}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸ ì¤‘...")

    for params in ParameterGrid(param_grid):
        scores = []
        mae_scores = []

        for train_idx, test_idx in tscv.split(X_clean):
            X_train, X_test = X_clean.iloc[train_idx], X_clean.iloc[test_idx]
            y_train, y_test = y_clean.iloc[train_idx], y_clean.iloc[test_idx]

            # ìŠ¤ì¼€ì¼ë§
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            try:
                model = Lasso(alpha=params['alpha'], max_iter=params['max_iter'], random_state=42)
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)

                score = r2_score(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)

                scores.append(score)
                mae_scores.append(mae)
            except:
                scores.append(-999)
                mae_scores.append(999)

        avg_score = np.mean(scores)
        std_score = np.std(scores)
        avg_mae = np.mean(mae_scores)

        results.append({
            'params': params,
            'mean_r2': avg_score,
            'std_r2': std_score,
            'mean_mae': avg_mae
        })

        if avg_score > best_score:
            best_score = avg_score
            best_params = params
            best_std = std_score

    # ê²°ê³¼ ì •ë ¬
    results.sort(key=lambda x: x['mean_r2'], reverse=True)

    print(f"\nğŸ† ìµœì  Lasso íŒŒë¼ë¯¸í„°:")
    print(f"   Alpha: {best_params['alpha']}")
    print(f"   Max Iter: {best_params['max_iter']}")
    print(f"   RÂ² = {best_score:.4f} Â± {best_std:.4f}")

    print(f"\nìƒìœ„ 5ê°œ ê²°ê³¼:")
    for i, result in enumerate(results[:5]):
        params = result['params']
        print(f"  {i+1}. Î±={params['alpha']:6.4f}, iter={params['max_iter']:4d}: "
              f"RÂ² = {result['mean_r2']:7.4f} Â± {result['std_r2']:.4f}")

    return best_params, best_score, results

def optimize_elasticnet_hyperparameters(X, y):
    """ElasticNet ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
    print("\nğŸ” ElasticNet í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...")

    # ì™„ì „í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
    combined_data = pd.concat([X, y], axis=1).dropna()
    X_clean = combined_data[X.columns]
    y_clean = combined_data[y.name]

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
    alpha_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]
    l1_ratio_values = [0.1, 0.3, 0.5, 0.7, 0.9]

    param_grid = {
        'alpha': alpha_values,
        'l1_ratio': l1_ratio_values
    }

    tscv = TimeSeriesSplit(n_splits=3)
    best_score = -999
    best_params = None

    results = []

    for params in ParameterGrid(param_grid):
        scores = []

        for train_idx, test_idx in tscv.split(X_clean):
            X_train, X_test = X_clean.iloc[train_idx], X_clean.iloc[test_idx]
            y_train, y_test = y_clean.iloc[train_idx], y_clean.iloc[test_idx]

            # ìŠ¤ì¼€ì¼ë§
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            try:
                model = ElasticNet(alpha=params['alpha'], l1_ratio=params['l1_ratio'],
                                 max_iter=3000, random_state=42)
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)

                score = r2_score(y_test, y_pred)
                scores.append(score)
            except:
                scores.append(-999)

        avg_score = np.mean(scores)
        results.append({
            'params': params,
            'mean_r2': avg_score
        })

        if avg_score > best_score:
            best_score = avg_score
            best_params = params

    results.sort(key=lambda x: x['mean_r2'], reverse=True)

    print(f"ğŸ† ìµœì  ElasticNet íŒŒë¼ë¯¸í„°:")
    print(f"   Alpha: {best_params['alpha']}")
    print(f"   L1 Ratio: {best_params['l1_ratio']}")
    print(f"   RÂ² = {best_score:.4f}")

    return best_params, best_score, results

def final_model_validation(X, y, best_lasso_params):
    """ìµœì¢… ëª¨ë¸ ê²€ì¦ ë° ì„±ëŠ¥ ë¹„êµ"""
    print("\nğŸ ìµœì¢… ëª¨ë¸ ê²€ì¦ ì¤‘...")

    # ì™„ì „í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
    combined_data = pd.concat([X, y], axis=1).dropna()
    X_clean = combined_data[X.columns]
    y_clean = combined_data[y.name]

    # ì‹œê°„ ìˆœì„œ ë¶„í•  (ìµœì¢… ê²€ì¦ìš©)
    split_idx = int(len(X_clean) * 0.8)
    X_train, X_test = X_clean.iloc[:split_idx], X_clean.iloc[split_idx:]
    y_train, y_test = y_clean.iloc[:split_idx], y_clean.iloc[split_idx:]

    print(f"ìµœì¢… ê²€ì¦ - í›ˆë ¨: {len(X_train)}, í…ŒìŠ¤íŠ¸: {len(X_test)}")

    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ìµœì í™”ëœ ëª¨ë¸ë“¤
    models = {
        'Optimized Lasso': Lasso(
            alpha=best_lasso_params['alpha'],
            max_iter=best_lasso_params['max_iter'],
            random_state=42
        ),
        'Baseline Ridge': Ridge(alpha=1.0),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=8),
        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5)
    }

    final_results = {}

    for name, model in models.items():
        # ìŠ¤ì¼€ì¼ë§ ì ìš© ì—¬ë¶€
        if 'Forest' in name or 'Boosting' in name:
            X_tr, X_te = X_train.values, X_test.values
        else:
            X_tr, X_te = X_train_scaled, X_test_scaled

        model.fit(X_tr, y_train)
        y_pred = model.predict(X_te)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mse)

        final_results[name] = {
            'r2_score': r2,
            'mse': mse,
            'rmse': rmse,
            'mae': mae
        }

        print(f"{name:20}: RÂ² = {r2:7.4f}, MAE = {mae:.6f}")

        # íŠ¹ì„± ì¤‘ìš”ë„ (Lassoë§Œ)
        if name == 'Optimized Lasso':
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'coefficient': np.abs(model.coef_)
            }).sort_values('coefficient', ascending=False)

            print(f"\nìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
            for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
                print(f"  {i+1:2d}. {row['feature']:25}: {row['coefficient']:.6f}")

    return final_results

def main():
    """ë©”ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í•¨ìˆ˜"""
    print("ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë° ìµœì¢… ê²€ì¦ ì‹œì‘")
    print("=" * 60)

    # 1. ë°ì´í„° ë¡œë“œ ë° íŠ¹ì„± ìƒì„± (ì´ì „ê³¼ ë™ì¼)
    spy_data = load_enhanced_spy_data()
    comprehensive_features = create_comprehensive_features(spy_data)
    targets = create_future_volatility_targets(spy_data)

    # 2. ì´ì „ì— ë°œê²¬í•œ ìµœê³  íŠ¹ì„±ë“¤ ì‚¬ìš©
    if 'target_vol_5d' in targets.columns:
        combined_for_selection = pd.concat([comprehensive_features, targets[['target_vol_5d']]], axis=1).dropna()

        if len(combined_for_selection) > 100:
            # ìƒê´€ê´€ê³„ ë¶„ì„
            correlations = combined_for_selection[comprehensive_features.columns].corrwith(
                combined_for_selection['target_vol_5d']
            ).abs().sort_values(ascending=False)

            # ìƒìœ„ 15ê°œ íŠ¹ì„± + ìƒí˜¸ì‘ìš©
            top_15_features = correlations.head(15).index
            top_features_df = comprehensive_features[top_15_features]
            interaction_features = create_interaction_features(top_features_df, n_top=8)
            final_features = pd.concat([top_features_df, interaction_features], axis=1)

            print(f"ğŸ“Š ìµœì í™”ìš© íŠ¹ì„± ìˆ˜: {len(final_features.columns)}")

            # 3. Lasso í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            best_lasso_params, best_lasso_score, lasso_results = optimize_lasso_hyperparameters(
                final_features, targets['target_vol_5d']
            )

            # 4. ElasticNet í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ë¹„êµìš©)
            best_elastic_params, best_elastic_score, elastic_results = optimize_elasticnet_hyperparameters(
                final_features, targets['target_vol_5d']
            )

            # 5. ìµœì¢… ëª¨ë¸ ê²€ì¦
            final_results = final_model_validation(
                final_features, targets['target_vol_5d'], best_lasso_params
            )

            # 6. ê²°ê³¼ ì €ì¥
            os.makedirs('results', exist_ok=True)

            optimization_results = {
                'timestamp': datetime.now().isoformat(),
                'optimization_summary': {
                    'best_lasso_r2': best_lasso_score,
                    'best_lasso_params': best_lasso_params,
                    'best_elastic_r2': best_elastic_score,
                    'best_elastic_params': best_elastic_params
                },
                'lasso_optimization': lasso_results[:10],  # ìƒìœ„ 10ê°œë§Œ ì €ì¥
                'elastic_optimization': elastic_results[:10],
                'final_validation': final_results,
                'feature_count': len(final_features.columns)
            }

            with open('results/hyperparameter_optimization.json', 'w') as f:
                json.dump(optimization_results, f, indent=2, default=str)

            print(f"\nğŸ’¾ ìµœì í™” ê²°ê³¼ ì €ì¥: results/hyperparameter_optimization.json")

            # ìµœì¢… ì„±ëŠ¥ ìš”ì•½
            print(f"\n" + "=" * 60)
            print("ğŸ¯ ìµœì í™” ì™„ë£Œ - ì„±ëŠ¥ ìš”ì•½")
            print("=" * 60)
            print(f"ìµœê³  Cross-Validation RÂ²: {best_lasso_score:.4f}")

            best_final = max(final_results.items(), key=lambda x: x[1]['r2_score'])
            print(f"ìµœê³  Final Test RÂ²:       {best_final[1]['r2_score']:.4f} ({best_final[0]})")
            print("=" * 60)

    return optimization_results if 'optimization_results' in locals() else {}

if __name__ == "__main__":
    results = main()