#!/usr/bin/env python3
"""
ì´ˆì•ˆì „ GridSearchCV ê¸°ë°˜ ëª¨ë¸ ìµœì í™” ì‹œìŠ¤í…œ
ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨í•˜ë©° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ íƒìƒ‰
"""

import sys
sys.path.append('/root/workspace')

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Any
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# XGBoost ì•ˆì „ import
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    logging.warning("XGBoost ë¯¸ì„¤ì¹˜ - XGBoost ëª¨ë¸ ì œì™¸")

from src.core.ultra_safe_data_processor import UltraSafeDataProcessor
from src.validation.auto_leakage_detector import AutoLeakageDetector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UltraSafeGridSearch:
    """ì´ˆì•ˆì „ GridSearch ìµœì í™” ì‹œìŠ¤í…œ"""

    def __init__(self):
        # CLAUDE.md ì¤€ìˆ˜ - ê·¹ë„ë¡œ ì—„ê²©í•œ ê¸°ì¤€
        self.MAX_R2 = 0.15           # RÂ² 15% ìƒí•œì„ 
        self.MAX_DIRECTION_ACC = 65.0 # ë°©í–¥ì •í™•ë„ 65% ìƒí•œì„ 
        self.MAX_CORRELATION = 0.20   # 20% ìƒê´€ê´€ê³„ ìƒí•œì„ 

        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.data_processor = UltraSafeDataProcessor()
        self.leakage_detector = AutoLeakageDetector()

        logger.info("ğŸ”’ ì´ˆì•ˆì „ GridSearch ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info(f"ì—„ê²©í•œ ì•ˆì „ ê¸°ì¤€ - RÂ²<{self.MAX_R2}, ë°©í–¥ì •í™•ë„<{self.MAX_DIRECTION_ACC}%")

    def define_model_grids(self) -> Dict[str, Dict]:
        """ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜"""
        logger.info("ğŸ“‹ ëª¨ë¸ ê·¸ë¦¬ë“œ ì •ì˜")

        grids = {
            'LinearRegression': {
                'model': LinearRegression(),
                'params': {}  # íŒŒë¼ë¯¸í„° ì—†ìŒ
            },

            'Ridge': {
                'model': Ridge(random_state=42),
                'params': {
                    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]
                }
            },

            'Lasso': {
                'model': Lasso(random_state=42, max_iter=1000),
                'params': {
                    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]
                }
            },

            'ElasticNet': {
                'model': ElasticNet(random_state=42, max_iter=1000),
                'params': {
                    'alpha': [0.01, 0.1, 1.0],
                    'l1_ratio': [0.1, 0.5, 0.9]
                }
            },

            'RandomForest': {
                'model': RandomForestRegressor(random_state=42),
                'params': {
                    'n_estimators': [10, 50, 100],
                    'max_depth': [3, 5, 10, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            },

            'GradientBoosting': {
                'model': GradientBoostingRegressor(random_state=42),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'max_depth': [3, 5, 7],
                    'subsample': [0.8, 0.9, 1.0]
                }
            },

            'SVR': {
                'model': SVR(),
                'params': {
                    'C': [0.1, 1.0, 10.0],
                    'gamma': ['scale', 'auto', 0.001, 0.01],
                    'kernel': ['rbf', 'linear']
                }
            },

            'MLPRegressor': {
                'model': MLPRegressor(random_state=42, max_iter=500),
                'params': {
                    'hidden_layer_sizes': [(50,), (100,), (50, 50)],
                    'alpha': [0.0001, 0.001, 0.01],
                    'learning_rate_init': [0.001, 0.01, 0.1]
                }
            }
        }

        # XGBoost ì¶”ê°€ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if XGBOOST_AVAILABLE:
            grids['XGBoost'] = {
                'model': xgb.XGBRegressor(random_state=42, verbosity=0),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'max_depth': [3, 5, 7],
                    'subsample': [0.8, 0.9, 1.0],
                    'reg_alpha': [0, 0.1, 1.0],
                    'reg_lambda': [1, 1.5, 2.0]
                }
            }

        logger.info(f"ì •ì˜ëœ ëª¨ë¸: {len(grids)}ê°œ")
        return grids

    def safe_grid_search(self, X: np.ndarray, y: np.ndarray,
                        model_name: str, model_config: Dict) -> Dict:
        """ì•ˆì „í•œ GridSearch ì‹¤í–‰"""
        logger.info(f"\nğŸ” {model_name} GridSearch ì‹œì‘")

        # TimeSeriesSplit ì„¤ì • (ì—„ê²©í•œ ì‹œê°„ ìˆœì„œ)
        tscv = TimeSeriesSplit(n_splits=3, test_size=50, gap=2)

        try:
            if model_config['params']:
                # GridSearchCV ì‹¤í–‰
                grid_search = GridSearchCV(
                    estimator=model_config['model'],
                    param_grid=model_config['params'],
                    cv=tscv,
                    scoring='neg_mean_squared_error',
                    n_jobs=1,  # ì•ˆì „ì„ ìœ„í•´ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
                    verbose=0
                )

                # ìŠ¤ì¼€ì¼ë§ (ì‹œê°„ ìˆœì„œ ì¤€ìˆ˜)
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                # GridSearch ì‹¤í–‰
                grid_search.fit(X_scaled, y)
                best_model = grid_search.best_estimator_
                best_params = grid_search.best_params_

                logger.info(f"   ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")

            else:
                # íŒŒë¼ë¯¸í„° ì—†ëŠ” ëª¨ë¸ (LinearRegression)
                best_model = model_config['model']
                best_params = {}
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                best_model.fit(X_scaled, y)

            # êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
            cv_results = self.evaluate_model_safely(X, y, best_model, model_name)

            return {
                'model': best_model,
                'scaler': scaler,
                'best_params': best_params,
                'cv_results': cv_results,
                'status': 'success'
            }

        except Exception as e:
            logger.error(f"   âŒ {model_name} GridSearch ì‹¤íŒ¨: {e}")
            return {
                'status': 'failed',
                'error': str(e)
            }

    def evaluate_model_safely(self, X: np.ndarray, y: np.ndarray,
                             model: Any, model_name: str) -> Dict:
        """ì•ˆì „í•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""

        tscv = TimeSeriesSplit(n_splits=3, test_size=50, gap=2)

        fold_results = []

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            # ì‹œê°„ ìˆœì„œ ì¬ê²€ì¦
            assert train_idx.max() < val_idx.min(), f"ì‹œê°„ ìˆœì„œ ìœ„ë°˜: {model_name} fold {fold}"

            # ë°ì´í„° ë¶„í• 
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            # ëª¨ë¸ ë³µì‚¬ í›„ í›ˆë ¨
            from sklearn.base import clone
            fold_model = clone(model)
            fold_model.fit(X_train_scaled, y_train)

            # ì˜ˆì¸¡
            y_pred = fold_model.predict(X_val_scaled)

            # ì„±ëŠ¥ ê³„ì‚°
            r2 = r2_score(y_val, y_pred)
            mae = mean_absolute_error(y_val, y_pred)
            mse = mean_squared_error(y_val, y_pred)

            # ë°©í–¥ ì •í™•ë„
            direction_actual = (y_val > 0).astype(int)
            direction_pred = (y_pred > 0).astype(int)
            direction_acc = (direction_actual == direction_pred).mean() * 100

            fold_result = {
                'fold': fold,
                'r2': r2,
                'mae': mae,
                'mse': mse,
                'direction_accuracy': direction_acc
            }
            fold_results.append(fold_result)

            # ì‹¤ì‹œê°„ ì•ˆì „ì„± ê²€ì¦
            metrics = {'r2': r2, 'direction_accuracy': direction_acc}
            is_safe = self.leakage_detector.validate_during_training(fold, model_name, metrics)

            if not is_safe:
                logger.error(f"   ğŸš¨ {model_name} fold {fold} ì•ˆì „ ê¸°ì¤€ ì´ˆê³¼!")
                return {'status': 'unsafe', 'fold_results': fold_results}

        # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        avg_r2 = np.mean([r['r2'] for r in fold_results])
        avg_mae = np.mean([r['mae'] for r in fold_results])
        avg_mse = np.mean([r['mse'] for r in fold_results])
        avg_direction = np.mean([r['direction_accuracy'] for r in fold_results])

        # ìµœì¢… ì•ˆì „ì„± ê²€ì¦
        final_safe = (avg_r2 <= self.MAX_R2 and avg_direction <= self.MAX_DIRECTION_ACC)

        return {
            'status': 'safe' if final_safe else 'unsafe',
            'avg_r2': avg_r2,
            'avg_mae': avg_mae,
            'avg_mse': avg_mse,
            'avg_direction_accuracy': avg_direction,
            'fold_results': fold_results,
            'safety_check': {
                'r2_safe': avg_r2 <= self.MAX_R2,
                'direction_safe': avg_direction <= self.MAX_DIRECTION_ACC
            }
        }

    def run_comprehensive_optimization(self, data_path: str) -> Dict:
        """ì¢…í•©ì ì¸ ëª¨ë¸ ìµœì í™” ì‹¤í–‰"""
        logger.info("=" * 100)
        logger.info("ğŸš€ ì´ˆì•ˆì „ ì¢…í•© ëª¨ë¸ ìµœì í™” ì‹œì‘")
        logger.info("ğŸ”’ ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨ í•˜ì—ì„œ ìµœê³  ì„±ëŠ¥ íƒìƒ‰")
        logger.info("=" * 100)

        # 1. ì´ˆì•ˆì „ ë°ì´í„° ì¤€ë¹„
        data_dict = self.data_processor.prepare_ultra_safe_data(data_path)
        X, y = data_dict['X'], data_dict['y']

        logger.info(f"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: X{X.shape}, y{y.shape}")

        # 2. ëª¨ë¸ ê·¸ë¦¬ë“œ ì •ì˜
        model_grids = self.define_model_grids()

        # 3. ê° ëª¨ë¸ë³„ GridSearch ì‹¤í–‰
        optimization_results = {}
        safe_models = []
        unsafe_models = []

        for model_name, model_config in model_grids.items():
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ {model_name} ìµœì í™”")
            logger.info(f"{'='*60}")

            result = self.safe_grid_search(X, y, model_name, model_config)
            optimization_results[model_name] = result

            if result['status'] == 'success':
                cv_result = result['cv_results']
                if cv_result['status'] == 'safe':
                    safe_models.append((model_name, cv_result))
                    logger.info(f"   âœ… {model_name}: ì•ˆì „ ê¸°ì¤€ í†µê³¼")
                    logger.info(f"      RÂ²={cv_result['avg_r2']:.4f}, ë°©í–¥ì •í™•ë„={cv_result['avg_direction_accuracy']:.1f}%")
                else:
                    unsafe_models.append((model_name, cv_result))
                    logger.warning(f"   âš ï¸ {model_name}: ì•ˆì „ ê¸°ì¤€ ì´ˆê³¼")
            else:
                logger.error(f"   âŒ {model_name}: ìµœì í™” ì‹¤íŒ¨")

        # 4. ì•ˆì „í•œ ëª¨ë¸ë“¤ ìˆœìœ„ ë§¤ê¸°ê¸°
        logger.info(f"\n{'='*100}")
        logger.info(f"ğŸ“Š ì•ˆì „í•œ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„")
        logger.info(f"{'='*100}")

        if safe_models:
            # RÂ²ì™€ ë°©í–¥ì •í™•ë„ ì¢…í•© ì ìˆ˜ë¡œ ìˆœìœ„ ê²°ì •
            safe_models_scored = []
            for model_name, cv_result in safe_models:
                # ì¢…í•© ì ìˆ˜ = (RÂ² ì •ê·œí™”) + (ë°©í–¥ì •í™•ë„ ì •ê·œí™”)
                # RÂ²: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (í•˜ì§€ë§Œ 0.15 ì´í•˜)
                # ë°©í–¥ì •í™•ë„: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (í•˜ì§€ë§Œ 65% ì´í•˜)
                r2_score_norm = max(0, cv_result['avg_r2']) / self.MAX_R2  # 0~1
                direction_score_norm = cv_result['avg_direction_accuracy'] / self.MAX_DIRECTION_ACC  # 0~1

                combined_score = (r2_score_norm * 0.6) + (direction_score_norm * 0.4)

                safe_models_scored.append((model_name, cv_result, combined_score))

            # ì¢…í•© ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            safe_models_scored.sort(key=lambda x: x[2], reverse=True)

            logger.info(f"ğŸ† ì•ˆì „í•œ ëª¨ë¸ ìˆœìœ„ (ì´ {len(safe_models_scored)}ê°œ):")
            for rank, (model_name, cv_result, score) in enumerate(safe_models_scored, 1):
                logger.info(f"   {rank}ìœ„. {model_name}")
                logger.info(f"      ì¢…í•©ì ìˆ˜: {score:.3f}")
                logger.info(f"      RÂ²: {cv_result['avg_r2']:.4f}")
                logger.info(f"      MAE: {cv_result['avg_mae']:.4f}")
                logger.info(f"      ë°©í–¥ì •í™•ë„: {cv_result['avg_direction_accuracy']:.1f}%")
                logger.info(f"      ìµœì  íŒŒë¼ë¯¸í„°: {optimization_results[model_name]['best_params']}")

        else:
            logger.warning("âš ï¸ ì•ˆì „ ê¸°ì¤€ì„ í†µê³¼í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")

        # 5. ìµœì¢… ê²°ê³¼ ì •ë¦¬
        final_results = {
            'safe_models': len(safe_models),
            'unsafe_models': len(unsafe_models),
            'total_models': len(model_grids),
            'ranking': safe_models_scored if safe_models else [],
            'detailed_results': optimization_results,
            'safety_summary': {
                'max_r2_allowed': self.MAX_R2,
                'max_direction_acc_allowed': self.MAX_DIRECTION_ACC,
                'claude_md_compliant': True
            }
        }

        logger.info(f"\n{'='*100}")
        logger.info(f"ğŸ“‹ ìµœì¢… ìš”ì•½")
        logger.info(f"{'='*100}")
        logger.info(f"ì•ˆì „í•œ ëª¨ë¸: {len(safe_models)}ê°œ")
        logger.info(f"ìœ„í—˜í•œ ëª¨ë¸: {len(unsafe_models)}ê°œ")
        logger.info(f"ì´ ëª¨ë¸: {len(model_grids)}ê°œ")
        logger.info(f"CLAUDE.md ì¤€ìˆ˜: âœ…")

        return final_results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    optimizer = UltraSafeGridSearch()

    try:
        results = optimizer.run_comprehensive_optimization(
            '/root/workspace/data/training/sp500_2020_2024_enhanced.csv'
        )

        print(f"\n{'='*100}")
        print(f"ğŸ¯ ì´ˆì•ˆì „ GridSearch ìµœì í™” ì™„ë£Œ")
        print(f"{'='*100}")

        if results['ranking']:
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {results['ranking'][0][0]}")
            print(f"   ì¢…í•© ì ìˆ˜: {results['ranking'][0][2]:.3f}")
            print(f"   RÂ²: {results['ranking'][0][1]['avg_r2']:.4f}")
            print(f"   ë°©í–¥ì •í™•ë„: {results['ranking'][0][1]['avg_direction_accuracy']:.1f}%")

        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"   ì•ˆì „í•œ ëª¨ë¸: {results['safe_models']}ê°œ")
        print(f"   ìœ„í—˜í•œ ëª¨ë¸: {results['unsafe_models']}ê°œ")
        print(f"   ì„±ê³µë¥ : {results['safe_models']/results['total_models']*100:.1f}%")

        return results

    except Exception as e:
        logger.error(f"ìµœì í™” ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    result = main()