"""
ê²½ì‚¬í•˜ê°•ë²• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œìŠ¤í…œ v5
ê°œì„ ì‚¬í•­: ì•™ìƒë¸” ì ‘ê·¼ë²• + ë‹¤ì¤‘ alpha ë™ì‹œ ìµœì í™”

V2 ì„±ê³µ: RÂ² = 0.3256 (alpha = 19.5)
ëª©í‘œ: RÂ² > 0.35 ë‹¬ì„±

V5 í˜ì‹ ì  ì ‘ê·¼:
- ë‹¤ì¤‘ Ridge ëª¨ë¸ ì•™ìƒë¸”
- ê°ê° ë‹¤ë¥¸ alpha ê°’ìœ¼ë¡œ ìµœì í™”
- ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡
- Bayesian Optimization ë„ì…
"""

import numpy as np
import pandas as pd
import yfinance as yf
import json
import time
from datetime import datetime
import logging
import warnings
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.ensemble import VotingRegressor
import skopt
from skopt import gp_minimize
from skopt.space import Real
from skopt.utils import use_named_args
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/workspace/data/raw/gradient_optimization_v5.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PurgedKFoldSklearn:
    """sklearn í˜¸í™˜ Purged K-Fold Cross-Validation"""

    def __init__(self, n_splits=5, purge_length=5, embargo_length=5):
        self.n_splits = n_splits
        self.purge_length = purge_length
        self.embargo_length = embargo_length

    def split(self, X, y=None):
        n_samples = len(X)
        indices = np.arange(n_samples)
        test_size = n_samples // self.n_splits
        splits = []

        for i in range(self.n_splits):
            test_start = i * test_size
            test_end = min((i + 1) * test_size, n_samples)
            test_indices = indices[test_start:test_end]

            purge_start = test_end
            purge_end = min(test_end + self.purge_length, n_samples)
            embargo_end = min(purge_end + self.embargo_length, n_samples)

            train_indices = np.concatenate([
                indices[:test_start],
                indices[embargo_end:]
            ])

            if len(train_indices) > 0 and len(test_indices) > 0:
                splits.append((train_indices, test_indices))

        return splits

    def get_n_splits(self, X=None, y=None, groups=None):
        return self.n_splits

class BayesianHyperparameterOptimizerV5:
    """Bayesian ìµœì í™” ê¸°ë°˜ ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ê¸° v5"""

    def __init__(self, n_calls=100):
        self.n_calls = n_calls
        self.cv = PurgedKFoldSklearn()
        self.scaler = StandardScaler()
        self.history = []
        self.call_count = 0

    def load_spy_data(self):
        """SPY ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        logger.info("ğŸ“Š SPY ë°ì´í„° ë¡œë”© ì¤‘...")

        spy = yf.Ticker("SPY")
        data = spy.history(start="2015-01-01", end="2024-12-31")
        data['returns'] = np.log(data['Close'] / data['Close'].shift(1))
        data = data.dropna()

        logger.info(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(data)}ê°œ ê´€ì¸¡ì¹˜")
        return data

    def create_premium_features(self, data):
        """í”„ë¦¬ë¯¸ì—„ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (40ê°œ+ íŠ¹ì„±)"""
        logger.info("ğŸ”§ í”„ë¦¬ë¯¸ì—„ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (40ê°œ+ íŠ¹ì„±)...")

        returns = data['returns']
        prices = data['Close']
        features = pd.DataFrame(index=data.index)

        # 1. ë‹¤ì¸µ ë³€ë™ì„± íŠ¹ì„±
        for window in [3, 5, 7, 10, 15, 20, 30, 50, 100]:
            features[f'vol_{window}'] = returns.rolling(window).std()

        # 2. ê³ ì°¨ ëª¨ë©˜íŠ¸ (í’ë¶€í•œ ë¶„í¬ ì •ë³´)
        for window in [5, 10, 15, 20, 30]:
            features[f'skew_{window}'] = returns.rolling(window).skew()
            features[f'kurt_{window}'] = returns.rolling(window).kurt()

        # 3. ê°€ê²© ê¸°ë°˜ íŠ¹ì„±
        for window in [10, 20, 50]:
            sma = prices.rolling(window).mean()
            features[f'price_dev_{window}'] = (prices - sma) / sma
            features[f'price_momentum_{window}'] = (prices / prices.shift(window)) - 1

        # 4. ë‹¤ì¸µ ë˜ê·¸ íŠ¹ì„±
        for lag in [1, 2, 3, 5, 7, 10]:
            features[f'return_lag_{lag}'] = returns.shift(lag)
            if f'vol_5' in features:
                features[f'vol_lag_{lag}'] = features['vol_5'].shift(lag)

        # 5. ë³€ë™ì„± ì²´ì œ ë° íŠ¸ë Œë“œ
        short_vol = features['vol_5']
        medium_vol = features['vol_20']
        long_vol = features['vol_50']

        features['vol_regime_sm'] = (short_vol > medium_vol).astype(float)
        features['vol_regime_ml'] = (medium_vol > long_vol).astype(float)
        features['vol_expansion'] = (short_vol / long_vol)
        features['vol_contraction'] = (long_vol / short_vol)

        # 6. í†µê³„ì  ì§€í‘œ í™•ì¥
        for window in [10, 20, 30, 50]:
            ma = returns.rolling(window).mean()
            std = returns.rolling(window).std()
            features[f'zscore_{window}'] = (returns - ma) / (std + 1e-8)
            features[f'sharpe_{window}'] = (ma * np.sqrt(252)) / (std + 1e-8)
            features[f'cvar_{window}'] = returns.rolling(window).quantile(0.05)

        # 7. ëª¨ë©˜í…€ ì§€í‘œ
        for window in [3, 5, 10, 15, 20, 30]:
            features[f'momentum_{window}'] = returns.rolling(window).sum()
            features[f'roc_{window}'] = (prices / prices.shift(window) - 1) * 100

        # 8. ë³€ë™ì„± ìƒí˜¸ì‘ìš©
        features['vol_5_20_ratio'] = features['vol_5'] / (features['vol_20'] + 1e-8)
        features['vol_10_50_ratio'] = features['vol_10'] / (features['vol_50'] + 1e-8)
        features['vol_momentum_interaction'] = features['vol_5'] * features['momentum_10']
        features['price_vol_interaction'] = features['price_dev_20'] * features['vol_20']

        # 9. ê·¹ê°’ ë° ë“œë¡œìš°ë‹¤ìš´
        for window in [5, 10, 15, 20]:
            cumret = returns.rolling(window).sum()
            features[f'max_drawdown_{window}'] = (cumret - cumret.rolling(window).max()).min()
            features[f'max_upward_{window}'] = (cumret - cumret.rolling(window).min()).max()

        # 10. ê³ ê¸‰ í†µê³„ì  íŠ¹ì„±
        for window in [15, 30]:
            features[f'q10_{window}'] = returns.rolling(window).quantile(0.1)
            features[f'q90_{window}'] = returns.rolling(window).quantile(0.9)
            features[f'iqr_{window}'] = (features[f'q90_{window}'] - features[f'q10_{window}'])

        # íƒ€ê²Ÿ: 5ì¼ í›„ ë³€ë™ì„±
        target = []
        for i in range(len(returns)):
            if i + 5 < len(returns):
                future_vol = returns.iloc[i+1:i+6].std()
                target.append(future_vol)
            else:
                target.append(np.nan)

        features['target_vol_5d'] = target
        features = features.dropna()

        X = features.drop('target_vol_5d', axis=1)
        y = features['target_vol_5d']

        logger.info(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {X.shape[1]}ê°œ íŠ¹ì„±, {len(X)}ê°œ ìƒ˜í”Œ")
        return X, y

    def create_ensemble_model(self, alpha1, alpha2, alpha3, alpha4, alpha5):
        """5ê°œ Ridge ëª¨ë¸ ì•™ìƒë¸” ìƒì„±"""
        models = [
            ('ridge1', Ridge(alpha=alpha1, random_state=42)),
            ('ridge2', Ridge(alpha=alpha2, random_state=43)),
            ('ridge3', Ridge(alpha=alpha3, random_state=44)),
            ('ridge4', Ridge(alpha=alpha4, random_state=45)),
            ('ridge5', Ridge(alpha=alpha5, random_state=46))
        ]

        return VotingRegressor(estimators=models)

    def objective_function(self, alpha1, alpha2, alpha3, alpha4, alpha5):
        """Bayesian ìµœì í™”ìš© ëª©ì í•¨ìˆ˜"""
        self.call_count += 1

        # ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
        model = self.create_ensemble_model(alpha1, alpha2, alpha3, alpha4, alpha5)

        # Purged K-Fold Cross-Validation
        cv_scores = []
        splits = self.cv.split(self.X_scaled)

        for train_idx, test_idx in splits:
            if len(train_idx) < 20 or len(test_idx) < 10:
                continue

            X_train, X_test = self.X_scaled[train_idx], self.X_scaled[test_idx]
            y_train, y_test = self.y.iloc[train_idx], self.y.iloc[test_idx]

            model.fit(X_train, y_train)
            predictions = model.predict(X_test)
            r2 = r2_score(y_test, predictions)
            cv_scores.append(r2)

        mean_r2 = np.mean(cv_scores) if cv_scores else -1.0

        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.history.append({
            'call': self.call_count,
            'alphas': [alpha1, alpha2, alpha3, alpha4, alpha5],
            'r2_score': mean_r2,
            'timestamp': datetime.now().isoformat()
        })

        # ë¡œê¹…
        if self.call_count % 5 == 0:
            logger.info(
                f"í˜¸ì¶œ {self.call_count:3d}: Î±=[{alpha1:.2f},{alpha2:.2f},{alpha3:.2f},{alpha4:.2f},{alpha5:.2f}] "
                f"RÂ²={mean_r2:.6f}"
            )

        if mean_r2 > 0.35:
            logger.info(f"ğŸš€ V5 ëª©í‘œ ë‹¬ì„±! RÂ² = {mean_r2:.6f} > 0.35")

        return -mean_r2  # ìµœì†Œí™” ë¬¸ì œ

    def optimize(self):
        """Bayesian ìµœì í™” ì‹¤í–‰"""
        logger.info("ğŸš€ Bayesian ìµœì í™” ê¸°ë°˜ ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” v5 ì‹œì‘")
        logger.info("ğŸ¯ v5 ëª©í‘œ: RÂ² > 0.35 (5-Ridge ì•™ìƒë¸”)")

        # ë°ì´í„° ì¤€ë¹„
        data = self.load_spy_data()
        X, y = self.create_premium_features(data)
        self.X_scaled = self.scaler.fit_transform(X)
        self.y = y

        logger.info(f"ğŸ“ˆ Bayesian ìµœì í™” ì‹œì‘: {self.n_calls}íšŒ í˜¸ì¶œ")

        # íƒìƒ‰ ê³µê°„ ì •ì˜ (V2 ì„±ê³µ ê¸°ì¤€ í™•ì¥)
        dimensions = [
            Real(1.0, 50.0, name='alpha1'),    # V2 ìµœì : 19.5 ì£¼ë³€
            Real(0.1, 20.0, name='alpha2'),    # ë‹¤ì–‘í•œ ì •ê·œí™” ê°•ë„
            Real(5.0, 100.0, name='alpha3'),   # ê°•í•œ ì •ê·œí™”
            Real(0.01, 5.0, name='alpha4'),    # ì•½í•œ ì •ê·œí™”
            Real(10.0, 200.0, name='alpha5')   # ë§¤ìš° ê°•í•œ ì •ê·œí™”
        ]

        # Bayesian ìµœì í™” ì‹¤í–‰
        result = gp_minimize(
            func=self.objective_function,
            dimensions=dimensions,
            n_calls=self.n_calls,
            n_initial_points=20,  # ì´ˆê¸° ëœë¤ íƒìƒ‰
            random_state=42,
            acq_func='EI'  # Expected Improvement
        )

        # ê²°ê³¼ ì²˜ë¦¬
        best_alphas = result.x
        best_score = -result.fun

        logger.info(f"âœ… v5 ìµœì í™” ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ìµœì  alphas: {[f'{a:.4f}' for a in best_alphas]}")
        logger.info(f"ğŸ“Š ìµœì  RÂ²: {best_score:.6f}")

        return best_alphas, best_score, self.history, result

    def save_results(self, best_alphas, best_score, history, bayesian_result):
        """ê²°ê³¼ ì €ì¥"""
        results = {
            'version': 'v5',
            'approach': 'Bayesian_Optimization_Ensemble',
            'improvements': [
                '5ê°œ Ridge ëª¨ë¸ ì•™ìƒë¸”',
                'Bayesian Optimization ì ìš©',
                '40ê°œ+ í”„ë¦¬ë¯¸ì—„ íŠ¹ì„±',
                'ë‹¤ì¤‘ alpha ë™ì‹œ ìµœì í™”',
                'Expected Improvement íšë“í•¨ìˆ˜'
            ],
            'optimization_completed': datetime.now().isoformat(),
            'best_hyperparameters': {
                'alpha1': float(best_alphas[0]),
                'alpha2': float(best_alphas[1]),
                'alpha3': float(best_alphas[2]),
                'alpha4': float(best_alphas[3]),
                'alpha5': float(best_alphas[4]),
                'model_type': 'VotingRegressor_5_Ridge'
            },
            'best_performance': {
                'r2_score': float(best_score),
                'method': 'Purged_K_Fold_CV_Ensemble'
            },
            'version_comparison': {
                'v1_r2': 0.2775,
                'v2_r2': 0.3256,
                'v3_r2': 0.2750,
                'v5_r2': float(best_score),
                'improvement_from_v2': float(best_score - 0.3256),
                'improvement_percent': float(((best_score - 0.3256) / 0.3256) * 100)
            },
            'bayesian_optimization': {
                'n_calls': self.n_calls,
                'acquisition_function': 'Expected_Improvement',
                'n_initial_points': 20
            },
            'optimization_history': history
        }

        # ê²°ê³¼ ì €ì¥
        save_path = '/root/workspace/data/raw/gradient_optimization_results_v5.json'
        with open(save_path, 'w') as f:
            json.dump(results, f, indent=2)

        logger.info(f"ğŸ’¾ v5 ê²°ê³¼ ì €ì¥ë¨: {save_path}")
        return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ¯ Bayesian ìµœì í™” ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” v5 ì‹œì‘")

    optimizer = BayesianHyperparameterOptimizerV5(n_calls=50)  # 50íšŒ í˜¸ì¶œ

    try:
        best_alphas, best_score, history, bayesian_result = optimizer.optimize()
        results = optimizer.save_results(best_alphas, best_score, history, bayesian_result)

        # ì„±ëŠ¥ ë¹„êµ
        v2_r2 = 0.3256
        improvement = best_score - v2_r2
        improvement_pct = (improvement / v2_r2) * 100

        logger.info("ğŸ“ˆ v5 ì„±ëŠ¥ ë¹„êµ:")
        logger.info(f"   V2 ìµœê³ ì : RÂ² = {v2_r2:.4f}")
        logger.info(f"   V5 ì•™ìƒë¸”: RÂ² = {best_score:.4f}")
        logger.info(f"   V2 ëŒ€ë¹„: {improvement:+.4f} ({improvement_pct:+.2f}%)")

        if best_score > 0.35:
            logger.info("ğŸ‰ v5 ëŒ€ì„±ê³µ: RÂ² > 0.35 ë‹¬ì„±!")
        elif best_score > v2_r2:
            logger.info("ğŸ‰ v5 ì„±ê³µ: V2 ì„±ëŠ¥ ì´ˆê³¼!")
        else:
            logger.info("âš ï¸ v5 ì•„ì‰¬ì›€: ì¶”ê°€ ìµœì í™” ê²€í† ")

        return results

    except Exception as e:
        logger.error(f"âŒ v5 ìµœì í™” ì‹¤íŒ¨: {str(e)}")
        raise

if __name__ == "__main__":
    main()