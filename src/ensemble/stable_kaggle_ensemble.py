#!/usr/bin/env python3
"""
ì•ˆì •ì ì¸ ìºê¸€ ì•™ìƒë¸” ì‹œìŠ¤í…œ (ì—ëŸ¬ ìˆ˜ì • ë²„ì „)
Jane Street, Optiver ë“± ìš°ìŠ¹ ê¸°ë²•ì„ ì•ˆì „í•˜ê²Œ ì ìš©
"""

import sys
sys.path.append('/root/workspace')

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Any
from sklearn.base import BaseEstimator, RegressorMixin, clone
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
import warnings
warnings.filterwarnings('ignore')

# XGBoost ì•ˆì „ import
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

from src.features.kaggle_advanced_features import KaggleAdvancedFeatureEngineer
from src.core.ultra_safe_data_processor import UltraSafeDataProcessor
from src.validation.auto_leakage_detector import AutoLeakageDetector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StableKaggleEnsemble:
    """ì•ˆì •ì ì¸ ìºê¸€ ì•™ìƒë³„ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.data_processor = UltraSafeDataProcessor()
        self.feature_engineer = KaggleAdvancedFeatureEngineer(safety_mode=True)
        self.leakage_detector = AutoLeakageDetector()

        # ì—„ê²©í•œ ì•ˆì „ ê¸°ì¤€
        self.MAX_R2 = 0.12
        self.MAX_DIRECTION_ACC = 62.0

        # ë² ì´ìŠ¤ ëª¨ë¸ë“¤
        self.base_models = self._initialize_base_models()
        self.fitted_models_ = {}
        self.ensemble_weights_ = None

        logger.info("ì•ˆì •ì ì¸ ìºê¸€ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”")

    def _initialize_base_models(self):
        """ë² ì´ìŠ¤ ëª¨ë¸ ì´ˆê¸°í™”"""
        models = {
            # Linear models (ê°•í•œ ì •ê·œí™”)
            'ridge_strong': Ridge(alpha=100.0),
            'ridge_moderate': Ridge(alpha=10.0),
            'lasso_conservative': Lasso(alpha=0.1, max_iter=1000),
            'elastic_balanced': ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=1000),

            # Tree models (ë³´ìˆ˜ì  ì„¤ì •)
            'rf_small': RandomForestRegressor(
                n_estimators=30, max_depth=4, min_samples_split=10,
                min_samples_leaf=5, random_state=42
            ),
            'rf_diverse': RandomForestRegressor(
                n_estimators=20, max_depth=6, min_samples_split=8,
                min_samples_leaf=4, random_state=123
            ),
        }

        # XGBoost ì¶”ê°€ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if XGBOOST_AVAILABLE:
            models['xgb_conservative'] = xgb.XGBRegressor(
                n_estimators=50, max_depth=3, learning_rate=0.1,
                subsample=0.8, colsample_bytree=0.8,
                reg_alpha=1.0, reg_lambda=1.0,
                random_state=42, verbosity=0
            )

        logger.info(f"ë² ì´ìŠ¤ ëª¨ë¸ {len(models)}ê°œ ì´ˆê¸°í™” ì™„ë£Œ")
        return models

    def _safe_time_series_split(self, X, n_splits=3, test_size=80, gap=5):
        """ì•ˆì „í•œ ì‹œê³„ì—´ ë¶„í• """
        n_samples = len(X)
        folds = []

        for i in range(n_splits):
            # í…ŒìŠ¤íŠ¸ êµ¬ê°„ ê³„ì‚°
            test_start = n_samples - (n_splits - i) * (test_size + gap)
            test_end = test_start + test_size

            if test_start <= gap:
                continue

            # í›ˆë ¨ êµ¬ê°„ (gap ì ìš©)
            train_end = test_start - gap
            train_start = 0

            if train_end <= train_start:
                continue

            train_idx = np.arange(train_start, train_end)
            test_idx = np.arange(test_start, test_end)

            folds.append((train_idx, test_idx))

        return folds

    def _generate_holdout_predictions(self, X, y):
        """Hold-out ë°©ì‹ìœ¼ë¡œ ë©”íƒ€ íŠ¹ì§• ìƒì„±"""
        logger.info("Hold-out ë°©ì‹ìœ¼ë¡œ ë©”íƒ€ íŠ¹ì§• ìƒì„±")

        n_samples = len(X)
        meta_predictions = np.zeros((n_samples, len(self.base_models)))

        # ì‹œê³„ì—´ ë¶„í• 
        folds = self._safe_time_series_split(X, n_splits=3)

        for fold_idx, (train_idx, val_idx) in enumerate(folds):
            logger.info(f"  Fold {fold_idx+1}/{len(folds)} ì²˜ë¦¬")

            X_fold_train = X[train_idx]
            y_fold_train = y[train_idx]
            X_fold_val = X[val_idx]

            # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡
            for model_idx, (model_name, model) in enumerate(self.base_models.items()):
                try:
                    # ëª¨ë¸ ë³µì‚¬ í›„ í›ˆë ¨
                    fold_model = clone(model)
                    fold_model.fit(X_fold_train, y_fold_train)

                    # ê²€ì¦ ì„¸íŠ¸ ì˜ˆì¸¡
                    val_pred = fold_model.predict(X_fold_val)
                    meta_predictions[val_idx, model_idx] = val_pred

                except Exception as e:
                    logger.warning(f"    {model_name} fold {fold_idx} ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨ ì‹œ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€
                    meta_predictions[val_idx, model_idx] = np.mean(y_fold_train)

        return meta_predictions

    def _optimize_ensemble_weights(self, meta_predictions, y_true):
        """ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™” (ê·¸ë¦¬ë“œ íƒìƒ‰)"""
        logger.info("ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”")

        n_models = meta_predictions.shape[1]
        best_weights = None
        best_score = float('inf')

        # ë‹¨ìˆœ ê·¸ë¦¬ë“œ íƒìƒ‰
        if n_models == 2:
            # 2ê°œ ëª¨ë¸
            for w1 in np.arange(0.1, 1.0, 0.1):
                weights = np.array([w1, 1-w1])
                pred = np.average(meta_predictions, weights=weights, axis=1)
                score = mean_squared_error(y_true, pred)
                if score < best_score:
                    best_score = score
                    best_weights = weights

        elif n_models >= 3:
            # 3ê°œ ì´ìƒ ëª¨ë¸
            for w1 in np.arange(0.2, 0.7, 0.1):
                for w2 in np.arange(0.2, 0.8-w1, 0.1):
                    if n_models == 3:
                        w3 = 1 - w1 - w2
                        if w3 > 0.1:
                            weights = np.array([w1, w2, w3])
                        else:
                            continue
                    else:
                        # 4ê°œ ì´ìƒì€ ë‚˜ë¨¸ì§€ë¥¼ ê· ë“±ë¶„í• 
                        remaining = 1 - w1 - w2
                        remaining_weight = remaining / (n_models - 2)
                        if remaining_weight > 0.05:
                            weights = np.array([w1, w2] + [remaining_weight] * (n_models - 2))
                        else:
                            continue

                    pred = np.average(meta_predictions, weights=weights, axis=1)
                    score = mean_squared_error(y_true, pred)
                    if score < best_score:
                        best_score = score
                        best_weights = weights

        # ê¸°ë³¸ê°’ (ê· ë“± ê°€ì¤‘ì¹˜)
        if best_weights is None:
            best_weights = np.ones(n_models) / n_models

        logger.info(f"ìµœì  ê°€ì¤‘ì¹˜: {dict(zip(self.base_models.keys(), best_weights))}")
        return best_weights

    def fit(self, X, y):
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        logger.info("=" * 80)
        logger.info("ğŸ† ì•ˆì •ì ì¸ ìºê¸€ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘")
        logger.info("=" * 80)

        # 1. ë©”íƒ€ íŠ¹ì§• ìƒì„± (Hold-out ë°©ì‹)
        meta_predictions = self._generate_holdout_predictions(X, y)

        # 2. ìœ íš¨í•œ ì˜ˆì¸¡ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
        valid_mask = ~np.isnan(meta_predictions).any(axis=1)
        if np.sum(valid_mask) < len(X) * 0.5:
            logger.warning("ìœ íš¨í•œ ë©”íƒ€ ì˜ˆì¸¡ì´ ë¶€ì¡±í•˜ì—¬ ê· ë“± ê°€ì¤‘ì¹˜ ì‚¬ìš©")
            self.ensemble_weights_ = np.ones(len(self.base_models)) / len(self.base_models)
        else:
            # 3. ê°€ì¤‘ì¹˜ ìµœì í™” (ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš©)
            valid_meta_pred = meta_predictions[valid_mask]
            valid_y = y[valid_mask]
            self.ensemble_weights_ = self._optimize_ensemble_weights(valid_meta_pred, valid_y)

        # 4. ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨
        logger.info("ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨")
        for model_name, model in self.base_models.items():
            try:
                fitted_model = clone(model)
                fitted_model.fit(X, y)
                self.fitted_models_[model_name] = fitted_model
                logger.info(f"  {model_name} í›ˆë ¨ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"  {model_name} í›ˆë ¨ ì‹¤íŒ¨: {e}")

        logger.info("ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ")
        return self

    def predict(self, X):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        if not self.fitted_models_:
            raise ValueError("ëª¨ë¸ì´ ë¨¼ì € í›ˆë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤")

        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ìˆ˜ì§‘
        predictions = []
        model_names = []

        for model_name, model in self.fitted_models_.items():
            try:
                pred = model.predict(X)
                predictions.append(pred)
                model_names.append(model_name)
            except Exception as e:
                logger.warning(f"{model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

        if len(predictions) == 0:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        predictions = np.array(predictions).T  # (n_samples, n_models)

        # ê°€ì¤‘ì¹˜ ì¡°ì • (ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ)
        available_weights = self.ensemble_weights_[:len(predictions[0])]
        available_weights = available_weights / np.sum(available_weights)

        ensemble_pred = np.average(predictions, weights=available_weights, axis=1)

        return ensemble_pred

    def run_stable_ensemble_training(self, data_path: str):
        """ì•ˆì •ì ì¸ ì•™ìƒë¸” í›ˆë ¨ ì‹¤í–‰"""
        logger.info("=" * 100)
        logger.info("ğŸ† ì•ˆì •ì ì¸ ìºê¸€ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì‹¤í–‰")
        logger.info("=" * 100)

        # 1. ê³ ê¸‰ íŠ¹ì§• ë°ì´í„° ì¤€ë¹„
        data_dict = self.data_processor.prepare_ultra_safe_data(data_path)
        X_base, y = data_dict['X'], data_dict['y']

        # ìºê¸€ ê³ ê¸‰ íŠ¹ì§• ì ìš©
        X_enhanced = self.feature_engineer.fit_transform(X_base)

        # 2. ë°ì´í„° ë¶„í• 
        split_point = int(len(X_enhanced) * 0.8)
        X_train = X_enhanced[:split_point]
        X_test = X_enhanced[split_point:]
        y_train = y[:split_point]
        y_test = y[split_point:]

        logger.info(f"ë°ì´í„° ì¤€ë¹„: train={X_train.shape}, test={X_test.shape}")

        # 3. ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        logger.info("\nê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        individual_results = {}

        for model_name, model in self.base_models.items():
            try:
                # ëª¨ë¸ í›ˆë ¨
                fitted_model = clone(model)
                fitted_model.fit(X_train, y_train)

                # ì˜ˆì¸¡ ë° í‰ê°€
                y_pred = fitted_model.predict(X_test)

                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = 1 - (mse / np.var(y_test))

                direction_actual = (y_test > 0).astype(int)
                direction_pred = (y_pred > 0).astype(int)
                direction_acc = np.mean(direction_actual == direction_pred) * 100

                individual_results[model_name] = {
                    'r2': r2, 'mae': mae, 'direction_acc': direction_acc
                }

                logger.info(f"  {model_name}: RÂ²={r2:.4f}, MAE={mae:.4f}, ë°©í–¥ì •í™•ë„={direction_acc:.1f}%")

            except Exception as e:
                logger.error(f"  {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")

        # 4. ì•™ìƒë¸” í›ˆë ¨
        logger.info("\nì•™ìƒë¸” í›ˆë ¨")
        self.fit(X_train, y_train)

        # 5. ì•™ìƒë¸” ì˜ˆì¸¡ ë° í‰ê°€
        ensemble_pred = self.predict(X_test)

        ensemble_mse = mean_squared_error(y_test, ensemble_pred)
        ensemble_mae = mean_absolute_error(y_test, ensemble_pred)
        ensemble_r2 = 1 - (ensemble_mse / np.var(y_test))

        ensemble_direction_actual = (y_test > 0).astype(int)
        ensemble_direction_pred = (ensemble_pred > 0).astype(int)
        ensemble_direction_acc = np.mean(ensemble_direction_actual == ensemble_direction_pred) * 100

        # 6. ì•ˆì „ì„± ê²€ì¦
        metrics = {
            'r2': ensemble_r2,
            'direction_accuracy': ensemble_direction_acc
        }

        safety_check = self.leakage_detector.validate_during_training(0, 'stable_ensemble', metrics)
        safe_performance = (ensemble_r2 <= self.MAX_R2 and ensemble_direction_acc <= self.MAX_DIRECTION_ACC)

        # 7. ê²°ê³¼ ì¶œë ¥
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ† ìµœì¢… ì•™ìƒë¸” ì„±ëŠ¥")
        logger.info(f"{'='*80}")
        logger.info(f"RÂ²: {ensemble_r2:.4f} ({'âœ… ì•ˆì „' if ensemble_r2 <= self.MAX_R2 else 'âŒ ìœ„í—˜'})")
        logger.info(f"MAE: {ensemble_mae:.4f}")
        logger.info(f"ë°©í–¥ì •í™•ë„: {ensemble_direction_acc:.1f}% ({'âœ… ì•ˆì „' if ensemble_direction_acc <= self.MAX_DIRECTION_ACC else 'âŒ ìœ„í—˜'})")

        logger.info(f"\nì•™ìƒë¸” êµ¬ì„±:")
        for i, (model_name, weight) in enumerate(zip(self.base_models.keys(), self.ensemble_weights_)):
            logger.info(f"  {model_name}: {weight:.3f}")

        success = safety_check and safe_performance
        logger.info(f"\nìµœì¢… ì•ˆì „ì„±: {'âœ… ì„±ê³µ' if success else 'âŒ ì‹¤íŒ¨'}")

        return {
            'ensemble_performance': {
                'r2': ensemble_r2,
                'mae': ensemble_mae,
                'direction_accuracy': ensemble_direction_acc
            },
            'individual_results': individual_results,
            'ensemble_weights': dict(zip(self.base_models.keys(), self.ensemble_weights_)),
            'safety_check': success,
            'model_count': len(self.fitted_models_),
            'feature_count': X_enhanced.shape[1]
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    system = StableKaggleEnsemble()

    try:
        results = system.run_stable_ensemble_training(
            '/root/workspace/data/training/sp500_2020_2024_enhanced.csv'
        )

        print(f"\n{'='*100}")
        print(f"ğŸ† ì•ˆì •ì ì¸ ìºê¸€ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì™„ë£Œ")
        print(f"{'='*100}")

        # ì•™ìƒë¸” ì„±ëŠ¥
        ensemble_perf = results['ensemble_performance']
        print(f"\nğŸ“Š ì•™ìƒë¸” ì„±ëŠ¥:")
        print(f"   RÂ²: {ensemble_perf['r2']:.4f}")
        print(f"   MAE: {ensemble_perf['mae']:.4f}")
        print(f"   ë°©í–¥ì •í™•ë„: {ensemble_perf['direction_accuracy']:.1f}%")

        # êµ¬ì„± ì •ë³´
        print(f"\nğŸ—ï¸ ì•™ìƒë¸” êµ¬ì„±:")
        print(f"   ì‚¬ìš©ëœ ëª¨ë¸: {results['model_count']}ê°œ")
        print(f"   íŠ¹ì§• ìˆ˜: {results['feature_count']}ê°œ")

        # ê°€ì¤‘ì¹˜ ì •ë³´
        print(f"\nâš–ï¸ ëª¨ë¸ ê°€ì¤‘ì¹˜:")
        for model_name, weight in results['ensemble_weights'].items():
            print(f"   {model_name}: {weight:.3f}")

        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        print(f"\nğŸ“ˆ ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:")
        for model_name, perf in results['individual_results'].items():
            print(f"   {model_name}: RÂ²={perf['r2']:.4f}, ë°©í–¥ì •í™•ë„={perf['direction_acc']:.1f}%")

        print(f"\nğŸ›¡ï¸ ì•ˆì „ì„±: {'âœ… í†µê³¼' if results['safety_check'] else 'âŒ ì‹¤íŒ¨'}")

        if results['safety_check']:
            print(f"\nğŸ‰ ì„±ê³µ: ì•ˆì •ì ì¸ ìºê¸€ ì•™ìƒë¸” ì‹œìŠ¤í…œìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±!")
        else:
            print(f"\nâš ï¸ ì£¼ì˜: ì•ˆì „ ê¸°ì¤€ ì´ˆê³¼ - ì¶”ê°€ ë³´ì • í•„ìš”")

        return results

    except Exception as e:
        logger.error(f"ì•ˆì •ì ì¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    result = main()