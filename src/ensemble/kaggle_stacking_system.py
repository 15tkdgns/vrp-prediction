#!/usr/bin/env python3
"""
ìºê¸€ ìš°ìŠ¹ì ê¸°ë²•: ì•ˆì „í•œ ìŠ¤íƒœí‚¹/ë¸”ë Œë”© ì•™ìƒë¸” ì‹œìŠ¤í…œ
Jane Street, Optiver ë“± ëŒ€íšŒ ìš°ìŠ¹ ì†”ë£¨ì…˜ì˜ ë‹¤ì¸µ ì•™ìƒë¸” ê¸°ë²• ì ìš©
"""

import sys
sys.path.append('/root/workspace')

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Any, Optional
from sklearn.model_selection import cross_val_predict
from sklearn.base import BaseEstimator, RegressorMixin, clone
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# XGBoost ì•ˆì „ import
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

# LightGBM ì•ˆì „ import
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

from src.features.kaggle_advanced_features import KaggleAdvancedFeatureEngineer
from src.core.ultra_safe_data_processor import UltraSafeDataProcessor
from src.validation.auto_leakage_detector import AutoLeakageDetector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SafeTimeSeriesSplit:
    """ì•ˆì „í•œ ì‹œê³„ì—´ êµì°¨ ê²€ì¦ (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)"""

    def __init__(self, n_splits: int = 3, test_size: int = 100, gap: int = 5):
        self.n_splits = n_splits
        self.test_size = test_size
        self.gap = gap

    def split(self, X, y=None, groups=None):
        """ì•ˆì „í•œ ì‹œê³„ì—´ ë¶„í• """
        n_samples = len(X)

        for i in range(self.n_splits):
            # í…ŒìŠ¤íŠ¸ ì‹œì‘ì  ê³„ì‚°
            test_start = n_samples - (self.n_splits - i) * (self.test_size + self.gap)
            test_end = test_start + self.test_size

            if test_start < self.gap:
                continue

            # í›ˆë ¨ êµ¬ê°„ (gap ì ìš©)
            train_end = test_start - self.gap
            train_start = 0

            if train_end <= train_start:
                continue

            train_idx = np.arange(train_start, train_end)
            test_idx = np.arange(test_start, test_end)

            yield train_idx, test_idx

class KaggleLevel0Models:
    """Level-0 ë² ì´ìŠ¤ ëª¨ë¸ë“¤ (ìºê¸€ ìš°ìŠ¹ì ì¡°í•©)"""

    def __init__(self, safety_mode: bool = True):
        self.safety_mode = safety_mode
        self.models = {}
        self._initialize_models()

    def _initialize_models(self):
        """ë‹¤ì–‘í•œ ë² ì´ìŠ¤ ëª¨ë¸ ì´ˆê¸°í™”"""
        logger.info("Level-0 ë² ì´ìŠ¤ ëª¨ë¸ ì´ˆê¸°í™”")

        # 1. Linear Models (ì•ˆì „ì„± ìš°ì„ )
        self.models['ridge_conservative'] = Ridge(alpha=100.0)
        self.models['ridge_moderate'] = Ridge(alpha=10.0)
        self.models['lasso_sparse'] = Lasso(alpha=0.1, max_iter=1000)
        self.models['elastic_balanced'] = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=1000)

        # 2. Tree Models (ë³´ìˆ˜ì  ì„¤ì •)
        self.models['rf_conservative'] = RandomForestRegressor(
            n_estimators=50, max_depth=5, min_samples_split=10,
            min_samples_leaf=5, random_state=42
        )
        self.models['rf_diverse'] = RandomForestRegressor(
            n_estimators=30, max_depth=7, min_samples_split=5,
            min_samples_leaf=3, random_state=123
        )

        # 3. Boosting Models (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if XGBOOST_AVAILABLE:
            self.models['xgb_conservative'] = xgb.XGBRegressor(
                n_estimators=100, max_depth=4, learning_rate=0.1,
                subsample=0.8, colsample_bytree=0.8, random_state=42, verbosity=0
            )
            self.models['xgb_regularized'] = xgb.XGBRegressor(
                n_estimators=50, max_depth=3, learning_rate=0.05,
                reg_alpha=1.0, reg_lambda=1.0, random_state=123, verbosity=0
            )

        if LIGHTGBM_AVAILABLE:
            self.models['lgb_conservative'] = lgb.LGBMRegressor(
                n_estimators=100, max_depth=4, learning_rate=0.1,
                feature_fraction=0.8, bagging_fraction=0.8, random_state=42, verbosity=-1
            )

        logger.info(f"ì´ {len(self.models)}ê°œ ë² ì´ìŠ¤ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

    def get_models(self):
        """ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        return self.models

class KaggleStackingEnsemble(BaseEstimator, RegressorMixin):
    """ìºê¸€ ìŠ¤íƒ€ì¼ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”"""

    def __init__(self, base_models: Dict, meta_model=None, cv_folds: int = 3):
        self.base_models = base_models
        self.meta_model = meta_model if meta_model else Ridge(alpha=10.0)
        self.cv_folds = cv_folds
        self.fitted_base_models_ = {}
        self.meta_features_scaler_ = StandardScaler()

    def _generate_meta_features(self, X, y):
        """ë©”íƒ€ íŠ¹ì§• ìƒì„± (CV ì˜ˆì¸¡ê°’)"""
        logger.info("ë©”íƒ€ íŠ¹ì§• ìƒì„±ì„ ìœ„í•œ êµì°¨ ê²€ì¦ ì‹¤í–‰")

        meta_features = np.zeros((len(X), len(self.base_models)))
        cv_splitter = SafeTimeSeriesSplit(n_splits=self.cv_folds)

        for i, (model_name, model) in enumerate(self.base_models.items()):
            logger.info(f"  {model_name} CV ì˜ˆì¸¡ ìƒì„±...")

            try:
                # ì•ˆì „í•œ êµì°¨ ê²€ì¦ ì˜ˆì¸¡
                cv_predictions = cross_val_predict(
                    model, X, y, cv=cv_splitter, method='predict'
                )
                meta_features[:, i] = cv_predictions
            except Exception as e:
                logger.warning(f"  {model_name} CV ì‹¤íŒ¨: {e} - ì˜ë²¡í„°ë¡œ ëŒ€ì²´")
                meta_features[:, i] = np.zeros(len(X))

        return meta_features

    def _create_additional_meta_features(self, X, meta_predictions):
        """ì¶”ê°€ ë©”íƒ€ íŠ¹ì§• ìƒì„± (ìºê¸€ ìš°ìŠ¹ì ê¸°ë²•)"""

        # 1. ì›ë³¸ íŠ¹ì§•ì˜ í†µê³„ ìš”ì•½
        feature_stats = np.column_stack([
            np.mean(X, axis=1),      # íŠ¹ì§•ë³„ í‰ê· 
            np.std(X, axis=1),       # íŠ¹ì§•ë³„ í‘œì¤€í¸ì°¨
            np.min(X, axis=1),       # íŠ¹ì§•ë³„ ìµœì†Œê°’
            np.max(X, axis=1)        # íŠ¹ì§•ë³„ ìµœëŒ€ê°’
        ])

        # 2. ëª¨ë¸ ì˜ˆì¸¡ê°’ì˜ í†µê³„
        pred_stats = np.column_stack([
            np.mean(meta_predictions, axis=1),     # ì˜ˆì¸¡ í‰ê· 
            np.std(meta_predictions, axis=1),      # ì˜ˆì¸¡ ë¶„ì‚° (ë¶ˆí™•ì‹¤ì„±)
            np.min(meta_predictions, axis=1),      # ì˜ˆì¸¡ ìµœì†Œê°’
            np.max(meta_predictions, axis=1),      # ì˜ˆì¸¡ ìµœëŒ€ê°’
        ])

        # 3. ëª¨ë¸ ê°„ í•©ì˜ë„
        agreement_features = np.column_stack([
            # ì˜ˆì¸¡ê°’ì˜ ìƒê´€ê´€ê³„ (ë‹¤ì–‘ì„± ì¸¡ì •)
            np.array([np.corrcoef(meta_predictions[i])[0, 1]
                     if len(meta_predictions[i]) > 1 else 0
                     for i in range(len(meta_predictions))]),
        ])

        # ëª¨ë“  ë©”íƒ€ íŠ¹ì§• ê²°í•©
        enhanced_meta_features = np.column_stack([
            meta_predictions,    # ê¸°ë³¸ CV ì˜ˆì¸¡ê°’
            feature_stats,       # ì›ë³¸ íŠ¹ì§• í†µê³„
            pred_stats,          # ì˜ˆì¸¡ í†µê³„
            agreement_features   # í•©ì˜ë„ íŠ¹ì§•
        ])

        return enhanced_meta_features

    def fit(self, X, y):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨"""
        logger.info("ìºê¸€ ìŠ¤íƒ€ì¼ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘")

        # 1. ë©”íƒ€ íŠ¹ì§• ìƒì„±
        meta_predictions = self._generate_meta_features(X, y)
        enhanced_meta_features = self._create_additional_meta_features(X, meta_predictions)

        # 2. ë©”íƒ€ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
        meta_features_scaled = self.meta_features_scaler_.fit_transform(enhanced_meta_features)

        # 3. ì „ì²´ ë°ì´í„°ë¡œ ë² ì´ìŠ¤ ëª¨ë¸ë“¤ ì¬í›ˆë ¨
        logger.info("ë² ì´ìŠ¤ ëª¨ë¸ë“¤ ì „ì²´ ë°ì´í„°ë¡œ ì¬í›ˆë ¨")
        for model_name, model in self.base_models.items():
            try:
                fitted_model = clone(model)
                fitted_model.fit(X, y)
                self.fitted_base_models_[model_name] = fitted_model
                logger.info(f"  {model_name} í›ˆë ¨ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"  {model_name} í›ˆë ¨ ì‹¤íŒ¨: {e}")

        # 4. ë©”íƒ€ ëª¨ë¸ í›ˆë ¨
        logger.info("ë©”íƒ€ ëª¨ë¸ í›ˆë ¨")
        self.meta_model.fit(meta_features_scaled, y)

        logger.info("ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ")
        return self

    def predict(self, X):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì˜ˆì¸¡"""

        # 1. ë² ì´ìŠ¤ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡
        base_predictions = np.zeros((len(X), len(self.fitted_base_models_)))

        for i, (model_name, model) in enumerate(self.fitted_base_models_.items()):
            try:
                base_predictions[:, i] = model.predict(X)
            except Exception as e:
                logger.warning(f"{model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e} - ì˜ë²¡í„°ë¡œ ëŒ€ì²´")
                base_predictions[:, i] = np.zeros(len(X))

        # 2. ì¶”ê°€ ë©”íƒ€ íŠ¹ì§• ìƒì„±
        enhanced_meta_features = self._create_additional_meta_features(X, base_predictions)

        # 3. ë©”íƒ€ íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
        meta_features_scaled = self.meta_features_scaler_.transform(enhanced_meta_features)

        # 4. ë©”íƒ€ ëª¨ë¸ ì˜ˆì¸¡
        final_predictions = self.meta_model.predict(meta_features_scaled)

        return final_predictions

class KaggleBlendingOptimizer:
    """ìºê¸€ ìŠ¤íƒ€ì¼ ë¸”ë Œë”© ìµœì í™”"""

    def __init__(self, models: List[Any]):
        self.models = models
        self.optimal_weights_ = None

    def _objective_function(self, weights, predictions, y_true):
        """ìµœì í™” ëª©ì  í•¨ìˆ˜ (MSE ìµœì†Œí™”)"""
        weighted_pred = np.average(predictions, weights=weights, axis=0)
        return mean_squared_error(y_true, weighted_pred)

    def optimize_weights(self, predictions: np.ndarray, y_true: np.ndarray):
        """ê°€ì¤‘ì¹˜ ìµœì í™” (ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ íƒìƒ‰)"""
        logger.info("ë¸”ë Œë”© ê°€ì¤‘ì¹˜ ìµœì í™”")

        n_models = predictions.shape[0]
        best_weights = None
        best_score = float('inf')

        # ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ íƒìƒ‰
        for w1 in np.arange(0.1, 1.0, 0.1):
            for w2 in np.arange(0.1, 1.0 - w1, 0.1):
                if n_models == 2:
                    weights = np.array([w1, 1 - w1])
                elif n_models == 3:
                    w3 = 1 - w1 - w2
                    if w3 > 0:
                        weights = np.array([w1, w2, w3])
                    else:
                        continue
                else:
                    # ë” ë§ì€ ëª¨ë¸ì˜ ê²½ìš° ê· ë“± ê°€ì¤‘ì¹˜
                    weights = np.ones(n_models) / n_models

                score = self._objective_function(weights, predictions, y_true)

                if score < best_score:
                    best_score = score
                    best_weights = weights.copy()

        self.optimal_weights_ = best_weights if best_weights is not None else np.ones(n_models) / n_models
        logger.info(f"ìµœì  ê°€ì¤‘ì¹˜: {self.optimal_weights_}")

        return self.optimal_weights_

class SafeKaggleEnsembleSystem:
    """ì•ˆì „í•œ ìºê¸€ ì•™ìƒë¸” ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.data_processor = UltraSafeDataProcessor()
        self.feature_engineer = KaggleAdvancedFeatureEngineer(safety_mode=True)
        self.leakage_detector = AutoLeakageDetector()

        # ì•ˆì „ ê¸°ì¤€
        self.MAX_R2 = 0.12
        self.MAX_DIRECTION_ACC = 62.0

        # ì•™ìƒë¸” êµ¬ì„±ìš”ì†Œ
        self.level0_models = KaggleLevel0Models(safety_mode=True)
        self.stacking_ensemble = None
        self.blending_optimizer = None

        logger.info("ì•ˆì „í•œ ìºê¸€ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”")

    def run_kaggle_ensemble_optimization(self, data_path: str):
        """ìºê¸€ ì•™ìƒë¸” ìµœì í™” ì‹¤í–‰"""
        logger.info("=" * 100)
        logger.info("ğŸ† ìºê¸€ ìŠ¤íƒœí‚¹/ë¸”ë Œë”© ì•™ìƒë¸” ìµœì í™” ì‹œì‘")
        logger.info("=" * 100)

        # 1. ê³ ê¸‰ íŠ¹ì§• ë°ì´í„° ì¤€ë¹„
        data_dict = self.data_processor.prepare_ultra_safe_data(data_path)
        X_base, y = data_dict['X'], data_dict['y']

        # ìºê¸€ ê³ ê¸‰ íŠ¹ì§• ì ìš©
        X_enhanced = self.feature_engineer.fit_transform(X_base)

        # 2. ë°ì´í„° ë¶„í• 
        split_point = int(len(X_enhanced) * 0.8)
        X_train = X_enhanced[:split_point]
        X_test = X_enhanced[split_point:]
        y_train = y[:split_point]
        y_test = y[split_point:]

        logger.info(f"ë°ì´í„° ì¤€ë¹„: train={X_train.shape}, test={X_test.shape}")

        # 3. Level-0 ë² ì´ìŠ¤ ëª¨ë¸ë“¤ í›ˆë ¨ ë° í‰ê°€
        base_models = self.level0_models.get_models()
        base_predictions_test = {}

        logger.info(f"Level-0 ë² ì´ìŠ¤ ëª¨ë¸ ê°œë³„ ì„±ëŠ¥ í‰ê°€")
        for model_name, model in base_models.items():
            try:
                # ëª¨ë¸ í›ˆë ¨
                fitted_model = clone(model)
                fitted_model.fit(X_train, y_train)

                # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
                y_pred = fitted_model.predict(X_test)
                base_predictions_test[model_name] = y_pred

                # ì„±ëŠ¥ ê³„ì‚°
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = 1 - (mse / np.var(y_test))

                direction_actual = (y_test > 0).astype(int)
                direction_pred = (y_pred > 0).astype(int)
                direction_acc = np.mean(direction_actual == direction_pred) * 100

                logger.info(f"  {model_name}: RÂ²={r2:.4f}, MAE={mae:.4f}, ë°©í–¥ì •í™•ë„={direction_acc:.1f}%")

            except Exception as e:
                logger.error(f"  {model_name} ì‹¤íŒ¨: {e}")

        # 4. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” êµ¬ì¶•
        logger.info(f"\nLevel-1 ìŠ¤íƒœí‚¹ ì•™ìƒë¸” êµ¬ì¶•")
        self.stacking_ensemble = KaggleStackingEnsemble(
            base_models=base_models,
            meta_model=Ridge(alpha=50.0),  # ê°•í•œ ì •ê·œí™”
            cv_folds=3
        )

        # ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨
        self.stacking_ensemble.fit(X_train, y_train)

        # ìŠ¤íƒœí‚¹ ì˜ˆì¸¡
        stacking_pred = self.stacking_ensemble.predict(X_test)

        # 5. ë¸”ë Œë”© ìµœì í™”
        logger.info(f"\nLevel-2 ë¸”ë Œë”© ìµœì í™”")

        # ìƒìœ„ 3ê°œ ê°œë³„ ëª¨ë¸ ì„ íƒ (ì„±ëŠ¥ ê¸°ì¤€)
        individual_performances = {}
        for model_name, y_pred in base_predictions_test.items():
            mse = mean_squared_error(y_test, y_pred)
            individual_performances[model_name] = mse

        top_models = sorted(individual_performances.items(), key=lambda x: x[1])[:3]
        logger.info(f"ë¸”ë Œë”©ìš© ìƒìœ„ ëª¨ë¸: {[name for name, _ in top_models]}")

        # ë¸”ë Œë”©í•  ì˜ˆì¸¡ê°’ë“¤
        predictions_for_blending = np.array([
            base_predictions_test[name] for name, _ in top_models
        ] + [stacking_pred])

        # ë¸”ë Œë”© ê°€ì¤‘ì¹˜ ìµœì í™”
        self.blending_optimizer = KaggleBlendingOptimizer(models=None)
        optimal_weights = self.blending_optimizer.optimize_weights(
            predictions_for_blending, y_test
        )

        # ìµœì¢… ë¸”ë Œë”© ì˜ˆì¸¡
        final_prediction = np.average(predictions_for_blending, weights=optimal_weights, axis=0)

        # 6. ìµœì¢… ì„±ëŠ¥ í‰ê°€
        final_mse = mean_squared_error(y_test, final_prediction)
        final_mae = mean_absolute_error(y_test, final_prediction)
        final_r2 = 1 - (final_mse / np.var(y_test))

        final_direction_actual = (y_test > 0).astype(int)
        final_direction_pred = (final_prediction > 0).astype(int)
        final_direction_acc = np.mean(final_direction_actual == final_direction_pred) * 100

        # 7. ì•ˆì „ì„± ê²€ì¦
        metrics = {
            'r2': final_r2,
            'direction_accuracy': final_direction_acc
        }

        safety_check = self.leakage_detector.validate_during_training(0, 'kaggle_ensemble', metrics)
        safe_performance = (final_r2 <= self.MAX_R2 and final_direction_acc <= self.MAX_DIRECTION_ACC)

        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ† ìºê¸€ ì•™ìƒë¸” ìµœì¢… ì„±ëŠ¥")
        logger.info(f"{'='*80}")
        logger.info(f"RÂ²: {final_r2:.4f} ({'âœ… ì•ˆì „' if final_r2 <= self.MAX_R2 else 'âŒ ìœ„í—˜'})")
        logger.info(f"MAE: {final_mae:.4f}")
        logger.info(f"ë°©í–¥ì •í™•ë„: {final_direction_acc:.1f}% ({'âœ… ì•ˆì „' if final_direction_acc <= self.MAX_DIRECTION_ACC else 'âŒ ìœ„í—˜'})")
        logger.info(f"ë² ì´ìŠ¤ ëª¨ë¸: {len(base_models)}ê°œ")
        logger.info(f"ë¸”ë Œë”© ê°€ì¤‘ì¹˜: {optimal_weights}")

        success = safety_check and safe_performance
        logger.info(f"ìµœì¢… ì•ˆì „ì„±: {'âœ… ì„±ê³µ' if success else 'âŒ ì‹¤íŒ¨'}")

        return {
            'final_performance': {
                'r2': final_r2,
                'mae': final_mae,
                'direction_accuracy': final_direction_acc
            },
            'base_models_count': len(base_models),
            'blending_weights': optimal_weights,
            'safety_check': success,
            'stacking_ensemble': self.stacking_ensemble,
            'blending_optimizer': self.blending_optimizer
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    system = SafeKaggleEnsembleSystem()

    try:
        results = system.run_kaggle_ensemble_optimization(
            '/root/workspace/data/training/sp500_2020_2024_enhanced.csv'
        )

        print(f"\n{'='*80}")
        print(f"ğŸ† ìºê¸€ ìŠ¤íƒœí‚¹/ë¸”ë Œë”© ì•™ìƒë¸” ì™„ë£Œ")
        print(f"{'='*80}")

        perf = results['final_performance']
        print(f"\nğŸ“Š ìµœì¢… ì•™ìƒë¸” ì„±ëŠ¥:")
        print(f"   RÂ²: {perf['r2']:.4f}")
        print(f"   MAE: {perf['mae']:.4f}")
        print(f"   ë°©í–¥ì •í™•ë„: {perf['direction_accuracy']:.1f}%")

        print(f"\nğŸ—ï¸ ì•™ìƒë¸” êµ¬ì¡°:")
        print(f"   ë² ì´ìŠ¤ ëª¨ë¸: {results['base_models_count']}ê°œ")
        print(f"   ë¸”ë Œë”© ê°€ì¤‘ì¹˜: {results['blending_weights']}")

        print(f"\nğŸ›¡ï¸ ì•ˆì „ì„±: {'âœ… í†µê³¼' if results['safety_check'] else 'âŒ ì‹¤íŒ¨'}")

        if results['safety_check']:
            print(f"\nğŸ‰ ì„±ê³µ: ìºê¸€ ìš°ìŠ¹ì ê¸°ë²•ìœ¼ë¡œ ì•ˆì „í•œ ì•™ìƒë¸” ì‹œìŠ¤í…œ êµ¬ì¶•!")
        else:
            print(f"\nâš ï¸ ì£¼ì˜: ì•ˆì „ ê¸°ì¤€ ì´ˆê³¼ - ì¶”ê°€ ë³´ì • í•„ìš”")

        return results

    except Exception as e:
        logger.error(f"ìºê¸€ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    result = main()