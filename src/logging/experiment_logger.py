#!/usr/bin/env python3
"""
ğŸ“ ì‹¤í—˜ ë¡œê±° ì‹œìŠ¤í…œ
í•™ìˆ  ë…¼ë¬¸ì„ ìœ„í•œ ì™„ì „í•œ ì‹¤í—˜ ê¸°ë¡ ë° ì¶”ì  ë„êµ¬

ì£¼ìš” ê¸°ëŠ¥:
- ì‹¤í—˜ ì„¤ì • ë° ê²°ê³¼ ìë™ ê¸°ë¡
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì 
- í™˜ê²½ ì •ë³´ ê¸°ë¡
- ì‹¤í—˜ ë¹„êµ ë° ë¶„ì„
- JSON/CSV í˜•íƒœë¡œ ì €ì¥
"""

import json
import csv
import pickle
import hashlib
import datetime
import os
import sys
import platform
import subprocess
import traceback
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass, asdict, field
from pathlib import Path
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

@dataclass
class ExperimentConfig:
    """ì‹¤í—˜ ì„¤ì • ì •ë³´"""
    experiment_id: str
    experiment_name: str
    description: str
    model_type: str
    model_parameters: Dict[str, Any]
    data_info: Dict[str, Any]
    preprocessing_steps: List[str]
    validation_method: str
    random_seed: int
    tags: List[str] = field(default_factory=list)
    notes: str = ""

@dataclass
class SystemInfo:
    """ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´"""
    python_version: str
    platform: str
    cpu_count: int
    memory_gb: float
    gpu_info: Optional[str]
    installed_packages: Dict[str, str]
    git_commit: Optional[str]
    git_branch: Optional[str]
    working_directory: str
    timestamp: str

@dataclass
class ExperimentMetrics:
    """ì‹¤í—˜ ì„±ëŠ¥ ì§€í‘œ"""
    mae: float
    mse: float
    rmse: float
    r2: float
    direction_accuracy: float
    training_time: float
    prediction_time: float
    memory_usage: float
    custom_metrics: Dict[str, float] = field(default_factory=dict)

@dataclass
class ExperimentRecord:
    """ì™„ì „í•œ ì‹¤í—˜ ê¸°ë¡"""
    config: ExperimentConfig
    system_info: SystemInfo
    metrics: ExperimentMetrics
    predictions: Optional[List[float]] = None
    feature_importance: Optional[Dict[str, float]] = None
    validation_scores: Optional[Dict[str, float]] = None
    error_log: Optional[str] = None
    duration_seconds: float = 0.0
    status: str = "completed"  # completed, failed, running

class ExperimentLogger:
    """
    í¬ê´„ì  ì‹¤í—˜ ë¡œê¹… ì‹œìŠ¤í…œ

    ëª¨ë“  ì‹¤í—˜ ê³¼ì •ì„ ìë™ìœ¼ë¡œ ê¸°ë¡í•˜ê³  ì¶”ì 
    """

    def __init__(self, log_dir: str = "experiments_log", auto_save: bool = True):
        """
        ì´ˆê¸°í™”

        Args:
            log_dir: ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            auto_save: ìë™ ì €ì¥ ì—¬ë¶€
        """
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True, parents=True)
        self.auto_save = auto_save

        # í˜„ì¬ ì‹¤í—˜ ê¸°ë¡
        self.current_experiment: Optional[ExperimentRecord] = None
        self.experiment_start_time: Optional[datetime.datetime] = None

        # ì‹¤í—˜ íˆìŠ¤í† ë¦¬
        self.experiment_history: List[ExperimentRecord] = []

        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        self.system_info = self._collect_system_info()

    def start_experiment(self, experiment_name: str,
                        description: str,
                        model_type: str,
                        model_parameters: Dict[str, Any],
                        data_info: Dict[str, Any],
                        preprocessing_steps: List[str],
                        validation_method: str,
                        random_seed: int,
                        tags: Optional[List[str]] = None,
                        notes: str = "") -> str:
        """
        ì‹¤í—˜ ì‹œì‘ ë° ì„¤ì • ê¸°ë¡

        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
            description: ì‹¤í—˜ ì„¤ëª…
            model_type: ëª¨ë¸ ìœ í˜•
            model_parameters: ëª¨ë¸ íŒŒë¼ë¯¸í„°
            data_info: ë°ì´í„° ì •ë³´
            preprocessing_steps: ì „ì²˜ë¦¬ ë‹¨ê³„
            validation_method: ê²€ì¦ ë°©ë²•
            random_seed: ëœë¤ ì‹œë“œ
            tags: ì‹¤í—˜ íƒœê·¸
            notes: ì¶”ê°€ ë…¸íŠ¸

        Returns:
            ì‹¤í—˜ ID
        """
        self.experiment_start_time = datetime.datetime.now()

        # ì‹¤í—˜ ID ìƒì„± (ì‹œê°„ + í•´ì‹œ)
        experiment_id = self._generate_experiment_id(experiment_name, model_parameters)

        # ì‹¤í—˜ ì„¤ì • ìƒì„±
        config = ExperimentConfig(
            experiment_id=experiment_id,
            experiment_name=experiment_name,
            description=description,
            model_type=model_type,
            model_parameters=model_parameters.copy(),
            data_info=data_info.copy(),
            preprocessing_steps=preprocessing_steps.copy(),
            validation_method=validation_method,
            random_seed=random_seed,
            tags=tags or [],
            notes=notes
        )

        # í˜„ì¬ ì‹¤í—˜ ê¸°ë¡ ì´ˆê¸°í™”
        self.current_experiment = ExperimentRecord(
            config=config,
            system_info=self.system_info,
            metrics=ExperimentMetrics(0, 0, 0, 0, 0, 0, 0, 0),  # ì„ì‹œê°’
            status="running"
        )

        print(f"ğŸ“ ì‹¤í—˜ ì‹œì‘: {experiment_name} (ID: {experiment_id})")
        return experiment_id

    def log_metrics(self, mae: float, mse: float, rmse: float, r2: float,
                   direction_accuracy: float, training_time: float,
                   prediction_time: float, memory_usage: float,
                   custom_metrics: Optional[Dict[str, float]] = None):
        """
        ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡

        Args:
            mae: Mean Absolute Error
            mse: Mean Squared Error
            rmse: Root Mean Squared Error
            r2: R-squared
            direction_accuracy: ë°©í–¥ ì •í™•ë„
            training_time: í›ˆë ¨ ì‹œê°„
            prediction_time: ì˜ˆì¸¡ ì‹œê°„
            memory_usage: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            custom_metrics: ì‚¬ìš©ì ì •ì˜ ì§€í‘œ
        """
        if self.current_experiment is None:
            raise ValueError("ì‹¤í—˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. start_experiment()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        self.current_experiment.metrics = ExperimentMetrics(
            mae=mae,
            mse=mse,
            rmse=rmse,
            r2=r2,
            direction_accuracy=direction_accuracy,
            training_time=training_time,
            prediction_time=prediction_time,
            memory_usage=memory_usage,
            custom_metrics=custom_metrics or {}
        )

        print(f"ğŸ“Š ì§€í‘œ ê¸°ë¡ ì™„ë£Œ: MAE={mae:.6f}, RÂ²={r2:.4f}, ë°©í–¥ì •í™•ë„={direction_accuracy:.1f}%")

    def log_predictions(self, predictions: np.ndarray):
        """ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë¡"""
        if self.current_experiment is None:
            raise ValueError("ì‹¤í—˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.current_experiment.predictions = predictions.tolist()
        print(f"ğŸ”® ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë¡: {len(predictions)}ê°œ ì˜ˆì¸¡ê°’")

    def log_feature_importance(self, feature_importance: Dict[str, float]):
        """íŠ¹ì§• ì¤‘ìš”ë„ ê¸°ë¡"""
        if self.current_experiment is None:
            raise ValueError("ì‹¤í—˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.current_experiment.feature_importance = feature_importance.copy()
        print(f"ğŸ¯ íŠ¹ì§• ì¤‘ìš”ë„ ê¸°ë¡: {len(feature_importance)}ê°œ íŠ¹ì§•")

    def log_validation_scores(self, validation_scores: Dict[str, float]):
        """êµì°¨ ê²€ì¦ ì ìˆ˜ ê¸°ë¡"""
        if self.current_experiment is None:
            raise ValueError("ì‹¤í—˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.current_experiment.validation_scores = validation_scores.copy()
        print(f"âœ… ê²€ì¦ ì ìˆ˜ ê¸°ë¡: {len(validation_scores)}ê°œ í´ë“œ")

    def log_error(self, error: Exception):
        """ì˜¤ë¥˜ ì •ë³´ ê¸°ë¡"""
        if self.current_experiment is None:
            raise ValueError("ì‹¤í—˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        error_info = {
            'error_type': type(error).__name__,
            'error_message': str(error),
            'traceback': traceback.format_exc(),
            'timestamp': datetime.datetime.now().isoformat()
        }

        self.current_experiment.error_log = json.dumps(error_info)
        self.current_experiment.status = "failed"
        print(f"âŒ ì˜¤ë¥˜ ê¸°ë¡: {type(error).__name__}")

    def end_experiment(self):
        """ì‹¤í—˜ ì¢…ë£Œ ë° ìµœì¢… ì €ì¥"""
        if self.current_experiment is None:
            raise ValueError("ì‹¤í—˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if self.experiment_start_time is not None:
            duration = datetime.datetime.now() - self.experiment_start_time
            self.current_experiment.duration_seconds = duration.total_seconds()

        if self.current_experiment.status == "running":
            self.current_experiment.status = "completed"

        # ì‹¤í—˜ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.experiment_history.append(self.current_experiment)

        # ìë™ ì €ì¥
        if self.auto_save:
            self._save_experiment(self.current_experiment)

        experiment_id = self.current_experiment.config.experiment_id
        print(f"âœ… ì‹¤í—˜ ì™„ë£Œ: {experiment_id} ({self.current_experiment.duration_seconds:.1f}ì´ˆ)")

        # í˜„ì¬ ì‹¤í—˜ ì´ˆê¸°í™”
        self.current_experiment = None
        self.experiment_start_time = None

        return experiment_id

    def _generate_experiment_id(self, experiment_name: str, parameters: Dict[str, Any]) -> str:
        """ì‹¤í—˜ ID ìƒì„±"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

        # íŒŒë¼ë¯¸í„° í•´ì‹œ ìƒì„±
        param_str = json.dumps(parameters, sort_keys=True, default=str)
        param_hash = hashlib.md5(param_str.encode()).hexdigest()[:8]

        return f"{experiment_name}_{timestamp}_{param_hash}"

    def _collect_system_info(self) -> SystemInfo:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        import psutil

        # GPU ì •ë³´ ìˆ˜ì§‘
        gpu_info = None
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu_info = f"{gpus[0].name} ({gpus[0].memoryTotal}MB)"
        except ImportError:
            try:
                # nvidia-smi ì‚¬ìš©
                result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total',
                                       '--format=csv,noheader,nounits'],
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    gpu_info = result.stdout.strip()
            except FileNotFoundError:
                pass

        # ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ì •ë³´
        try:
            import pkg_resources
            installed_packages = {pkg.project_name: pkg.version
                                for pkg in pkg_resources.working_set}
        except Exception:
            installed_packages = {}

        # Git ì •ë³´
        git_commit, git_branch = self._get_git_info()

        return SystemInfo(
            python_version=platform.python_version(),
            platform=platform.platform(),
            cpu_count=psutil.cpu_count(),
            memory_gb=psutil.virtual_memory().total / (1024**3),
            gpu_info=gpu_info,
            installed_packages=installed_packages,
            git_commit=git_commit,
            git_branch=git_branch,
            working_directory=os.getcwd(),
            timestamp=datetime.datetime.now().isoformat()
        )

    def _get_git_info(self) -> Tuple[Optional[str], Optional[str]]:
        """Git ì •ë³´ ìˆ˜ì§‘"""
        try:
            # Git commit hash
            commit_result = subprocess.run(['git', 'rev-parse', 'HEAD'],
                                         capture_output=True, text=True)
            commit = commit_result.stdout.strip() if commit_result.returncode == 0 else None

            # Git branch
            branch_result = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
                                         capture_output=True, text=True)
            branch = branch_result.stdout.strip() if branch_result.returncode == 0 else None

            return commit, branch
        except FileNotFoundError:
            return None, None

    def _save_experiment(self, experiment: ExperimentRecord):
        """ì‹¤í—˜ ê¸°ë¡ ì €ì¥"""
        experiment_id = experiment.config.experiment_id

        # JSON í˜•íƒœë¡œ ì €ì¥
        json_file = self.log_dir / f"{experiment_id}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(experiment), f, indent=2, ensure_ascii=False, default=str)

        # ë°”ì´ë„ˆë¦¬ í˜•íƒœë¡œë„ ì €ì¥ (ì˜ˆì¸¡ê°’ ë“± ëŒ€ìš©ëŸ‰ ë°ì´í„°)
        pickle_file = self.log_dir / f"{experiment_id}.pkl"
        with open(pickle_file, 'wb') as f:
            pickle.dump(experiment, f)

        print(f"ğŸ’¾ ì‹¤í—˜ ê¸°ë¡ ì €ì¥: {json_file}")

    def load_experiment(self, experiment_id: str) -> ExperimentRecord:
        """ì‹¤í—˜ ê¸°ë¡ ë¡œë“œ"""
        pickle_file = self.log_dir / f"{experiment_id}.pkl"

        if pickle_file.exists():
            with open(pickle_file, 'rb') as f:
                return pickle.load(f)
        else:
            # JSON íŒŒì¼ì—ì„œ ë¡œë“œ (pickleì´ ì—†ëŠ” ê²½ìš°)
            json_file = self.log_dir / f"{experiment_id}.json"
            if json_file.exists():
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # ë°ì´í„°í´ë˜ìŠ¤ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë²„ì „)
                # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë³€í™˜ ë¡œì§ í•„ìš”
                return data
            else:
                raise FileNotFoundError(f"ì‹¤í—˜ {experiment_id}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def list_experiments(self, tags: Optional[List[str]] = None,
                        model_type: Optional[str] = None,
                        status: Optional[str] = None) -> List[str]:
        """ì‹¤í—˜ ëª©ë¡ ì¡°íšŒ"""
        experiment_files = list(self.log_dir.glob("*.json"))
        experiments = []

        for file in experiment_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # í•„í„°ë§
                if tags and not any(tag in data['config'].get('tags', []) for tag in tags):
                    continue

                if model_type and data['config'].get('model_type') != model_type:
                    continue

                if status and data.get('status') != status:
                    continue

                experiments.append(data['config']['experiment_id'])

            except Exception as e:
                print(f"ì‹¤í—˜ íŒŒì¼ {file} ë¡œë“œ ì˜¤ë¥˜: {str(e)}")

        return experiments

    def compare_experiments(self, experiment_ids: List[str]) -> pd.DataFrame:
        """ì‹¤í—˜ ë¹„êµ í…Œì´ë¸” ìƒì„±"""
        comparison_data = []

        for exp_id in experiment_ids:
            try:
                with open(self.log_dir / f"{exp_id}.json", 'r', encoding='utf-8') as f:
                    data = json.load(f)

                config = data['config']
                metrics = data['metrics']

                row = {
                    'experiment_id': exp_id,
                    'experiment_name': config['experiment_name'],
                    'model_type': config['model_type'],
                    'mae': metrics['mae'],
                    'rmse': metrics['rmse'],
                    'r2': metrics['r2'],
                    'direction_accuracy': metrics['direction_accuracy'],
                    'training_time': metrics['training_time'],
                    'status': data.get('status', 'unknown'),
                    'duration': data.get('duration_seconds', 0)
                }

                # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¼ë¶€ ì¶”ê°€
                for key, value in config['model_parameters'].items():
                    if isinstance(value, (int, float, str, bool)):
                        row[f"param_{key}"] = value

                comparison_data.append(row)

            except Exception as e:
                print(f"ì‹¤í—˜ {exp_id} ë¹„êµ ì˜¤ë¥˜: {str(e)}")

        return pd.DataFrame(comparison_data)

    def generate_experiment_summary(self) -> str:
        """ì‹¤í—˜ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        experiments = self.list_experiments()

        if not experiments:
            return "ê¸°ë¡ëœ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤."

        # ì‹¤í—˜ í†µê³„
        total_experiments = len(experiments)
        completed_experiments = len(self.list_experiments(status="completed"))
        failed_experiments = len(self.list_experiments(status="failed"))

        # ì„±ëŠ¥ í†µê³„ (ì™„ë£Œëœ ì‹¤í—˜ë§Œ)
        completed_exp_data = []
        for exp_id in self.list_experiments(status="completed"):
            try:
                with open(self.log_dir / f"{exp_id}.json", 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    completed_exp_data.append(data['metrics'])
            except Exception:
                continue

        report = [
            "ğŸ“Š ì‹¤í—˜ ìš”ì•½ ë³´ê³ ì„œ",
            "=" * 50,
            "",
            f"ğŸ“ˆ ì‹¤í—˜ í†µê³„:",
            f"   ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}",
            f"   ì™„ë£Œëœ ì‹¤í—˜: {completed_experiments}",
            f"   ì‹¤íŒ¨í•œ ì‹¤í—˜: {failed_experiments}",
            f"   ì„±ê³µë¥ : {completed_experiments/total_experiments*100:.1f}%" if total_experiments > 0 else "   ì„±ê³µë¥ : N/A",
            ""
        ]

        if completed_exp_data:
            mae_values = [exp['mae'] for exp in completed_exp_data]
            r2_values = [exp['r2'] for exp in completed_exp_data]
            dir_acc_values = [exp['direction_accuracy'] for exp in completed_exp_data]

            report.extend([
                f"ğŸ¯ ì„±ëŠ¥ í†µê³„ (ì™„ë£Œëœ ì‹¤í—˜):",
                f"   MAE - í‰ê· : {np.mean(mae_values):.6f}, ìµœê³ : {np.min(mae_values):.6f}, ìµœì €: {np.max(mae_values):.6f}",
                f"   RÂ² - í‰ê· : {np.mean(r2_values):.4f}, ìµœê³ : {np.max(r2_values):.4f}, ìµœì €: {np.min(r2_values):.4f}",
                f"   ë°©í–¥ì •í™•ë„ - í‰ê· : {np.mean(dir_acc_values):.1f}%, ìµœê³ : {np.max(dir_acc_values):.1f}%, ìµœì €: {np.min(dir_acc_values):.1f}%",
                ""
            ])

        # ìµœê·¼ ì‹¤í—˜ë“¤
        recent_experiments = sorted(experiments, reverse=True)[:5]
        report.extend([
            f"ğŸ•’ ìµœê·¼ ì‹¤í—˜ (ìµœëŒ€ 5ê°œ):",
        ])

        for exp_id in recent_experiments:
            try:
                with open(self.log_dir / f"{exp_id}.json", 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    config = data['config']
                    metrics = data['metrics']
                    status = data.get('status', 'unknown')

                    report.append(f"   - {config['experiment_name']}: MAE={metrics['mae']:.6f}, ìƒíƒœ={status}")
            except Exception:
                report.append(f"   - {exp_id}: ë¡œë“œ ì˜¤ë¥˜")

        return "\n".join(report)

def main():
    """í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸ“ ì‹¤í—˜ ë¡œê±° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # ì‹¤í—˜ ë¡œê±° ì´ˆê¸°í™”
    logger = ExperimentLogger(log_dir="test_experiments_log")

    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    X_train = np.random.randn(100, 5)
    y_train = np.random.randn(100)
    X_test = np.random.randn(30, 5)
    y_test = np.random.randn(30)

    # í…ŒìŠ¤íŠ¸ ì‹¤í—˜ 1
    print("\n1. ì²« ë²ˆì§¸ ì‹¤í—˜")
    exp_id1 = logger.start_experiment(
        experiment_name="RandomForest_Test",
        description="Random Forest ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸",
        model_type="RandomForestRegressor",
        model_parameters={"n_estimators": 100, "max_depth": 10, "random_state": 42},
        data_info={"n_samples": 100, "n_features": 5, "target_type": "regression"},
        preprocessing_steps=["StandardScaler", "train_test_split"],
        validation_method="TimeSeriesSplit",
        random_seed=42,
        tags=["baseline", "random_forest"],
        notes="ê¸°ë³¸ ì„±ëŠ¥ í™•ì¸ìš©"
    )

    # ëª¨ë¸ ì‹œë®¬ë ˆì´ì…˜ ë° ì§€í‘œ ê¸°ë¡
    import time
    start_time = time.time()
    time.sleep(0.1)  # í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜
    training_time = time.time() - start_time

    start_time = time.time()
    predictions = np.random.randn(30)  # ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜
    prediction_time = time.time() - start_time

    # ì§€í‘œ ê³„ì‚° ë° ê¸°ë¡
    mae = np.mean(np.abs(y_test - predictions))
    mse = np.mean((y_test - predictions)**2)
    rmse = np.sqrt(mse)
    r2 = 1 - mse / np.var(y_test) if np.var(y_test) > 0 else 0

    logger.log_metrics(
        mae=mae, mse=mse, rmse=rmse, r2=r2,
        direction_accuracy=65.0, training_time=training_time,
        prediction_time=prediction_time, memory_usage=50.0,
        custom_metrics={"sharpe_ratio": 1.2, "max_drawdown": 0.15}
    )

    logger.log_predictions(predictions)
    logger.log_feature_importance({"feature_0": 0.3, "feature_1": 0.25, "feature_2": 0.2})
    logger.log_validation_scores({"fold_1": 0.85, "fold_2": 0.82, "fold_3": 0.88})

    logger.end_experiment()

    # í…ŒìŠ¤íŠ¸ ì‹¤í—˜ 2 (ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜)
    print("\n2. ë‘ ë²ˆì§¸ ì‹¤í—˜ (ì‹¤íŒ¨)")
    exp_id2 = logger.start_experiment(
        experiment_name="LinearRegression_Test",
        description="Linear Regression ì‹¤íŒ¨ ì¼€ì´ìŠ¤",
        model_type="LinearRegression",
        model_parameters={"fit_intercept": True},
        data_info={"n_samples": 100, "n_features": 5},
        preprocessing_steps=["StandardScaler"],
        validation_method="cross_val_score",
        random_seed=42,
        tags=["linear_model", "failed_test"]
    )

    # ì˜¤ë¥˜ ì‹œë®¬ë ˆì´ì…˜
    try:
        raise ValueError("ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜: ë°ì´í„° í˜•ì‹ ë¶ˆì¼ì¹˜")
    except Exception as e:
        logger.log_error(e)

    logger.end_experiment()

    # ì‹¤í—˜ ëª©ë¡ ë° ë¹„êµ
    print("\n3. ì‹¤í—˜ ëª©ë¡ ì¡°íšŒ")
    all_experiments = logger.list_experiments()
    print(f"ì „ì²´ ì‹¤í—˜: {all_experiments}")

    completed_experiments = logger.list_experiments(status="completed")
    print(f"ì™„ë£Œëœ ì‹¤í—˜: {completed_experiments}")

    # ì‹¤í—˜ ë¹„êµ
    if len(all_experiments) >= 2:
        print("\n4. ì‹¤í—˜ ë¹„êµ")
        comparison_df = logger.compare_experiments(all_experiments[:2])
        print(comparison_df.to_string(index=False))

    # ìš”ì•½ ë³´ê³ ì„œ
    print("\n5. ì‹¤í—˜ ìš”ì•½ ë³´ê³ ì„œ")
    summary = logger.generate_experiment_summary()
    print(summary)

if __name__ == "__main__":
    main()