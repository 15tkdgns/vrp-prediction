#!/usr/bin/env python3
"""
ğŸ”„ ì¬í˜„ì„± ì²´ì»¤ ì‹œìŠ¤í…œ
í•™ìˆ  ë…¼ë¬¸ì„ ìœ„í•œ ì‹¤í—˜ ì¬í˜„ì„± ê²€ì¦ ë„êµ¬

ì£¼ìš” ê¸°ëŠ¥:
- ì‹¤í—˜ ì¬í˜„ì„± ê²€ì¦
- í™˜ê²½ ì¼ê´€ì„± ì²´í¬
- ê²°ê³¼ ë¹„êµ ë° ë¶„ì„
- ì¬í˜„ì„± ë³´ê³ ì„œ ìƒì„±
"""

import json
import numpy as np
import pandas as pd
import hashlib
import subprocess
import platform
import sys
import os
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from dataclasses import dataclass
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

@dataclass
class ReproducibilityResult:
    """ì¬í˜„ì„± ê²€ì¦ ê²°ê³¼"""
    is_reproducible: bool
    similarity_score: float
    environment_match: bool
    seed_consistency: bool
    data_consistency: bool
    code_consistency: bool
    metric_differences: Dict[str, float]
    warnings: List[str]
    recommendations: List[str]

class ReproducibilityChecker:
    """
    ì‹¤í—˜ ì¬í˜„ì„± ê²€ì¦ ì‹œìŠ¤í…œ

    ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ì‹¤í—˜ì„ ì¬ì‹¤í–‰í•˜ê³  ê²°ê³¼ì˜ ì¼ê´€ì„±ì„ ê²€ì¦
    """

    def __init__(self, tolerance: float = 1e-6, strict_mode: bool = False):
        """
        ì´ˆê¸°í™”

        Args:
            tolerance: ìˆ˜ì¹˜ ë¹„êµ í—ˆìš© ì˜¤ì°¨
            strict_mode: ì—„ê²© ëª¨ë“œ (í™˜ê²½ê¹Œì§€ ì™„ì „íˆ ì¼ì¹˜í•´ì•¼ í•¨)
        """
        self.tolerance = tolerance
        self.strict_mode = strict_mode

    def verify_experiment_reproducibility(self,
                                        original_experiment_path: str,
                                        rerun_function: Callable,
                                        n_runs: int = 3) -> ReproducibilityResult:
        """
        ì‹¤í—˜ ì¬í˜„ì„± ê²€ì¦

        Args:
            original_experiment_path: ì›ë³¸ ì‹¤í—˜ ê¸°ë¡ ê²½ë¡œ
            rerun_function: ì¬ì‹¤í–‰ í•¨ìˆ˜
            n_runs: ì¬ì‹¤í–‰ íšŸìˆ˜

        Returns:
            ì¬í˜„ì„± ê²€ì¦ ê²°ê³¼
        """
        print(f"ğŸ”„ ì¬í˜„ì„± ê²€ì¦ ì‹œì‘: {original_experiment_path}")

        # ì›ë³¸ ì‹¤í—˜ ë¡œë“œ
        original_data = self._load_experiment_data(original_experiment_path)
        if original_data is None:
            raise ValueError(f"ì›ë³¸ ì‹¤í—˜ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {original_experiment_path}")

        # í™˜ê²½ ì¼ê´€ì„± ì²´í¬
        environment_match = self._check_environment_consistency(original_data)

        # ì—¬ëŸ¬ ë²ˆ ì¬ì‹¤í–‰
        rerun_results = []
        for i in range(n_runs):
            print(f"  ì¬ì‹¤í–‰ {i+1}/{n_runs}")
            try:
                rerun_result = rerun_function()
                rerun_results.append(rerun_result)
            except Exception as e:
                print(f"  ì¬ì‹¤í–‰ {i+1} ì‹¤íŒ¨: {str(e)}")
                rerun_results.append(None)

        # ê²°ê³¼ ë¶„ì„
        return self._analyze_reproducibility(original_data, rerun_results, environment_match)

    def _load_experiment_data(self, experiment_path: str) -> Optional[Dict]:
        """ì‹¤í—˜ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(experiment_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"ì‹¤í—˜ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return None

    def _check_environment_consistency(self, original_data: Dict) -> bool:
        """í™˜ê²½ ì¼ê´€ì„± ì²´í¬"""
        print("  í™˜ê²½ ì¼ê´€ì„± ì²´í¬ ì¤‘...")

        if 'system_info' not in original_data:
            print("  ê²½ê³ : ì›ë³¸ ì‹¤í—˜ì— ì‹œìŠ¤í…œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        original_system = original_data['system_info']
        current_system = self._get_current_system_info()

        # í•µì‹¬ í™˜ê²½ ìš”ì†Œ ë¹„êµ
        checks = {
            'python_version': original_system.get('python_version') == current_system['python_version'],
            'platform': original_system.get('platform') == current_system['platform'],
            'working_directory': original_system.get('working_directory') == current_system['working_directory']
        }

        # ì¤‘ìš”í•œ íŒ¨í‚¤ì§€ ë²„ì „ ë¹„êµ
        original_packages = original_system.get('installed_packages', {})
        current_packages = current_system['installed_packages']

        critical_packages = ['numpy', 'pandas', 'scikit-learn', 'tensorflow', 'torch']
        package_consistency = True

        for package in critical_packages:
            if package in original_packages and package in current_packages:
                if original_packages[package] != current_packages[package]:
                    print(f"  ê²½ê³ : {package} ë²„ì „ ë¶ˆì¼ì¹˜ - ì›ë³¸: {original_packages[package]}, í˜„ì¬: {current_packages[package]}")
                    package_consistency = False

        all_consistent = all(checks.values()) and package_consistency

        if not all_consistent:
            print("  âš ï¸ í™˜ê²½ ë¶ˆì¼ì¹˜ ê°ì§€")
            for check_name, result in checks.items():
                if not result:
                    print(f"    - {check_name}: ë¶ˆì¼ì¹˜")

        return all_consistent

    def _get_current_system_info(self) -> Dict:
        """í˜„ì¬ ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        import psutil

        # ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ì •ë³´
        try:
            import pkg_resources
            installed_packages = {pkg.project_name: pkg.version
                                for pkg in pkg_resources.working_set}
        except Exception:
            installed_packages = {}

        return {
            'python_version': platform.python_version(),
            'platform': platform.platform(),
            'working_directory': os.getcwd(),
            'installed_packages': installed_packages
        }

    def _analyze_reproducibility(self, original_data: Dict,
                               rerun_results: List[Optional[Dict]],
                               environment_match: bool) -> ReproducibilityResult:
        """ì¬í˜„ì„± ë¶„ì„"""
        print("  ê²°ê³¼ ë¶„ì„ ì¤‘...")

        # ì„±ê³µí•œ ì¬ì‹¤í–‰ë§Œ í•„í„°ë§
        valid_reruns = [result for result in rerun_results if result is not None]

        if not valid_reruns:
            return ReproducibilityResult(
                is_reproducible=False,
                similarity_score=0.0,
                environment_match=environment_match,
                seed_consistency=False,
                data_consistency=False,
                code_consistency=False,
                metric_differences={},
                warnings=["ëª¨ë“  ì¬ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."],
                recommendations=["ì‹¤í—˜ ì½”ë“œì™€ í™˜ê²½ì„ ì ê²€í•˜ì„¸ìš”."]
            )

        # ë©”íŠ¸ë¦­ ë¹„êµ
        original_metrics = original_data.get('metrics', {})
        metric_differences = self._compare_metrics(original_metrics, valid_reruns)

        # ì‹œë“œ ì¼ê´€ì„± ì²´í¬
        seed_consistency = self._check_seed_consistency(original_data, valid_reruns)

        # ë°ì´í„° ì¼ê´€ì„± ì²´í¬
        data_consistency = self._check_data_consistency(original_data, valid_reruns)

        # ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_score = self._calculate_similarity_score(metric_differences)

        # ì¬í˜„ì„± íŒì •
        is_reproducible = (
            similarity_score > 0.95 and  # 95% ì´ìƒ ìœ ì‚¬
            seed_consistency and
            data_consistency and
            (environment_match or not self.strict_mode)
        )

        # ê²½ê³  ë° ê¶Œê³ ì‚¬í•­ ìƒì„±
        warnings, recommendations = self._generate_warnings_and_recommendations(
            similarity_score, environment_match, seed_consistency,
            data_consistency, metric_differences
        )

        return ReproducibilityResult(
            is_reproducible=is_reproducible,
            similarity_score=similarity_score,
            environment_match=environment_match,
            seed_consistency=seed_consistency,
            data_consistency=data_consistency,
            code_consistency=True,  # ì½”ë“œê°€ ì‹¤í–‰ë˜ë©´ ì¼ê´€ì„± ìˆë‹¤ê³  ê°€ì •
            metric_differences=metric_differences,
            warnings=warnings,
            recommendations=recommendations
        )

    def _compare_metrics(self, original_metrics: Dict, rerun_results: List[Dict]) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ ë¹„êµ"""
        differences = {}

        if not original_metrics:
            return differences

        for metric_name in original_metrics.keys():
            if metric_name in ['custom_metrics']:  # ì¤‘ì²© ë”•ì…”ë„ˆë¦¬ ê±´ë„ˆë›°ê¸°
                continue

            original_value = original_metrics[metric_name]
            if not isinstance(original_value, (int, float)):
                continue

            rerun_values = []
            for rerun in rerun_results:
                if metric_name in rerun.get('metrics', {}):
                    rerun_values.append(rerun['metrics'][metric_name])

            if rerun_values:
                mean_rerun = np.mean(rerun_values)
                std_rerun = np.std(rerun_values) if len(rerun_values) > 1 else 0

                # ì ˆëŒ€ ì°¨ì´ì™€ ìƒëŒ€ ì°¨ì´ ê³„ì‚°
                abs_diff = abs(original_value - mean_rerun)
                rel_diff = abs_diff / abs(original_value) if original_value != 0 else abs_diff

                differences[metric_name] = {
                    'original': original_value,
                    'rerun_mean': mean_rerun,
                    'rerun_std': std_rerun,
                    'absolute_difference': abs_diff,
                    'relative_difference': rel_diff,
                    'within_tolerance': abs_diff <= self.tolerance
                }

        return differences

    def _check_seed_consistency(self, original_data: Dict, rerun_results: List[Dict]) -> bool:
        """ì‹œë“œ ì¼ê´€ì„± ì²´í¬"""
        original_seed = original_data.get('config', {}).get('random_seed')

        if original_seed is None:
            return False

        # ì¬ì‹¤í–‰ ê²°ê³¼ì—ì„œ ë™ì¼í•œ ì‹œë“œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        for rerun in rerun_results:
            rerun_seed = rerun.get('config', {}).get('random_seed')
            if rerun_seed != original_seed:
                return False

        return True

    def _check_data_consistency(self, original_data: Dict, rerun_results: List[Dict]) -> bool:
        """ë°ì´í„° ì¼ê´€ì„± ì²´í¬"""
        original_data_info = original_data.get('config', {}).get('data_info', {})

        if not original_data_info:
            return True  # ë°ì´í„° ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¼ê´€ì„± ìˆë‹¤ê³  ê°€ì •

        for rerun in rerun_results:
            rerun_data_info = rerun.get('config', {}).get('data_info', {})

            # ì¤‘ìš”í•œ ë°ì´í„° ì†ì„± ë¹„êµ
            for key in ['n_samples', 'n_features', 'target_type']:
                if key in original_data_info and key in rerun_data_info:
                    if original_data_info[key] != rerun_data_info[key]:
                        return False

        return True

    def _calculate_similarity_score(self, metric_differences: Dict) -> float:
        """ì „ì²´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°"""
        if not metric_differences:
            return 1.0

        total_score = 0.0
        count = 0

        for metric_name, diff_info in metric_differences.items():
            if isinstance(diff_info, dict) and 'within_tolerance' in diff_info:
                if diff_info['within_tolerance']:
                    score = 1.0
                else:
                    # ìƒëŒ€ ì°¨ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
                    rel_diff = diff_info['relative_difference']
                    score = max(0.0, 1.0 - rel_diff)

                total_score += score
                count += 1

        return total_score / count if count > 0 else 1.0

    def _generate_warnings_and_recommendations(self, similarity_score: float,
                                             environment_match: bool,
                                             seed_consistency: bool,
                                             data_consistency: bool,
                                             metric_differences: Dict) -> Tuple[List[str], List[str]]:
        """ê²½ê³  ë° ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        warnings = []
        recommendations = []

        # ìœ ì‚¬ë„ ê¸°ë°˜ ê²½ê³ 
        if similarity_score < 0.95:
            warnings.append(f"ê²°ê³¼ ìœ ì‚¬ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({similarity_score:.3f})")
            recommendations.append("ë©”íŠ¸ë¦­ ì°¨ì´ê°€ í° ì›ì¸ì„ ë¶„ì„í•˜ì„¸ìš”")

        # í™˜ê²½ ë¶ˆì¼ì¹˜ ê²½ê³ 
        if not environment_match:
            warnings.append("ì‹¤í–‰ í™˜ê²½ì´ ì›ë³¸ê³¼ ë‹¤ë¦…ë‹ˆë‹¤")
            if self.strict_mode:
                recommendations.append("ë™ì¼í•œ í™˜ê²½ì—ì„œ ì‹¤í—˜ì„ ì¬ì‹¤í–‰í•˜ì„¸ìš”")
            else:
                recommendations.append("í™˜ê²½ ì°¨ì´ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³ ë ¤í•˜ì„¸ìš”")

        # ì‹œë“œ ë¶ˆì¼ì¹˜ ê²½ê³ 
        if not seed_consistency:
            warnings.append("ëœë¤ ì‹œë“œê°€ ì¼ê´€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            recommendations.append("ë™ì¼í•œ ëœë¤ ì‹œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")

        # ë°ì´í„° ë¶ˆì¼ì¹˜ ê²½ê³ 
        if not data_consistency:
            warnings.append("ë°ì´í„° ì„¤ì •ì´ ì¼ê´€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            recommendations.append("ë™ì¼í•œ ë°ì´í„°ì…‹ê³¼ ì „ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")

        # íŠ¹ì • ë©”íŠ¸ë¦­ ì°¨ì´ ê²½ê³ 
        large_diff_metrics = []
        for metric_name, diff_info in metric_differences.items():
            if isinstance(diff_info, dict) and diff_info.get('relative_difference', 0) > 0.1:
                large_diff_metrics.append(metric_name)

        if large_diff_metrics:
            warnings.append(f"í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” ë©”íŠ¸ë¦­: {', '.join(large_diff_metrics)}")
            recommendations.append("í•´ë‹¹ ë©”íŠ¸ë¦­ë“¤ì˜ ë³€ë™ ì›ì¸ì„ ì¡°ì‚¬í•˜ì„¸ìš”")

        # ì¼ë°˜ì ì¸ ê¶Œê³ ì‚¬í•­
        if not warnings:
            recommendations.append("ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤")
        else:
            recommendations.append("ì¬í˜„ì„± ê°œì„ ì„ ìœ„í•´ í™˜ê²½ê³¼ ì„¤ì •ì„ í‘œì¤€í™”í•˜ì„¸ìš”")
            recommendations.append("Docker ì»¨í…Œì´ë„ˆ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”")

        return warnings, recommendations

    def generate_reproducibility_report(self, result: ReproducibilityResult,
                                      experiment_name: str) -> str:
        """ì¬í˜„ì„± ë³´ê³ ì„œ ìƒì„±"""
        report = [
            f"ğŸ”„ ì¬í˜„ì„± ê²€ì¦ ë³´ê³ ì„œ: {experiment_name}",
            "=" * 60,
            ""
        ]

        # ì „ì²´ ê²°ê³¼
        status = "âœ… ì¬í˜„ ê°€ëŠ¥" if result.is_reproducible else "âŒ ì¬í˜„ ë¶ˆê°€ëŠ¥"
        report.extend([
            f"ğŸ“Š ì „ì²´ ê²°ê³¼: {status}",
            f"ğŸ“ˆ ìœ ì‚¬ë„ ì ìˆ˜: {result.similarity_score:.3f}",
            ""
        ])

        # ì„¸ë¶€ ê²€ì¦ ê²°ê³¼
        report.extend([
            "ğŸ” ì„¸ë¶€ ê²€ì¦ ê²°ê³¼:",
            f"   í™˜ê²½ ì¼ì¹˜: {'âœ…' if result.environment_match else 'âŒ'}",
            f"   ì‹œë“œ ì¼ê´€ì„±: {'âœ…' if result.seed_consistency else 'âŒ'}",
            f"   ë°ì´í„° ì¼ê´€ì„±: {'âœ…' if result.data_consistency else 'âŒ'}",
            f"   ì½”ë“œ ì¼ê´€ì„±: {'âœ…' if result.code_consistency else 'âŒ'}",
            ""
        ])

        # ë©”íŠ¸ë¦­ ì°¨ì´ ë¶„ì„
        if result.metric_differences:
            report.extend([
                "ğŸ“Š ë©”íŠ¸ë¦­ ì°¨ì´ ë¶„ì„:",
                "-" * 40
            ])

            for metric_name, diff_info in result.metric_differences.items():
                if isinstance(diff_info, dict):
                    original = diff_info['original']
                    rerun_mean = diff_info['rerun_mean']
                    rel_diff = diff_info['relative_difference']
                    within_tol = diff_info['within_tolerance']

                    tolerance_status = "âœ…" if within_tol else "âŒ"
                    report.append(f"   {metric_name}: {tolerance_status}")
                    report.append(f"     ì›ë³¸: {original:.6f}")
                    report.append(f"     ì¬ì‹¤í–‰: {rerun_mean:.6f}")
                    report.append(f"     ìƒëŒ€ ì°¨ì´: {rel_diff:.3%}")

            report.append("")

        # ê²½ê³ ì‚¬í•­
        if result.warnings:
            report.extend([
                "âš ï¸ ê²½ê³ ì‚¬í•­:",
            ])
            for warning in result.warnings:
                report.append(f"   - {warning}")
            report.append("")

        # ê¶Œê³ ì‚¬í•­
        if result.recommendations:
            report.extend([
                "ğŸ’¡ ê¶Œê³ ì‚¬í•­:",
            ])
            for recommendation in result.recommendations:
                report.append(f"   - {recommendation}")
            report.append("")

        # ì¬í˜„ì„± ê°œì„  ê°€ì´ë“œ
        if not result.is_reproducible:
            report.extend([
                "ğŸ› ï¸ ì¬í˜„ì„± ê°œì„  ê°€ì´ë“œ:",
                "   1. ëª¨ë“  ëœë¤ ì‹œë“œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •",
                "   2. í™˜ê²½ ì¢…ì†ì„±ì„ requirements.txtì— ëª…ì‹œ",
                "   3. ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„ ì™„ì „íˆ ë¬¸ì„œí™”",
                "   4. Docker ì»¨í…Œì´ë„ˆë¥¼ ì‚¬ìš©í•œ í™˜ê²½ í‘œì¤€í™”",
                "   5. ì‹¤í—˜ ì½”ë“œì˜ ë¹„ê²°ì •ì  ìš”ì†Œ ì œê±°",
                ""
            ])

        return "\n".join(report)

    def batch_verify_experiments(self, experiment_dir: str,
                                rerun_functions: Dict[str, Callable]) -> Dict[str, ReproducibilityResult]:
        """ì—¬ëŸ¬ ì‹¤í—˜ì˜ ì¬í˜„ì„± ì¼ê´„ ê²€ì¦"""
        results = {}
        experiment_files = list(Path(experiment_dir).glob("*.json"))

        for exp_file in experiment_files:
            exp_id = exp_file.stem
            if exp_id in rerun_functions:
                print(f"ê²€ì¦ ì¤‘: {exp_id}")
                try:
                    result = self.verify_experiment_reproducibility(
                        str(exp_file), rerun_functions[exp_id], n_runs=2
                    )
                    results[exp_id] = result
                except Exception as e:
                    print(f"ê²€ì¦ ì‹¤íŒ¨ ({exp_id}): {str(e)}")

        return results

def main():
    """í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸ”„ ì¬í˜„ì„± ì²´ì»¤ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # ì¬í˜„ì„± ì²´ì»¤ ì´ˆê¸°í™”
    checker = ReproducibilityChecker(tolerance=1e-3, strict_mode=False)

    # í…ŒìŠ¤íŠ¸ìš© ì‹¤í—˜ ë°ì´í„° ìƒì„±
    test_experiment = {
        'config': {
            'experiment_id': 'test_experiment_001',
            'experiment_name': 'Test_Reproducibility',
            'model_type': 'RandomForestRegressor',
            'model_parameters': {'n_estimators': 100, 'random_state': 42},
            'data_info': {'n_samples': 1000, 'n_features': 10},
            'random_seed': 42
        },
        'metrics': {
            'mae': 0.123456,
            'mse': 0.234567,
            'rmse': 0.484334,
            'r2': 0.789012,
            'direction_accuracy': 75.5
        },
        'system_info': {
            'python_version': platform.python_version(),
            'platform': platform.platform(),
            'working_directory': os.getcwd(),
            'installed_packages': {'numpy': '1.21.0', 'pandas': '1.3.0'}
        }
    }

    # ì„ì‹œ ì‹¤í—˜ íŒŒì¼ ì €ì¥
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(test_experiment, f)
        temp_file = f.name

    # ì¬ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜ (ì™„ì „ ë™ì¼í•œ ê²°ê³¼)
    def perfect_rerun():
        return {
            'config': test_experiment['config'],
            'metrics': test_experiment['metrics']  # ë™ì¼í•œ ê²°ê³¼
        }

    # ì¬ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜ (ì•½ê°„ì˜ ì°¨ì´)
    def slightly_different_rerun():
        import random
        metrics = test_experiment['metrics'].copy()
        # ì‘ì€ ë…¸ì´ì¦ˆ ì¶”ê°€
        for key in metrics:
            if isinstance(metrics[key], (int, float)):
                noise = random.uniform(-0.001, 0.001)
                metrics[key] += noise

        return {
            'config': test_experiment['config'],
            'metrics': metrics
        }

    # ì¬ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜ (í° ì°¨ì´)
    def very_different_rerun():
        metrics = test_experiment['metrics'].copy()
        # í° ì°¨ì´ ì¶”ê°€
        for key in metrics:
            if isinstance(metrics[key], (int, float)):
                metrics[key] *= 1.1  # 10% ì°¨ì´

        return {
            'config': test_experiment['config'],
            'metrics': metrics
        }

    try:
        # 1. ì™„ë²½í•œ ì¬í˜„ í…ŒìŠ¤íŠ¸
        print("\n1. ì™„ë²½í•œ ì¬í˜„ í…ŒìŠ¤íŠ¸")
        result1 = checker.verify_experiment_reproducibility(
            temp_file, perfect_rerun, n_runs=3
        )

        print(f"ì¬í˜„ ê°€ëŠ¥: {result1.is_reproducible}")
        print(f"ìœ ì‚¬ë„: {result1.similarity_score:.3f}")

        # 2. ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆëŠ” ì¬í˜„ í…ŒìŠ¤íŠ¸
        print("\n2. ì•½ê°„ì˜ ì°¨ì´ í…ŒìŠ¤íŠ¸")
        result2 = checker.verify_experiment_reproducibility(
            temp_file, slightly_different_rerun, n_runs=3
        )

        print(f"ì¬í˜„ ê°€ëŠ¥: {result2.is_reproducible}")
        print(f"ìœ ì‚¬ë„: {result2.similarity_score:.3f}")

        # 3. í° ì°¨ì´ê°€ ìˆëŠ” ì¬í˜„ í…ŒìŠ¤íŠ¸
        print("\n3. í° ì°¨ì´ í…ŒìŠ¤íŠ¸")
        result3 = checker.verify_experiment_reproducibility(
            temp_file, very_different_rerun, n_runs=3
        )

        print(f"ì¬í˜„ ê°€ëŠ¥: {result3.is_reproducible}")
        print(f"ìœ ì‚¬ë„: {result3.similarity_score:.3f}")

        # ë³´ê³ ì„œ ìƒì„±
        print("\n4. ì¬í˜„ì„± ë³´ê³ ì„œ (í° ì°¨ì´ ì¼€ì´ìŠ¤)")
        print("-" * 60)
        report = checker.generate_reproducibility_report(result3, "Test_Experiment")
        print(report)

    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_file)

if __name__ == "__main__":
    main()