#!/usr/bin/env python3
"""
ğŸ§ª í†µì œ ì‹¤í—˜ ì‹œìŠ¤í…œ
í•™ìˆ  ë…¼ë¬¸ì„ ìœ„í•œ ì²´ê³„ì ì¸ ì‹¤í—˜ ì„¤ê³„ ë° ì‹¤í–‰ í”„ë ˆì„ì›Œí¬

ì£¼ìš” ê¸°ëŠ¥:
- í†µì œ ë³€ìˆ˜ ê´€ë¦¬
- ì‹¤í—˜ ì¡°ê±´ ì„¤ì •
- A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
- ê²°ê³¼ ë¶„ì„ ë° í•´ì„
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
import itertools
from sklearn.model_selection import ParameterGrid
from sklearn.base import BaseEstimator, clone
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

class ExperimentType(Enum):
    """ì‹¤í—˜ ìœ í˜• ì—´ê±°í˜•"""
    AB_TEST = "ab_test"
    MULTI_ARM = "multi_arm"
    FACTORIAL = "factorial"
    ABLATION = "ablation"
    SENSITIVITY = "sensitivity"

@dataclass
class ExperimentalCondition:
    """
    ì‹¤í—˜ ì¡°ê±´ ì •ì˜ í´ë˜ìŠ¤
    """
    name: str
    description: str
    parameters: Dict[str, Any]
    control_variables: Dict[str, Any] = field(default_factory=dict)
    expected_outcome: Optional[str] = None

@dataclass
class ExperimentResult:
    """
    ì‹¤í—˜ ê²°ê³¼ í´ë˜ìŠ¤
    """
    condition_name: str
    metrics: Dict[str, float]
    sample_size: int
    execution_time: float
    additional_info: Dict[str, Any] = field(default_factory=dict)

class ControlledExperimentFramework:
    """
    í†µì œ ì‹¤í—˜ì„ ìœ„í•œ í¬ê´„ì  í”„ë ˆì„ì›Œí¬

    ë³€ìˆ˜ í†µì œ, ì‹¤í—˜ ì„¤ê³„, ê²°ê³¼ ë¶„ì„ì„ ì²´ê³„ì ìœ¼ë¡œ ìˆ˜í–‰
    """

    def __init__(self, random_state=42):
        """
        ì´ˆê¸°í™”

        Args:
            random_state: ëœë¤ ì‹œë“œ
        """
        self.random_state = random_state
        np.random.seed(random_state)

        self.experiments: Dict[str, List[ExperimentalCondition]] = {}
        self.results: Dict[str, List[ExperimentResult]] = {}
        self.control_variables: Dict[str, Any] = {}

    def set_control_variables(self, **kwargs):
        """
        í†µì œ ë³€ìˆ˜ ì„¤ì •

        ëª¨ë“  ì‹¤í—˜ì—ì„œ ë™ì¼í•˜ê²Œ ìœ ì§€ë  ë³€ìˆ˜ë“¤
        """
        self.control_variables.update(kwargs)

    def design_ab_test(self, experiment_name: str,
                      condition_a: Dict[str, Any],
                      condition_b: Dict[str, Any],
                      condition_a_name: str = "Control",
                      condition_b_name: str = "Treatment") -> str:
        """
        A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„

        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
            condition_a: ì¡°ê±´ A íŒŒë¼ë¯¸í„°
            condition_b: ì¡°ê±´ B íŒŒë¼ë¯¸í„°
            condition_a_name: ì¡°ê±´ A ì´ë¦„
            condition_b_name: ì¡°ê±´ B ì´ë¦„

        Returns:
            ì‹¤í—˜ ID
        """
        conditions = [
            ExperimentalCondition(
                name=condition_a_name,
                description=f"A/B Test - {condition_a_name} condition",
                parameters=condition_a,
                control_variables=self.control_variables.copy()
            ),
            ExperimentalCondition(
                name=condition_b_name,
                description=f"A/B Test - {condition_b_name} condition",
                parameters=condition_b,
                control_variables=self.control_variables.copy()
            )
        ]

        self.experiments[experiment_name] = conditions
        return experiment_name

    def design_factorial_experiment(self, experiment_name: str,
                                  factors: Dict[str, List[Any]]) -> str:
        """
        ìš”ì¸ ì„¤ê³„ ì‹¤í—˜ (Factorial Design)

        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
            factors: ìš”ì¸ë³„ ìˆ˜ì¤€ ë”•ì…”ë„ˆë¦¬

        Returns:
            ì‹¤í—˜ ID
        """
        # ëª¨ë“  ìš”ì¸ ì¡°í•© ìƒì„±
        factor_names = list(factors.keys())
        factor_levels = list(factors.values())

        conditions = []
        for i, combination in enumerate(itertools.product(*factor_levels)):
            condition_params = dict(zip(factor_names, combination))
            condition_name = f"Condition_{i+1}"

            # ì¡°ê±´ ì„¤ëª… ìƒì„±
            desc_parts = [f"{name}={value}" for name, value in condition_params.items()]
            description = f"Factorial - {', '.join(desc_parts)}"

            conditions.append(ExperimentalCondition(
                name=condition_name,
                description=description,
                parameters=condition_params,
                control_variables=self.control_variables.copy()
            ))

        self.experiments[experiment_name] = conditions
        return experiment_name

    def design_ablation_study(self, experiment_name: str,
                            full_model_params: Dict[str, Any],
                            components_to_ablate: List[str]) -> str:
        """
        ì†Œê±° ì—°êµ¬ (Ablation Study) ì„¤ê³„

        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
            full_model_params: ì „ì²´ ëª¨ë¸ íŒŒë¼ë¯¸í„°
            components_to_ablate: ì œê±°í•  êµ¬ì„±ìš”ì†Œ ëª©ë¡

        Returns:
            ì‹¤í—˜ ID
        """
        conditions = []

        # ì „ì²´ ëª¨ë¸ (ë² ì´ìŠ¤ë¼ì¸)
        conditions.append(ExperimentalCondition(
            name="Full_Model",
            description="Complete model with all components",
            parameters=full_model_params.copy(),
            control_variables=self.control_variables.copy()
        ))

        # ê° êµ¬ì„±ìš”ì†Œë¥¼ í•˜ë‚˜ì”© ì œê±°í•œ ëª¨ë¸ë“¤
        for component in components_to_ablate:
            ablated_params = full_model_params.copy()

            # êµ¬ì„±ìš”ì†Œ ì œê±° ë°©ë²• (íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ë‹¤ë¦„)
            if component in ablated_params:
                if isinstance(ablated_params[component], bool):
                    ablated_params[component] = False
                elif isinstance(ablated_params[component], (int, float)):
                    ablated_params[component] = 0
                elif isinstance(ablated_params[component], list):
                    ablated_params[component] = []
                else:
                    del ablated_params[component]

            conditions.append(ExperimentalCondition(
                name=f"Without_{component}",
                description=f"Model without {component}",
                parameters=ablated_params,
                control_variables=self.control_variables.copy()
            ))

        self.experiments[experiment_name] = conditions
        return experiment_name

    def design_sensitivity_analysis(self, experiment_name: str,
                                  base_params: Dict[str, Any],
                                  sensitivity_params: Dict[str, List[Any]]) -> str:
        """
        ë¯¼ê°ë„ ë¶„ì„ ì„¤ê³„

        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
            base_params: ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            sensitivity_params: ë¯¼ê°ë„ ë¶„ì„í•  íŒŒë¼ë¯¸í„°ì™€ ê·¸ ê°’ë“¤

        Returns:
            ì‹¤í—˜ ID
        """
        conditions = []

        # ë² ì´ìŠ¤ë¼ì¸ ì¡°ê±´
        conditions.append(ExperimentalCondition(
            name="Baseline",
            description="Baseline configuration",
            parameters=base_params.copy(),
            control_variables=self.control_variables.copy()
        ))

        # ê° íŒŒë¼ë¯¸í„°ë³„ ë¯¼ê°ë„ í…ŒìŠ¤íŠ¸
        for param_name, param_values in sensitivity_params.items():
            for i, param_value in enumerate(param_values):
                test_params = base_params.copy()
                test_params[param_name] = param_value

                condition_name = f"{param_name}_{i+1}"
                description = f"Sensitivity test - {param_name}={param_value}"

                conditions.append(ExperimentalCondition(
                    name=condition_name,
                    description=description,
                    parameters=test_params,
                    control_variables=self.control_variables.copy()
                ))

        self.experiments[experiment_name] = conditions
        return experiment_name

    def run_experiment(self, experiment_name: str,
                      model_factory: Callable,
                      X_train: np.ndarray, y_train: np.ndarray,
                      X_test: np.ndarray, y_test: np.ndarray,
                      custom_metrics: Optional[Dict[str, Callable]] = None,
                      n_runs: int = 1) -> Dict[str, List[ExperimentResult]]:
        """
        ì‹¤í—˜ ì‹¤í–‰

        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
            model_factory: ëª¨ë¸ ìƒì„± í•¨ìˆ˜
            X_train, y_train: í›ˆë ¨ ë°ì´í„°
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            custom_metrics: ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­
            n_runs: ì‹¤í–‰ íšŸìˆ˜ (ê²°ê³¼ ì•ˆì •ì„± í™•ë³´)

        Returns:
            ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if experiment_name not in self.experiments:
            raise ValueError(f"ì‹¤í—˜ '{experiment_name}'ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        conditions = self.experiments[experiment_name]
        results = []

        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì •ì˜
        default_metrics = {
            'mae': lambda y_true, y_pred: mean_absolute_error(y_true, y_pred),
            'mse': lambda y_true, y_pred: mean_squared_error(y_true, y_pred),
            'rmse': lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),
            'r2': lambda y_true, y_pred: r2_score(y_true, y_pred)
        }

        # ì‚¬ìš©ì ë©”íŠ¸ë¦­ ì¶”ê°€
        if custom_metrics:
            default_metrics.update(custom_metrics)

        for condition in conditions:
            print(f"ì‹¤í–‰ ì¤‘: {condition.name}")

            condition_results = []

            for run in range(n_runs):
                try:
                    import time
                    start_time = time.time()

                    # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
                    model = model_factory(**condition.parameters, **condition.control_variables)
                    model.fit(X_train, y_train)

                    # ì˜ˆì¸¡
                    y_pred = model.predict(X_test)

                    # ë©”íŠ¸ë¦­ ê³„ì‚°
                    metrics = {}
                    for metric_name, metric_func in default_metrics.items():
                        try:
                            metrics[metric_name] = metric_func(y_test, y_pred)
                        except Exception as e:
                            print(f"ë©”íŠ¸ë¦­ {metric_name} ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
                            metrics[metric_name] = np.nan

                    # ë°©í–¥ ì •í™•ë„ ì¶”ê°€
                    metrics['direction_accuracy'] = self._calculate_direction_accuracy(y_test, y_pred)

                    execution_time = time.time() - start_time

                    # ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
                    additional_info = {
                        'run_number': run + 1,
                        'model_type': type(model).__name__,
                        'parameters_used': condition.parameters.copy()
                    }

                    # íŠ¹ì§• ì¤‘ìš”ë„ (ê°€ëŠ¥í•œ ê²½ìš°)
                    if hasattr(model, 'feature_importances_'):
                        additional_info['feature_importances'] = model.feature_importances_.tolist()
                    elif hasattr(model, 'coef_'):
                        additional_info['coefficients'] = model.coef_.tolist()

                    result = ExperimentResult(
                        condition_name=condition.name,
                        metrics=metrics,
                        sample_size=len(y_test),
                        execution_time=execution_time,
                        additional_info=additional_info
                    )

                    condition_results.append(result)

                except Exception as e:
                    print(f"ì¡°ê±´ {condition.name}, ì‹¤í–‰ {run+1} ì˜¤ë¥˜: {str(e)}")
                    # ì˜¤ë¥˜ ê²°ê³¼ë„ ê¸°ë¡
                    error_result = ExperimentResult(
                        condition_name=condition.name,
                        metrics={'error': str(e)},
                        sample_size=0,
                        execution_time=0,
                        additional_info={'error': True, 'run_number': run + 1}
                    )
                    condition_results.append(error_result)

            results.extend(condition_results)

        self.results[experiment_name] = results
        return {experiment_name: results}

    def _calculate_direction_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """ë°©í–¥ ì •í™•ë„ ê³„ì‚°"""
        if len(y_true) <= 1:
            return 0.5

        true_directions = np.sign(y_true)
        pred_directions = np.sign(y_pred)

        # 0ì¸ ê²½ìš° ì²˜ë¦¬
        true_directions[true_directions == 0] = 1
        pred_directions[pred_directions == 0] = 1

        accuracy = np.mean(true_directions == pred_directions)
        return accuracy * 100

    def analyze_experiment_results(self, experiment_name: str,
                                 primary_metric: str = 'mae') -> Dict[str, Any]:
        """
        ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
            primary_metric: ì£¼ìš” í‰ê°€ ì§€í‘œ

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if experiment_name not in self.results:
            raise ValueError(f"ì‹¤í—˜ '{experiment_name}' ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        results = self.results[experiment_name]

        # ì¡°ê±´ë³„ ê²°ê³¼ ì§‘ê³„
        condition_summaries = {}
        for result in results:
            if 'error' in result.metrics:
                continue

            condition = result.condition_name
            if condition not in condition_summaries:
                condition_summaries[condition] = {
                    'metrics': {},
                    'execution_times': [],
                    'n_runs': 0
                }

            # ë©”íŠ¸ë¦­ë³„ ê°’ ìˆ˜ì§‘
            for metric, value in result.metrics.items():
                if metric not in condition_summaries[condition]['metrics']:
                    condition_summaries[condition]['metrics'][metric] = []
                condition_summaries[condition]['metrics'][metric].append(value)

            condition_summaries[condition]['execution_times'].append(result.execution_time)
            condition_summaries[condition]['n_runs'] += 1

        # í†µê³„ ê³„ì‚°
        analysis = {
            'experiment_name': experiment_name,
            'primary_metric': primary_metric,
            'conditions': {},
            'comparison': {}
        }

        for condition, summary in condition_summaries.items():
            condition_stats = {
                'n_runs': summary['n_runs'],
                'avg_execution_time': np.mean(summary['execution_times']),
                'metrics': {}
            }

            for metric, values in summary['metrics'].items():
                condition_stats['metrics'][metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values, ddof=1) if len(values) > 1 else 0,
                    'min': np.min(values),
                    'max': np.max(values),
                    'median': np.median(values)
                }

            analysis['conditions'][condition] = condition_stats

        # ì¡°ê±´ ê°„ ë¹„êµ (ì£¼ìš” ë©”íŠ¸ë¦­ ê¸°ì¤€)
        if len(condition_summaries) >= 2 and primary_metric in list(condition_summaries.values())[0]['metrics']:
            condition_means = {}
            for condition, summary in condition_summaries.items():
                condition_means[condition] = np.mean(summary['metrics'][primary_metric])

            # ìµœê³  ì„±ëŠ¥ ì¡°ê±´
            if primary_metric in ['mae', 'mse', 'rmse']:  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                best_condition = min(condition_means.items(), key=lambda x: x[1])
                worst_condition = max(condition_means.items(), key=lambda x: x[1])
            else:  # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
                best_condition = max(condition_means.items(), key=lambda x: x[1])
                worst_condition = min(condition_means.items(), key=lambda x: x[1])

            analysis['comparison'] = {
                'best_condition': best_condition[0],
                'best_value': best_condition[1],
                'worst_condition': worst_condition[0],
                'worst_value': worst_condition[1],
                'improvement': abs(best_condition[1] - worst_condition[1]),
                'relative_improvement': abs(best_condition[1] - worst_condition[1]) / abs(worst_condition[1]) * 100 if worst_condition[1] != 0 else 0
            }

        return analysis

    def generate_experiment_report(self, experiment_name: str,
                                 primary_metric: str = 'mae') -> str:
        """
        ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„±

        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
            primary_metric: ì£¼ìš” í‰ê°€ ì§€í‘œ

        Returns:
            ì‹¤í—˜ ë³´ê³ ì„œ ë¬¸ìì—´
        """
        if experiment_name not in self.results:
            return f"ì‹¤í—˜ '{experiment_name}' ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        analysis = self.analyze_experiment_results(experiment_name, primary_metric)

        report = [f"ğŸ§ª ì‹¤í—˜ ë³´ê³ ì„œ: {experiment_name}", "=" * 60, ""]

        # ì‹¤í—˜ ê°œìš”
        experiment_type = "Unknown"
        if experiment_name in self.experiments:
            n_conditions = len(self.experiments[experiment_name])
            experiment_type = f"{n_conditions}ê°œ ì¡°ê±´ ë¹„êµ"

        report.append(f"ğŸ“‹ ì‹¤í—˜ ê°œìš”:")
        report.append(f"   ì‹¤í—˜ ìœ í˜•: {experiment_type}")
        report.append(f"   ì£¼ìš” ì§€í‘œ: {primary_metric.upper()}")
        report.append(f"   ë¶„ì„ ì¡°ê±´: {len(analysis['conditions'])}ê°œ")
        report.append("")

        # ì¡°ê±´ë³„ ì„±ëŠ¥ í‘œ
        report.append("ğŸ“Š ì¡°ê±´ë³„ ì„±ëŠ¥ ê²°ê³¼:")
        report.append("-" * 50)

        headers = ["Condition", f"{primary_metric.upper()}", "Std", "Dir Acc(%)", "Exec Time(s)"]
        rows = []

        for condition, stats in analysis['conditions'].items():
            metrics = stats['metrics']
            primary_mean = metrics.get(primary_metric, {}).get('mean', 0)
            primary_std = metrics.get(primary_metric, {}).get('std', 0)
            dir_acc = metrics.get('direction_accuracy', {}).get('mean', 0)

            rows.append([
                condition,
                f"{primary_mean:.6f}",
                f"{primary_std:.6f}",
                f"{dir_acc:.1f}",
                f"{stats['avg_execution_time']:.3f}"
            ])

        # í‘œ í¬ë§·íŒ…
        col_widths = [max(len(str(row[i])) for row in [headers] + rows) for i in range(len(headers))]

        header_line = " | ".join(f"{headers[i]:<{col_widths[i]}}" for i in range(len(headers)))
        report.append(header_line)
        report.append("-" * len(header_line))

        for row in rows:
            data_line = " | ".join(f"{row[i]:<{col_widths[i]}}" for i in range(len(row)))
            report.append(data_line)

        report.append("")

        # ë¹„êµ ë¶„ì„
        if 'best_condition' in analysis['comparison']:
            comp = analysis['comparison']
            report.append("ğŸ† ì„±ëŠ¥ ë¹„êµ:")
            report.append(f"   ìµœê³  ì„±ëŠ¥: {comp['best_condition']} ({comp['best_value']:.6f})")
            report.append(f"   ìµœì € ì„±ëŠ¥: {comp['worst_condition']} ({comp['worst_value']:.6f})")
            report.append(f"   ì„±ëŠ¥ ì°¨ì´: {comp['improvement']:.6f} ({comp['relative_improvement']:.1f}%)")
            report.append("")

        # í†µê³„ì  ìœ ì˜ì„± (ì¡°ê±´ì´ 2ê°œì¸ ê²½ìš°)
        if len(analysis['conditions']) == 2:
            conditions = list(analysis['conditions'].keys())
            cond1, cond2 = conditions[0], conditions[1]

            # ì›ë³¸ ë°ì´í„°ì—ì„œ ê°’ ì¶”ì¶œ
            cond1_values = []
            cond2_values = []

            for result in self.results[experiment_name]:
                if result.condition_name == cond1 and primary_metric in result.metrics:
                    cond1_values.append(result.metrics[primary_metric])
                elif result.condition_name == cond2 and primary_metric in result.metrics:
                    cond2_values.append(result.metrics[primary_metric])

            if len(cond1_values) > 1 and len(cond2_values) > 1:
                from scipy import stats
                t_stat, p_value = stats.ttest_ind(cond1_values, cond2_values)

                report.append("ğŸ“ˆ í†µê³„ì  ìœ ì˜ì„±:")
                report.append(f"   t-í†µê³„ëŸ‰: {t_stat:.4f}")
                report.append(f"   p-value: {p_value:.6f}")

                if p_value < 0.05:
                    report.append("   âœ… í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ (p < 0.05)")
                else:
                    report.append("   âŒ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŒ (p â‰¥ 0.05)")

                report.append("")

        # ê¶Œê³ ì‚¬í•­
        report.append("ğŸ’¡ ê¶Œê³ ì‚¬í•­:")
        if 'best_condition' in analysis['comparison']:
            best_cond = analysis['comparison']['best_condition']
            improvement = analysis['comparison']['relative_improvement']

            if improvement > 5:  # 5% ì´ìƒ ê°œì„ 
                report.append(f"   âœ… '{best_cond}' ì¡°ê±´ ì‚¬ìš© ê¶Œì¥ ({improvement:.1f}% ì„±ëŠ¥ í–¥ìƒ)")
            else:
                report.append(f"   âš ï¸  ì¡°ê±´ ê°„ ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸ ({improvement:.1f}%)")
                report.append("   ë³µì¡ì„±ê³¼ ì„±ëŠ¥ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ ê³ ë ¤ í•„ìš”")
        else:
            report.append("   ëª¨ë“  ì¡°ê±´ì—ì„œ ìœ ì‚¬í•œ ì„±ëŠ¥. ë‹¨ìˆœí•œ ëª¨ë¸ ìš°ì„  ê³ ë ¤")

        # ì‹¤í—˜ í•œê³„ì 
        report.append("")
        report.append("âš ï¸  ì‹¤í—˜ í•œê³„ì :")
        total_runs = sum(stats['n_runs'] for stats in analysis['conditions'].values())
        report.append(f"   - ì´ ì‹¤í–‰ íšŸìˆ˜: {total_runs}íšŒ (ë” ë§ì€ ë°˜ë³µìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´ ê°€ëŠ¥)")
        report.append("   - ë‹¨ì¼ ë°ì´í„°ì…‹ ê¸°ì¤€ (ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ê²€ì¦ í•„ìš”)")
        report.append("   - íŠ¹ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ë‚´ ê²°ê³¼")

        return "\n".join(report)

def main():
    """í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸ§ª í†µì œ ì‹¤í—˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
    np.random.seed(42)

    n_samples = 300
    n_features = 10
    X = np.random.randn(n_samples, n_features)
    y = 0.1 * X[:, 0] + 0.05 * X[:, 1] + np.random.normal(0, 0.1, n_samples)

    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    split_idx = int(0.8 * n_samples)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # ì‹¤í—˜ í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™”
    framework = ControlledExperimentFramework(random_state=42)

    # í†µì œ ë³€ìˆ˜ ì„¤ì •
    framework.set_control_variables(
        random_state=42,
        max_iter=1000
    )

    # ê°„ë‹¨í•œ ëª¨ë¸ íŒ©í† ë¦¬ (Linear Regression ë³€í˜•)
    def model_factory(alpha=1.0, fit_intercept=True, **kwargs):
        from sklearn.linear_model import Ridge
        return Ridge(alpha=alpha, fit_intercept=fit_intercept, **kwargs)

    # 1. A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„ ë° ì‹¤í–‰
    print("1. A/B í…ŒìŠ¤íŠ¸: ì •ê·œí™” ìœ ë¬´")
    ab_experiment = framework.design_ab_test(
        experiment_name="regularization_test",
        condition_a={'alpha': 0.0},  # ì •ê·œí™” ì—†ìŒ
        condition_b={'alpha': 1.0},  # ì •ê·œí™” ìˆìŒ
        condition_a_name="No_Regularization",
        condition_b_name="With_Regularization"
    )

    ab_results = framework.run_experiment(
        ab_experiment, model_factory,
        X_train, y_train, X_test, y_test,
        n_runs=3
    )

    ab_report = framework.generate_experiment_report(ab_experiment)
    print(ab_report)
    print("\n" + "="*60 + "\n")

    # 2. ìš”ì¸ ì„¤ê³„ ì‹¤í—˜
    print("2. ìš”ì¸ ì„¤ê³„: ì •ê·œí™” ê°•ë„ Ã— ì ˆí¸ ìœ ë¬´")
    factorial_experiment = framework.design_factorial_experiment(
        experiment_name="factorial_test",
        factors={
            'alpha': [0.1, 1.0, 10.0],
            'fit_intercept': [True, False]
        }
    )

    factorial_results = framework.run_experiment(
        factorial_experiment, model_factory,
        X_train, y_train, X_test, y_test,
        n_runs=2
    )

    factorial_report = framework.generate_experiment_report(factorial_experiment)
    print(factorial_report)
    print("\n" + "="*60 + "\n")

    # 3. ë¯¼ê°ë„ ë¶„ì„
    print("3. ë¯¼ê°ë„ ë¶„ì„: ì •ê·œí™” ê°•ë„")
    sensitivity_experiment = framework.design_sensitivity_analysis(
        experiment_name="alpha_sensitivity",
        base_params={'alpha': 1.0, 'fit_intercept': True},
        sensitivity_params={
            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
        }
    )

    sensitivity_results = framework.run_experiment(
        sensitivity_experiment, model_factory,
        X_train, y_train, X_test, y_test,
        n_runs=2
    )

    sensitivity_report = framework.generate_experiment_report(sensitivity_experiment)
    print(sensitivity_report)

    # 4. ì „ì²´ ì‹¤í—˜ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ¯ ì‹¤í—˜ ì„¸ì…˜ ìš”ì•½")
    print("="*60)
    print(f"ì‹¤í–‰ëœ ì‹¤í—˜ ìˆ˜: {len(framework.experiments)}")
    print(f"ì´ ì‹¤í—˜ ì¡°ê±´ ìˆ˜: {sum(len(conditions) for conditions in framework.experiments.values())}")
    print(f"ì´ ì‹¤í–‰ íšŸìˆ˜: {sum(len(results) for results in framework.results.values())}")

    # ê° ì‹¤í—˜ì˜ ìµœê³  ì„±ëŠ¥ ì¡°ê±´
    print("\nìµœê³  ì„±ëŠ¥ ì¡°ê±´ ìš”ì•½:")
    for exp_name in framework.experiments.keys():
        analysis = framework.analyze_experiment_results(exp_name)
        if 'best_condition' in analysis['comparison']:
            best = analysis['comparison']['best_condition']
            best_value = analysis['comparison']['best_value']
            print(f"  {exp_name}: {best} (MAE: {best_value:.6f})")

if __name__ == "__main__":
    main()