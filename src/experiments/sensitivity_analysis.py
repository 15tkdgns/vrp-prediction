#!/usr/bin/env python3
"""
ğŸ“Š ë¯¼ê°ë„ ë¶„ì„ ì‹œìŠ¤í…œ
í•™ìˆ  ë…¼ë¬¸ì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ëª¨ë¸ ë¯¼ê°ë„ ë¶„ì„

ì£¼ìš” ê¸°ëŠ¥:
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„
- ë°ì´í„° í¬ê¸° ë¯¼ê°ë„ ë¶„ì„
- íŠ¹ì§• ì¤‘ìš”ë„ ë¯¼ê°ë„ ë¶„ì„
- ë…¸ì´ì¦ˆ ì˜í–¥ ë¶„ì„
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from dataclasses import dataclass
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.base import BaseEstimator, clone
from sklearn.model_selection import validation_curve, learning_curve
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

@dataclass
class SensitivityResult:
    """ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ í´ë˜ìŠ¤"""
    parameter_name: str
    parameter_values: List[Any]
    mean_scores: List[float]
    std_scores: List[float]
    optimal_value: Any
    optimal_score: float
    sensitivity_score: float
    interpretation: str

class SensitivityAnalyzer:
    """
    í¬ê´„ì  ë¯¼ê°ë„ ë¶„ì„ ë„êµ¬

    ëª¨ë¸ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ë¯¼ê°ë„ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„
    """

    def __init__(self, random_state=42):
        """
        ì´ˆê¸°í™”

        Args:
            random_state: ëœë¤ ì‹œë“œ
        """
        self.random_state = random_state
        np.random.seed(random_state)

        self.results: Dict[str, SensitivityResult] = {}

    def hyperparameter_sensitivity(self, model: BaseEstimator,
                                 parameter_name: str,
                                 parameter_range: List[Any],
                                 X_train: np.ndarray, y_train: np.ndarray,
                                 X_test: np.ndarray, y_test: np.ndarray,
                                 scoring: str = 'neg_mean_absolute_error',
                                 cv: int = 5) -> SensitivityResult:
        """
        í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„

        Args:
            model: ë¶„ì„í•  ëª¨ë¸
            parameter_name: íŒŒë¼ë¯¸í„° ì´ë¦„
            parameter_range: íŒŒë¼ë¯¸í„° ê°’ ë²”ìœ„
            X_train, y_train: í›ˆë ¨ ë°ì´í„°
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            scoring: í‰ê°€ ì§€í‘œ
            cv: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜

        Returns:
            ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼
        """
        print(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„: {parameter_name}")

        # Validation curve ê³„ì‚°
        train_scores, test_scores = validation_curve(
            model, X_train, y_train,
            param_name=parameter_name,
            param_range=parameter_range,
            cv=cv,
            scoring=scoring,
            n_jobs=-1
        )

        # ì ìˆ˜ ë³€í™˜ (ìŒìˆ˜ ì ìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œ)
        if scoring.startswith('neg_'):
            test_scores = -test_scores

        # í†µê³„ ê³„ì‚°
        mean_scores = np.mean(test_scores, axis=1)
        std_scores = np.std(test_scores, axis=1)

        # ìµœì ê°’ ì°¾ê¸°
        if scoring in ['neg_mean_absolute_error', 'neg_mean_squared_error']:
            optimal_idx = np.argmax(mean_scores)  # ìŒìˆ˜ ì ìˆ˜ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (ì›ë˜ëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        else:
            optimal_idx = np.argmax(mean_scores)  # ì¼ë°˜ì ìœ¼ë¡œ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ

        optimal_value = parameter_range[optimal_idx]
        optimal_score = mean_scores[optimal_idx]

        # ë¯¼ê°ë„ ì ìˆ˜ ê³„ì‚° (ë³€ë™ ê³„ìˆ˜)
        sensitivity_score = np.std(mean_scores) / np.mean(mean_scores) if np.mean(mean_scores) != 0 else 0

        # í•´ì„
        if sensitivity_score < 0.05:
            interpretation = f"{parameter_name}ì— ëŒ€í•´ ë§¤ìš° ì•ˆì •ì  (ë¯¼ê°ë„ ë‚®ìŒ)"
        elif sensitivity_score < 0.15:
            interpretation = f"{parameter_name}ì— ëŒ€í•´ ì ë‹¹íˆ ë¯¼ê°"
        else:
            interpretation = f"{parameter_name}ì— ëŒ€í•´ ë§¤ìš° ë¯¼ê° (ì‹ ì¤‘í•œ íŠœë‹ í•„ìš”)"

        result = SensitivityResult(
            parameter_name=parameter_name,
            parameter_values=parameter_range,
            mean_scores=mean_scores.tolist(),
            std_scores=std_scores.tolist(),
            optimal_value=optimal_value,
            optimal_score=optimal_score,
            sensitivity_score=sensitivity_score,
            interpretation=interpretation
        )

        self.results[parameter_name] = result
        return result

    def data_size_sensitivity(self, model: BaseEstimator,
                            X_train: np.ndarray, y_train: np.ndarray,
                            X_test: np.ndarray, y_test: np.ndarray,
                            train_sizes: Optional[List[float]] = None,
                            cv: int = 5) -> SensitivityResult:
        """
        ë°ì´í„° í¬ê¸° ë¯¼ê°ë„ ë¶„ì„ (Learning Curve)

        Args:
            model: ë¶„ì„í•  ëª¨ë¸
            X_train, y_train: í›ˆë ¨ ë°ì´í„°
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            train_sizes: í›ˆë ¨ í¬ê¸° ë¹„ìœ¨ë“¤
            cv: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜

        Returns:
            ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼
        """
        print("ë°ì´í„° í¬ê¸° ë¯¼ê°ë„ ë¶„ì„ (Learning Curve)")

        if train_sizes is None:
            train_sizes = np.linspace(0.1, 1.0, 10)

        # Learning curve ê³„ì‚°
        train_sizes_abs, train_scores, test_scores = learning_curve(
            model, X_train, y_train,
            train_sizes=train_sizes,
            cv=cv,
            scoring='neg_mean_absolute_error',
            n_jobs=-1
        )

        # ì ìˆ˜ ë³€í™˜
        test_scores = -test_scores
        mean_scores = np.mean(test_scores, axis=1)
        std_scores = np.std(test_scores, axis=1)

        # ìµœì  ë°ì´í„° í¬ê¸° (ì„±ëŠ¥ í¬í™”ì )
        # ì„±ëŠ¥ì´ ë” ì´ìƒ í¬ê²Œ ê°œì„ ë˜ì§€ ì•ŠëŠ” ì§€ì  ì°¾ê¸°
        score_improvements = np.diff(mean_scores)
        plateau_threshold = 0.001  # ê°œì„ ì´ 0.1% ë¯¸ë§Œì¸ ì§€ì 

        plateau_idx = None
        for i, improvement in enumerate(score_improvements):
            if improvement < plateau_threshold:
                plateau_idx = i + 1
                break

        if plateau_idx is None:
            plateau_idx = len(mean_scores) - 1

        optimal_size = train_sizes[plateau_idx]
        optimal_score = mean_scores[plateau_idx]

        # ë¯¼ê°ë„ ê³„ì‚° (ë°ì´í„° í¬ê¸° ì¦ê°€ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”ìœ¨)
        sensitivity_score = abs(score_improvements).mean() if len(score_improvements) > 0 else 0

        # í•´ì„
        if sensitivity_score > 0.01:
            interpretation = "ë°ì´í„° í¬ê¸°ì— ë§¤ìš° ë¯¼ê°. ë” ë§ì€ ë°ì´í„°ë¡œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥"
        elif sensitivity_score > 0.005:
            interpretation = "ë°ì´í„° í¬ê¸°ì— ì ë‹¹íˆ ë¯¼ê°. ì¶”ê°€ ë°ì´í„° ê³ ë ¤"
        else:
            interpretation = "ë°ì´í„° í¬ê¸°ì— ë‘”ê°. í˜„ì¬ ë°ì´í„°ë¡œ ì¶©ë¶„"

        result = SensitivityResult(
            parameter_name="training_data_size",
            parameter_values=train_sizes.tolist(),
            mean_scores=mean_scores.tolist(),
            std_scores=std_scores.tolist(),
            optimal_value=optimal_size,
            optimal_score=optimal_score,
            sensitivity_score=sensitivity_score,
            interpretation=interpretation
        )

        self.results["data_size"] = result
        return result

    def feature_importance_sensitivity(self, model: BaseEstimator,
                                     X_train: np.ndarray, y_train: np.ndarray,
                                     X_test: np.ndarray, y_test: np.ndarray,
                                     feature_names: Optional[List[str]] = None,
                                     n_permutations: int = 10) -> Dict[str, SensitivityResult]:
        """
        íŠ¹ì§• ì¤‘ìš”ë„ ë¯¼ê°ë„ ë¶„ì„ (Permutation Importance)

        Args:
            model: ë¶„ì„í•  ëª¨ë¸
            X_train, y_train: í›ˆë ¨ ë°ì´í„°
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            feature_names: íŠ¹ì§• ì´ë¦„ë“¤
            n_permutations: ìˆœì—´ ë°˜ë³µ íšŸìˆ˜

        Returns:
            íŠ¹ì§•ë³„ ë¯¼ê°ë„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("íŠ¹ì§• ì¤‘ìš”ë„ ë¯¼ê°ë„ ë¶„ì„")

        if feature_names is None:
            feature_names = [f"feature_{i}" for i in range(X_train.shape[1])]

        # ëª¨ë¸ í›ˆë ¨
        model_fitted = clone(model)
        model_fitted.fit(X_train, y_train)

        # ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥
        baseline_pred = model_fitted.predict(X_test)
        baseline_score = mean_absolute_error(y_test, baseline_pred)

        feature_results = {}

        for i, feature_name in enumerate(feature_names):
            print(f"  ë¶„ì„ ì¤‘: {feature_name}")

            # í•´ë‹¹ íŠ¹ì§• ìˆœì—´í•˜ì—¬ ì„±ëŠ¥ ë³€í™” ì¸¡ì •
            permutation_scores = []

            for _ in range(n_permutations):
                X_test_permuted = X_test.copy()
                np.random.shuffle(X_test_permuted[:, i])  # ië²ˆì§¸ íŠ¹ì§• ìˆœì—´

                permuted_pred = model_fitted.predict(X_test_permuted)
                permuted_score = mean_absolute_error(y_test, permuted_pred)
                permutation_scores.append(permuted_score)

            # ì„±ëŠ¥ ì €í•˜ ê³„ì‚°
            importance_scores = np.array(permutation_scores) - baseline_score
            mean_importance = np.mean(importance_scores)
            std_importance = np.std(importance_scores)

            # ë¯¼ê°ë„ ê³„ì‚° (ì„±ëŠ¥ ì €í•˜ ì •ë„)
            sensitivity = mean_importance / baseline_score if baseline_score != 0 else 0

            # í•´ì„
            if sensitivity > 0.1:  # 10% ì´ìƒ ì„±ëŠ¥ ì €í•˜
                interpretation = f"{feature_name}ì€ ë§¤ìš° ì¤‘ìš”í•œ íŠ¹ì§• (ì œê±° ì‹œ {sensitivity*100:.1f}% ì„±ëŠ¥ ì €í•˜)"
            elif sensitivity > 0.05:  # 5% ì´ìƒ ì„±ëŠ¥ ì €í•˜
                interpretation = f"{feature_name}ì€ ì¤‘ìš”í•œ íŠ¹ì§• (ì œê±° ì‹œ {sensitivity*100:.1f}% ì„±ëŠ¥ ì €í•˜)"
            elif sensitivity > 0.01:  # 1% ì´ìƒ ì„±ëŠ¥ ì €í•˜
                interpretation = f"{feature_name}ì€ ì•½ê°„ ì¤‘ìš”í•œ íŠ¹ì§•"
            else:
                interpretation = f"{feature_name}ì€ ì¤‘ìš”ë„ê°€ ë‚®ì€ íŠ¹ì§•"

            result = SensitivityResult(
                parameter_name=feature_name,
                parameter_values=[0, 1],  # íŠ¹ì§• ìœ ë¬´
                mean_scores=[baseline_score, np.mean(permutation_scores)],
                std_scores=[0, std_importance],
                optimal_value=1,  # íŠ¹ì§• ì‚¬ìš©
                optimal_score=baseline_score,
                sensitivity_score=sensitivity,
                interpretation=interpretation
            )

            feature_results[feature_name] = result

        return feature_results

    def noise_sensitivity(self, model: BaseEstimator,
                         X_train: np.ndarray, y_train: np.ndarray,
                         X_test: np.ndarray, y_test: np.ndarray,
                         noise_levels: Optional[List[float]] = None,
                         n_trials: int = 5) -> SensitivityResult:
        """
        ë…¸ì´ì¦ˆ ë¯¼ê°ë„ ë¶„ì„

        Args:
            model: ë¶„ì„í•  ëª¨ë¸
            X_train, y_train: í›ˆë ¨ ë°ì´í„°
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            noise_levels: ë…¸ì´ì¦ˆ ìˆ˜ì¤€ë“¤ (í‘œì¤€í¸ì°¨ ë¹„ìœ¨)
            n_trials: ê° ë…¸ì´ì¦ˆ ìˆ˜ì¤€ë³„ ì‹œí–‰ íšŸìˆ˜

        Returns:
            ë…¸ì´ì¦ˆ ë¯¼ê°ë„ ê²°ê³¼
        """
        print("ë…¸ì´ì¦ˆ ë¯¼ê°ë„ ë¶„ì„")

        if noise_levels is None:
            noise_levels = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]

        mean_scores = []
        std_scores = []

        original_std = np.std(X_train)

        for noise_level in noise_levels:
            trial_scores = []

            for trial in range(n_trials):
                # ë…¸ì´ì¦ˆ ì¶”ê°€
                noise = np.random.normal(0, noise_level * original_std, X_train.shape)
                X_train_noisy = X_train + noise

                # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ë„ ë™ì¼í•œ ë…¸ì´ì¦ˆ ì¶”ê°€
                noise_test = np.random.normal(0, noise_level * original_std, X_test.shape)
                X_test_noisy = X_test + noise_test

                # ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
                model_trial = clone(model)
                model_trial.fit(X_train_noisy, y_train)

                pred = model_trial.predict(X_test_noisy)
                score = mean_absolute_error(y_test, pred)
                trial_scores.append(score)

            mean_scores.append(np.mean(trial_scores))
            std_scores.append(np.std(trial_scores))

        # ìµœì  ë…¸ì´ì¦ˆ ìˆ˜ì¤€ (ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥)
        optimal_idx = np.argmin(mean_scores)
        optimal_noise = noise_levels[optimal_idx]
        optimal_score = mean_scores[optimal_idx]

        # ë¯¼ê°ë„ ê³„ì‚° (ë…¸ì´ì¦ˆ ì¦ê°€ì— ë”°ë¥¸ ì„±ëŠ¥ ì €í•˜ìœ¨)
        clean_score = mean_scores[0]  # ë…¸ì´ì¦ˆ 0ì¸ ê²½ìš°
        max_degradation = max(mean_scores) - clean_score
        sensitivity_score = max_degradation / clean_score if clean_score != 0 else 0

        # í•´ì„
        if sensitivity_score > 0.2:
            interpretation = "ë…¸ì´ì¦ˆì— ë§¤ìš° ë¯¼ê°. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì¤‘ìš”"
        elif sensitivity_score > 0.1:
            interpretation = "ë…¸ì´ì¦ˆì— ì ë‹¹íˆ ë¯¼ê°. ì¼ë¶€ ë…¸ì´ì¦ˆ ì œê±° ê¶Œì¥"
        else:
            interpretation = "ë…¸ì´ì¦ˆì— ê°•ê±´í•¨. í˜„ì¬ ë°ì´í„° í’ˆì§ˆë¡œ ì¶©ë¶„"

        result = SensitivityResult(
            parameter_name="noise_level",
            parameter_values=noise_levels,
            mean_scores=mean_scores,
            std_scores=std_scores,
            optimal_value=optimal_noise,
            optimal_score=optimal_score,
            sensitivity_score=sensitivity_score,
            interpretation=interpretation
        )

        self.results["noise"] = result
        return result

    def cross_parameter_sensitivity(self, model: BaseEstimator,
                                  parameter_grid: Dict[str, List[Any]],
                                  X_train: np.ndarray, y_train: np.ndarray,
                                  X_test: np.ndarray, y_test: np.ndarray,
                                  cv: int = 3) -> Dict[str, Any]:
        """
        ë‹¤ì¤‘ íŒŒë¼ë¯¸í„° êµí˜¸ì‘ìš© ë¶„ì„

        Args:
            model: ë¶„ì„í•  ëª¨ë¸
            parameter_grid: íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
            X_train, y_train: í›ˆë ¨ ë°ì´í„°
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            cv: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜

        Returns:
            êµí˜¸ì‘ìš© ë¶„ì„ ê²°ê³¼
        """
        print("ë‹¤ì¤‘ íŒŒë¼ë¯¸í„° êµí˜¸ì‘ìš© ë¶„ì„")

        from sklearn.model_selection import GridSearchCV

        # ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ëª¨ë“  ì¡°í•© í‰ê°€
        grid_search = GridSearchCV(
            model, parameter_grid,
            cv=cv,
            scoring='neg_mean_absolute_error',
            n_jobs=-1
        )

        grid_search.fit(X_train, y_train)

        # ê²°ê³¼ ë¶„ì„
        results_df = pd.DataFrame(grid_search.cv_results_)

        # ê° íŒŒë¼ë¯¸í„°ë³„ ì£¼íš¨ê³¼ ê³„ì‚°
        main_effects = {}
        for param_name in parameter_grid.keys():
            param_scores = results_df.groupby(f'param_{param_name}')['mean_test_score'].mean()
            main_effects[param_name] = {
                'values': param_scores.index.tolist(),
                'scores': (-param_scores).tolist(),  # ìŒìˆ˜ ì ìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œ ë³€í™˜
                'effect_size': np.std(-param_scores)
            }

        # êµí˜¸ì‘ìš© ê°•ë„ ê³„ì‚°
        interaction_strength = {}
        param_names = list(parameter_grid.keys())

        if len(param_names) >= 2:
            for i in range(len(param_names)):
                for j in range(i+1, len(param_names)):
                    param1, param2 = param_names[i], param_names[j]

                    # 2-way êµí˜¸ì‘ìš© ë¶„ì„
                    interaction_scores = results_df.groupby([f'param_{param1}', f'param_{param2}'])['mean_test_score'].mean()

                    # êµí˜¸ì‘ìš© íš¨ê³¼ í¬ê¸° (ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´)
                    predicted_scores = []
                    actual_scores = []

                    for (val1, val2), actual_score in interaction_scores.items():
                        # ì£¼íš¨ê³¼ë¡œ ì˜ˆì¸¡í•œ ì ìˆ˜
                        main1_score = main_effects[param1]['scores'][main_effects[param1]['values'].index(val1)]
                        main2_score = main_effects[param2]['scores'][main_effects[param2]['values'].index(val2)]
                        predicted_score = (main1_score + main2_score) / 2

                        predicted_scores.append(predicted_score)
                        actual_scores.append(-actual_score)

                    interaction_effect = np.std(np.array(actual_scores) - np.array(predicted_scores))
                    interaction_strength[f"{param1}_x_{param2}"] = interaction_effect

        return {
            'best_params': grid_search.best_params_,
            'best_score': -grid_search.best_score_,
            'main_effects': main_effects,
            'interaction_effects': interaction_strength,
            'cv_results': results_df
        }

    def generate_sensitivity_report(self) -> str:
        """ë¯¼ê°ë„ ë¶„ì„ ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        if not self.results:
            return "ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        report = ["ğŸ“Š ë¯¼ê°ë„ ë¶„ì„ ì¢…í•© ë³´ê³ ì„œ", "=" * 60, ""]

        # ê° ë¶„ì„ë³„ ê²°ê³¼ ìš”ì•½
        for analysis_name, result in self.results.items():
            report.append(f"## {analysis_name.upper()} ë¯¼ê°ë„ ë¶„ì„")
            report.append("-" * 40)

            if analysis_name == "data_size":
                report.append(f"**ìµœì  ë°ì´í„° í¬ê¸°**: {result.optimal_value:.1%}")
                report.append(f"**í•´ë‹¹ ì„±ëŠ¥**: {result.optimal_score:.6f}")
            else:
                report.append(f"**ìµœì  ê°’**: {result.optimal_value}")
                report.append(f"**ìµœì  ì„±ëŠ¥**: {result.optimal_score:.6f}")

            report.append(f"**ë¯¼ê°ë„ ì ìˆ˜**: {result.sensitivity_score:.4f}")
            report.append(f"**í•´ì„**: {result.interpretation}")
            report.append("")

            # ì„±ëŠ¥ ë³€í™” ìš”ì•½
            if len(result.mean_scores) > 1:
                min_score = min(result.mean_scores)
                max_score = max(result.mean_scores)
                score_range = max_score - min_score
                relative_range = score_range / min_score * 100 if min_score != 0 else 0

                report.append(f"**ì„±ëŠ¥ ë³€í™” ë²”ìœ„**: {score_range:.6f} ({relative_range:.1f}%)")
                report.append("")

        # ì „ì²´ ê²°ë¡ 
        report.append("## ğŸ¯ ì¢…í•© ê²°ë¡ ")
        report.append("-" * 30)

        # ê°€ì¥ ë¯¼ê°í•œ ìš”ì†Œ ì°¾ê¸°
        if self.results:
            most_sensitive = max(self.results.items(), key=lambda x: x[1].sensitivity_score)
            least_sensitive = min(self.results.items(), key=lambda x: x[1].sensitivity_score)

            report.append(f"**ê°€ì¥ ë¯¼ê°í•œ ìš”ì†Œ**: {most_sensitive[0]} (ë¯¼ê°ë„: {most_sensitive[1].sensitivity_score:.4f})")
            report.append(f"**ê°€ì¥ ì•ˆì •ì  ìš”ì†Œ**: {least_sensitive[0]} (ë¯¼ê°ë„: {least_sensitive[1].sensitivity_score:.4f})")
            report.append("")

        # ê¶Œê³ ì‚¬í•­
        report.append("## ğŸ’¡ ê¶Œê³ ì‚¬í•­")
        report.append("-" * 20)

        high_sensitivity_items = [name for name, result in self.results.items() if result.sensitivity_score > 0.1]
        low_sensitivity_items = [name for name, result in self.results.items() if result.sensitivity_score < 0.05]

        if high_sensitivity_items:
            report.append("**ì‹ ì¤‘í•œ íŠœë‹ í•„ìš”**:")
            for item in high_sensitivity_items:
                report.append(f"  - {item}: {self.results[item].interpretation}")
            report.append("")

        if low_sensitivity_items:
            report.append("**ì•ˆì •ì  ìš”ì†Œë“¤**:")
            for item in low_sensitivity_items:
                report.append(f"  - {item}: ê¸°ë³¸ê°’ ì‚¬ìš© ê°€ëŠ¥")
            report.append("")

        report.append("**ì¼ë°˜ì  ê¶Œê³ **:")
        report.append("- ë¯¼ê°ë„ê°€ ë†’ì€ íŒŒë¼ë¯¸í„°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ íŠœë‹")
        report.append("- ì•ˆì •ì ì¸ íŒŒë¼ë¯¸í„°ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©ìœ¼ë¡œ ë³µì¡ì„± ê°ì†Œ")
        report.append("- ë°ì´í„° í¬ê¸°ì™€ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì„ ê³ ë ¤í•œ ê²¬ê³ í•œ ëª¨ë¸ êµ¬ì¶•")

        return "\n".join(report)

def main():
    """í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸ“Š ë¯¼ê°ë„ ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
    np.random.seed(42)

    n_samples = 500
    n_features = 8
    X = np.random.randn(n_samples, n_features)
    # ì•½ê°„ì˜ ë¹„ì„ í˜•ì„±ê³¼ ë…¸ì´ì¦ˆ ì¶”ê°€
    y = (0.3 * X[:, 0] + 0.2 * X[:, 1] ** 2 + 0.1 * X[:, 2] * X[:, 3] +
         np.random.normal(0, 0.1, n_samples))

    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    split_idx = int(0.8 * n_samples)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # ë¯¼ê°ë„ ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = SensitivityAnalyzer(random_state=42)

    # í…ŒìŠ¤íŠ¸ ëª¨ë¸ (Random Forest)
    from sklearn.ensemble import RandomForestRegressor
    model = RandomForestRegressor(random_state=42)

    # 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„
    print("1. n_estimators ë¯¼ê°ë„ ë¶„ì„")
    estimators_result = analyzer.hyperparameter_sensitivity(
        model=model,
        parameter_name='n_estimators',
        parameter_range=[10, 25, 50, 100, 200, 300],
        X_train=X_train, y_train=y_train,
        X_test=X_test, y_test=y_test,
        cv=3
    )

    print(f"ìµœì ê°’: {estimators_result.optimal_value}")
    print(f"ë¯¼ê°ë„: {estimators_result.sensitivity_score:.4f}")
    print(f"í•´ì„: {estimators_result.interpretation}")
    print()

    # 2. max_depth ë¯¼ê°ë„ ë¶„ì„
    print("2. max_depth ë¯¼ê°ë„ ë¶„ì„")
    depth_result = analyzer.hyperparameter_sensitivity(
        model=model,
        parameter_name='max_depth',
        parameter_range=[3, 5, 7, 10, 15, 20, None],
        X_train=X_train, y_train=y_train,
        X_test=X_test, y_test=y_test,
        cv=3
    )

    print(f"ìµœì ê°’: {depth_result.optimal_value}")
    print(f"ë¯¼ê°ë„: {depth_result.sensitivity_score:.4f}")
    print()

    # 3. ë°ì´í„° í¬ê¸° ë¯¼ê°ë„ ë¶„ì„
    print("3. ë°ì´í„° í¬ê¸° ë¯¼ê°ë„ ë¶„ì„")
    data_size_result = analyzer.data_size_sensitivity(
        model=model,
        X_train=X_train, y_train=y_train,
        X_test=X_test, y_test=y_test,
        cv=3
    )

    print(f"ìµœì  ë°ì´í„° í¬ê¸°: {data_size_result.optimal_value:.1%}")
    print(f"í•´ì„: {data_size_result.interpretation}")
    print()

    # 4. íŠ¹ì§• ì¤‘ìš”ë„ ë¯¼ê°ë„ ë¶„ì„ (ì¼ë¶€ íŠ¹ì§•ë§Œ)
    print("4. íŠ¹ì§• ì¤‘ìš”ë„ ë¯¼ê°ë„ ë¶„ì„")
    feature_names = [f"feature_{i}" for i in range(min(4, n_features))]  # ì²˜ìŒ 4ê°œ íŠ¹ì§•ë§Œ
    X_train_subset = X_train[:, :len(feature_names)]
    X_test_subset = X_test[:, :len(feature_names)]

    feature_results = analyzer.feature_importance_sensitivity(
        model=RandomForestRegressor(n_estimators=50, random_state=42),
        X_train=X_train_subset, y_train=y_train,
        X_test=X_test_subset, y_test=y_test,
        feature_names=feature_names,
        n_permutations=5
    )

    for feature, result in feature_results.items():
        print(f"{feature}: {result.interpretation}")

    print()

    # 5. ë…¸ì´ì¦ˆ ë¯¼ê°ë„ ë¶„ì„
    print("5. ë…¸ì´ì¦ˆ ë¯¼ê°ë„ ë¶„ì„")
    noise_result = analyzer.noise_sensitivity(
        model=RandomForestRegressor(n_estimators=50, random_state=42),
        X_train=X_train, y_train=y_train,
        X_test=X_test, y_test=y_test,
        noise_levels=[0.0, 0.05, 0.1, 0.2],
        n_trials=3
    )

    print(f"ìµœì  ë…¸ì´ì¦ˆ ìˆ˜ì¤€: {noise_result.optimal_value}")
    print(f"í•´ì„: {noise_result.interpretation}")
    print()

    # 6. ë‹¤ì¤‘ íŒŒë¼ë¯¸í„° êµí˜¸ì‘ìš© ë¶„ì„
    print("6. ë‹¤ì¤‘ íŒŒë¼ë¯¸í„° êµí˜¸ì‘ìš© ë¶„ì„")
    param_grid = {
        'n_estimators': [50, 100],
        'max_depth': [5, 10, None]
    }

    interaction_results = analyzer.cross_parameter_sensitivity(
        model=model,
        parameter_grid=param_grid,
        X_train=X_train, y_train=y_train,
        X_test=X_test, y_test=y_test,
        cv=3
    )

    print(f"ìµœì  íŒŒë¼ë¯¸í„°: {interaction_results['best_params']}")
    print(f"ìµœê³  ì„±ëŠ¥: {interaction_results['best_score']:.6f}")

    if interaction_results['interaction_effects']:
        for interaction, effect in interaction_results['interaction_effects'].items():
            print(f"êµí˜¸ì‘ìš© {interaction}: {effect:.6f}")

    print()

    # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
    print("ğŸ“‹ ë¯¼ê°ë„ ë¶„ì„ ì¢…í•© ë³´ê³ ì„œ:")
    print("=" * 60)
    report = analyzer.generate_sensitivity_report()
    print(report)

if __name__ == "__main__":
    main()