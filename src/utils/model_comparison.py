#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ê°ì •ë¶„ì„ í¬í•¨/ë¯¸í¬í•¨ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
import logging
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelComparison:
    """ë‰´ìŠ¤ ê°ì •ë¶„ì„ í¬í•¨/ë¯¸í¬í•¨ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, data_file: str = "data/raw/integrated_spy_news_data.csv"):
        self.data_file = data_file
        self.data = None
        self.baseline_models = {}
        self.enhanced_models = {}
        self.results = {}
        
        # íŠ¹ì„± ê·¸ë£¹ ì •ì˜
        self.technical_features = None
        self.news_features = None
        self.all_features = None
        
    def load_data(self) -> pd.DataFrame:
        """í†µí•© ë°ì´í„° ë¡œë“œ"""
        if not os.path.exists(self.data_file):
            raise FileNotFoundError(f"Data file not found: {self.data_file}")
            
        logger.info(f"Loading integrated data from {self.data_file}")
        self.data = pd.read_csv(self.data_file)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
        self.data['date'] = pd.to_datetime(self.data['date'])
        
        # íŠ¹ì„± ê·¸ë£¹ ì •ì˜
        self._define_feature_groups()
        
        logger.info(f"Loaded {len(self.data)} records with {len(self.data.columns)} features")
        return self.data
    
    def _define_feature_groups(self):
        """íŠ¹ì„± ê·¸ë£¹ ì •ì˜"""
        # ì œì™¸í•  ì»¬ëŸ¼ë“¤
        exclude_cols = ['date', 'target', 'next_day_return', 'open', 'high', 'low', 'close', 'volume']
        
        # ë‰´ìŠ¤ ê´€ë ¨ íŠ¹ì„±
        news_keywords = ['sentiment', 'news', 'article', 'impact']
        self.news_features = [col for col in self.data.columns 
                             if any(keyword in col.lower() for keyword in news_keywords) 
                             and col not in exclude_cols]
        
        # ê¸°ìˆ ì  ì§€í‘œ íŠ¹ì„± (ë‰´ìŠ¤ íŠ¹ì„±ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€)
        self.technical_features = [col for col in self.data.columns 
                                  if col not in exclude_cols 
                                  and col not in self.news_features]
        
        # ì „ì²´ íŠ¹ì„±
        self.all_features = self.technical_features + self.news_features
        
        logger.info(f"Technical features ({len(self.technical_features)}): {self.technical_features}")
        logger.info(f"News features ({len(self.news_features)}): {self.news_features}")
    
    def prepare_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """ë°ì´í„° ì¤€ë¹„ ë° ë¶„í• """
        if self.data is None:
            self.load_data()
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        self.data = self.data.fillna(self.data.mean(numeric_only=True))
        
        X = self.data[self.all_features]
        y = self.data['target']
        
        # ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ ì‹œê°„ ìˆœì„œ ìœ ì§€í•˜ì—¬ ë¶„í• 
        # ì²˜ìŒ 70%ëŠ” í›ˆë ¨ìš©, ë‚˜ë¨¸ì§€ 30%ëŠ” í…ŒìŠ¤íŠ¸ìš©
        split_idx = int(len(self.data) * 0.7)
        
        X_train = X.iloc[:split_idx]
        X_test = X.iloc[split_idx:]
        y_train = y.iloc[:split_idx]
        y_test = y.iloc[split_idx:]
        
        logger.info(f"Train set: {len(X_train)} samples")
        logger.info(f"Test set: {len(X_test)} samples")
        logger.info(f"Train period: {self.data['date'].iloc[0]} ~ {self.data['date'].iloc[split_idx-1]}")
        logger.info(f"Test period: {self.data['date'].iloc[split_idx]} ~ {self.data['date'].iloc[-1]}")
        
        return X_train, X_test, y_train, y_test
    
    def train_models(self):
        """ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ê³¼ í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨"""
        logger.info("ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        X_train, X_test, y_train, y_test = self.prepare_data()
        
        # ëª¨ë¸ ì •ì˜
        models = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),
            'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=6),
            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)
        }
        
        # 1. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ (ê¸°ìˆ ì  ì§€í‘œë§Œ)
        logger.info("ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í›ˆë ¨ (ê¸°ìˆ ì  ì§€í‘œë§Œ)")
        X_train_technical = X_train[self.technical_features]
        X_test_technical = X_test[self.technical_features]
        
        # ìŠ¤ì¼€ì¼ë§
        scaler_baseline = StandardScaler()
        X_train_technical_scaled = scaler_baseline.fit_transform(X_train_technical)
        X_test_technical_scaled = scaler_baseline.transform(X_test_technical)
        
        for name, model in models.items():
            logger.info(f"Training baseline {name}...")
            model_copy = type(model)(**model.get_params())
            model_copy.fit(X_train_technical_scaled, y_train)
            self.baseline_models[name] = {
                'model': model_copy,
                'scaler': scaler_baseline,
                'features': self.technical_features
            }
        
        # 2. í–¥ìƒëœ ëª¨ë¸ (ê¸°ìˆ ì  ì§€í‘œ + ë‰´ìŠ¤)
        logger.info("ğŸ“ˆ í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨ (ê¸°ìˆ ì  ì§€í‘œ + ë‰´ìŠ¤)")
        
        # ìŠ¤ì¼€ì¼ë§
        scaler_enhanced = StandardScaler()
        X_train_scaled = scaler_enhanced.fit_transform(X_train)
        X_test_scaled = scaler_enhanced.transform(X_test)
        
        for name, model in models.items():
            logger.info(f"Training enhanced {name}...")
            model_copy = type(model)(**model.get_params())
            model_copy.fit(X_train_scaled, y_train)
            self.enhanced_models[name] = {
                'model': model_copy,
                'scaler': scaler_enhanced,
                'features': self.all_features
            }
        
        logger.info("âœ… ëª¨ë“  ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
    
    def evaluate_models(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        logger.info("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        
        X_train, X_test, y_train, y_test = self.prepare_data()
        results = {}
        
        # ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í‰ê°€
        logger.info("ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í‰ê°€...")
        X_test_technical = X_test[self.technical_features]
        
        for name, model_info in self.baseline_models.items():
            model = model_info['model']
            scaler = model_info['scaler']
            
            X_test_scaled = scaler.transform(X_test_technical)
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
            
            results[f'Baseline_{name}'] = self._calculate_metrics(y_test, y_pred, y_pred_proba)
        
        # í–¥ìƒëœ ëª¨ë¸ í‰ê°€
        logger.info("í–¥ìƒëœ ëª¨ë¸ í‰ê°€...")
        for name, model_info in self.enhanced_models.items():
            model = model_info['model']
            scaler = model_info['scaler']
            
            X_test_scaled = scaler.transform(X_test)
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
            
            results[f'Enhanced_{name}'] = self._calculate_metrics(y_test, y_pred, y_pred_proba)
        
        self.results = results
        return results
    
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, y_pred_proba: np.ndarray) -> Dict[str, float]:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1_score': f1_score(y_true, y_pred, zero_division=0),
            'auc': roc_auc_score(y_true, y_pred_proba) if len(np.unique(y_true)) > 1 else 0.5
        }
    
    def analyze_feature_importance(self) -> Dict[str, Any]:
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        logger.info("ğŸ” íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
        
        importance_analysis = {}
        
        # RandomForest ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        for model_type in ['Baseline', 'Enhanced']:
            rf_model = self.baseline_models['RandomForest'] if model_type == 'Baseline' else self.enhanced_models['RandomForest']
            features = rf_model['features']
            importances = rf_model['model'].feature_importances_
            
            # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            feature_importance = pd.DataFrame({
                'feature': features,
                'importance': importances
            }).sort_values('importance', ascending=False)
            
            importance_analysis[model_type] = feature_importance
        
        # ë‰´ìŠ¤ ê´€ë ¨ íŠ¹ì„±ì˜ ê¸°ì—¬ë„ ë¶„ì„
        enhanced_importance = importance_analysis['Enhanced']
        news_importance = enhanced_importance[enhanced_importance['feature'].isin(self.news_features)]
        technical_importance = enhanced_importance[enhanced_importance['feature'].isin(self.technical_features)]
        
        news_total_importance = news_importance['importance'].sum()
        technical_total_importance = technical_importance['importance'].sum()
        
        importance_analysis['news_contribution'] = {
            'news_total_importance': news_total_importance,
            'technical_total_importance': technical_total_importance,
            'news_percentage': news_total_importance / (news_total_importance + technical_total_importance) * 100,
            'top_news_features': news_importance.head(5).to_dict('records'),
            'top_technical_features': technical_importance.head(5).to_dict('records')
        }
        
        return importance_analysis
    
    def create_performance_comparison_chart(self, save_path: str = "results/model_comparison_chart.png"):
        """ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        logger.info("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ê²°ê³¼ ë°ì´í„° ì¤€ë¹„
        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']
        
        baseline_data = []
        enhanced_data = []
        model_names = []
        
        for key, values in self.results.items():
            if key.startswith('Baseline_'):
                model_name = key.replace('Baseline_', '')
                model_names.append(model_name)
                baseline_data.append([values[metric] for metric in metrics])
            elif key.startswith('Enhanced_'):
                model_name = key.replace('Enhanced_', '')
                enhanced_data.append([values[metric] for metric in metrics])
        
        # ì°¨íŠ¸ ìƒì„±
        fig, axes = plt.subplots(1, 3, figsize=(20, 6))
        fig.suptitle('ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ: ê¸°ìˆ ì  ì§€í‘œ vs ê¸°ìˆ ì  ì§€í‘œ + ë‰´ìŠ¤', fontsize=16)
        
        # 1. ì „ì²´ ì„±ëŠ¥ ë¹„êµ
        x = np.arange(len(metrics))
        width = 0.35
        
        baseline_means = np.mean(baseline_data, axis=0)
        enhanced_means = np.mean(enhanced_data, axis=0)
        
        axes[0].bar(x - width/2, baseline_means, width, label='ê¸°ìˆ ì  ì§€í‘œë§Œ', alpha=0.8)
        axes[0].bar(x + width/2, enhanced_means, width, label='ê¸°ìˆ ì  ì§€í‘œ + ë‰´ìŠ¤', alpha=0.8)
        axes[0].set_xlabel('ì„±ëŠ¥ ì§€í‘œ')
        axes[0].set_ylabel('ì ìˆ˜')
        axes[0].set_title('í‰ê·  ì„±ëŠ¥ ë¹„êµ')
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(metrics)
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # 2. ëª¨ë¸ë³„ ì •í™•ë„ ë¹„êµ
        model_baseline_acc = [self.results[f'Baseline_{name}']['accuracy'] for name in model_names]
        model_enhanced_acc = [self.results[f'Enhanced_{name}']['accuracy'] for name in model_names]
        
        x_models = np.arange(len(model_names))
        axes[1].bar(x_models - width/2, model_baseline_acc, width, label='ê¸°ìˆ ì  ì§€í‘œë§Œ', alpha=0.8)
        axes[1].bar(x_models + width/2, model_enhanced_acc, width, label='ê¸°ìˆ ì  ì§€í‘œ + ë‰´ìŠ¤', alpha=0.8)
        axes[1].set_xlabel('ëª¨ë¸')
        axes[1].set_ylabel('ì •í™•ë„')
        axes[1].set_title('ëª¨ë¸ë³„ ì •í™•ë„ ë¹„êµ')
        axes[1].set_xticks(x_models)
        axes[1].set_xticklabels(model_names)
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # 3. ì„±ëŠ¥ í–¥ìƒë¥ 
        improvements = [(enhanced_means[i] - baseline_means[i]) / baseline_means[i] * 100 
                       for i in range(len(metrics))]
        
        colors = ['green' if imp > 0 else 'red' for imp in improvements]
        axes[2].bar(metrics, improvements, color=colors, alpha=0.7)
        axes[2].set_xlabel('ì„±ëŠ¥ ì§€í‘œ')
        axes[2].set_ylabel('í–¥ìƒë¥  (%)')
        axes[2].set_title('ë‰´ìŠ¤ ê°ì •ë¶„ì„ ì¶”ê°€ì‹œ ì„±ëŠ¥ í–¥ìƒë¥ ')
        axes[2].axhline(y=0, color='black', linestyle='-', alpha=0.3)
        axes[2].grid(True, alpha=0.3)
        
        # ìˆ˜ì¹˜ í‘œì‹œ
        for i, imp in enumerate(improvements):
            axes[2].text(i, imp + (1 if imp > 0 else -1), f'{imp:.1f}%', 
                        ha='center', va='bottom' if imp > 0 else 'top')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ì°¨íŠ¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        return save_path
    
    def save_results(self, output_file: str = "results/model_comparison_results.json"):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        importance_analysis = self.analyze_feature_importance()
        
        # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ë³€í™˜ í•¨ìˆ˜
        def convert_to_json_serializable(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.bool_, np.int64, np.float64)):
                return obj.item()
            elif isinstance(obj, dict):
                return {key: convert_to_json_serializable(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_json_serializable(item) for item in obj]
            elif hasattr(obj, 'to_dict'):
                return convert_to_json_serializable(obj.to_dict())
            else:
                return obj
        
        # ì „ì²´ ê²°ê³¼ êµ¬ì„±
        full_results = {
            'timestamp': datetime.now().isoformat(),
            'data_info': {
                'total_samples': int(len(self.data)),
                'total_features': len(self.all_features),
                'technical_features': len(self.technical_features),
                'news_features': len(self.news_features),
                'target_distribution': {str(k): int(v) for k, v in self.data['target'].value_counts().to_dict().items()}
            },
            'performance_results': convert_to_json_serializable(self.results),
            'feature_importance': {
                'news_contribution_percentage': float(importance_analysis['news_contribution']['news_percentage']),
                'top_news_features': convert_to_json_serializable(importance_analysis['news_contribution']['top_news_features']),
                'top_technical_features': convert_to_json_serializable(importance_analysis['news_contribution']['top_technical_features'])
            },
            'summary': convert_to_json_serializable(self._generate_summary())
        }
        
        # JSONìœ¼ë¡œ ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(full_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        return output_file
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        baseline_avg = {}
        enhanced_avg = {}
        
        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']
        
        for metric in metrics:
            baseline_values = [results[metric] for key, results in self.results.items() if key.startswith('Baseline_')]
            enhanced_values = [results[metric] for key, results in self.results.items() if key.startswith('Enhanced_')]
            
            baseline_avg[metric] = np.mean(baseline_values)
            enhanced_avg[metric] = np.mean(enhanced_values)
        
        # í–¥ìƒë¥  ê³„ì‚°
        improvements = {}
        for metric in metrics:
            improvements[metric] = (enhanced_avg[metric] - baseline_avg[metric]) / baseline_avg[metric] * 100
        
        return {
            'baseline_average': baseline_avg,
            'enhanced_average': enhanced_avg,
            'improvements': improvements,
            'best_baseline_model': max([(k, v['accuracy']) for k, v in self.results.items() if k.startswith('Baseline_')], key=lambda x: x[1])[0],
            'best_enhanced_model': max([(k, v['accuracy']) for k, v in self.results.items() if k.startswith('Enhanced_')], key=lambda x: x[1])[0],
            'news_helps': enhanced_avg['accuracy'] > baseline_avg['accuracy']
        }
    
    def print_results(self):
        """ê²°ê³¼ ì¶œë ¥"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        logger.info("="*80)
        
        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥
        logger.info("\nğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:")
        for model_name, metrics in self.results.items():
            logger.info(f"\n{model_name}:")
            for metric, value in metrics.items():
                logger.info(f"  {metric}: {value:.4f}")
        
        # ìš”ì•½
        summary = self._generate_summary()
        logger.info(f"\nğŸ” ì¢…í•© ë¶„ì„:")
        logger.info(f"ìµœê³  ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸: {summary['best_baseline_model']} ({self.results[summary['best_baseline_model']]['accuracy']:.4f})")
        logger.info(f"ìµœê³  í–¥ìƒ ëª¨ë¸: {summary['best_enhanced_model']} ({self.results[summary['best_enhanced_model']]['accuracy']:.4f})")
        logger.info(f"ë‰´ìŠ¤ ê°ì •ë¶„ì„ íš¨ê³¼: {'âœ… ë„ì›€ë¨' if summary['news_helps'] else 'âŒ ë„ì›€ì•ˆë¨'}")
        
        logger.info(f"\nğŸ“ˆ í‰ê·  ì„±ëŠ¥ í–¥ìƒë¥ :")
        for metric, improvement in summary['improvements'].items():
            status = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰"
            logger.info(f"  {metric}: {status} {improvement:+.2f}%")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ë‰´ìŠ¤ ê°ì •ë¶„ì„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    
    try:
        # ë¹„êµê¸° ì´ˆê¸°í™”
        comparator = ModelComparison()
        
        # ë°ì´í„° ë¡œë“œ
        comparator.load_data()
        
        # ëª¨ë¸ í›ˆë ¨
        comparator.train_models()
        
        # ì„±ëŠ¥ í‰ê°€
        comparator.evaluate_models()
        
        # ê²°ê³¼ ì¶œë ¥
        comparator.print_results()
        
        # ì°¨íŠ¸ ìƒì„±
        comparator.create_performance_comparison_chart()
        
        # ê²°ê³¼ ì €ì¥
        comparator.save_results()
        
        logger.info("âœ… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹¤íŒ¨: {str(e)}")
        raise


if __name__ == "__main__":
    main()