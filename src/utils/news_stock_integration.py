#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ê°ì • ë¶„ì„ê³¼ ì£¼ì‹ ë°ì´í„°ë¥¼ í†µí•©í•˜ëŠ” ëª¨ë“ˆ
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NewsStockIntegration:
    """ë‰´ìŠ¤ ê°ì • ë¶„ì„ê³¼ SPY ì£¼ì‹ ë°ì´í„°ë¥¼ í†µí•©í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: str = "data/raw"):
        self.data_dir = data_dir
        self.spy_data = None
        self.news_data = None
        self.integrated_data = None
        
    def load_spy_data(self) -> pd.DataFrame:
        """SPY ì£¼ì‹ ë°ì´í„° ë¡œë“œ"""
        spy_file = os.path.join(self.data_dir, "spy_2025_h1.json")
        
        if not os.path.exists(spy_file):
            raise FileNotFoundError(f"SPY data file not found: {spy_file}")
            
        logger.info(f"Loading SPY data from {spy_file}")
        
        with open(spy_file, 'r') as f:
            spy_json = json.load(f)
            
        # DataFrameìœ¼ë¡œ ë³€í™˜
        spy_df = pd.DataFrame(spy_json['data'])
        spy_df['date'] = pd.to_datetime(spy_df['date'])
        spy_df = spy_df.sort_values('date')
        
        # ê¸°ë³¸ì ì¸ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        spy_df = self._calculate_technical_indicators(spy_df)
        
        logger.info(f"Loaded {len(spy_df)} SPY records")
        self.spy_data = spy_df
        return spy_df
    
    def load_news_data(self) -> pd.DataFrame:
        """ë‰´ìŠ¤ ê°ì • ë¶„ì„ ë°ì´í„° ë¡œë“œ"""
        # ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ
        timeseries_file = os.path.join(self.data_dir, "sentiment_timeseries.json")
        summary_file = os.path.join(self.data_dir, "news_sentiment_summary.json")
        
        if not os.path.exists(timeseries_file):
            raise FileNotFoundError(f"News timeseries file not found: {timeseries_file}")
        if not os.path.exists(summary_file):
            raise FileNotFoundError(f"News summary file not found: {summary_file}")
            
        logger.info(f"Loading news data from {timeseries_file} and {summary_file}")
        
        # ì‹œê³„ì—´ ë°ì´í„°
        with open(timeseries_file, 'r') as f:
            timeseries = json.load(f)
            
        # ìš”ì•½ ë°ì´í„°
        with open(summary_file, 'r') as f:
            summary = json.load(f)
            
        # ì‹œê³„ì—´ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        news_df = pd.DataFrame({
            'date': pd.to_datetime(timeseries['dates']),
            'sentiment_score': timeseries['sentiment_scores'],
            'article_count': timeseries['article_counts']
        })
        
        # ìš”ì•½ ë°ì´í„°ì—ì„œ ì¶”ê°€ íŠ¹ì§• ì¶”ì¶œ
        news_df = self._add_news_features(news_df, summary)
        
        logger.info(f"Loaded {len(news_df)} news records")
        self.news_data = news_df
        return news_df
    
    def _calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        df = df.copy()
        
        # ê°€ê²© ë³€í™”
        df['price_change'] = df['close'].pct_change()
        df['price_change_abs'] = df['price_change'].abs()
        
        # ë³¼ë¥¨ ë³€í™”
        df['volume_change'] = df['volume'].pct_change()
        df['volume_ma_5'] = df['volume'].rolling(5).mean()
        df['unusual_volume'] = (df['volume'] > df['volume_ma_5'] * 1.5).astype(int)
        
        # ì´ë™í‰ê· 
        for period in [5, 10, 20]:
            df[f'ma_{period}'] = df['close'].rolling(period).mean()
            df[f'price_to_ma{period}'] = df['close'] / df[f'ma_{period}']
        
        # RSI ê³„ì‚°
        df['rsi'] = self._calculate_rsi(df['close'])
        
        # ë³¼ë¦°ì € ë°´ë“œ
        df = self._calculate_bollinger_bands(df)
        
        # MACD
        df = self._calculate_macd(df)
        
        # ë³€ë™ì„±
        df['volatility_20'] = df['price_change'].rolling(20).std()
        df['volatility_5'] = df['price_change'].rolling(5).std()
        
        # ê°€ê²© ìŠ¤íŒŒì´í¬ ê°ì§€
        df['price_spike'] = (df['price_change_abs'] > df['volatility_20'] * 2).astype(int)
        
        return df
    
    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """RSI ê³„ì‚°"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_bollinger_bands(self, df: pd.DataFrame, window: int = 20, std_dev: int = 2) -> pd.DataFrame:
        """ë³¼ë¦°ì € ë°´ë“œ ê³„ì‚°"""
        df = df.copy()
        df[f'bb_middle'] = df['close'].rolling(window).mean()
        bb_std = df['close'].rolling(window).std()
        df[f'bb_upper'] = df[f'bb_middle'] + (bb_std * std_dev)
        df[f'bb_lower'] = df[f'bb_middle'] - (bb_std * std_dev)
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        return df
    
    def _calculate_macd(self, df: pd.DataFrame, fast: int = 12, slow: int = 26, signal: int = 9) -> pd.DataFrame:
        """MACD ê³„ì‚°"""
        df = df.copy()
        ema_fast = df['close'].ewm(span=fast).mean()
        ema_slow = df['close'].ewm(span=slow).mean()
        df['macd'] = ema_fast - ema_slow
        df['macd_signal'] = df['macd'].ewm(span=signal).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        return df
    
    def _add_news_features(self, news_df: pd.DataFrame, summary_data: Dict) -> pd.DataFrame:
        """ë‰´ìŠ¤ ë°ì´í„°ì— ì¶”ê°€ íŠ¹ì§• ìƒì„±"""
        news_df = news_df.copy()
        
        # ë‰´ìŠ¤ ê°ì • ì ìˆ˜ íŠ¹ì§•
        news_df['sentiment_positive'] = (news_df['sentiment_score'] > 0.1).astype(int)
        news_df['sentiment_negative'] = (news_df['sentiment_score'] < -0.1).astype(int)
        news_df['sentiment_neutral'] = ((news_df['sentiment_score'] >= -0.1) & (news_df['sentiment_score'] <= 0.1)).astype(int)
        news_df['sentiment_abs'] = news_df['sentiment_score'].abs()
        
        # ë‰´ìŠ¤ ê°œìˆ˜ íŠ¹ì§•
        news_df['high_news_volume'] = (news_df['article_count'] > news_df['article_count'].quantile(0.75)).astype(int)
        news_df['low_news_volume'] = (news_df['article_count'] < news_df['article_count'].quantile(0.25)).astype(int)
        
        # ì´ë™í‰ê·  ê¸°ë°˜ íŠ¹ì§•
        for window in [3, 7]:
            news_df[f'sentiment_ma_{window}'] = news_df['sentiment_score'].rolling(window).mean()
            news_df[f'news_count_ma_{window}'] = news_df['article_count'].rolling(window).mean()
        
        # ë³€í™”ìœ¨ íŠ¹ì§•
        news_df['sentiment_change'] = news_df['sentiment_score'].diff()
        news_df['news_count_change'] = news_df['article_count'].pct_change()
        
        # ë³€ë™ì„± íŠ¹ì§•
        news_df['sentiment_volatility'] = news_df['sentiment_score'].rolling(7).std()
        
        # ê³ ì„íŒ©íŠ¸ ë‰´ìŠ¤ íŠ¹ì§• (ìš”ì•½ ë°ì´í„°ì—ì„œ)
        if 'recent_news' in summary_data:
            high_impact_dates = self._extract_high_impact_dates(summary_data['recent_news'])
            news_df['high_impact_news'] = news_df['date'].dt.date.isin(high_impact_dates).astype(int)
        
        return news_df
    
    def _extract_high_impact_dates(self, recent_news: List[Dict]) -> set:
        """ê³ ì„íŒ©íŠ¸ ë‰´ìŠ¤ê°€ ìˆëŠ” ë‚ ì§œë“¤ ì¶”ì¶œ"""
        high_impact_dates = set()
        
        for news in recent_news:
            impact_score = news.get('market_impact', 0) * (news.get('spy_relevance', 0.8) if 'spy_relevance' in news else 0.8)
            if impact_score > 0.6:  # ì„ê³„ê°’
                news_date = datetime.strptime(news['date'], '%Y-%m-%d').date()
                high_impact_dates.add(news_date)
                
        return high_impact_dates
    
    def integrate_data(self) -> pd.DataFrame:
        """ë‰´ìŠ¤ ë°ì´í„°ì™€ ì£¼ì‹ ë°ì´í„°ë¥¼ í†µí•©"""
        if self.spy_data is None:
            self.load_spy_data()
        if self.news_data is None:
            self.load_news_data()
            
        logger.info("Integrating SPY and news data...")
        
        # ë‚ ì§œë³„ë¡œ ì¡°ì¸
        integrated = pd.merge(
            self.spy_data,
            self.news_data,
            on='date',
            how='left'  # SPY ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸
        )
        
        # ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
        news_columns = self.news_data.columns.drop('date')
        for col in news_columns:
            if col in ['sentiment_score', 'sentiment_change', 'sentiment_ma_3', 'sentiment_ma_7', 'sentiment_volatility']:
                integrated[col] = integrated[col].fillna(0)  # ê°ì • ì ìˆ˜ ê´€ë ¨ì€ 0ìœ¼ë¡œ
            elif col in ['article_count', 'news_count_change', 'news_count_ma_3', 'news_count_ma_7']:
                integrated[col] = integrated[col].fillna(1)  # ë‰´ìŠ¤ ê°œìˆ˜ëŠ” 1ë¡œ (0ì€ ë¹„í˜„ì‹¤ì )
            else:
                integrated[col] = integrated[col].fillna(0)  # ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ
        
        # ë¼ë²¨ ìƒì„± (ë‹¤ìŒ ë‚  ê°€ê²© ìƒìŠ¹ ì—¬ë¶€)
        integrated['next_day_return'] = integrated['close'].shift(-1) / integrated['close'] - 1
        integrated['target'] = (integrated['next_day_return'] > 0).astype(int)
        
        # NaN ì œê±°
        integrated = integrated.dropna()
        
        logger.info(f"Integrated dataset created with {len(integrated)} records")
        logger.info(f"Features: {len(integrated.columns)} total")
        
        self.integrated_data = integrated
        return integrated
    
    def save_integrated_data(self, output_file: str = "integrated_spy_news_data.csv"):
        """í†µí•© ë°ì´í„° ì €ì¥"""
        if self.integrated_data is None:
            self.integrate_data()
            
        output_path = os.path.join(self.data_dir, output_file)
        self.integrated_data.to_csv(output_path, index=False)
        logger.info(f"Integrated data saved to {output_path}")
        
        # ë°ì´í„° ìš”ì•½ ì •ë³´ ì¶œë ¥
        self._print_data_summary()
        
        return output_path
    
    def _print_data_summary(self):
        """ë°ì´í„° ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        if self.integrated_data is None:
            return
            
        df = self.integrated_data
        
        logger.info("\n=== í†µí•© ë°ì´í„° ìš”ì•½ ===")
        logger.info(f"ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
        logger.info(f"ë‚ ì§œ ë²”ìœ„: {df['date'].min()} ~ {df['date'].max()}")
        logger.info(f"ì´ íŠ¹ì„± ìˆ˜: {len(df.columns)}")
        
        logger.info(f"\n=== ê¸°ìˆ ì  ì§€í‘œ íŠ¹ì„± ===")
        technical_features = [col for col in df.columns if any(indicator in col for indicator in 
                            ['rsi', 'macd', 'bb_', 'ma_', 'volatility', 'price_', 'volume_', 'unusual_volume', 'price_spike'])]
        logger.info(f"ê¸°ìˆ ì  ì§€í‘œ íŠ¹ì„± ìˆ˜: {len(technical_features)}")
        
        logger.info(f"\n=== ë‰´ìŠ¤ ê´€ë ¨ íŠ¹ì„± ===")
        news_features = [col for col in df.columns if any(keyword in col for keyword in 
                        ['sentiment', 'news', 'article', 'impact'])]
        logger.info(f"ë‰´ìŠ¤ ê´€ë ¨ íŠ¹ì„± ìˆ˜: {len(news_features)}")
        logger.info(f"ë‰´ìŠ¤ íŠ¹ì„±: {news_features}")
        
        logger.info(f"\n=== íƒ€ê²Ÿ ë¶„í¬ ===")
        target_dist = df['target'].value_counts()
        logger.info(f"ìƒìŠ¹ (1): {target_dist.get(1, 0)} ({target_dist.get(1, 0)/len(df)*100:.1f}%)")
        logger.info(f"í•˜ë½ (0): {target_dist.get(0, 0)} ({target_dist.get(0, 0)/len(df)*100:.1f}%)")
        
        logger.info(f"\n=== ë‰´ìŠ¤ ê°ì • ë¶„ì„ í†µê³„ ===")
        logger.info(f"í‰ê·  ê°ì • ì ìˆ˜: {df['sentiment_score'].mean():.3f}")
        logger.info(f"ê°ì • ì ìˆ˜ í‘œì¤€í¸ì°¨: {df['sentiment_score'].std():.3f}")
        logger.info(f"í‰ê·  ì¼ë³„ ë‰´ìŠ¤ ê°œìˆ˜: {df['article_count'].mean():.1f}")
        logger.info(f"ê³ ì„íŒ©íŠ¸ ë‰´ìŠ¤ ì¼ìˆ˜: {df['high_impact_news'].sum()}ì¼")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ¤– ë‰´ìŠ¤-ì£¼ì‹ ë°ì´í„° í†µí•© ì‹œì‘")
    
    try:
        # í†µí•©ê¸° ì´ˆê¸°í™”
        integrator = NewsStockIntegration()
        
        # ë°ì´í„° í†µí•© ë° ì €ì¥
        output_file = integrator.save_integrated_data()
        
        logger.info("âœ… ë‰´ìŠ¤-ì£¼ì‹ ë°ì´í„° í†µí•© ì™„ë£Œ!")
        logger.info(f"ğŸ“ ì €ì¥ íŒŒì¼: {output_file}")
        
        return output_file
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° í†µí•© ì‹¤íŒ¨: {str(e)}")
        raise


if __name__ == "__main__":
    main()