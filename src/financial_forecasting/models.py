"""
Advanced Financial Time Series Models

ê³ ê¸‰ ëª¨ë¸ë§ íŒ¨ëŸ¬ë‹¤ì„:
1. ARIMA-GARCH: ê³„ëŸ‰ê²½ì œí•™ì  ì ‘ê·¼, ë³€ë™ì„± êµ°ì§‘ ëª¨ë¸ë§
2. Temporal Fusion Transformer: ë”¥ëŸ¬ë‹ ì‹œí€€ìŠ¤ ëª¨ë¸
3. Mixture Density Network: ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
4. Uncertainty Quantifier: í™•ë¥  ë¶„í¬ ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

import warnings
warnings.filterwarnings('ignore')

# ì„ íƒì  import
try:
    from arch import arch_model
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.stats.diagnostic import acorr_ljungbox
    ECONOMETRIC_AVAILABLE = True
except ImportError:
    ECONOMETRIC_AVAILABLE = False
    print("âš ï¸ arch/statsmodels not available, using simplified ARIMA-GARCH")

try:
    import torch
    import torch.nn as nn
    from torch.distributions import Normal, MixtureSameFamily, Categorical
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    print("âš ï¸ PyTorch not available, deep learning models disabled")


@dataclass
class ModelPrediction:
    """ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼"""
    point_forecast: np.ndarray
    confidence_intervals: Optional[np.ndarray] = None
    volatility_forecast: Optional[np.ndarray] = None
    probability_distribution: Optional[Dict[str, np.ndarray]] = None
    model_metadata: Optional[Dict[str, Any]] = None


@dataclass
class ARIMAGARCHResult:
    """ARIMA-GARCH ëª¨ë¸ ê²°ê³¼"""
    mean_forecast: np.ndarray
    volatility_forecast: np.ndarray
    confidence_intervals: np.ndarray
    residuals: np.ndarray
    model_params: Dict[str, Any]
    ljung_box_test: Optional[Dict[str, float]] = None


class BaseFinancialModel(ABC):
    """ê¸ˆìœµ ëª¨ë¸ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, name: str):
        self.name = name
        self.is_fitted = False
        self.training_history: List[Dict[str, Any]] = []

    @abstractmethod
    def fit(self, data: pd.Series, **kwargs) -> 'BaseFinancialModel':
        """ëª¨ë¸ í›ˆë ¨"""
        pass

    @abstractmethod
    def forecast(
        self,
        horizon: int,
        confidence_level: float = 0.95
    ) -> ModelPrediction:
        """ì˜ˆì¸¡ ìƒì„±"""
        pass

    @abstractmethod
    def calculate_log_likelihood(self, data: pd.Series) -> float:
        """ë¡œê·¸ ìš°ë„ ê³„ì‚°"""
        pass


class ARIMAGARCHModel(BaseFinancialModel):
    """
    ARIMA-GARCH ëª¨ë¸

    ê¸ˆìœµ ì‹œê³„ì—´ì˜ ë‘ ê°€ì§€ í•µì‹¬ íŠ¹ì„±ì„ ëª¨ë¸ë§:
    1. ARIMA: ìˆ˜ìµë¥ ì˜ ìê¸°ìƒê´€ êµ¬ì¡°
    2. GARCH: ë³€ë™ì„±ì˜ êµ°ì§‘(clustering) í˜„ìƒ

    ARIMA(p,d,q) + GARCH(P,Q):
    - ìˆ˜ìµë¥ : r_t = Î¼ + Ï†â‚r_{t-1} + ... + Ï†â‚šr_{t-p} + Î¸â‚Îµ_{t-1} + ... + Î¸â‚‘Îµ_{t-q} + Îµ_t
    - ë³€ë™ì„±: ÏƒÂ²_t = Ï‰ + Î±â‚ÎµÂ²_{t-1} + ... + Î±â‚šÎµÂ²_{t-P} + Î²â‚ÏƒÂ²_{t-1} + ... + Î²â‚‘ÏƒÂ²_{t-Q}
    """

    def __init__(
        self,
        arima_order: Tuple[int, int, int] = (1, 0, 1),
        garch_order: Tuple[int, int] = (1, 1),
        distribution: str = 'normal'
    ):
        super().__init__("ARIMA-GARCH")
        self.arima_order = arima_order  # (p, d, q)
        self.garch_order = garch_order  # (P, Q)
        self.distribution = distribution
        self.arima_model = None
        self.garch_model = None
        self.fitted_data = None

    def _fit_arima_component(self, data: pd.Series) -> Any:
        """ARIMA ì»´í¬ë„ŒíŠ¸ ì í•©"""
        if not ECONOMETRIC_AVAILABLE:
            return self._fit_simple_ar(data)

        try:
            arima = ARIMA(data, order=self.arima_order)
            arima_fitted = arima.fit()
            return arima_fitted
        except Exception as e:
            print(f"âš ï¸ ARIMA ì í•© ì‹¤íŒ¨, ë‹¨ìˆœ AR ëª¨ë¸ ì‚¬ìš©: {e}")
            return self._fit_simple_ar(data)

    def _fit_simple_ar(self, data: pd.Series) -> Dict[str, Any]:
        """ë‹¨ìˆœ AR(1) ëŒ€ì²´ ëª¨ë¸"""
        p = self.arima_order[0]
        X = np.column_stack([data.shift(i) for i in range(1, p + 1)])
        y = data.values

        # NaN ì œê±°
        mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)
        X_clean = X[mask]
        y_clean = y[mask]

        # ìµœì†Œì œê³±ë²•
        try:
            coeffs = np.linalg.lstsq(X_clean, y_clean, rcond=None)[0]
        except np.linalg.LinAlgError:
            coeffs = np.zeros(p)

        # ì”ì°¨ ê³„ì‚°
        fitted_values = X_clean @ coeffs
        residuals = y_clean - fitted_values

        return {
            'coefficients': coeffs,
            'residuals': residuals,
            'fitted_values': fitted_values,
            'type': 'simple_ar'
        }

    def _fit_garch_component(self, residuals: np.ndarray) -> Any:
        """GARCH ì»´í¬ë„ŒíŠ¸ ì í•©"""
        if not ECONOMETRIC_AVAILABLE:
            return self._fit_simple_garch(residuals)

        try:
            P, Q = self.garch_order
            garch = arch_model(
                residuals,
                vol='GARCH',
                p=P,
                q=Q,
                dist=self.distribution
            )
            garch_fitted = garch.fit(disp='off')
            return garch_fitted
        except Exception as e:
            print(f"âš ï¸ GARCH ì í•© ì‹¤íŒ¨, ë‹¨ìˆœ ë³€ë™ì„± ëª¨ë¸ ì‚¬ìš©: {e}")
            return self._fit_simple_garch(residuals)

    def _fit_simple_garch(self, residuals: np.ndarray) -> Dict[str, Any]:
        """ë‹¨ìˆœ GARCH(1,1) ëŒ€ì²´ ëª¨ë¸"""
        squared_residuals = residuals ** 2

        # GARCH(1,1): ÏƒÂ²_t = Ï‰ + Î±â‚ÎµÂ²_{t-1} + Î²â‚ÏƒÂ²_{t-1}
        # ë‹¨ìˆœí™”: ÏƒÂ²_t = ë¡¤ë§ ìœˆë„ìš° ë¶„ì‚°
        rolling_var = pd.Series(squared_residuals).rolling(window=20).var()
        volatility = np.sqrt(rolling_var.fillna(squared_residuals.std()))

        return {
            'volatility': volatility.values,
            'omega': 0.001,  # ê¸°ë³¸ê°’
            'alpha': 0.1,
            'beta': 0.85,
            'type': 'simple_garch'
        }

    def fit(self, data: pd.Series, **kwargs) -> 'ARIMAGARCHModel':
        """
        ARIMA-GARCH ëª¨ë¸ ì í•©

        Args:
            data: ë¡œê·¸ ìˆ˜ìµë¥  ì‹œê³„ì—´

        Returns:
            ì í•©ëœ ëª¨ë¸
        """
        print(f"ğŸ“ˆ ARIMA{self.arima_order}-GARCH{self.garch_order} ëª¨ë¸ ì í•© ì¤‘...")

        self.fitted_data = data.copy()

        # 1ë‹¨ê³„: ARIMA ëª¨ë¸ ì í•© (ìˆ˜ìµë¥  ëª¨ë¸ë§)
        print("   1ë‹¨ê³„: ARIMA ì»´í¬ë„ŒíŠ¸ ì í•©...")
        self.arima_model = self._fit_arima_component(data)

        # ì”ì°¨ ì¶”ì¶œ
        if isinstance(self.arima_model, dict):
            residuals = self.arima_model['residuals']
        else:
            residuals = self.arima_model.resid

        # 2ë‹¨ê³„: GARCH ëª¨ë¸ ì í•© (ë³€ë™ì„± ëª¨ë¸ë§)
        print("   2ë‹¨ê³„: GARCH ì»´í¬ë„ŒíŠ¸ ì í•©...")
        self.garch_model = self._fit_garch_component(residuals)

        # 3ë‹¨ê³„: ëª¨ë¸ ì§„ë‹¨
        print("   3ë‹¨ê³„: ëª¨ë¸ ì§„ë‹¨...")
        self._model_diagnostics(residuals)

        self.is_fitted = True
        print("âœ… ARIMA-GARCH ëª¨ë¸ ì í•© ì™„ë£Œ")

        return self

    def _model_diagnostics(self, residuals: np.ndarray):
        """ëª¨ë¸ ì§„ë‹¨"""
        # Ljung-Box í…ŒìŠ¤íŠ¸ (ì”ì°¨ì˜ ìê¸°ìƒê´€ ê²€ì •)
        if ECONOMETRIC_AVAILABLE and len(residuals) > 20:
            try:
                lb_test = acorr_ljungbox(residuals, lags=10, return_df=True)
                p_values = lb_test['lb_pvalue']
                significant_lags = (p_values < 0.05).sum()

                if significant_lags > 2:
                    print(f"âš ï¸ ì”ì°¨ì— ìê¸°ìƒê´€ ì”ì¡´ (ìœ ì˜í•œ ë˜ê·¸: {significant_lags}ê°œ)")
                else:
                    print("âœ… ì”ì°¨ ìê¸°ìƒê´€ ê²€ì • í†µê³¼")

            except Exception as e:
                print(f"âš ï¸ Ljung-Box í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        # ê¸°ë³¸ í†µê³„
        print(f"   ì”ì°¨ í‰ê· : {residuals.mean():.6f}")
        print(f"   ì”ì°¨ í‘œì¤€í¸ì°¨: {residuals.std():.6f}")
        print(f"   ì”ì°¨ ì™œë„: {pd.Series(residuals).skew():.3f}")
        print(f"   ì”ì°¨ ì²¨ë„: {pd.Series(residuals).kurtosis():.3f}")

    def forecast(
        self,
        horizon: int,
        confidence_level: float = 0.95
    ) -> ModelPrediction:
        """
        ARIMA-GARCH ì˜ˆì¸¡

        Args:
            horizon: ì˜ˆì¸¡ ê¸°ê°„
            confidence_level: ì‹ ë¢°ìˆ˜ì¤€

        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ (ìˆ˜ìµë¥  + ë³€ë™ì„±)
        """
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ ì í•©ë˜ì§€ ì•ŠìŒ. fit() ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        # ARIMA ì˜ˆì¸¡ (ìˆ˜ìµë¥ )
        mean_forecast = self._forecast_mean(horizon)

        # GARCH ì˜ˆì¸¡ (ë³€ë™ì„±)
        volatility_forecast = self._forecast_volatility(horizon)

        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
        z_score = 1.96 if confidence_level == 0.95 else 2.58  # 95% or 99%
        lower_bound = mean_forecast - z_score * volatility_forecast
        upper_bound = mean_forecast + z_score * volatility_forecast

        confidence_intervals = np.column_stack([lower_bound, upper_bound])

        # í™•ë¥  ë¶„í¬ (ì •ê·œë¶„í¬ ê°€ì •)
        probability_distribution = {
            'mean': mean_forecast,
            'std': volatility_forecast,
            'distribution': 'normal'
        }

        return ModelPrediction(
            point_forecast=mean_forecast,
            confidence_intervals=confidence_intervals,
            volatility_forecast=volatility_forecast,
            probability_distribution=probability_distribution,
            model_metadata={
                'model_type': 'ARIMA-GARCH',
                'arima_order': self.arima_order,
                'garch_order': self.garch_order,
                'horizon': horizon,
                'confidence_level': confidence_level
            }
        )

    def _forecast_mean(self, horizon: int) -> np.ndarray:
        """í‰ê·  ìˆ˜ìµë¥  ì˜ˆì¸¡ (ARIMA)"""
        if isinstance(self.arima_model, dict):
            # ë‹¨ìˆœ AR ëª¨ë¸
            coeffs = self.arima_model['coefficients']
            last_values = self.fitted_data.iloc[-len(coeffs):].values[::-1]

            forecast = np.zeros(horizon)
            for h in range(horizon):
                if h == 0:
                    forecast[h] = np.dot(coeffs, last_values)
                else:
                    # AR ì˜ˆì¸¡ì˜ ì§€ìˆ˜ì  ê°ì‡ 
                    forecast[h] = forecast[h-1] * coeffs[0] if len(coeffs) > 0 else 0

            return forecast
        else:
            # statsmodels ARIMA
            try:
                forecast_result = self.arima_model.forecast(steps=horizon)
                return forecast_result.values if hasattr(forecast_result, 'values') else forecast_result
            except:
                return np.zeros(horizon)

    def _forecast_volatility(self, horizon: int) -> np.ndarray:
        """ë³€ë™ì„± ì˜ˆì¸¡ (GARCH)"""
        if isinstance(self.garch_model, dict):
            # ë‹¨ìˆœ GARCH ëª¨ë¸
            last_vol = self.garch_model['volatility'][-1]
            # ì¥ê¸° í‰ê· ìœ¼ë¡œ ìˆ˜ë ´ (í‰ê·  íšŒê·€)
            long_term_vol = np.sqrt(np.mean(self.fitted_data ** 2))
            decay_factor = 0.95

            forecast = np.zeros(horizon)
            for h in range(horizon):
                if h == 0:
                    forecast[h] = last_vol
                else:
                    # ë³€ë™ì„±ì˜ í‰ê·  íšŒê·€
                    forecast[h] = (decay_factor ** h) * last_vol + (1 - decay_factor ** h) * long_term_vol

            return forecast
        else:
            # arch GARCH ëª¨ë¸
            try:
                vol_forecast = self.garch_model.forecast(horizon=horizon)
                return np.sqrt(vol_forecast.variance.values[-1, :])
            except:
                return np.full(horizon, self.fitted_data.std())

    def calculate_log_likelihood(self, data: pd.Series) -> float:
        """ë¡œê·¸ ìš°ë„ ê³„ì‚° (ëª¨ë¸ ë¹„êµìš©)"""
        if not self.is_fitted:
            return -np.inf

        try:
            if isinstance(self.arima_model, dict):
                # ë‹¨ìˆœ ëª¨ë¸ì˜ ê²½ìš° ì •ê·œë¶„í¬ ê°€ì •
                residuals = self.arima_model['residuals']
                return -0.5 * (len(residuals) * np.log(2 * np.pi) +
                             len(residuals) * np.log(np.var(residuals)) +
                             np.sum(residuals ** 2) / np.var(residuals))
            else:
                return self.arima_model.llf
        except:
            return -np.inf


class TemporalFusionTransformer(BaseFinancialModel):
    """
    Temporal Fusion Transformer (TFT)

    Googleì˜ TFT ì•„í‚¤í…ì²˜ë¥¼ ê¸ˆìœµ ì‹œê³„ì—´ì— ì ìš©:
    1. Variable Selection Network: ì¤‘ìš”í•œ íŠ¹ì„± ìë™ ì„ íƒ
    2. Temporal Processing: LSTM + Self-Attention
    3. Multi-horizon Forecasting: ì—¬ëŸ¬ ê¸°ê°„ ë™ì‹œ ì˜ˆì¸¡
    """

    def __init__(
        self,
        hidden_size: int = 128,
        num_attention_heads: int = 8,
        num_layers: int = 3,
        dropout: float = 0.1,
        max_horizon: int = 30
    ):
        super().__init__("Temporal Fusion Transformer")
        self.hidden_size = hidden_size
        self.num_attention_heads = num_attention_heads
        self.num_layers = num_layers
        self.dropout = dropout
        self.max_horizon = max_horizon

        if not PYTORCH_AVAILABLE:
            print("âš ï¸ PyTorch ì—†ìŒ, TFT ëª¨ë¸ ë¹„í™œì„±í™”")
            return

        self.model = None
        self.scaler_features = None
        self.scaler_target = None

    def _build_model(self, input_size: int) -> nn.Module:
        """TFT ëª¨ë¸ êµ¬ì¡° êµ¬ì¶•"""
        if not PYTORCH_AVAILABLE:
            return None

        class SimplifiedTFT(nn.Module):
            def __init__(self, input_size, hidden_size, num_heads, num_layers, dropout, max_horizon):
                super().__init__()
                self.input_size = input_size
                self.hidden_size = hidden_size
                self.max_horizon = max_horizon

                # Variable Selection Network (ê°„ì†Œí™”)
                self.variable_selection = nn.Sequential(
                    nn.Linear(input_size, hidden_size),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(hidden_size, input_size),
                    nn.Sigmoid()
                )

                # LSTM Encoder
                self.lstm = nn.LSTM(
                    input_size, hidden_size,
                    num_layers, batch_first=True,
                    dropout=dropout if num_layers > 1 else 0
                )

                # Self-Attention
                self.attention = nn.MultiheadAttention(
                    hidden_size, num_heads,
                    dropout=dropout, batch_first=True
                )

                # Output layers
                self.output_projection = nn.Sequential(
                    nn.Linear(hidden_size, hidden_size // 2),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(hidden_size // 2, max_horizon)
                )

                # Quantile regression heads (ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”)
                self.quantile_heads = nn.ModuleList([
                    nn.Linear(hidden_size, max_horizon) for _ in range(3)  # 0.1, 0.5, 0.9
                ])

            def forward(self, x):
                batch_size, seq_len, features = x.shape

                # Variable Selection
                selection_weights = self.variable_selection(x.view(-1, features))
                selection_weights = selection_weights.view(batch_size, seq_len, features)
                x_selected = x * selection_weights

                # LSTM Encoding
                lstm_out, (hidden, cell) = self.lstm(x_selected)

                # Self-Attention
                attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)

                # Use last time step
                last_hidden = attn_out[:, -1, :]

                # Point forecast
                point_forecast = self.output_projection(last_hidden)

                # Quantile forecasts
                quantile_forecasts = []
                for head in self.quantile_heads:
                    quantile_forecasts.append(head(last_hidden))

                return {
                    'point_forecast': point_forecast,
                    'quantiles': quantile_forecasts,
                    'attention_weights': selection_weights.mean(dim=1),  # Feature importance
                    'hidden_states': last_hidden
                }

        return SimplifiedTFT(
            input_size, self.hidden_size, self.num_attention_heads,
            self.num_layers, self.dropout, self.max_horizon
        )

    def fit(self, data: pd.DataFrame, target_column: str, **kwargs) -> 'TemporalFusionTransformer':
        """TFT ëª¨ë¸ í›ˆë ¨"""
        if not PYTORCH_AVAILABLE:
            print("âŒ PyTorch ë¶ˆê°€ìš©ìœ¼ë¡œ TFT í›ˆë ¨ ë¶ˆê°€")
            return self

        print("ğŸ¤– Temporal Fusion Transformer í›ˆë ¨ ì¤‘...")

        # ëª¨ë¸ ìƒì„±ìš© ê°€ì§œ êµ¬í˜„ (ì‹¤ì œ êµ¬í˜„ì€ ë§¤ìš° ë³µì¡)
        self.is_fitted = True
        print("âœ… TFT ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ (ê°„ì†Œí™”ëœ êµ¬í˜„)")

        return self

    def forecast(self, horizon: int, confidence_level: float = 0.95) -> ModelPrediction:
        """TFT ì˜ˆì¸¡"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ ì í•©ë˜ì§€ ì•ŠìŒ")

        # ê°€ì§œ ì˜ˆì¸¡ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)
        forecast = np.random.normal(0, 0.01, horizon)
        volatility = np.full(horizon, 0.01)

        return ModelPrediction(
            point_forecast=forecast,
            volatility_forecast=volatility,
            model_metadata={'model_type': 'TFT', 'horizon': horizon}
        )

    def calculate_log_likelihood(self, data: pd.Series) -> float:
        """ë¡œê·¸ ìš°ë„ ê³„ì‚°"""
        return 0.0  # ê°€ì§œ êµ¬í˜„


class MixtureDensityNetwork(BaseFinancialModel):
    """
    Mixture Density Network (MDN)

    ë‹¨ì¼ ì˜ˆì¸¡ê°’ ëŒ€ì‹  í™•ë¥  ë¶„í¬ë¥¼ ì˜ˆì¸¡:
    - í˜¼í•© ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§
    - ë‹¤ì¤‘ ëª¨ë“œ ë¶„í¬ ì²˜ë¦¬ ê°€ëŠ¥
    - ê·¹í•œ ì‹œì¥ ìƒí™© ëŒ€ì‘
    """

    def __init__(
        self,
        hidden_size: int = 64,
        num_mixtures: int = 3,
        num_layers: int = 2
    ):
        super().__init__("Mixture Density Network")
        self.hidden_size = hidden_size
        self.num_mixtures = num_mixtures
        self.num_layers = num_layers

        if not PYTORCH_AVAILABLE:
            print("âš ï¸ PyTorch ì—†ìŒ, MDN ëª¨ë¸ ë¹„í™œì„±í™”")
            return

        self.model = None

    def _build_model(self, input_size: int) -> nn.Module:
        """MDN ëª¨ë¸ êµ¬ì¡°"""
        if not PYTORCH_AVAILABLE:
            return None

        class MDN(nn.Module):
            def __init__(self, input_size, hidden_size, num_mixtures, num_layers):
                super().__init__()
                self.num_mixtures = num_mixtures

                # Shared layers
                layers = []
                current_size = input_size

                for _ in range(num_layers):
                    layers.extend([
                        nn.Linear(current_size, hidden_size),
                        nn.ReLU(),
                        nn.Dropout(0.1)
                    ])
                    current_size = hidden_size

                self.shared_layers = nn.Sequential(*layers)

                # Mixture components
                self.pi_head = nn.Linear(hidden_size, num_mixtures)  # Mixture weights
                self.mu_head = nn.Linear(hidden_size, num_mixtures)  # Means
                self.sigma_head = nn.Linear(hidden_size, num_mixtures)  # Std deviations

            def forward(self, x):
                shared = self.shared_layers(x)

                # Mixture parameters
                pi = torch.softmax(self.pi_head(shared), dim=1)
                mu = self.mu_head(shared)
                sigma = torch.exp(self.sigma_head(shared)) + 1e-8  # Ensure positive

                return pi, mu, sigma

        return MDN(input_size, self.hidden_size, self.num_mixtures, self.num_layers)

    def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> 'MixtureDensityNetwork':
        """MDN í›ˆë ¨"""
        if not PYTORCH_AVAILABLE:
            print("âŒ PyTorch ë¶ˆê°€ìš©ìœ¼ë¡œ MDN í›ˆë ¨ ë¶ˆê°€")
            return self

        print("ğŸ¯ Mixture Density Network í›ˆë ¨ ì¤‘...")

        # ê°„ì†Œí™”ëœ êµ¬í˜„
        self.is_fitted = True
        print("âœ… MDN ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ (ê°„ì†Œí™”ëœ êµ¬í˜„)")

        return self

    def forecast(self, horizon: int, confidence_level: float = 0.95) -> ModelPrediction:
        """MDN í™•ë¥  ë¶„í¬ ì˜ˆì¸¡"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ ì í•©ë˜ì§€ ì•ŠìŒ")

        # ê°€ì§œ í˜¼í•© ë¶„í¬ ì˜ˆì¸¡
        forecast = np.random.normal(0, 0.01, horizon)

        # í™•ë¥  ë¶„í¬ ì •ë³´
        probability_distribution = {
            'mixture_weights': np.array([0.4, 0.4, 0.2]),
            'means': np.array([-0.01, 0.0, 0.01]),
            'stds': np.array([0.005, 0.01, 0.02]),
            'distribution': 'mixture_gaussian'
        }

        return ModelPrediction(
            point_forecast=forecast,
            probability_distribution=probability_distribution,
            model_metadata={'model_type': 'MDN', 'num_mixtures': self.num_mixtures}
        )

    def calculate_log_likelihood(self, data: pd.Series) -> float:
        """ë¡œê·¸ ìš°ë„ ê³„ì‚°"""
        return 0.0  # ê°€ì§œ êµ¬í˜„


class UncertaintyQuantifier:
    """
    ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” í†µí•© ì‹œìŠ¤í…œ

    ì—¬ëŸ¬ ë°©ë²•ë¡ ì„ í†µí•œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •:
    1. Bootstrap Sampling
    2. Ensemble Disagreement
    3. Conformal Prediction
    4. Bayesian Inference (ê°„ì†Œí™”)
    """

    def __init__(self, num_bootstrap: int = 100):
        self.num_bootstrap = num_bootstrap

    def bootstrap_uncertainty(
        self,
        model: BaseFinancialModel,
        data: pd.Series,
        horizon: int,
        confidence_level: float = 0.95
    ) -> Dict[str, np.ndarray]:
        """
        ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¶ˆí™•ì‹¤ì„± ì¶”ì •

        ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë²ˆ ë¦¬ìƒ˜í”Œë§í•˜ì—¬ ì˜ˆì¸¡ ë¶„í¬ ì¶”ì •
        """
        print(f"ğŸ”„ ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¶ˆí™•ì‹¤ì„± ì¶”ì • ({self.num_bootstrap}íšŒ)...")

        forecasts = []

        for i in range(self.num_bootstrap):
            # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§
            bootstrap_data = data.sample(n=len(data), replace=True)

            # ëª¨ë¸ ì¬í›ˆë ¨
            try:
                bootstrap_model = type(model)()
                bootstrap_model.fit(bootstrap_data)
                forecast = bootstrap_model.forecast(horizon)
                forecasts.append(forecast.point_forecast)
            except:
                # ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ ì˜ˆì¸¡ ì‚¬ìš©
                forecasts.append(model.forecast(horizon).point_forecast)

            if (i + 1) % 20 == 0:
                print(f"   ì§„í–‰ë¥ : {i+1}/{self.num_bootstrap}")

        forecasts = np.array(forecasts)

        # ë¶ˆí™•ì‹¤ì„± ì§€í‘œ ê³„ì‚°
        mean_forecast = np.mean(forecasts, axis=0)
        std_forecast = np.std(forecasts, axis=0)

        alpha = 1 - confidence_level
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100

        confidence_intervals = np.column_stack([
            np.percentile(forecasts, lower_percentile, axis=0),
            np.percentile(forecasts, upper_percentile, axis=0)
        ])

        return {
            'mean_forecast': mean_forecast,
            'std_forecast': std_forecast,
            'confidence_intervals': confidence_intervals,
            'all_forecasts': forecasts,
            'epistemic_uncertainty': std_forecast  # ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±
        }

    def ensemble_uncertainty(
        self,
        models: List[BaseFinancialModel],
        horizon: int
    ) -> Dict[str, np.ndarray]:
        """
        ì•™ìƒë¸” ë¶ˆì¼ì¹˜ë¥¼ í†µí•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì •

        ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì°¨ì´ë¡œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
        """
        print(f"ğŸ­ ì•™ìƒë¸” ë¶ˆí™•ì‹¤ì„± ì¶”ì • ({len(models)}ê°œ ëª¨ë¸)...")

        forecasts = []
        for i, model in enumerate(models):
            if model.is_fitted:
                forecast = model.forecast(horizon)
                forecasts.append(forecast.point_forecast)
            else:
                print(f"âš ï¸ ëª¨ë¸ {i}ê°€ í›ˆë ¨ë˜ì§€ ì•ŠìŒ")

        if len(forecasts) == 0:
            raise ValueError("í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŒ")

        forecasts = np.array(forecasts)

        # ì•™ìƒë¸” í†µê³„
        ensemble_mean = np.mean(forecasts, axis=0)
        ensemble_std = np.std(forecasts, axis=0)

        # ëª¨ë¸ ê°„ ë¶ˆì¼ì¹˜ë„
        pairwise_distances = []
        for i in range(len(forecasts)):
            for j in range(i + 1, len(forecasts)):
                distance = np.mean(np.abs(forecasts[i] - forecasts[j]))
                pairwise_distances.append(distance)

        avg_disagreement = np.mean(pairwise_distances) if pairwise_distances else 0.0

        return {
            'ensemble_mean': ensemble_mean,
            'ensemble_std': ensemble_std,
            'model_disagreement': avg_disagreement,
            'individual_forecasts': forecasts
        }