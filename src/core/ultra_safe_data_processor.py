#!/usr/bin/env python3
"""
ì´ˆì•ˆì „ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ
ê°€ì¥ ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨
"""

import numpy as np
import pandas as pd
import logging
from typing import Tuple, List, Dict, Optional
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UltraSafeDataProcessor:
    """ì´ˆì•ˆì „ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self):
        # ê·¹ë„ë¡œ ì—„ê²©í•œ ê¸°ì¤€
        self.MAX_R2 = 0.10           # 10% ì´ìƒ ê¸ˆì§€
        self.MAX_DIRECTION_ACC = 60.0 # 60% ì´ìƒ ê¸ˆì§€
        self.MAX_CORRELATION = 0.20   # 20% ì´ìƒ ìƒê´€ê´€ê³„ ê¸ˆì§€
        self.MIN_LAG_DAYS = 2         # ìµœì†Œ 2ì¼ ì§€ì—°

        logger.info("ì´ˆì•ˆì „ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info(f"ê·¹ë„ë¡œ ì—„ê²©í•œ ê¸°ì¤€ - ìƒê´€ê´€ê³„: <{self.MAX_CORRELATION}")

    def create_ultra_safe_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì´ˆì•ˆì „ íŠ¹ì§• ìƒì„± (ê·¹ë„ë¡œ ë³´ìˆ˜ì )"""
        logger.info("=== ì´ˆì•ˆì „ íŠ¹ì§• ìƒì„± ===")

        df = df.copy()

        # 1. ê³¼ê±° ìˆ˜ìµë¥ ë§Œ ì‚¬ìš© (ìµœì†Œ 2ì¼ ì§€ì—°)
        logger.info("1. ê³¼ê±° ìˆ˜ìµë¥  (2ì¼+ ì§€ì—°)")
        df['returns'] = df['Close'].pct_change()
        df['returns_lag2'] = df['returns'].shift(2)
        df['returns_lag3'] = df['returns'].shift(3)
        df['returns_lag5'] = df['returns'].shift(5)
        df['returns_lag10'] = df['returns'].shift(10)

        # 2. ê³¼ê±° ë³€ë™ì„± (ë³´ìˆ˜ì )
        logger.info("2. ê³¼ê±° ë³€ë™ì„± (2ì¼+ ì§€ì—°)")
        df['vol_5_lag2'] = df['returns_lag2'].rolling(5).std()
        df['vol_10_lag2'] = df['returns_lag2'].rolling(10).std()

        # 3. ê³¼ê±° ìˆ˜ìµë¥  í†µê³„ (ë³´ìˆ˜ì )
        logger.info("3. ê³¼ê±° ìˆ˜ìµë¥  í†µê³„")
        df['returns_mean_5_lag2'] = df['returns_lag2'].rolling(5).mean()
        df['returns_std_5_lag2'] = df['returns_lag2'].rolling(5).std()

        # 4. ë‹¨ìˆœ ì§€ì—° ì°¨ì´ (ì•ˆì „í•œ íŠ¹ì§•)
        logger.info("4. ë‹¨ìˆœ ì§€ì—° ì°¨ì´")
        df['returns_diff_2_5'] = df['returns_lag2'] - df['returns_lag5']
        df['returns_diff_3_10'] = df['returns_lag3'] - df['returns_lag10']

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.dropna()

        # ì•ˆì „í•œ íŠ¹ì§•ë§Œ ì„ íƒ
        safe_features = [
            'returns_lag2', 'returns_lag3', 'returns_lag5', 'returns_lag10',
            'vol_5_lag2', 'vol_10_lag2',
            'returns_mean_5_lag2', 'returns_std_5_lag2',
            'returns_diff_2_5', 'returns_diff_3_10'
        ]

        logger.info(f"ì´ˆì•ˆì „ íŠ¹ì§•: {len(safe_features)}ê°œ")

        return df, safe_features

    def validate_ultra_safe_features(self, df: pd.DataFrame, feature_cols: List[str], target_col: str) -> bool:
        """ì´ˆì•ˆì „ íŠ¹ì§• ê²€ì¦"""
        logger.info("=== ì´ˆì•ˆì „ íŠ¹ì§• ê²€ì¦ ===")

        max_corr = 0
        dangerous_features = []

        for col in feature_cols:
            if col in df.columns:
                corr = abs(df[col].corr(df[target_col]))
                max_corr = max(max_corr, corr)

                if corr > self.MAX_CORRELATION:
                    dangerous_features.append((col, corr))

        logger.info(f"ìµœëŒ€ ìƒê´€ê´€ê³„: {max_corr:.3f}")

        if dangerous_features:
            logger.error(f"ìœ„í—˜í•œ íŠ¹ì§• ë°œê²¬: {len(dangerous_features)}ê°œ")
            for col, corr in dangerous_features:
                logger.error(f"   {col}: {corr:.3f}")
            return False

        logger.info("âœ… ëª¨ë“  íŠ¹ì§•ì´ ì´ˆì•ˆì „ ê¸°ì¤€ í†µê³¼")
        return True

    def prepare_ultra_safe_data(self, data_path: str, target_type: str = 'return') -> Dict:
        """ì´ˆì•ˆì „ ë°ì´í„° ì¤€ë¹„"""
        logger.info("=" * 80)
        logger.info("ğŸ›¡ï¸ ì´ˆì•ˆì „ ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
        logger.info("ğŸš« ê·¹ë„ë¡œ ì—„ê²©í•œ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€")
        logger.info("=" * 80)

        # 1. ë°ì´í„° ë¡œë”©
        df = pd.read_csv(data_path)
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date').reset_index(drop=True)

        logger.info(f"ì›ë³¸ ë°ì´í„°: {df.shape}")

        # 2. ì´ˆì•ˆì „ íŠ¹ì§• ìƒì„±
        df, safe_features = self.create_ultra_safe_features(df)

        # 3. íƒ€ê²Ÿ ìƒì„±
        if target_type == 'return':
            target = df['Close'].pct_change().shift(-1)
            target_name = 'next_return'
        else:  # direction
            returns = df['Close'].pct_change().shift(-1)
            target = (returns > 0).astype(int)
            target_name = 'next_direction'

        df[target_name] = target
        df = df.dropna()

        # 4. ì´ˆì•ˆì „ ê²€ì¦
        is_safe = self.validate_ultra_safe_features(df, safe_features, target_name)

        if not is_safe:
            raise ValueError("ì´ˆì•ˆì „ ê²€ì¦ ì‹¤íŒ¨")

        # 5. ìµœì¢… ë°ì´í„° ì¤€ë¹„
        X = df[safe_features].values
        y = df[target_name].values

        # 6. ì‹œê°„ ë¶„í• 
        tscv = TimeSeriesSplit(n_splits=3, test_size=50, gap=2)  # 2ì¼ ê°„ê²©
        splits = list(tscv.split(X))

        logger.info(f"ìµœì¢… ì´ˆì•ˆì „ ë°ì´í„°: X{X.shape}, y{y.shape}")

        # 7. ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì • (ì´ˆì•ˆì „ í™•ì¸)
        baseline_r2 = self._measure_baseline_performance(X, y)

        if baseline_r2 > self.MAX_R2:
            logger.warning(f"ê¸°ì¤€ì„  RÂ² {baseline_r2:.3f} > {self.MAX_R2} - ì—¬ì „íˆ ìœ„í—˜í•  ìˆ˜ ìˆìŒ")

        logger.info("âœ… ì´ˆì•ˆì „ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")

        return {
            'X': X,
            'y': y,
            'feature_names': safe_features,
            'splits': splits,
            'target_type': target_type,
            'baseline_r2': baseline_r2,
            'safety_level': 'ULTRA_SAFE'
        }

    def _measure_baseline_performance(self, X: np.ndarray, y: np.ndarray) -> float:
        """ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì •"""
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import r2_score

        # ê°„ë‹¨í•œ ë¶„í• 
        split_point = int(len(X) * 0.8)
        X_train, X_test = X[:split_point], X[split_point:]
        y_train, y_test = y[:split_point], y[split_point:]

        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ì„ í˜• íšŒê·€
        model = LinearRegression()
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)

        r2 = r2_score(y_test, y_pred)
        logger.info(f"ê¸°ì¤€ì„  RÂ²: {r2:.4f}")

        return r2

def main():
    """í…ŒìŠ¤íŠ¸"""
    processor = UltraSafeDataProcessor()

    try:
        data_dict = processor.prepare_ultra_safe_data(
            '/root/workspace/data/training/sp500_2020_2024_enhanced.csv',
            target_type='return'
        )

        print(f"\nâœ… ì´ˆì•ˆì „ ë°ì´í„° ì¤€ë¹„ ì„±ê³µ:")
        print(f"   ë°ì´í„° í¬ê¸°: {data_dict['X'].shape}")
        print(f"   íŠ¹ì§•: {data_dict['feature_names']}")
        print(f"   ê¸°ì¤€ì„  RÂ²: {data_dict['baseline_r2']:.4f}")
        print(f"   ì•ˆì „ ìˆ˜ì¤€: {data_dict['safety_level']}")

        return data_dict

    except Exception as e:
        print(f"\nâŒ ì´ˆì•ˆì „ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    result = main()