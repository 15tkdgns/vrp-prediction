#!/usr/bin/env python3
"""
ì•ˆì „í•œ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ
ë°ì´í„° ëˆ„ì¶œ ì‚¬ì „ ë°©ì§€ ê¸°ì¤€ ì¤€ìˆ˜
"""

import numpy as np
import pandas as pd
import logging
from typing import Tuple, List, Dict, Optional
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SafeDataProcessor:
    """ì•ˆì „í•œ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self):
        # ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ ê¸°ì¤€
        self.MAX_R2 = 0.15
        self.MAX_DIRECTION_ACC = 65.0
        self.MAX_CORRELATION = 0.30
        self.MAX_IC = 0.08
        self.MIN_LAG_DAYS = 1

        logger.info("ì•ˆì „í•œ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info(f"í—ˆìš© ê¸°ì¤€ - RÂ²: <{self.MAX_R2}, ë°©í–¥ì •í™•ë„: <{self.MAX_DIRECTION_ACC}%")

    def load_data(self, data_path: str) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ ê²€ì¦"""
        logger.info("=== ì•ˆì „í•œ ë°ì´í„° ë¡œë”© ===")

        df = pd.read_csv(data_path)
        logger.info(f"ì›ë³¸ ë°ì´í„°: {df.shape}")

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_cols = ['Date', 'Close']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")

        # ë‚ ì§œ ì •ë ¬ í™•ì¸
        df['Date'] = pd.to_datetime(df['Date'])
        if not df['Date'].is_monotonic_increasing:
            logger.warning("ë‚ ì§œ ìˆœì„œ ì •ë ¬")
            df = df.sort_values('Date').reset_index(drop=True)

        # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
        logger.info(f"ë°ì´í„° ê¸°ê°„: {df['Date'].min()} ~ {df['Date'].max()}")
        logger.info(f"ì´ ê±°ë˜ì¼: {len(df)}ì¼")

        return df

    def create_safe_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì•ˆì „í•œ íŠ¹ì§• ìƒì„± (ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ì—†ìŒ)"""
        logger.info("=== ì•ˆì „í•œ íŠ¹ì§• ìƒì„± ===")

        df = df.copy()

        # 1. ê¸°ë³¸ ê°€ê²© íŠ¹ì§• (ìµœì†Œ 1ì¼ ì§€ì—°)
        logger.info("1. ê³¼ê±° ê°€ê²© íŠ¹ì§• ìƒì„±")
        df['close_lag1'] = df['Close'].shift(1)
        df['close_lag2'] = df['Close'].shift(2)
        df['close_lag3'] = df['Close'].shift(3)
        df['close_lag5'] = df['Close'].shift(5)

        # 2. ì•ˆì „í•œ ìˆ˜ìµë¥  íŠ¹ì§• (ì§€ì—° ì ìš©)
        logger.info("2. ê³¼ê±° ìˆ˜ìµë¥  íŠ¹ì§• ìƒì„±")
        df['returns'] = df['Close'].pct_change()
        df['returns_lag1'] = df['returns'].shift(1)
        df['returns_lag2'] = df['returns'].shift(2)
        df['returns_lag3'] = df['returns'].shift(3)
        df['returns_lag5'] = df['returns'].shift(5)

        # 3. ì•ˆì „í•œ ì´ë™í‰ê·  (ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        logger.info("3. ê³¼ê±° ê¸°ë°˜ ì´ë™í‰ê·  ìƒì„±")
        df['ma5_lag1'] = df['close_lag1'].rolling(5).mean()
        df['ma10_lag1'] = df['close_lag1'].rolling(10).mean()
        df['ma20_lag1'] = df['close_lag1'].rolling(20).mean()

        # 4. ì•ˆì „í•œ ë³€ë™ì„± (ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        logger.info("4. ê³¼ê±° ê¸°ë°˜ ë³€ë™ì„± ìƒì„±")
        df['vol5_lag1'] = df['returns_lag1'].rolling(5).std()
        df['vol10_lag1'] = df['returns_lag1'].rolling(10).std()
        df['vol20_lag1'] = df['returns_lag1'].rolling(20).std()

        # 5. ì•ˆì „í•œ ëª¨ë©˜í…€ íŠ¹ì§• (ì¶”ê°€ ì§€ì—° ìƒì„± í›„)
        logger.info("5. ì¶”ê°€ ê³¼ê±° ê°€ê²© ìƒì„±")
        df['close_lag10'] = df['Close'].shift(10)

        logger.info("6. ê³¼ê±° ê¸°ë°˜ ëª¨ë©˜í…€ ìƒì„±")
        df['momentum_5_lag1'] = (df['close_lag1'] / df['close_lag5']) - 1
        df['momentum_10_lag1'] = (df['close_lag1'] / df['close_lag10']) - 1

        # 6. ì•ˆì „í•œ ìƒëŒ€ íŠ¹ì§•
        logger.info("7. ê³¼ê±° ê¸°ë°˜ ìƒëŒ€ íŠ¹ì§• ìƒì„±")
        df['price_ma_ratio_lag1'] = df['close_lag1'] / df['ma20_lag1']
        df['ma_crossover_lag1'] = (df['ma5_lag1'] > df['ma10_lag1']).astype(float)

        # 8. ì•ˆì „í•œ í†µê³„ íŠ¹ì§•
        logger.info("8. ê³¼ê±° ê¸°ë°˜ í†µê³„ íŠ¹ì§• ìƒì„±")
        df['returns_mean_5_lag1'] = df['returns_lag1'].rolling(5).mean()
        df['returns_std_5_lag1'] = df['returns_lag1'].rolling(5).std()
        df['returns_skew_10_lag1'] = df['returns_lag1'].rolling(10).skew()

        # ê²°ì¸¡ê°’ ì •ë¦¬
        feature_cols = [col for col in df.columns if col.endswith('_lag1') or 'momentum_' in col or 'crossover' in col]
        logger.info(f"ìƒì„±ëœ ì•ˆì „í•œ íŠ¹ì§•: {len(feature_cols)}ê°œ")

        return df

    def validate_feature_safety(self, df: pd.DataFrame, feature_cols: List[str], target_col: str) -> bool:
        """íŠ¹ì§• ì•ˆì „ì„± ê²€ì¦"""
        logger.info("=== íŠ¹ì§• ì•ˆì „ì„± ê²€ì¦ ===")

        issues = []

        # 1. ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ê²€ì‚¬
        logger.info("1. ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ê²€ì‚¬")
        future_leak_features = []

        for col in feature_cols:
            if col in df.columns and df[col].notna().sum() > 10:
                # í˜„ì¬ íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ (ë™ì‹œì  ëˆ„ì¶œ ê²€ì‚¬)
                current_corr = abs(df[col].corr(df[target_col]))

                if current_corr > self.MAX_CORRELATION:
                    future_leak_features.append((col, current_corr))
                    issues.append(f"ê³¼ë„í•œ ë™ì‹œì  ìƒê´€ê´€ê³„: {col} ({current_corr:.3f})")

        if future_leak_features:
            logger.error(f"ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ì˜ì‹¬: {len(future_leak_features)}ê°œ")
            for col, corr in future_leak_features[:5]:
                logger.error(f"   {col}: {corr:.3f}")

        # 2. íŠ¹ì§•ëª… ê²€ì¦ (ì§€ì—° í™•ì¸)
        logger.info("2. íŠ¹ì§•ëª… ì•ˆì „ì„± ê²€ì¦")
        unsafe_features = []

        for col in feature_cols:
            # ì§€ì—°ì´ í¬í•¨ë˜ì§€ ì•Šì€ íŠ¹ì§• ê²€ì‚¬
            if not any(lag in col for lag in ['_lag', 'momentum_', 'crossover']):
                unsafe_features.append(col)
                issues.append(f"ì§€ì—° ì—†ëŠ” íŠ¹ì§•: {col}")

        if unsafe_features:
            logger.error(f"ì§€ì—° ì—†ëŠ” íŠ¹ì§•: {len(unsafe_features)}ê°œ")

        # 3. ì»¬ëŸ¼ëª… ê¸°ë°˜ ê²€ì¦
        logger.info("3. ì»¬ëŸ¼ëª… ê¸°ë°˜ ì•ˆì „ì„± ê²€ì¦")
        forbidden_keywords = ['future', 'next', 'ahead', 'forward', 'current']

        for col in feature_cols:
            for keyword in forbidden_keywords:
                if keyword in col.lower():
                    issues.append(f"ê¸ˆì§€ëœ í‚¤ì›Œë“œ í¬í•¨: {col} ({keyword})")

        # ê²°ê³¼ ì¶œë ¥
        is_safe = len(issues) == 0

        if is_safe:
            logger.info("âœ… ëª¨ë“  íŠ¹ì§•ì´ ì•ˆì „ì„± ê²€ì¦ í†µê³¼")
        else:
            logger.error(f"âŒ ì•ˆì „ì„± ê²€ì¦ ì‹¤íŒ¨: {len(issues)}ê°œ ë¬¸ì œ")
            for issue in issues[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥
                logger.error(f"   {issue}")

        return is_safe

    def create_safe_target(self, df: pd.DataFrame, target_type: str = 'return') -> pd.Series:
        """ì•ˆì „í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±"""
        logger.info(f"=== ì•ˆì „í•œ íƒ€ê²Ÿ ìƒì„± (íƒ€ì…: {target_type}) ===")

        if target_type == 'return':
            # ë‹¤ìŒë‚  ìˆ˜ìµë¥  (ë¯¸ë˜ ì •ë³´ì´ì§€ë§Œ ì˜ˆì¸¡ íƒ€ê²Ÿìœ¼ë¡œ í—ˆìš©)
            target = df['Close'].pct_change().shift(-1)
            logger.info("íƒ€ê²Ÿ: ë‹¤ìŒë‚  ìˆ˜ìµë¥ ")

        elif target_type == 'direction':
            # ë‹¤ìŒë‚  ë°©í–¥ (ìƒìŠ¹=1, í•˜ë½=0)
            returns = df['Close'].pct_change().shift(-1)
            target = (returns > 0).astype(int)
            logger.info("íƒ€ê²Ÿ: ë‹¤ìŒë‚  ë°©í–¥ (ìƒìŠ¹/í•˜ë½)")

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ê²Ÿ íƒ€ì…: {target_type}")

        # íƒ€ê²Ÿ í†µê³„
        valid_target = target.dropna()
        logger.info(f"íƒ€ê²Ÿ í†µê³„:")
        logger.info(f"   ìœ íš¨ ìƒ˜í”Œ: {len(valid_target)}ê°œ")

        if target_type == 'return':
            logger.info(f"   í‰ê· : {valid_target.mean():.6f}")
            logger.info(f"   í‘œì¤€í¸ì°¨: {valid_target.std():.6f}")
            logger.info(f"   ë²”ìœ„: [{valid_target.min():.6f}, {valid_target.max():.6f}]")
        else:
            logger.info(f"   ìƒìŠ¹: {valid_target.sum()}ê°œ ({valid_target.mean()*100:.1f}%)")
            logger.info(f"   í•˜ë½: {len(valid_target)-valid_target.sum()}ê°œ ({(1-valid_target.mean())*100:.1f}%)")

        return target

    def create_safe_splits(self, X: np.ndarray, y: np.ndarray, n_splits: int = 3) -> List[Tuple[np.ndarray, np.ndarray]]:
        """ì•ˆì „í•œ ì‹œê°„ ë¶„í• """
        logger.info(f"=== ì•ˆì „í•œ ì‹œê°„ ë¶„í•  (splits: {n_splits}) ===")

        # TimeSeriesSplit ì‚¬ìš© (í•„ìˆ˜)
        tscv = TimeSeriesSplit(
            n_splits=n_splits,
            test_size=max(100, len(X) // 10),  # ìµœì†Œ 100ê°œ ë˜ëŠ” 10%
            gap=1  # ìµœì†Œ 1ì¼ ê°„ê²©
        )

        splits = []
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            # ì‹œê°„ ìˆœì„œ ê²€ì¦
            if train_idx.max() >= val_idx.min():
                raise ValueError(f"Fold {fold}: ì‹œê°„ ìˆœì„œ ìœ„ë°˜")

            logger.info(f"Fold {fold}: í›ˆë ¨ {len(train_idx)}, ê²€ì¦ {len(val_idx)}")
            logger.info(f"   í›ˆë ¨ ë²”ìœ„: {train_idx.min()}-{train_idx.max()}")
            logger.info(f"   ê²€ì¦ ë²”ìœ„: {val_idx.min()}-{val_idx.max()}")
            logger.info(f"   ê°„ê²©: {val_idx.min() - train_idx.max()}ì¼")

            splits.append((train_idx, val_idx))

        return splits

    def safe_scale_data(self, X_train: np.ndarray, X_val: np.ndarray) -> Tuple[np.ndarray, np.ndarray, StandardScaler]:
        """ì•ˆì „í•œ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ì—†ìŒ)"""

        # í›ˆë ¨ ë°ì´í„°ë§Œìœ¼ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)

        # ê²€ì¦ ë°ì´í„°ëŠ” í›ˆë ¨ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
        X_val_scaled = scaler.transform(X_val)

        return X_train_scaled, X_val_scaled, scaler

    def prepare_safe_ml_data(self, data_path: str, target_type: str = 'return') -> Dict:
        """ì•ˆì „í•œ ML ë°ì´í„° ì¤€ë¹„ (ì „ì²´ íŒŒì´í”„ë¼ì¸)"""
        logger.info("=" * 80)
        logger.info("ğŸ›¡ï¸ ì•ˆì „í•œ ML ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
        logger.info("=" * 80)

        # 1. ë°ì´í„° ë¡œë”©
        df = self.load_data(data_path)

        # 2. ì•ˆì „í•œ íŠ¹ì§• ìƒì„±
        df = self.create_safe_features(df)

        # 3. ì•ˆì „í•œ íƒ€ê²Ÿ ìƒì„±
        target = self.create_safe_target(df, target_type)

        # 4. íŠ¹ì§• ì„ íƒ (ì•ˆì „í•œ íŠ¹ì§•ë§Œ)
        safe_feature_cols = [col for col in df.columns
                           if (col.endswith('_lag1') or
                               'momentum_' in col or
                               'crossover' in col or
                               col.endswith('_lag2') or
                               col.endswith('_lag3') or
                               col.endswith('_lag5') or
                               col.endswith('_lag10') or
                               'ratio_lag1' in col)]

        logger.info(f"ì„ íƒëœ ì•ˆì „í•œ íŠ¹ì§•: {len(safe_feature_cols)}ê°œ")

        # 5. íŠ¹ì§• ì•ˆì „ì„± ê²€ì¦
        is_safe = self.validate_feature_safety(df, safe_feature_cols, 'returns')

        if not is_safe:
            raise ValueError("íŠ¹ì§• ì•ˆì „ì„± ê²€ì¦ ì‹¤íŒ¨ - ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜")

        # 6. ê²°ì¸¡ê°’ ì²˜ë¦¬ ë° ë°ì´í„° ì •ë¦¬
        df['target'] = target
        df = df.dropna()

        if len(df) < 200:
            raise ValueError(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ: {len(df)}ê°œ (ìµœì†Œ 200ê°œ í•„ìš”)")

        X = df[safe_feature_cols].values
        y = df['target'].values

        logger.info(f"ìµœì¢… ë°ì´í„°: X{X.shape}, y{y.shape}")

        # 7. ì•ˆì „í•œ ì‹œê°„ ë¶„í• 
        splits = self.create_safe_splits(X, y, n_splits=3)

        # 8. ìµœì¢… ê²€ì¦
        self._final_safety_check(X, y, safe_feature_cols)

        logger.info("âœ… ì•ˆì „í•œ ML ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")

        return {
            'X': X,
            'y': y,
            'feature_names': safe_feature_cols,
            'splits': splits,
            'target_type': target_type,
            'data_shape': X.shape,
            'safety_validated': True
        }

    def _final_safety_check(self, X: np.ndarray, y: np.ndarray, feature_names: List[str]) -> None:
        """ìµœì¢… ì•ˆì „ì„± ê²€ì‚¬"""
        logger.info("=== ìµœì¢… ì•ˆì „ì„± ê²€ì‚¬ ===")

        # 1. ìƒê´€ê´€ê³„ ì¬ê²€ì¦
        max_corr = 0
        for i in range(X.shape[1]):
            corr = abs(np.corrcoef(X[:, i], y)[0, 1])
            if not np.isnan(corr):
                max_corr = max(max_corr, corr)

        logger.info(f"ìµœëŒ€ íŠ¹ì§•-íƒ€ê²Ÿ ìƒê´€ê´€ê³„: {max_corr:.3f}")

        if max_corr > self.MAX_CORRELATION:
            raise ValueError(f"ê³¼ë„í•œ ìƒê´€ê´€ê³„: {max_corr:.3f} > {self.MAX_CORRELATION}")

        # 2. ê¸°ë³¸ í†µê³„ ê²€ì¦
        logger.info("ê¸°ë³¸ í†µê³„:")
        logger.info(f"   íƒ€ê²Ÿ í‰ê· : {np.mean(y):.6f}")
        logger.info(f"   íƒ€ê²Ÿ í‘œì¤€í¸ì°¨: {np.std(y):.6f}")
        logger.info(f"   íŠ¹ì§• í‰ê·  ë²”ìœ„: [{np.mean(X, axis=0).min():.3f}, {np.mean(X, axis=0).max():.3f}]")

        # 3. ë¬´í•œê°’/NaN ê²€ì‚¬
        if np.any(np.isinf(X)) or np.any(np.isnan(X)):
            raise ValueError("íŠ¹ì§•ì— ë¬´í•œê°’ ë˜ëŠ” NaN í¬í•¨")

        if np.any(np.isinf(y)) or np.any(np.isnan(y)):
            raise ValueError("íƒ€ê²Ÿì— ë¬´í•œê°’ ë˜ëŠ” NaN í¬í•¨")

        logger.info("âœ… ìµœì¢… ì•ˆì „ì„± ê²€ì‚¬ í†µê³¼")

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    processor = SafeDataProcessor()

    data_path = '/root/workspace/data/training/sp500_2020_2024_enhanced.csv'

    try:
        # ì•ˆì „í•œ íšŒê·€ ë°ì´í„° ì¤€ë¹„
        result = processor.prepare_safe_ml_data(data_path, target_type='return')

        print(f"\nâœ… ì•ˆì „í•œ ë°ì´í„° ì¤€ë¹„ ì„±ê³µ:")
        print(f"   ë°ì´í„° í¬ê¸°: {result['data_shape']}")
        print(f"   íŠ¹ì§• ìˆ˜: {len(result['feature_names'])}")
        print(f"   ë¶„í•  ìˆ˜: {len(result['splits'])}")
        print(f"   ì•ˆì „ì„± ê²€ì¦: {result['safety_validated']}")

        return result

    except Exception as e:
        print(f"\nâŒ ì•ˆì „í•œ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    result = main()