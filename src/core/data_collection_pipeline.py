import pandas as pd
from datetime import datetime, timedelta
import os
import numpy as np
from textblob import TextBlob
import ta
from transformers import pipeline
import logging

from tqdm import tqdm
from src.core.api_config import APIManager
from src.utils.yfinance_manager import get_yfinance_manager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class SP500DataCollector:
    """
    S&P500 ì£¼ì‹ ê´€ë ¨ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì²˜ë¦¬, ê°€ê³µí•˜ì—¬ ëª¨ë¸ í›ˆë ¨ìš© ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤.

    ì£¼ìš” ê¸°ëŠ¥:
    1. S&P500 ì¢…ëª© í‹°ì»¤ ìˆ˜ì§‘
    2. ê°œë³„ ì¢…ëª©ì˜ ì£¼ê°€ ë°ì´í„° ë° ê±°ë˜ëŸ‰ ìˆ˜ì§‘ (yfinance)
    3. ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ê°ì„± ë¶„ì„ (NewsAPI, HuggingFace Transformers)
    4. ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (TA-Lib)
    5. ì´ë²¤íŠ¸ ë¼ë²¨ë§ (ê°€ê²©, ê±°ë˜ëŸ‰, ë³€ë™ì„± ê¸°ë°˜)
    6. ìµœì¢… í›ˆë ¨ìš© ë°ì´í„°ì…‹ ìƒì„± ë° ì €ì¥
    """

    def __init__(self, data_dir="data/raw"):
        """
        SP500DataCollector ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            data_dir (str): ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        """
        self.data_dir = data_dir
        self.sp500_tickers = []
        # ê¸ˆìœµ í…ìŠ¤íŠ¸ì— íŠ¹í™”ëœ FinBERT ëª¨ë¸ ë¡œë“œ
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis", model="ProsusAI/finbert"
        )
        self.api_manager = APIManager()  # APIManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

        # ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)

    def get_sp500_tickers(self):
        """
        S&P500 êµ¬ì„± ì¢…ëª©ì˜ ìµœì‹  í‹°ì»¤ ëª©ë¡ì„ ì›¹ì—ì„œ ê°€ì ¸ì™€ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        url = "https://datahub.io/core/s-and-p-500-companies/r/constituents.csv"
        try:
            df = pd.read_csv(url)
            self.sp500_tickers = df["Symbol"].tolist()
            df.to_csv(f"{self.data_dir}/sp500_constituents.csv", index=False)
            logging.info(f"S&P500 í‹°ì»¤ {len(self.sp500_tickers)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")
        except Exception as e:
            logging.error(f"S&P500 í‹°ì»¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")


    def collect_stock_data(self, period="1y", num_tickers=10):
        """
        ì§€ì •ëœ ê¸°ê°„ ë™ì•ˆì˜ ì£¼ê°€ ë° ê±°ë˜ëŸ‰ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        ì‹¤íŒ¨í•œ ê²½ìš° ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.

        Args:
            period (str): ìˆ˜ì§‘í•  ë°ì´í„° ê¸°ê°„ (e.g., '1y', '6mo').
            num_tickers (int): ìˆ˜ì§‘í•  í‹°ì»¤ì˜ ìˆ˜ (í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ ì‚¬ìš©).
        """
        if not self.sp500_tickers:
            self.get_sp500_tickers()

        # YFinanceManager ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        yf_manager = get_yfinance_manager()
        
        # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¼ë¶€ í‹°ì»¤ë§Œ ì‚¬ìš©
        tickers_to_fetch = self.sp500_tickers[:num_tickers]
        successful_collections = 0
        failed_collections = 0

        for ticker in tqdm(tickers_to_fetch, desc="Collecting stock data"):
            try:
                # ìƒˆë¡œìš´ YFinanceManagerë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘
                result = yf_manager.get_stock_history(ticker, period=period)
                
                if result['success']:
                    # ì„±ê³µì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ ê²½ìš°
                    hist_data = result['data']
                    hist_df = pd.DataFrame(hist_data)
                    
                    # Date ì»¬ëŸ¼ì´ ë¬¸ìì—´ì¸ ê²½ìš° datetimeìœ¼ë¡œ ë³€í™˜
                    if 'Date' in hist_df.columns:
                        hist_df['Date'] = pd.to_datetime(hist_df['Date'])
                    
                    hist_df.to_csv(f"{self.data_dir}/stock_{ticker}.csv", index=False)
                    logging.info(f"âœ… {ticker} ì£¼ê°€ ë°ì´í„° ì €ì¥ ì™„ë£Œ ({len(hist_data)}ê°œ ë ˆì½”ë“œ)")
                    successful_collections += 1
                    
                else:
                    # ì‹¤íŒ¨í•œ ê²½ìš° - ëª¨ì˜ ë°ì´í„° ìƒì„±í•˜ì§€ ì•Šê³  ëª…í™•í•œ ì˜¤ë¥˜ ë¡œê¹…
                    error_msg = result.get('message', 'Unknown error')
                    logging.error(f"âŒ {ticker} ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {error_msg}")
                    failed_collections += 1
                    
                    # ì‹¤íŒ¨í•œ í‹°ì»¤ë¥¼ ë³„ë„ íŒŒì¼ì— ê¸°ë¡
                    self._log_failed_collection(ticker, 'stock_data', error_msg)
                    
            except Exception as e:
                logging.error(f"âŒ {ticker} ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                failed_collections += 1
                self._log_failed_collection(ticker, 'stock_data', str(e))
        
        # ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
        total_attempted = len(tickers_to_fetch)
        success_rate = (successful_collections / total_attempted) * 100 if total_attempted > 0 else 0
        
        logging.info(f"ğŸ“Š ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {successful_collections}/{total_attempted} ì„±ê³µ ({success_rate:.1f}%)")
        
        if failed_collections > 0:
            logging.warning(f"âš ï¸ {failed_collections}ê°œ í‹°ì»¤ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨. ì‹¤íŒ¨ ëª©ë¡ì€ failed_collections.log ì°¸ì¡°")

    def calculate_technical_indicators(self, df):
        """
        ì£¼ê°€ ë°ì´í„°í”„ë ˆì„ì— ë‹¤ì–‘í•œ ê¸°ìˆ ì  ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        df_ti = df.copy()
        # ì´ë™í‰ê· , RSI, MACD ë“± ê¸°ë³¸ ì§€í‘œ ì¶”ê°€
        df_ti["sma_20"] = ta.trend.sma_indicator(df_ti["Close"], window=20)
        df_ti["sma_50"] = ta.trend.sma_indicator(df_ti["Close"], window=50)
        df_ti["rsi"] = ta.momentum.rsi(df_ti["Close"], window=14)
        df_ti["macd"] = ta.trend.macd_diff(df_ti["Close"])
        # ë³¼ë¦°ì € ë°´ë“œ
        df_ti["bb_upper"] = ta.volatility.bollinger_hband(df_ti["Close"])
        df_ti["bb_lower"] = ta.volatility.bollinger_lband(df_ti["Close"])
        # ë³€ë™ì„± ë° ê±°ë˜ëŸ‰ ê´€ë ¨ ì§€í‘œ
        df_ti["atr"] = ta.volatility.average_true_range(
            df_ti["High"], df_ti["Low"], df_ti["Close"]
        )
        df_ti["volatility"] = df_ti["Close"].rolling(window=20).std()
        df_ti["obv"] = ta.volume.on_balance_volume(df_ti["Close"], df_ti["Volume"])

        return df_ti

    def collect_news_and_sentiment(self, num_tickers=5):
        """
        APIManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³ , FinBERTì™€ TextBlobìœ¼ë¡œ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        ì‹¤íŒ¨í•œ ê²½ìš° ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
        """
        all_news = []
        tickers_to_fetch = self.sp500_tickers[:num_tickers]

        for ticker in tqdm(tickers_to_fetch, desc="Collecting news data"):
            articles = []
            try:
                # APIManagerë¥¼ í†µí•´ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
                articles = self.api_manager.get_news_data(ticker)
                if not articles:  # If API returns empty or fails
                    logging.warning(f"âš ï¸ {ticker} ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    self._log_failed_collection(ticker, 'news_data', 'No articles returned from API')
                    continue  # ë‹¤ìŒ í‹°ì»¤ë¡œ ë„˜ì–´ê°€ê¸°
            except Exception as e:
                logging.error(f"âŒ {ticker} ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                self._log_failed_collection(ticker, 'news_data', str(e))
                continue  # ë‹¤ìŒ í‹°ì»¤ë¡œ ë„˜ì–´ê°€ê¸°

            for article in articles:
                title = article.get("title", "")
                description = article.get("description", "")
                full_text = f"{title}. {description}"
                if not full_text.strip() or full_text == ". ":  # ". "ì¸ ê²½ìš°ë„ í•„í„°ë§
                    continue

                # FinBERT ë¶„ì„
                finbert_sentiment = self.sentiment_analyzer(full_text[:512])[0]
                # TextBlob ë¶„ì„
                blob = TextBlob(full_text)

                all_news.append(
                    {
                        "ticker": ticker,
                        "publishedAt": article.get("publishedAt"),
                        "date": (
                            datetime.fromisoformat(
                                article.get("publishedAt").replace("Z", "+00:00")
                            ).date()
                            if article.get("publishedAt")
                            else None
                        ),
                        "title": title,
                        "finbert_label": finbert_sentiment["label"],
                        "finbert_score": finbert_sentiment["score"],
                        "textblob_polarity": blob.sentiment.polarity,
                    }
                )
            logging.info(f"{ticker} ë‰´ìŠ¤ {len(articles)}ê°œ ìˆ˜ì§‘ ë° ë¶„ì„ ì™„ë£Œ.")

        news_df = pd.DataFrame(all_news)
        news_df.to_csv(f"{self.data_dir}/news_sentiment_data.csv", index=False)

    def _log_failed_collection(self, ticker, data_type, error_message):
        """
        ì‹¤íŒ¨í•œ ë°ì´í„° ìˆ˜ì§‘ì„ ë³„ë„ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤.
        """
        failed_log_path = os.path.join(self.data_dir, "failed_collections.log")
        timestamp = datetime.now().isoformat()
        
        with open(failed_log_path, "a", encoding="utf-8") as f:
            f.write(f"{timestamp} | {data_type} | {ticker} | {error_message}\n")
        
        logging.debug(f"ì‹¤íŒ¨í•œ ìˆ˜ì§‘ ê¸°ë¡ë¨: {ticker} ({data_type})")

    def create_training_dataset(self, num_tickers=10):
        """
        ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  ê°€ê³µí•˜ì—¬ ìµœì¢… í›ˆë ¨ìš© ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ì´ ê³¼ì •ì—ëŠ” ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€, ë‰´ìŠ¤ ê°ì„± ë°ì´í„° ë³‘í•©, ì´ë²¤íŠ¸ ë¼ë²¨ë§ì´ í¬í•¨ë©ë‹ˆë‹¤.
        """
        all_features = []
        all_labels = []

        tickers_to_process = self.sp500_tickers[:num_tickers]

        # ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
        try:
            news_df = pd.read_csv(f"{self.data_dir}/news_sentiment_data.csv")
            news_df["publishedAt"] = pd.to_datetime(news_df["publishedAt"]).dt.date
        except FileNotFoundError:
            news_df = pd.DataFrame()

        for ticker in tqdm(tickers_to_process):
            stock_file_path = f"{self.data_dir}/stock_{ticker}.csv"
            if not os.path.exists(stock_file_path):
                logging.warning(
                    f"{ticker}ì˜ ì£¼ê°€ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ í‹°ì»¤ëŠ” ê±´ë„ˆëœë‹ˆë‹¤."
                )
                continue

            try:
                # ì£¼ê°€ ë°ì´í„° ë¡œë“œ ë° ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                stock_df = pd.read_csv(stock_file_path, parse_dates=["Date"])
                logging.info(
                    f"Columns read from CSV for {ticker}: {stock_df.columns.tolist()}"
                )

                if stock_df.empty:
                    logging.warning(
                        f"{ticker}ì˜ ì£¼ê°€ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì´ í‹°ì»¤ëŠ” ê±´ë„ˆëœë‹ˆë‹¤."
                    )
                    continue

                stock_df["Date"] = pd.to_datetime(
                    stock_df["Date"], utc=True
                ).dt.tz_localize(None)
                stock_df_ti = self.calculate_technical_indicators(stock_df)
                stock_df_ti["date_key"] = stock_df_ti["Date"].dt.date

                # ë‰´ìŠ¤ ë°ì´í„°ì™€ ë³‘í•©
                if not news_df.empty:
                    ticker_news = news_df[news_df["ticker"] == ticker]
                    daily_sentiment = (
                        ticker_news.groupby("publishedAt")
                        .agg(
                            news_sentiment=("finbert_score", "mean"),
                            news_polarity=("textblob_polarity", "mean"),
                            news_count=("title", "count"),
                        )
                        .reset_index()
                    )
                    daily_sentiment.rename(
                        columns={"publishedAt": "date_key"}, inplace=True
                    )
                    stock_df_ti = pd.merge(
                        stock_df_ti, daily_sentiment, on="date_key", how="left"
                    )

                # ì´ë²¤íŠ¸ ë¼ë²¨ ìƒì„±
                stock_df_ti["price_change"] = stock_df_ti["Close"].pct_change()
                stock_df_ti["volume_change"] = stock_df_ti["Volume"].pct_change()
                stock_df_ti["unusual_volume"] = (
                    stock_df_ti["Volume"]
                    > stock_df_ti["Volume"].rolling(window=20).mean() * 2
                ).astype(int)
                stock_df_ti["price_spike"] = (
                    abs(stock_df_ti["price_change"]) > 0.05
                ).astype(int)

                # ì£¼ìš” ì´ë²¤íŠ¸ ì •ì˜: ê°€ê²© ìŠ¤íŒŒì´í¬ ë˜ëŠ” ì´ë¡€ì  ê±°ë˜ëŸ‰ ë°œìƒ ì‹œ
                stock_df_ti["major_event"] = (
                    (stock_df_ti["price_spike"] == 1)
                    | (stock_df_ti["unusual_volume"] == 1)
                ).astype(int)

                # ë°ì´í„° ì •ë¦¬
                stock_df_ti.fillna(0, inplace=True)
                stock_df_ti["ticker"] = ticker

                # íŠ¹ì„±ê³¼ ë¼ë²¨ ë¶„ë¦¬
                feature_cols = [
                    "ticker",
                    "Date",
                    "Open",
                    "High",
                    "Low",
                    "Close",
                    "Volume",
                    "sma_20",
                    "sma_50",
                    "rsi",
                    "macd",
                    "bb_upper",
                    "bb_lower",
                    "atr",
                    "volatility",
                    "obv",
                    "price_change",
                    "volume_change",
                    "unusual_volume",
                    "price_spike",
                    "news_sentiment",
                    "news_polarity",
                    "news_count",
                ]
                label_cols = [
                    "ticker",
                    "Date",
                    "major_event",
                    "price_spike",
                    "unusual_volume",
                ]

                # news ê´€ë ¨ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„
                for col in ["news_sentiment", "news_polarity", "news_count"]:
                    if col not in stock_df_ti:
                        stock_df_ti[col] = 0

                all_features.append(stock_df_ti[feature_cols])
                all_labels.append(stock_df_ti[label_cols])

            except Exception as e:
                logging.error(f"{ticker} ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

        # ìµœì¢… ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
        if all_features:
            features_df = pd.concat(all_features, ignore_index=True)
            labels_df = pd.concat(all_labels, ignore_index=True)

            features_df.to_csv(f"{self.data_dir}/training_features.csv", index=False)
            labels_df.to_csv(f"{self.data_dir}/event_labels.csv", index=False)
            logging.info("ìµœì¢… í›ˆë ¨ìš© íŠ¹ì„± ë° ë¼ë²¨ íŒŒì¼ ìƒì„± ì™„ë£Œ.")
        else:
            logging.warning(
                "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ í›ˆë ¨ìš© íŠ¹ì„± ë° ë¼ë²¨ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            )


if __name__ == "__main__":
    # --- ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---
    collector = SP500DataCollector()

    # 1. S&P500 í‹°ì»¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    collector.get_sp500_tickers()

    # 2. ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
    collector.collect_stock_data(num_tickers=10)  # ëª¨ë“  ì¢…ëª© ìˆ˜ì§‘

    # 3. ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ê°ì„± ë¶„ì„
    collector.collect_news_and_sentiment(num_tickers=10)

    # 4. ëª¨ë“  ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ìµœì¢… í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±
    collector.create_training_dataset(num_tickers=10)
