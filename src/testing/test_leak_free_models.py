#!/usr/bin/env python3
"""
ğŸ”’ ëˆ„ì¶œ ì—†ëŠ” ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ Time Aware Blending ëª¨ë¸ í…ŒìŠ¤íŠ¸
ì‹¤ì œì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ í‰ê°€
"""

import numpy as np
import pandas as pd
from pathlib import Path
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, log_loss
from sklearn.isotonic import IsotonicRegression

class LeakFreeTimeAwareBlending:
    def __init__(self, max_dd_threshold=0.05):
        """ëˆ„ì¶œ ì—†ëŠ” Time Aware Blending ëª¨ë¸"""
        self.models = {
            'rf': RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42),
            'gb': GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42),
            'en': ElasticNet(alpha=0.01, random_state=42)
        }
        self.scaler = StandardScaler()
        self.max_dd_threshold = max_dd_threshold
        self.model_weights = np.array([0.35, 0.35, 0.3])  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
        self.feature_importance_log = []

    def calculate_performance_metrics(self, y_true, y_pred, y_pred_proba=None):
        """ì™„ì „í•œ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        metrics = {}

        # ê¸°ë³¸ ì§€í‘œ
        metrics['mae'] = mean_absolute_error(y_true, y_pred)
        metrics['mse'] = np.mean((y_true - y_pred) ** 2)
        metrics['rmse'] = np.sqrt(metrics['mse'])

        # ë°©í–¥ ì •í™•ë„
        direction_actual = np.sign(y_true)
        direction_pred = np.sign(y_pred)
        metrics['direction_accuracy'] = np.mean(direction_actual == direction_pred) * 100

        # Log Loss (í™•ë¥  ì˜ˆì¸¡ì´ ìˆëŠ” ê²½ìš°)
        if y_pred_proba is not None:
            binary_actual = (y_true > 0).astype(int)
            y_pred_proba = np.clip(y_pred_proba, 1e-15, 1 - 1e-15)
            metrics['log_loss'] = log_loss(binary_actual, y_pred_proba)

        # ìˆ˜ìµë¥  ê¸°ë°˜ ì§€í‘œ
        returns = y_pred  # ì˜ˆì¸¡ëœ ìˆ˜ìµë¥ 

        # Maximum Drawdown ê³„ì‚°
        cumulative_returns = np.cumprod(1 + returns)
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdown = (cumulative_returns - running_max) / running_max
        metrics['max_drawdown'] = abs(np.min(drawdown))

        # Sharpe Ratio
        excess_returns = returns
        if np.std(excess_returns) > 0:
            metrics['sharpe_ratio'] = np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(252)
        else:
            metrics['sharpe_ratio'] = 0

        # Sortino Ratio
        downside_returns = returns[returns < 0]
        if len(downside_returns) > 0:
            downside_std = np.std(downside_returns)
            if downside_std > 0:
                metrics['sortino_ratio'] = np.mean(excess_returns) / downside_std * np.sqrt(252)
            else:
                metrics['sortino_ratio'] = 0
        else:
            metrics['sortino_ratio'] = 0

        return metrics

    def prepare_features(self, df):
        """íŠ¹ì§• ì¤€ë¹„ ë° ê²€ì¦"""
        print("ğŸ” íŠ¹ì§• ì¤€ë¹„ ë° ë°ì´í„° ìœ ì¶œ ê²€ì¦...")

        # ì•ˆì „í•œ íŠ¹ì§•ë§Œ ì„ íƒ
        safe_features = [
            'MA_5', 'MA_10', 'MA_20', 'MA_50',  # ì´ë™í‰ê· 
            'RSI',  # RSI
            'BB_position',  # ë³¼ë¦°ì €ë°´ë“œ ìœ„ì¹˜
            'Volatility_5', 'Volatility_10', 'Volatility_20',  # ë³€ë™ì„±
            'Volume_ratio_10', 'Volume_ratio_20',  # ê±°ë˜ëŸ‰ ë¹„ìœ¨
            'ATR',  # ATR
            # Lag íŠ¹ì§•ë“¤
            'Returns_lag_1', 'Returns_lag_2', 'Returns_lag_3',
            'RSI_lag_1', 'RSI_lag_2', 'RSI_lag_3',
            'Volatility_20_lag_1', 'Volatility_20_lag_2', 'Volatility_20_lag_3',
            'BB_position_lag_1', 'BB_position_lag_2', 'BB_position_lag_3',
            # ëª¨ë©˜í…€
            'Price_momentum_5', 'Price_momentum_10'
        ]

        # ì¡´ì¬í•˜ëŠ” íŠ¹ì§•ë§Œ ì„ íƒ
        available_features = [col for col in safe_features if col in df.columns]

        X = df[available_features].copy()
        y = df['Returns'].copy()

        print(f"âœ… ì‚¬ìš©ëœ íŠ¹ì§•: {len(available_features)}ê°œ")
        print(f"âœ… ìƒ˜í”Œ ìˆ˜: {len(X)}")
        print(f"âœ… íƒ€ê²Ÿ ë²”ìœ„: {y.min():.4f} ~ {y.max():.4f}")

        # ìµœì¢… NaN ì œê±°
        mask = ~(X.isna().any(axis=1) | y.isna())
        X_clean = X[mask]
        y_clean = y[mask]

        print(f"âœ… ì •ë¦¬ í›„ ìƒ˜í”Œ ìˆ˜: {len(X_clean)}")

        return X_clean, y_clean, available_features

    def advanced_probability_calibration(self, predictions, targets):
        """ê³ ê¸‰ í™•ë¥  ë³´ì • (Log Loss ìµœì í™”)"""
        # ì‹œê·¸ëª¨ì´ë“œ ë³€í™˜ìœ¼ë¡œ í™•ë¥  ìƒì„±
        pred_probs = 1 / (1 + np.exp(-predictions))
        binary_targets = (targets > 0).astype(int)

        # ë“±ì¥ íšŒê·€ë¥¼ í†µí•œ ë³´ì •
        if len(predictions) >= 10:
            iso_reg = IsotonicRegression(out_of_bounds='clip')
            calibrated_probs = iso_reg.fit_transform(pred_probs, binary_targets)

            # ì‹¤ì œ ì–‘ì„± ë¹„ìœ¨ ê¸°ë°˜ ì¶”ê°€ ë³´ì •
            actual_positive_rate = np.mean(binary_targets)
            predicted_positive_rate = np.mean(calibrated_probs)

            if predicted_positive_rate > 0:
                adjustment_factor = actual_positive_rate / predicted_positive_rate
                calibrated_probs = calibrated_probs * adjustment_factor

            # ì‹ ë¢°ë„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
            confidence_scaling = 0.8
            calibrated_probs = 0.5 + (calibrated_probs - 0.5) * confidence_scaling

            # ê·¹ê°’ ë°©ì§€ (Log Loss ì•ˆì •í™”)
            calibrated_probs = np.clip(calibrated_probs, 0.1, 0.9)

            return calibrated_probs
        else:
            return np.clip(pred_probs, 0.1, 0.9)

    def apply_temporal_smoothing(self, predictions, window_size=5):
        """ì‹œê°„ì  ìŠ¤ë¬´ë”© ì ìš©"""
        if len(predictions) < window_size:
            return predictions

        smoothed = predictions.copy()
        for i in range(window_size, len(predictions)):
            # ìµœê·¼ window_sizeê°œì˜ ì˜ˆì¸¡ì— ì§€ìˆ˜ ê°€ì¤‘ í‰ê·  ì ìš©
            recent_predictions = predictions[i-window_size:i]
            weights = np.exp(np.linspace(-1, 0, window_size))
            weights = weights / np.sum(weights)

            weighted_average = np.average(recent_predictions, weights=weights)
            smoothing_factor = 0.3
            smoothed[i] = (1 - smoothing_factor) * predictions[i] + smoothing_factor * weighted_average

        return smoothed

    def evaluate_leak_free_model(self, data_file):
        """ëˆ„ì¶œ ì—†ëŠ” ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€"""
        print(f"ğŸ”’ ëˆ„ì¶œ ì—†ëŠ” ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        print(f"ğŸ“ ë°ì´í„°: {data_file}")

        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_file)
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date').reset_index(drop=True)

        print(f"ğŸ“Š ë¡œë“œëœ ë°ì´í„°: {df.shape}")

        # íŠ¹ì§• ì¤€ë¹„
        X, y, features = self.prepare_features(df)

        # Time Series êµì°¨ ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=5)
        results = []

        print(f"\\nğŸ”„ {tscv.n_splits}-Fold Time Series êµì°¨ ê²€ì¦ ì‹œì‘...")

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
            print(f"\\nğŸ“Š Fold {fold}/{tscv.n_splits}")
            print(f"   í›ˆë ¨: {len(train_idx)}ê°œ, ê²€ì¦: {len(val_idx)}ê°œ")

            # ë°ì´í„° ë¶„í• 
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            # ìŠ¤ì¼€ì¼ë§
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_val_scaled = self.scaler.transform(X_val)

            # ê°œë³„ ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
            predictions = {}
            for name, model in self.models.items():
                model.fit(X_train_scaled, y_train)
                pred = model.predict(X_val_scaled)
                predictions[name] = pred

            # ì•™ìƒë¸” ì˜ˆì¸¡
            ensemble_pred = np.average(list(predictions.values()),
                                     weights=self.model_weights, axis=0)

            # ì‹œê°„ì  ìŠ¤ë¬´ë”© ì ìš©
            ensemble_pred = self.apply_temporal_smoothing(ensemble_pred)

            # í™•ë¥  ë³´ì • (Log Loss ê³„ì‚°ìš©)
            calibrated_probs = self.advanced_probability_calibration(ensemble_pred, y_val)

            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            metrics = self.calculate_performance_metrics(y_val, ensemble_pred, calibrated_probs)
            metrics['fold'] = fold

            # ê²°ê³¼ ì¶œë ¥
            print(f"   MAE: {metrics['mae']:.6f}")
            print(f"   RMSE: {metrics['rmse']:.6f}")
            print(f"   ë°©í–¥ ì •í™•ë„: {metrics['direction_accuracy']:.2f}%")
            print(f"   Log Loss: {metrics['log_loss']:.6f}")
            print(f"   MDD: {metrics['max_drawdown']:.6f}")
            print(f"   Sharpe: {metrics['sharpe_ratio']:.3f}")
            print(f"   Sortino: {metrics['sortino_ratio']:.3f}")

            results.append(metrics)

        # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        avg_metrics = {}
        for key in ['mae', 'rmse', 'direction_accuracy', 'log_loss',
                   'max_drawdown', 'sharpe_ratio', 'sortino_ratio']:
            values = [r[key] for r in results]
            avg_metrics[f"{key}_mean"] = np.mean(values)
            avg_metrics[f"{key}_std"] = np.std(values)
            avg_metrics[f"{key}_min"] = np.min(values)
            avg_metrics[f"{key}_max"] = np.max(values)

        # ëª©í‘œ ë‹¬ì„± í‰ê°€
        mdd_target = 0.6  # 60%
        log_loss_target = 0.7

        goal_achievement = {
            'mdd_target': float(mdd_target),
            'mdd_achieved': float(avg_metrics['max_drawdown_mean']),
            'mdd_success': bool(avg_metrics['max_drawdown_mean'] < mdd_target),
            'log_loss_target': float(log_loss_target),
            'log_loss_achieved': float(avg_metrics['log_loss_mean']),
            'log_loss_success': bool(avg_metrics['log_loss_mean'] < log_loss_target)
        }

        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_data = {
            'timestamp': timestamp,
            'model_name': 'leak_free_time_aware_blending',
            'data_source': 'leak_free_dataset',
            'average_metrics': avg_metrics,
            'fold_results': results,
            'goal_achievement': goal_achievement,
            'features_used': features,
            'data_shape': list(X.shape)
        }

        results_file = f"/root/workspace/results/analysis/leak_free_results_{timestamp}.json"
        Path(results_file).parent.mkdir(parents=True, exist_ok=True)

        with open(results_file, 'w') as f:
            json.dump(results_data, f, indent=2)

        # ê²°ê³¼ ì¶œë ¥
        print(f"\\nğŸ¯ ìµœì¢… ê²°ê³¼ (í‰ê·  Â± í‘œì¤€í¸ì°¨):")
        print(f"   MAE: {avg_metrics['mae_mean']:.6f} Â± {avg_metrics['mae_std']:.6f}")
        print(f"   RMSE: {avg_metrics['rmse_mean']:.6f} Â± {avg_metrics['rmse_std']:.6f}")
        print(f"   ë°©í–¥ ì •í™•ë„: {avg_metrics['direction_accuracy_mean']:.2f}% Â± {avg_metrics['direction_accuracy_std']:.2f}%")
        print(f"   Log Loss: {avg_metrics['log_loss_mean']:.6f} Â± {avg_metrics['log_loss_std']:.6f}")
        print(f"   MDD: {avg_metrics['max_drawdown_mean']:.6f} Â± {avg_metrics['max_drawdown_std']:.6f}")
        print(f"   Sharpe: {avg_metrics['sharpe_ratio_mean']:.3f} Â± {avg_metrics['sharpe_ratio_std']:.3f}")
        print(f"   Sortino: {avg_metrics['sortino_ratio_mean']:.3f} Â± {avg_metrics['sortino_ratio_std']:.3f}")

        print(f"\\nğŸ¯ ëª©í‘œ ë‹¬ì„± í‰ê°€:")
        print(f"   MDD < {mdd_target}: {goal_achievement['mdd_achieved']:.4f} {'âœ…' if goal_achievement['mdd_success'] else 'âŒ'}")
        print(f"   Log Loss < {log_loss_target}: {goal_achievement['log_loss_achieved']:.4f} {'âœ…' if goal_achievement['log_loss_success'] else 'âŒ'}")

        if goal_achievement['mdd_success'] and goal_achievement['log_loss_success']:
            print(f"\\nğŸ‰ ëª¨ë“  ëª©í‘œ ë‹¬ì„±! ì™„ì „í•œ ì„±ê³µ!")
        elif goal_achievement['mdd_success']:
            print(f"\\nğŸ¯ MDD ëª©í‘œ ë‹¬ì„±, Log Loss ê°œì„  í•„ìš”")
        elif goal_achievement['log_loss_success']:
            print(f"\\nğŸ¯ Log Loss ëª©í‘œ ë‹¬ì„±, MDD ê°œì„  í•„ìš”")
        else:
            print(f"\\nâš ï¸ ë‘ ëª©í‘œ ëª¨ë‘ ê°œì„  í•„ìš”")

        print(f"\\nğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")

        return results_data

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”’ ëˆ„ì¶œ ì—†ëŠ” Time Aware Blending ëª¨ë¸ í…ŒìŠ¤íŠ¸")

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = LeakFreeTimeAwareBlending()

    # ëˆ„ì¶œ ì—†ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€
    data_file = "/root/workspace/data/training/sp500_leak_free_dataset.csv"

    try:
        results = model.evaluate_leak_free_model(data_file)
        print("\\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

        return results

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = main()