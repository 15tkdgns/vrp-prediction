#!/usr/bin/env python3
"""
ê°œì„ ëœ ëª¨ë¸ì„ í›ˆë ¨ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (ì‹ ë¢°ë„ í™•ì¸)
"""

import os
import json
import numpy as np
import pandas as pd
import joblib
from datetime import datetime
import logging
from tensorflow.keras.models import load_model
import warnings
warnings.filterwarnings('ignore')


def test_improved_models_with_training_data():
    """ê°œì„ ëœ ëª¨ë¸ì„ í›ˆë ¨ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¯ ê°œì„ ëœ ëª¨ë¸ ì‹ ë¢°ë„ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    models_dir = "data/models"
    data_dir = "data/raw"
    
    try:
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        scaler = joblib.load(f"{models_dir}/scaler_improved.pkl")
        print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        
        # ëª¨ë¸ë“¤ ë¡œë“œ
        models = {}
        models['random_forest'] = joblib.load(f"{models_dir}/random_forest_improved_model.pkl")
        models['gradient_boosting'] = joblib.load(f"{models_dir}/gradient_boosting_improved_model.pkl")
        models['lstm'] = load_model(f"{models_dir}/lstm_improved_model.h5")
        print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        features_df = pd.read_csv(f"{data_dir}/training_features.csv")
        labels_df = pd.read_csv(f"{data_dir}/event_labels.csv")
        
        # ë°ì´í„° ë³‘í•©
        merged_df = pd.merge(features_df, labels_df, on=["ticker", "Date"], how="inner")
        
        # íŠ¹ì„± ì„ íƒ (ìˆ«ìí˜•ë§Œ)
        numeric_columns = merged_df.select_dtypes(include=[np.number]).columns.tolist()
        target_columns = ['major_event', 'price_spike', 'unusual_volume']
        feature_columns = [col for col in numeric_columns if col not in target_columns]
        
        X = merged_df[feature_columns].fillna(0)
        y = merged_df['major_event']
        
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ")
        print(f"ğŸ¯ ì‹¤ì œ ì´ë²¤íŠ¸ ë¹„ìœ¨: {y.mean():.3f}")
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        X_scaled = scaler.transform(X)
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ ë° ì‹ ë¢°ë„ ë¶„ì„
        results = {}
        
        for model_name, model in models.items():
            print(f"\nğŸ¤– {model_name.upper()} ì‹ ë¢°ë„ ë¶„ì„:")
            print("-" * 30)
            
            if model_name == 'lstm':
                X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))
                predictions = model.predict(X_lstm, verbose=0).flatten()
            else:
                predictions = model.predict_proba(X_scaled)[:, 1]
            
            # ì‹ ë¢°ë„ í†µê³„
            avg_confidence = np.mean(predictions)
            confidence_std = np.std(predictions)
            median_confidence = np.median(predictions)
            min_confidence = np.min(predictions)
            max_confidence = np.max(predictions)
            
            # ì´ë²¤íŠ¸ë³„ ì‹ ë¢°ë„
            event_predictions = predictions[y == 1]
            normal_predictions = predictions[y == 0]
            
            event_avg = np.mean(event_predictions) if len(event_predictions) > 0 else 0
            normal_avg = np.mean(normal_predictions) if len(normal_predictions) > 0 else 0
            
            # ì‹ ë¢°ë„ êµ¬ê°„ë³„ ë¶„í¬
            low_conf = np.sum(predictions < 0.2) / len(predictions)
            mid_conf = np.sum((predictions >= 0.2) & (predictions <= 0.8)) / len(predictions)
            high_conf = np.sum(predictions > 0.8) / len(predictions)
            
            results[model_name] = {
                'avg_confidence': float(avg_confidence),
                'confidence_std': float(confidence_std),
                'median_confidence': float(median_confidence),
                'min_confidence': float(min_confidence),
                'max_confidence': float(max_confidence),
                'event_avg_confidence': float(event_avg),
                'normal_avg_confidence': float(normal_avg),
                'confidence_distribution': {
                    'low_confidence_pct': float(low_conf),
                    'mid_confidence_pct': float(mid_conf),
                    'high_confidence_pct': float(high_conf)
                }
            }
            
            print(f"  í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.4f} Â± {confidence_std:.4f}")
            print(f"  ì¤‘ì•™ê°’: {median_confidence:.4f}")
            print(f"  ë²”ìœ„: {min_confidence:.4f} ~ {max_confidence:.4f}")
            print(f"  ì´ë²¤íŠ¸ì‹œ í‰ê· : {event_avg:.4f}")
            print(f"  ì •ìƒì‹œ í‰ê· : {normal_avg:.4f}")
            print(f"  ì‹ ë¢°ë„ ë¶„í¬:")
            print(f"    ë‚®ìŒ (<0.2): {low_conf*100:.1f}%")
            print(f"    ì¤‘ê°„ (0.2-0.8): {mid_conf*100:.1f}%")
            print(f"    ë†’ìŒ (>0.8): {high_conf*100:.1f}%")
            
            # ê·¹ë‹¨ì ì¸ ì˜ˆì¸¡ ìƒ˜í”Œ í‘œì‹œ
            high_indices = np.argsort(predictions)[-3:][::-1]
            print(f"  ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ 3ê°œ:")
            for i, idx in enumerate(high_indices):
                row = merged_df.iloc[idx]
                actual = "ì´ë²¤íŠ¸" if y.iloc[idx] == 1 else "ì •ìƒ"
                print(f"    {i+1}. {row['ticker']} - {predictions[idx]:.4f} (ì‹¤ì œ: {actual})")
        
        # ê²°ê³¼ ì €ì¥
        test_results = {
            'test_timestamp': datetime.now().isoformat(),
            'test_info': {
                'samples': len(X),
                'features': len(feature_columns),
                'actual_event_rate': float(y.mean())
            },
            'model_results': results
        }
        
        with open(f"{data_dir}/improved_model_confidence_test.json", "w") as f:
            json.dump(test_results, f, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {data_dir}/improved_model_confidence_test.json")
        
        # ëª¨ë¸ ë¹„êµ ë° í‰ê°€
        print(f"\nğŸ“Š ëª¨ë¸ ì‹ ë¢°ë„ ë¹„êµ:")
        print("-" * 40)
        
        for model_name in results:
            r = results[model_name]
            # í˜„ì‹¤ì ì¸ ì‹ ë¢°ë„ ì ìˆ˜ (0.1-0.3 ì‚¬ì´ê°€ ì´ìƒì )
            realism_score = 1.0 - abs(r['avg_confidence'] - 0.15)  # 0.15ë¥¼ ì´ìƒì ìœ¼ë¡œ ê°€ì •
            
            print(f"{model_name.upper()}:")
            print(f"  í‰ê·  ì‹ ë¢°ë„: {r['avg_confidence']:.4f}")
            print(f"  í‘œì¤€í¸ì°¨: {r['confidence_std']:.4f}")
            print(f"  í˜„ì‹¤ì„± ì ìˆ˜: {realism_score:.4f}")
            print()
        
        # ê°€ì¥ í˜„ì‹¤ì ì¸ ëª¨ë¸ ì°¾ê¸°
        best_model = min(results.keys(), 
                        key=lambda x: abs(results[x]['avg_confidence'] - 0.15))
        
        print(f"ğŸ† ê°€ì¥ í˜„ì‹¤ì ì¸ ëª¨ë¸: {best_model.upper()}")
        print(f"   í‰ê·  ì‹ ë¢°ë„: {results[best_model]['avg_confidence']:.4f}")
        print(f"   í‘œì¤€í¸ì°¨: {results[best_model]['confidence_std']:.4f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    success = test_improved_models_with_training_data()
    
    if success:
        print("\nâœ… ê°œì„ ëœ ëª¨ë¸ ì‹ ë¢°ë„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("   ì´ì œ í˜„ì‹¤ì ì¸ ì‹ ë¢°ë„ë¡œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")