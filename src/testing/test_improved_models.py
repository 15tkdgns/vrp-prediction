#!/usr/bin/env python3
"""
ê°œì„ ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
í˜„ì‹¤ì ì¸ ì‹ ë¢°ë„ë¡œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
"""

import os
import json
import numpy as np
import pandas as pd
import yfinance as yf
import joblib
from datetime import datetime, timedelta
import logging
from tensorflow.keras.models import load_model
import warnings
warnings.filterwarnings('ignore')


class ImprovedModelTester:
    def __init__(self, data_dir="data/raw", models_dir="data/models"):
        self.data_dir = data_dir
        self.models_dir = models_dir
        self.models = {}
        self.scaler = None
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
        self.logger = logging.getLogger(__name__)

    def load_improved_models(self):
        """ê°œì„ ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        print("ğŸ”„ ê°œì„ ëœ ëª¨ë¸ ë¡œë”©...")
        
        try:
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            scaler_path = f"{self.models_dir}/scaler_improved.pkl"
            if os.path.exists(scaler_path):
                self.scaler = joblib.load(scaler_path)
                print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            else:
                print("âš ï¸ ê°œì„ ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©")
                self.scaler = joblib.load(f"{self.models_dir}/scaler.pkl")
            
            # Random Forest ëª¨ë¸
            rf_path = f"{self.models_dir}/random_forest_improved_model.pkl"
            if os.path.exists(rf_path):
                self.models['random_forest'] = joblib.load(rf_path)
                print("âœ… ê°œì„ ëœ Random Forest ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # Gradient Boosting ëª¨ë¸
            gb_path = f"{self.models_dir}/gradient_boosting_improved_model.pkl"
            if os.path.exists(gb_path):
                self.models['gradient_boosting'] = joblib.load(gb_path)
                print("âœ… ê°œì„ ëœ Gradient Boosting ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # LSTM ëª¨ë¸
            lstm_path = f"{self.models_dir}/lstm_improved_model.h5"
            if os.path.exists(lstm_path):
                self.models['lstm'] = load_model(lstm_path)
                print("âœ… ê°œì„ ëœ LSTM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            if not self.models:
                raise FileNotFoundError("ê°œì„ ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def get_realtime_data(self, tickers=['AAPL', 'MSFT', 'GOOGL'], days=5):
        """ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìµœê·¼ ë°ì´í„° ìˆ˜ì§‘"""
        print(f"ğŸ“Š ìµœê·¼ {days}ì¼ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        
        # ì£¼ë§/íœ´ì¼ ëŒ€ì‘ì„ ìœ„í•´ ë” ê¸´ ê¸°ê°„ ì„¤ì •
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days+60)  # ë” ë§ì€ ë°ì´í„° í™•ë³´
        
        all_data = []
        
        for ticker in tickers:
            try:
                stock = yf.Ticker(ticker)
                data = stock.history(start=start_date, end=end_date)
                
                if data.empty:
                    continue
                
                # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼)
                data['Returns'] = data['Close'].pct_change()
                data['Volatility'] = data['Returns'].rolling(window=5).std()
                data['Volume_MA'] = data['Volume'].rolling(window=20).mean()
                data['Price_MA_5'] = data['Close'].rolling(window=5).mean()
                data['Price_MA_20'] = data['Close'].rolling(window=20).mean()
                data['Price_MA_50'] = data['Close'].rolling(window=50).mean()
                
                # ì¶”ê°€ ê¸°ìˆ ì  ì§€í‘œ
                data['RSI'] = self.calculate_rsi(data['Close'])
                data['MACD'], data['MACD_Signal'] = self.calculate_macd(data['Close'])
                data['BB_Upper'], data['BB_Lower'] = self.calculate_bollinger_bands(data['Close'])
                data['ATR'] = self.calculate_atr(data)
                
                data['Price_Change'] = data['Returns'].abs()
                data['Volume_Spike'] = data['Volume'] / data['Volume_MA']
                
                data['ticker'] = ticker
                data = data.reset_index()
                data = data.dropna()
                
                # ìµœê·¼ ë°ì´í„°ë§Œ ì„ íƒ (ìµœì†Œ 1ê°œ ë³´ì¥)
                recent_data = data.tail(max(days, 10))  # ìµœì†Œ 10ê°œ ë°ì´í„° í™•ë³´
                all_data.append(recent_data)
                
                print(f"âœ… {ticker}: {len(recent_data)}ê°œ ìµœì‹  ë ˆì½”ë“œ")
                
            except Exception as e:
                print(f"âŒ {ticker} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        if not all_data:
            return None
        
        return pd.concat(all_data, ignore_index=True)

    def calculate_rsi(self, prices, window=14):
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi

    def calculate_macd(self, prices, fast=12, slow=26, signal=9):
        exp1 = prices.ewm(span=fast).mean()
        exp2 = prices.ewm(span=slow).mean()
        macd = exp1 - exp2
        signal_line = macd.ewm(span=signal).mean()
        return macd, signal_line

    def calculate_bollinger_bands(self, prices, window=20, num_std=2):
        rolling_mean = prices.rolling(window=window).mean()
        rolling_std = prices.rolling(window=window).std()
        upper_band = rolling_mean + (rolling_std * num_std)
        lower_band = rolling_mean - (rolling_std * num_std)
        return upper_band, lower_band

    def calculate_atr(self, data, window=14):
        high_low = data['High'] - data['Low']
        high_close = np.abs(data['High'] - data['Close'].shift())
        low_close = np.abs(data['Low'] - data['Close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = np.max(ranges, axis=1)
        atr = true_range.rolling(window=window).mean()
        return atr

    def prepare_test_features(self, data):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì„± ì¤€ë¹„"""
        feature_columns = [
            'Open', 'High', 'Low', 'Close', 'Volume',
            'Returns', 'Volatility', 'Volume_MA', 'Price_MA_5', 'Price_MA_20', 'Price_MA_50',
            'RSI', 'MACD', 'MACD_Signal', 'BB_Upper', 'BB_Lower', 'ATR',
            'Price_Change', 'Volume_Spike'
        ]
        
        X = data[feature_columns].fillna(0)
        return X

    def run_realtime_test(self):
        """ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\nğŸ¯ ê°œì„ ëœ ëª¨ë¸ ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 50)
        
        if not self.load_improved_models():
            return False
        
        # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
        test_data = self.get_realtime_data()
        if test_data is None:
            print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # íŠ¹ì„± ì¤€ë¹„
        X_test = self.prepare_test_features(test_data)
        X_test_scaled = self.scaler.transform(X_test)
        
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)}ê°œ ìƒ˜í”Œ")
        print(f"ğŸ“ˆ ì¢…ëª©: {', '.join(test_data['ticker'].unique())}")
        print(f"ğŸ“… ê¸°ê°„: {test_data['Date'].min().strftime('%Y-%m-%d')} ~ {test_data['Date'].max().strftime('%Y-%m-%d')}")
        
        # ëª¨ë¸ë³„ ì˜ˆì¸¡ ê²°ê³¼
        results = {}
        
        for model_name, model in self.models.items():
            print(f"\nğŸ¤– {model_name.upper()} ì˜ˆì¸¡ ê²°ê³¼:")
            print("-" * 30)
            
            if model_name == 'lstm':
                # LSTMìš© ë°ì´í„° reshape
                X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
                predictions = model.predict(X_test_lstm, verbose=0).flatten()
            else:
                predictions = model.predict_proba(X_test_scaled)[:, 1]
            
            # ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„
            avg_confidence = np.mean(predictions)
            confidence_std = np.std(predictions)
            max_confidence = np.max(predictions)
            min_confidence = np.min(predictions)
            high_confidence_count = np.sum(predictions > 0.5)
            
            results[model_name] = {
                'predictions': predictions.tolist(),
                'avg_confidence': float(avg_confidence),
                'confidence_std': float(confidence_std),
                'max_confidence': float(max_confidence),
                'min_confidence': float(min_confidence),
                'high_confidence_count': int(high_confidence_count),
                'total_samples': len(predictions)
            }
            
            print(f"  í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.4f} Â± {confidence_std:.4f}")
            print(f"  ì‹ ë¢°ë„ ë²”ìœ„: {min_confidence:.4f} ~ {max_confidence:.4f}")
            print(f"  ê³ ì‹ ë¢°ë„ ì˜ˆì¸¡ (>0.5): {high_confidence_count}ê°œ ({high_confidence_count/len(predictions)*100:.1f}%)")
            
            # ìƒìœ„ ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ
            top_indices = np.argsort(predictions)[-3:][::-1]
            print(f"  ìƒìœ„ 3ê°œ ì˜ˆì¸¡:")
            for i, idx in enumerate(top_indices):
                row = test_data.iloc[idx]
                print(f"    {i+1}. {row['ticker']} ({row['Date'].strftime('%m-%d')}): {predictions[idx]:.4f}")
        
        # ê²°ê³¼ ì €ì¥
        test_results = {
            'test_timestamp': datetime.now().isoformat(),
            'test_data_info': {
                'samples': len(test_data),
                'tickers': test_data['ticker'].unique().tolist(),
                'date_range': {
                    'start': test_data['Date'].min().isoformat(),
                    'end': test_data['Date'].max().isoformat()
                }
            },
            'model_results': results
        }
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        with open(f"{self.data_dir}/improved_realtime_test_results.json", "w") as f:
            json.dump(test_results, f, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {self.data_dir}/improved_realtime_test_results.json")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ìš”ì•½:")
        print(f"  í…ŒìŠ¤íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  ì´ ìƒ˜í”Œ: {len(test_data)}ê°œ")
        print(f"  í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸: {len(results)}ê°œ")
        
        best_model = min(results.keys(), key=lambda x: abs(results[x]['avg_confidence'] - 0.15))  # 0.15ì— ê°€ì¥ ê°€ê¹Œìš´ ëª¨ë¸
        print(f"  ê°€ì¥ í˜„ì‹¤ì ì¸ ëª¨ë¸: {best_model} (í‰ê·  ì‹ ë¢°ë„: {results[best_model]['avg_confidence']:.4f})")
        
        return True


if __name__ == "__main__":
    tester = ImprovedModelTester()
    success = tester.run_realtime_test()
    
    if success:
        print("\nâœ… ê°œì„ ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("   ì´ì œ í˜„ì‹¤ì ì¸ ì‹ ë¢°ë„ë¡œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")