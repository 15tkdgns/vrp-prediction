#!/usr/bin/env python3
"""
ğŸ”’ ì™„ì „ ëˆ„ì¶œ ë°©ì§€ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì‹¤í—˜

ë‹¤ì–‘í•œ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ë°ì´í„° ëˆ„ì¶œ ì—†ì´ ì•ˆì „í•˜ê²Œ í…ŒìŠ¤íŠ¸
"""

import sys
sys.path.append('/root/workspace/src')

import pandas as pd
import numpy as np
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# Core imports
from core.data_processor import DataProcessor

# Deep Learning imports
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score

# TensorFlow imports for comparison
try:
    import tensorflow as tf
    from tensorflow.keras import layers, models
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("âš ï¸ TensorFlow ë¯¸ì„¤ì¹˜ - TensorFlow ëª¨ë¸ ì œì™¸")

class SafeNeuralNetworks:
    """ì™„ì „ ëˆ„ì¶œ ë°©ì§€ ì‹ ê²½ë§ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.data_processor = DataProcessor()
        self.max_allowed_correlation = 0.25
        self.realistic_performance_max = 0.7
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"ğŸ”’ ì•ˆì „í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì‹¤í—˜ ì‹œìŠ¤í…œ")
        print(f"   ğŸš¨ ìµœëŒ€ í—ˆìš© ìƒê´€ê´€ê³„: {self.max_allowed_correlation}")
        print(f"   ğŸ“Š í˜„ì‹¤ì  ì„±ëŠ¥ ìƒí•œ: {self.realistic_performance_max}")
        print(f"   ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")

    def create_safe_features(self, df):
        """ì•ˆì „í•œ íŠ¹ì„± ìƒì„±"""
        print("ğŸ”’ ì•ˆì „í•œ ì‹ ê²½ë§ íŠ¹ì„± ìƒì„±...")

        safe_df = df.copy()

        # ê¸°ë³¸ ìˆ˜ìµë¥ 
        safe_df['returns'] = safe_df['Close'].pct_change()

        # ì•ˆì „í•œ ê³¼ê±° ê¸°ìˆ ì  ì§€í‘œ
        for period in [5, 10, 20, 30]:
            # ëª¨ë©˜í…€ (ê³¼ê±°ë§Œ)
            safe_df[f'momentum_past_{period}'] = (
                safe_df['Close'] / safe_df['Close'].shift(period) - 1
            )

            # ë³€ë™ì„± (ê³¼ê±°ë§Œ)
            safe_df[f'volatility_past_{period}'] = (
                safe_df['returns'].rolling(period).std()
            )

            # ì´ë™í‰ê·  ë¹„ìœ¨ (ê³¼ê±°ë§Œ)
            safe_df[f'sma_ratio_past_{period}'] = (
                safe_df['Close'] / safe_df['Close'].rolling(period).mean()
            )

        # ë³¼ë¥¨ ê¸°ë°˜ íŠ¹ì„±
        for period in [10, 20]:
            safe_df[f'volume_sma_{period}'] = safe_df['Volume'].rolling(period).mean()
            safe_df[f'volume_ratio_past_{period}'] = (
                safe_df['Volume'] / safe_df[f'volume_sma_{period}']
            )

        # ê°€ê²© ë ˆì¸ì§€ íŠ¹ì„±
        safe_df['hl_range'] = (safe_df['High'] - safe_df['Low']) / safe_df['Close']
        safe_df['oc_change'] = (safe_df['Close'] - safe_df['Open']) / safe_df['Open']

        # ì•ˆì „í•œ ë˜ê·¸ íŠ¹ì„±
        for lag in [1, 2, 3, 5]:
            safe_df[f'returns_lag_{lag}'] = safe_df['returns'].shift(lag)

        # íƒ€ê²Ÿ ë³€ìˆ˜ (ë¯¸ë˜ ì •ë³´, ìœ ì¼í•œ ì˜ˆì™¸)
        safe_df['future_return'] = safe_df['Close'].pct_change().shift(-1)
        safe_df['direction_target'] = (safe_df['future_return'] > 0).astype(int)

        # NaN ì²˜ë¦¬
        safe_df = safe_df.fillna(method='ffill').fillna(0)
        safe_df = safe_df.replace([np.inf, -np.inf], 0)

        print(f"   âœ… ì‹ ê²½ë§ ì•ˆì „ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {safe_df.shape}")
        return safe_df

    def validate_neural_safety(self, df):
        """ì‹ ê²½ë§ìš© ì•ˆì „ì„± ê²€ì¦"""
        print("ğŸ” ì‹ ê²½ë§ ë°ì´í„° ëˆ„ì¶œ ê²€ì¦...")

        safe_features = []
        for col in df.columns:
            if col not in ['direction_target', 'future_return', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']:
                safe_features.append(col)

        print(f"   ê²€ì¦í•  íŠ¹ì„± ìˆ˜: {len(safe_features)}")

        # ìƒê´€ê´€ê³„ ê²€ì‚¬
        suspicious_features = []
        for feature in safe_features:
            if feature in df.columns:
                corr = abs(df[feature].corr(df['direction_target']))
                if not pd.isna(corr):
                    if corr > self.max_allowed_correlation:
                        suspicious_features.append((feature, corr))
                        print(f"   âš ï¸ ì˜ì‹¬ íŠ¹ì„±: {feature} (ìƒê´€ê´€ê³„: {corr:.4f})")

        if suspicious_features:
            print(f"   ğŸš¨ ì˜ì‹¬ íŠ¹ì„± {len(suspicious_features)}ê°œ ì œê±°!")
            for feature, _ in suspicious_features:
                if feature in safe_features:
                    safe_features.remove(feature)
        else:
            print("   âœ… ëª¨ë“  íŠ¹ì„±ì´ ì‹ ê²½ë§ ì•ˆì „ ê¸°ì¤€ í†µê³¼")

        return safe_features

    class SimpleNN(nn.Module):
        """ë‹¨ìˆœ ì‹ ê²½ë§"""
        def __init__(self, input_size):
            super().__init__()
            self.network = nn.Sequential(
                nn.Linear(input_size, 64),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(32, 1),
                nn.Sigmoid()
            )

        def forward(self, x):
            return self.network(x)

    class DeepNN(nn.Module):
        """ê¹Šì€ ì‹ ê²½ë§"""
        def __init__(self, input_size):
            super().__init__()
            self.network = nn.Sequential(
                nn.Linear(input_size, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.4),

                nn.Linear(128, 96),
                nn.BatchNorm1d(96),
                nn.ReLU(),
                nn.Dropout(0.3),

                nn.Linear(96, 64),
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.Dropout(0.3),

                nn.Linear(64, 32),
                nn.BatchNorm1d(32),
                nn.ReLU(),
                nn.Dropout(0.2),

                nn.Linear(32, 1),
                nn.Sigmoid()
            )

        def forward(self, x):
            return self.network(x)

    class WideNN(nn.Module):
        """ë„“ì€ ì‹ ê²½ë§"""
        def __init__(self, input_size):
            super().__init__()
            self.network = nn.Sequential(
                nn.Linear(input_size, 256),
                nn.ReLU(),
                nn.Dropout(0.4),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 1),
                nn.Sigmoid()
            )

        def forward(self, x):
            return self.network(x)

    class ResidualNN(nn.Module):
        """ì”ì°¨ ì—°ê²° ì‹ ê²½ë§"""
        def __init__(self, input_size):
            super().__init__()
            self.input_layer = nn.Linear(input_size, 64)
            self.hidden1 = nn.Linear(64, 64)
            self.hidden2 = nn.Linear(64, 64)
            self.output_layer = nn.Linear(64, 1)
            self.dropout = nn.Dropout(0.3)
            self.relu = nn.ReLU()
            self.sigmoid = nn.Sigmoid()

        def forward(self, x):
            x = self.relu(self.input_layer(x))

            # ì²« ë²ˆì§¸ ì”ì°¨ ë¸”ë¡
            residual = x
            x = self.relu(self.hidden1(x))
            x = self.dropout(x)
            x = self.hidden2(x) + residual  # ì”ì°¨ ì—°ê²°
            x = self.relu(x)

            x = self.sigmoid(self.output_layer(x))
            return x

    def train_pytorch_model(self, model, X_train, y_train, X_val, y_val, model_name):
        """PyTorch ëª¨ë¸ í›ˆë ¨"""
        # ë°ì´í„° ì¤€ë¹„
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train),
            torch.FloatTensor(y_train)
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(X_val),
            torch.FloatTensor(y_val)
        )

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)  # ì‹œê°„ ìˆœì„œ ìœ ì§€
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        model = model.to(self.device)

        # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤í•¨ìˆ˜
        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
        criterion = nn.BCELoss()

        # ì¡°ê¸° ì¤‘ë‹¨ ì„¤ì •
        best_val_loss = float('inf')
        patience = 10
        patience_counter = 0

        # í›ˆë ¨
        for epoch in range(100):
            model.train()
            train_loss = 0

            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)

                optimizer.zero_grad()
                outputs = model(batch_X).squeeze()
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            # ê²€ì¦
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                    outputs = model(batch_X).squeeze()
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()

            val_loss /= len(val_loader)

            # ì¡°ê¸° ì¤‘ë‹¨ ì²´í¬
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"      ì¡°ê¸° ì¤‘ë‹¨: epoch {epoch+1}")
                    break

        # ìµœì¢… ì˜ˆì¸¡
        model.eval()
        with torch.no_grad():
            X_val_tensor = torch.FloatTensor(X_val).to(self.device)
            y_pred = model(X_val_tensor).squeeze().cpu().numpy()

        return y_pred

    def create_tensorflow_model(self, input_size, architecture='simple'):
        """TensorFlow ëª¨ë¸ ìƒì„±"""
        if not TENSORFLOW_AVAILABLE:
            return None

        if architecture == 'simple':
            model = models.Sequential([
                layers.Dense(64, activation='relu', input_shape=(input_size,)),
                layers.Dropout(0.3),
                layers.Dense(32, activation='relu'),
                layers.Dropout(0.2),
                layers.Dense(1, activation='sigmoid')
            ])
        elif architecture == 'deep':
            model = models.Sequential([
                layers.Dense(128, activation='relu', input_shape=(input_size,)),
                layers.BatchNormalization(),
                layers.Dropout(0.4),
                layers.Dense(96, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                layers.Dense(64, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                layers.Dense(32, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.2),
                layers.Dense(1, activation='sigmoid')
            ])
        elif architecture == 'wide':
            model = models.Sequential([
                layers.Dense(256, activation='relu', input_shape=(input_size,)),
                layers.Dropout(0.4),
                layers.Dense(128, activation='relu'),
                layers.Dropout(0.3),
                layers.Dense(1, activation='sigmoid')
            ])

        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy']
        )

        return model

    def run_neural_experiments(self, data_path='/root/workspace/data/training/sp500_2020_2024_enhanced.csv'):
        """ì‹ ê²½ë§ ì‹¤í—˜ ì‹¤í–‰"""
        print("ğŸ”’ ì•ˆì „í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì‹¤í—˜ ì‹œì‘")
        print("="*70)

        try:
            # 1. ë°ì´í„° ë¡œë”© ë° ì•ˆì „ ì²˜ë¦¬
            df = self.data_processor.load_and_validate_data(data_path)
            safe_df = self.create_safe_features(df)

            # 2. ì•ˆì „ì„± ê²€ì¦
            safe_features = self.validate_neural_safety(safe_df)

            if len(safe_features) < 5:
                print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì•ˆì „ íŠ¹ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤!")
                return None

            # 3. ì‹ ê²½ë§ ì‹¤í—˜
            neural_results = self._run_neural_architectures(safe_df, safe_features)

            # 4. ê²°ê³¼ ê²€ì¦
            self._validate_neural_results(neural_results)

            return neural_results

        except Exception as e:
            print(f"âŒ ì‹ ê²½ë§ ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def _run_neural_architectures(self, safe_df, safe_features):
        """ë‹¤ì–‘í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì‹¤í–‰"""
        print(f"\nğŸ§  ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì‹¤í—˜ (íŠ¹ì„± ìˆ˜: {len(safe_features)})")

        # ë°ì´í„° ì¤€ë¹„
        X = safe_df[safe_features].values
        y = safe_df['direction_target'].values

        # ì•ˆì „ ì²˜ë¦¬
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        y = np.nan_to_num(y, nan=0.5, posinf=1.0, neginf=0.0).astype(float)

        # ìœ íš¨ ë°ì´í„°ë§Œ ì„ íƒ
        valid_idx = ~pd.isna(safe_df['direction_target'])
        X = X[valid_idx]
        y = y[valid_idx]

        print(f"   ìµœì¢… ë°ì´í„°: X={X.shape}, y=í´ë˜ìŠ¤ë¶„í¬{np.bincount(y.astype(int))}")

        # ì‹œê°„ ìˆœì„œ êµì°¨ ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=3)

        # PyTorch ëª¨ë¸ë“¤
        pytorch_models = {
            'SimpleNN': self.SimpleNN,
            'DeepNN': self.DeepNN,
            'WideNN': self.WideNN,
            'ResidualNN': self.ResidualNN
        }

        neural_results = {}

        # PyTorch ëª¨ë¸ ì‹¤í—˜
        for model_name, model_class in pytorch_models.items():
            print(f"\n   ğŸ”¬ {model_name} (PyTorch) ì‹¤í—˜...")

            fold_accuracies = []
            fold_maes = []
            fold_r2s = []

            for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                # ìŠ¤ì¼€ì¼ë§
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)

                try:
                    # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
                    model = model_class(X_train_scaled.shape[1])
                    y_pred = self.train_pytorch_model(
                        model, X_train_scaled, y_train, X_val_scaled, y_val, model_name
                    )

                    # ì„±ëŠ¥ ê³„ì‚°
                    mae = mean_absolute_error(y_val, y_pred)
                    r2 = r2_score(y_val, y_pred)

                    # ë°©í–¥ ì •í™•ë„
                    y_pred_direction = (y_pred > 0.5).astype(int)
                    direction_acc = np.mean(y_pred_direction == y_val.astype(int))

                    fold_accuracies.append(direction_acc)
                    fold_maes.append(mae)
                    fold_r2s.append(r2)

                    print(f"      Fold {fold+1}: ë°©í–¥ì •í™•ë„={direction_acc:.4f}, MAE={mae:.6f}, RÂ²={r2:.4f}")

                except Exception as e:
                    print(f"      Fold {fold+1} ì‹¤íŒ¨: {e}")
                    fold_accuracies.append(0.5)
                    fold_maes.append(1.0)
                    fold_r2s.append(-1.0)

            # í‰ê·  ì„±ëŠ¥
            avg_accuracy = np.mean(fold_accuracies)
            avg_mae = np.mean(fold_maes)
            avg_r2 = np.mean(fold_r2s)

            neural_results[model_name] = {
                'framework': 'PyTorch',
                'direction_accuracy': avg_accuracy,
                'mae': avg_mae,
                'r2': avg_r2,
                'fold_accuracies': fold_accuracies
            }

            print(f"   âœ… {model_name} í‰ê· : ë°©í–¥ì •í™•ë„={avg_accuracy:.4f}, MAE={avg_mae:.6f}, RÂ²={avg_r2:.4f}")

        # TensorFlow ëª¨ë¸ ì‹¤í—˜ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if TENSORFLOW_AVAILABLE:
            tf_architectures = ['simple', 'deep', 'wide']

            for arch in tf_architectures:
                model_name = f'TF_{arch.capitalize()}NN'
                print(f"\n   ğŸ”¬ {model_name} (TensorFlow) ì‹¤í—˜...")

                fold_accuracies = []
                fold_maes = []
                fold_r2s = []

                for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
                    X_train, X_val = X[train_idx], X[val_idx]
                    y_train, y_val = y[train_idx], y[val_idx]

                    # ìŠ¤ì¼€ì¼ë§
                    scaler = StandardScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_val_scaled = scaler.transform(X_val)

                    try:
                        # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
                        model = self.create_tensorflow_model(X_train_scaled.shape[1], arch)

                        # ì¡°ê¸° ì¤‘ë‹¨ ì„¤ì •
                        early_stopping = tf.keras.callbacks.EarlyStopping(
                            monitor='val_loss', patience=10, restore_best_weights=True
                        )

                        # í›ˆë ¨
                        history = model.fit(
                            X_train_scaled, y_train,
                            validation_data=(X_val_scaled, y_val),
                            epochs=100,
                            batch_size=32,
                            verbose=0,
                            callbacks=[early_stopping]
                        )

                        # ì˜ˆì¸¡
                        y_pred = model.predict(X_val_scaled, verbose=0).flatten()

                        # ì„±ëŠ¥ ê³„ì‚°
                        mae = mean_absolute_error(y_val, y_pred)
                        r2 = r2_score(y_val, y_pred)

                        # ë°©í–¥ ì •í™•ë„
                        y_pred_direction = (y_pred > 0.5).astype(int)
                        direction_acc = np.mean(y_pred_direction == y_val.astype(int))

                        fold_accuracies.append(direction_acc)
                        fold_maes.append(mae)
                        fold_r2s.append(r2)

                        print(f"      Fold {fold+1}: ë°©í–¥ì •í™•ë„={direction_acc:.4f}, MAE={mae:.6f}, RÂ²={r2:.4f}")

                    except Exception as e:
                        print(f"      Fold {fold+1} ì‹¤íŒ¨: {e}")
                        fold_accuracies.append(0.5)
                        fold_maes.append(1.0)
                        fold_r2s.append(-1.0)

                # í‰ê·  ì„±ëŠ¥
                avg_accuracy = np.mean(fold_accuracies)
                avg_mae = np.mean(fold_maes)
                avg_r2 = np.mean(fold_r2s)

                neural_results[model_name] = {
                    'framework': 'TensorFlow',
                    'direction_accuracy': avg_accuracy,
                    'mae': avg_mae,
                    'r2': avg_r2,
                    'fold_accuracies': fold_accuracies
                }

                print(f"   âœ… {model_name} í‰ê· : ë°©í–¥ì •í™•ë„={avg_accuracy:.4f}, MAE={avg_mae:.6f}, RÂ²={avg_r2:.4f}")

        return neural_results

    def _validate_neural_results(self, results):
        """ì‹ ê²½ë§ ê²°ê³¼ ê²€ì¦"""
        print("\nğŸš¨ ì‹ ê²½ë§ ê²°ê³¼ ê²€ì¦ ë° ê²½ê³  ì‹œìŠ¤í…œ")
        print("="*60)

        for model_name, metrics in results.items():
            accuracy = metrics['direction_accuracy']
            r2 = metrics['r2']
            framework = metrics['framework']

            # ì„±ëŠ¥ ê²€ì¦
            if accuracy > 0.9:
                print(f"ğŸš¨ {model_name} ({framework}): {accuracy:.1%} - ëˆ„ì¶œ ì˜ì‹¬!")
            elif accuracy > 0.75:
                print(f"âš ï¸ {model_name} ({framework}): {accuracy:.1%} - ë†’ì€ ì„±ëŠ¥, ì¬ê²€ì¦ ê¶Œì¥")
            elif accuracy > 0.6:
                print(f"âœ… {model_name} ({framework}): {accuracy:.1%} - ì–‘í˜¸í•œ ì„±ëŠ¥")
            else:
                print(f"ğŸ“Š {model_name} ({framework}): {accuracy:.1%} - í˜„ì‹¤ì  ì„±ëŠ¥")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = max(results.keys(), key=lambda k: results[k]['direction_accuracy'])
        best_acc = results[best_model]['direction_accuracy']
        best_framework = results[best_model]['framework']

        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model} ({best_framework}) - {best_acc:.1%}")

        if best_acc > 0.85:
            print("ğŸš¨ ê²½ê³ : ì—¬ì „íˆ ë†’ì€ ì„±ëŠ¥, ì¶”ê°€ ëˆ„ì¶œ ê²€ì¦ í•„ìš”!")
        elif best_acc > 0.7:
            print("ğŸ“Š ì–‘í˜¸: í•©ë¦¬ì  ì„±ëŠ¥ ë²”ìœ„")
        else:
            print("âœ… ì•ˆì „: í˜„ì‹¤ì  ì„±ëŠ¥, ëˆ„ì¶œ ì—†ìŒ í™•ì¸")

        # ê²°ê³¼ ì €ì¥
        output_path = f"/root/workspace/data/results/safe_neural_networks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        try:
            with open(output_path, 'w') as f:
                json.dump({
                    'timestamp': datetime.now().isoformat(),
                    'experiment_type': 'safe_neural_networks',
                    'max_allowed_correlation': self.max_allowed_correlation,
                    'device': str(self.device),
                    'results': {k: {**v, 'fold_accuracies': [float(x) for x in v['fold_accuracies']]}
                              for k, v in results.items()}
                }, f, indent=2)
            print(f"\nğŸ’¾ ì‹ ê²½ë§ ê²°ê³¼ ì €ì¥: {output_path}")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    system = SafeNeuralNetworks()
    results = system.run_neural_experiments()

    if results:
        print("\nğŸ‰ ì•ˆì „í•œ ì‹ ê²½ë§ ì‹¤í—˜ ì™„ë£Œ!")
        print("âœ… ëª¨ë“  ê²°ê³¼ê°€ ëˆ„ì¶œ ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ì‹ ê²½ë§ ì‹¤í—˜ ì‹¤íŒ¨!")

    return results

if __name__ == "__main__":
    main()