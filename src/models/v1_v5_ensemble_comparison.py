"""
V1 + V5 ì•™ìƒë¸” ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ
ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ ì•™ìƒë¸” íš¨ê³¼ ê²€ì¦

ë¹„êµ ëŒ€ìƒ:
- V1 ë‹¨ë… (RÂ² = 0.314)
- V5 ë‹¨ë… (RÂ² = 0.302)
- V1 + V5 ì•™ìƒë¸” (ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜)

ëª©í‘œ: ì•™ìƒë¸”ë¡œ ì„±ëŠ¥ í–¥ìƒ ë° ì•ˆì •ì„± í™•ë³´
"""

import numpy as np
import pandas as pd
import yfinance as yf
import json
import logging
from datetime import datetime
import warnings
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.ensemble import VotingRegressor

warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/workspace/data/raw/v1_v5_ensemble_comparison.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PurgedKFoldSklearn:
    """sklearn í˜¸í™˜ Purged K-Fold Cross-Validation"""

    def __init__(self, n_splits=5, purge_length=5, embargo_length=5):
        self.n_splits = n_splits
        self.purge_length = purge_length
        self.embargo_length = embargo_length

    def split(self, X, y=None, groups=None):
        n_samples = len(X)
        indices = np.arange(n_samples)
        test_size = n_samples // self.n_splits
        splits = []

        for i in range(self.n_splits):
            test_start = i * test_size
            test_end = min((i + 1) * test_size, n_samples)
            test_indices = indices[test_start:test_end]

            purge_start = test_end
            purge_end = min(test_end + self.purge_length, n_samples)
            embargo_end = min(purge_end + self.embargo_length, n_samples)

            train_indices = np.concatenate([
                indices[:test_start],
                indices[embargo_end:]
            ])

            if len(train_indices) > 0 and len(test_indices) > 0:
                splits.append((train_indices, test_indices))

        return splits

    def get_n_splits(self, X=None, y=None, groups=None):
        return self.n_splits

class V1V5EnsembleComparator:
    """V1 + V5 ì•™ìƒë¸” ì„±ëŠ¥ ë¹„êµê¸°"""

    def __init__(self):
        self.cv = PurgedKFoldSklearn()
        self.scaler_v1 = StandardScaler()
        self.scaler_v5 = StandardScaler()
        self.results = {}

        # ëª¨ë¸ ì„¤ì •
        self.v1_config = {
            'alpha': 1.8523,
            'expected_r2': 0.314,
            'features': 12
        }

        self.v5_config = {
            'alphas': [31.7068, 17.4421, 161.7161],
            'expected_r2': 0.302,
            'features': 50
        }

    def load_spy_data(self):
        """SPY ë°ì´í„° ë¡œë“œ"""
        logger.info("ğŸ“Š SPY ë°ì´í„° ë¡œë”©...")

        spy = yf.Ticker("SPY")
        data = spy.history(start="2015-01-01", end="2024-12-31")
        data['returns'] = np.log(data['Close'] / data['Close'].shift(1))
        data = data.dropna()

        logger.info(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(data)}ê°œ ê´€ì¸¡ì¹˜")
        return data

    def create_v1_features(self, data):
        """V1 íŠ¹ì„± ìƒì„± (12ê°œ)"""
        returns = data['returns']
        features = pd.DataFrame(index=data.index)

        # ê¸°ë³¸ ë³€ë™ì„± (3ê°œ)
        features['vol_5'] = returns.rolling(5).std()
        features['vol_10'] = returns.rolling(10).std()
        features['vol_20'] = returns.rolling(20).std()

        # ê¸°ë³¸ ë˜ê·¸ (3ê°œ)
        features['return_lag_1'] = returns.shift(1)
        features['return_lag_2'] = returns.shift(2)
        features['return_lag_3'] = returns.shift(3)

        # ê¸°ë³¸ í†µê³„ (4ê°œ)
        for window in [10, 20]:
            ma = returns.rolling(window).mean()
            std = returns.rolling(window).std()
            features[f'zscore_{window}'] = (returns - ma) / (std + 1e-8)
            features[f'momentum_{window}'] = returns.rolling(window).sum()

        # ê¸°ë³¸ ë¹„ìœ¨ (2ê°œ)
        features['vol_5_20_ratio'] = features['vol_5'] / (features['vol_20'] + 1e-8)
        features['vol_regime'] = (features['vol_5'] > features['vol_10']).astype(float)

        return self.finalize_features(features, returns)

    def create_v5_features(self, data):
        """V5 íŠ¹ì„± ìƒì„± (50ê°œ - ê°„ì†Œí™”)"""
        returns = data['returns']
        prices = data['Close']
        volume = data['Volume']
        features = pd.DataFrame(index=data.index)

        # V1 ê¸°ë³¸ íŠ¹ì„±ë“¤ í¬í•¨
        v1_features, _ = self.create_v1_features(data)
        for col in v1_features.columns:
            if col != 'target_vol_5d':
                features[col] = v1_features[col]

        # ì¶”ê°€ ë³€ë™ì„± (10ê°œ)
        for window in [3, 7, 12, 15, 25, 30, 40, 50, 60, 100]:
            features[f'vol_{window}'] = returns.rolling(window).std()

        # ê³ ì°¨ ëª¨ë©˜íŠ¸ (8ê°œ)
        for window in [5, 10, 15, 20]:
            features[f'skew_{window}'] = returns.rolling(window).skew()
            features[f'kurt_{window}'] = returns.rolling(window).kurt()

        # ê°€ê²© íŠ¹ì„± (10ê°œ)
        for window in [5, 10, 20, 30, 50]:
            sma = prices.rolling(window).mean()
            features[f'price_sma_dev_{window}'] = (prices - sma) / sma
            features[f'price_mom_{window}'] = (prices / prices.shift(window)) - 1

        # ê±°ë˜ëŸ‰ íŠ¹ì„± (4ê°œ)
        for window in [10, 20]:
            vol_sma = volume.rolling(window).mean()
            features[f'volume_ratio_{window}'] = volume / (vol_sma + 1)
            features[f'price_volume_{window}'] = returns * (volume / vol_sma)

        # ì¶”ê°€ ë˜ê·¸ (6ê°œ)
        for lag in [5, 7, 10]:
            features[f'return_lag_{lag}'] = returns.shift(lag)
            features[f'vol_lag_{lag}'] = features['vol_5'].shift(lag)

        # ìƒìœ„ 50ê°œë§Œ ì„ íƒ (ì¤‘ë³µ ì œê±°)
        feature_cols = [col for col in features.columns if col != 'target_vol_5d']
        selected_cols = feature_cols[:50]  # ìƒìœ„ 50ê°œ

        final_features = pd.DataFrame(index=data.index)
        for col in selected_cols:
            final_features[col] = features[col]

        return self.finalize_features(final_features, returns)

    def finalize_features(self, features, returns):
        """íŠ¹ì„± ë§ˆë¬´ë¦¬ ì²˜ë¦¬"""
        # íƒ€ê²Ÿ: 5ì¼ í›„ ë³€ë™ì„±
        target = []
        for i in range(len(returns)):
            if i + 5 < len(returns):
                future_vol = returns.iloc[i+1:i+6].std()
                target.append(future_vol)
            else:
                target.append(np.nan)

        features['target_vol_5d'] = target
        features = features.dropna()

        X = features.drop('target_vol_5d', axis=1)
        y = features['target_vol_5d']

        return X, y

    def test_single_models(self):
        """ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")

        data = self.load_spy_data()

        # V1 ëª¨ë¸ í…ŒìŠ¤íŠ¸
        logger.info("   V1 ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
        X_v1, y_v1 = self.create_v1_features(data)
        X_v1_scaled = self.scaler_v1.fit_transform(X_v1)

        v1_model = Ridge(alpha=self.v1_config['alpha'])
        v1_scores = []

        splits = list(self.cv.split(X_v1_scaled))
        for train_idx, val_idx in splits:
            X_train, X_val = X_v1_scaled[train_idx], X_v1_scaled[val_idx]
            y_train, y_val = y_v1.iloc[train_idx], y_v1.iloc[val_idx]

            v1_model.fit(X_train, y_train)
            val_pred = v1_model.predict(X_val)
            r2 = r2_score(y_val, val_pred)
            v1_scores.append(r2)

        v1_performance = {
            'scores': [float(s) for s in v1_scores],
            'mean_r2': float(np.mean(v1_scores)),
            'std_r2': float(np.std(v1_scores)),
            'features_count': X_v1.shape[1],
            'samples_count': len(X_v1)
        }

        logger.info(f"   V1 ì„±ëŠ¥: RÂ² = {v1_performance['mean_r2']:.4f} Â± {v1_performance['std_r2']:.4f}")

        # V5 ëª¨ë¸ í…ŒìŠ¤íŠ¸
        logger.info("   V5 ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
        X_v5, y_v5 = self.create_v5_features(data)
        X_v5_scaled = self.scaler_v5.fit_transform(X_v5)

        v5_models = [
            ('ridge1', Ridge(alpha=self.v5_config['alphas'][0], random_state=42)),
            ('ridge2', Ridge(alpha=self.v5_config['alphas'][1], random_state=43)),
            ('ridge3', Ridge(alpha=self.v5_config['alphas'][2], random_state=44))
        ]
        v5_ensemble = VotingRegressor(estimators=v5_models)
        v5_scores = []

        splits = list(self.cv.split(X_v5_scaled))
        for train_idx, val_idx in splits:
            X_train, X_val = X_v5_scaled[train_idx], X_v5_scaled[val_idx]
            y_train, y_val = y_v5.iloc[train_idx], y_v5.iloc[val_idx]

            v5_ensemble.fit(X_train, y_train)
            val_pred = v5_ensemble.predict(X_val)
            r2 = r2_score(y_val, val_pred)
            v5_scores.append(r2)

        v5_performance = {
            'scores': [float(s) for s in v5_scores],
            'mean_r2': float(np.mean(v5_scores)),
            'std_r2': float(np.std(v5_scores)),
            'features_count': X_v5.shape[1],
            'samples_count': len(X_v5)
        }

        logger.info(f"   V5 ì„±ëŠ¥: RÂ² = {v5_performance['mean_r2']:.4f} Â± {v5_performance['std_r2']:.4f}")

        return v1_performance, v5_performance, (X_v1_scaled, y_v1), (X_v5_scaled, y_v5)

    def test_ensemble_combinations(self, v1_data, v5_data):
        """ë‹¤ì–‘í•œ ì•™ìƒë¸” ì¡°í•© í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” V1 + V5 ì•™ìƒë¸” ì¡°í•© í…ŒìŠ¤íŠ¸...")

        X_v1, y_v1 = v1_data
        X_v5, y_v5 = v5_data

        # ê³µí†µ ì¸ë±ìŠ¤ ì°¾ê¸° (ë‘ ë°ì´í„°ì…‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        common_indices = set(y_v1.index) & set(y_v5.index)
        common_indices = sorted(list(common_indices))

        if len(common_indices) < 1000:
            logger.warning(f"âš ï¸ ê³µí†µ ìƒ˜í”Œ ìˆ˜ê°€ ì ìŒ: {len(common_indices)}ê°œ")

        # ê³µí†µ ìƒ˜í”Œë¡œ ì •ë ¬
        v1_common_mask = y_v1.index.isin(common_indices)
        v5_common_mask = y_v5.index.isin(common_indices)

        X_v1_common = X_v1[v1_common_mask]
        y_v1_common = y_v1[v1_common_mask]
        X_v5_common = X_v5[v5_common_mask]
        y_v5_common = y_v5[v5_common_mask]

        logger.info(f"   ê³µí†µ ìƒ˜í”Œ: {len(y_v1_common)}ê°œ")

        # ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸
        weight_combinations = [
            (1.0, 0.0),    # V1 only
            (0.0, 1.0),    # V5 only
            (0.7, 0.3),    # V1 ìš°ì„¸
            (0.6, 0.4),    # V1 ìš°ì„¸ (ì•½ê°„)
            (0.5, 0.5),    # ê· ë“±
            (0.4, 0.6),    # V5 ìš°ì„¸ (ì•½ê°„)
            (0.3, 0.7),    # V5 ìš°ì„¸
        ]

        ensemble_results = {}

        for w1, w5 in weight_combinations:
            logger.info(f"   ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸: V1={w1:.1f}, V5={w5:.1f}")

            ensemble_scores = []
            splits = list(self.cv.split(X_v1_common))

            for train_idx, val_idx in splits:
                # V1 ëª¨ë¸
                X_v1_train, X_v1_val = X_v1_common[train_idx], X_v1_common[val_idx]
                y_train, y_val = y_v1_common.iloc[train_idx], y_v1_common.iloc[val_idx]

                v1_model = Ridge(alpha=self.v1_config['alpha'])
                v1_model.fit(X_v1_train, y_train)
                v1_pred = v1_model.predict(X_v1_val)

                # V5 ëª¨ë¸
                X_v5_train, X_v5_val = X_v5_common[train_idx], X_v5_common[val_idx]

                v5_models = [
                    ('ridge1', Ridge(alpha=self.v5_config['alphas'][0], random_state=42)),
                    ('ridge2', Ridge(alpha=self.v5_config['alphas'][1], random_state=43)),
                    ('ridge3', Ridge(alpha=self.v5_config['alphas'][2], random_state=44))
                ]
                v5_ensemble = VotingRegressor(estimators=v5_models)
                v5_ensemble.fit(X_v5_train, y_train)
                v5_pred = v5_ensemble.predict(X_v5_val)

                # ê°€ì¤‘ ì•™ìƒë¸”
                ensemble_pred = w1 * v1_pred + w5 * v5_pred

                # ì„±ëŠ¥ ê³„ì‚°
                r2 = r2_score(y_val, ensemble_pred)
                ensemble_scores.append(r2)

            ensemble_performance = {
                'weights': (w1, w5),
                'scores': [float(s) for s in ensemble_scores],
                'mean_r2': float(np.mean(ensemble_scores)),
                'std_r2': float(np.std(ensemble_scores))
            }

            ensemble_results[f'w{w1:.1f}_{w5:.1f}'] = ensemble_performance

            logger.info(f"      ê²°ê³¼: RÂ² = {ensemble_performance['mean_r2']:.4f} Â± {ensemble_performance['std_r2']:.4f}")

        return ensemble_results

    def find_optimal_ensemble(self, ensemble_results):
        """ìµœì  ì•™ìƒë¸” ì¡°í•© ì°¾ê¸°"""
        logger.info("ğŸ¯ ìµœì  ì•™ìƒë¸” ì¡°í•© ë¶„ì„...")

        best_combination = None
        best_r2 = -1
        best_stability = float('inf')

        performance_summary = []

        for combo_name, results in ensemble_results.items():
            mean_r2 = results['mean_r2']
            std_r2 = results['std_r2']
            weights = results['weights']

            # ì„±ëŠ¥-ì•ˆì •ì„± ìŠ¤ì½”ì–´ (ë†’ì€ RÂ² + ë‚®ì€ í‘œì¤€í¸ì°¨)
            stability_penalty = std_r2 * 2  # ì•ˆì •ì„± ì¤‘ìš”ë„ 2ë°°
            composite_score = mean_r2 - stability_penalty

            performance_summary.append({
                'combination': combo_name,
                'weights': weights,
                'mean_r2': mean_r2,
                'std_r2': std_r2,
                'composite_score': composite_score
            })

            # ìµœê³  RÂ² ì—…ë°ì´íŠ¸
            if mean_r2 > best_r2:
                best_r2 = mean_r2
                best_combination = combo_name
                best_stability = std_r2

        # ì •ë ¬ (composite_score ê¸°ì¤€)
        performance_summary.sort(key=lambda x: x['composite_score'], reverse=True)

        logger.info("   ğŸ“Š ì„±ëŠ¥-ì•ˆì •ì„± ìˆœìœ„:")
        for i, result in enumerate(performance_summary[:5], 1):
            w1, w5 = result['weights']
            logger.info(f"   {i}ìœ„: V1={w1:.1f}/V5={w5:.1f} â†’ RÂ²={result['mean_r2']:.4f}Â±{result['std_r2']:.4f}")

        optimal_result = performance_summary[0]
        logger.info(f"ğŸ† ìµœì  ì¡°í•©: {optimal_result['combination']} (ì¢…í•© ì ìˆ˜: {optimal_result['composite_score']:.4f})")

        return optimal_result, performance_summary

    def generate_comparison_report(self, v1_perf, v5_perf, ensemble_results, optimal_result):
        """ì¢…í•© ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""
        logger.info("ğŸ“‹ V1 + V5 ì•™ìƒë¸” ì¢…í•© ë¹„êµ ë³´ê³ ì„œ ìƒì„±...")

        # ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
        best_ensemble = max(ensemble_results.values(), key=lambda x: x['mean_r2'])

        comparison_report = {
            'analysis_date': datetime.now().isoformat(),
            'single_model_performance': {
                'V1': v1_perf,
                'V5': v5_perf
            },
            'ensemble_results': ensemble_results,
            'optimal_ensemble': optimal_result,
            'performance_comparison': {
                'V1_only': v1_perf['mean_r2'],
                'V5_only': v5_perf['mean_r2'],
                'best_ensemble': best_ensemble['mean_r2'],
                'optimal_ensemble': optimal_result['mean_r2']
            },
            'improvement_analysis': {
                'ensemble_vs_v1': float(best_ensemble['mean_r2'] - v1_perf['mean_r2']),
                'ensemble_vs_v5': float(best_ensemble['mean_r2'] - v5_perf['mean_r2']),
                'optimal_vs_best_single': float(optimal_result['mean_r2'] - max(v1_perf['mean_r2'], v5_perf['mean_r2'])),
                'improvement_percentage': float((optimal_result['mean_r2'] / max(v1_perf['mean_r2'], v5_perf['mean_r2']) - 1) * 100)
            },
            'stability_analysis': {
                'V1_std': v1_perf['std_r2'],
                'V5_std': v5_perf['std_r2'],
                'optimal_ensemble_std': optimal_result['std_r2'],
                'stability_improvement': max(v1_perf['std_r2'], v5_perf['std_r2']) - optimal_result['std_r2']
            },
            'recommendations': self.generate_recommendations(v1_perf, v5_perf, optimal_result)
        }

        # ê²°ê³¼ ì €ì¥
        save_path = '/root/workspace/data/raw/v1_v5_ensemble_comparison_results.json'
        with open(save_path, 'w') as f:
            json.dump(comparison_report, f, indent=2)

        logger.info("="*70)
        logger.info("ğŸ¯ V1 + V5 ì•™ìƒë¸” ë¹„êµ ì™„ë£Œ")
        logger.info(f"ğŸ“Š V1 ë‹¨ë…: RÂ² = {v1_perf['mean_r2']:.4f}")
        logger.info(f"ğŸ“Š V5 ë‹¨ë…: RÂ² = {v5_perf['mean_r2']:.4f}")
        logger.info(f"ğŸ† ìµœì  ì•™ìƒë¸”: RÂ² = {optimal_result['mean_r2']:.4f}")
        logger.info(f"ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ: {comparison_report['improvement_analysis']['improvement_percentage']:+.2f}%")
        logger.info(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼: {save_path}")
        logger.info("="*70)

        return comparison_report

    def generate_recommendations(self, v1_perf, v5_perf, optimal_result):
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        optimal_weights = optimal_result['weights']
        improvement_pct = (optimal_result['mean_r2'] / max(v1_perf['mean_r2'], v5_perf['mean_r2']) - 1) * 100

        if improvement_pct > 2:
            recommendations.extend([
                f"âœ… ì•™ìƒë¸” ê¶Œì¥: V1={optimal_weights[0]:.1f}, V5={optimal_weights[1]:.1f}",
                f"âœ… ì„±ëŠ¥ í–¥ìƒ: {improvement_pct:+.2f}% ê°œì„ ",
                "âœ… ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ ì•™ìƒë¸” ìš°ìˆ˜"
            ])
        elif improvement_pct > 0:
            recommendations.extend([
                f"ğŸ”¶ ì•™ìƒë¸” ê³ ë ¤: ì†Œí­ ê°œì„  ({improvement_pct:+.2f}%)",
                f"ğŸ”¶ ê°€ì¤‘ì¹˜: V1={optimal_weights[0]:.1f}, V5={optimal_weights[1]:.1f}",
                "ğŸ”¶ ë³µì¡ë„ vs ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„ ê²€í† "
            ])
        else:
            recommendations.extend([
                "âŒ ì•™ìƒë¸” ë¶ˆí•„ìš”: ë‹¨ì¼ ëª¨ë¸ì´ ë” ìš°ìˆ˜",
                f"âŒ V1 ë‹¨ë… ì‚¬ìš© ê¶Œì¥ (RÂ² = {v1_perf['mean_r2']:.4f})",
                "âŒ ë³µì¡ì„± ì¦ê°€ ëŒ€ë¹„ ì„±ëŠ¥ ì´ìµ ì—†ìŒ"
            ])

        # ì•ˆì •ì„± ë¶„ì„
        optimal_std = optimal_result['std_r2']
        best_single_std = min(v1_perf['std_r2'], v5_perf['std_r2'])

        if optimal_std < best_single_std:
            recommendations.append("âœ… ì•™ìƒë¸”ì´ ì•ˆì •ì„±ë„ í–¥ìƒ")
        else:
            recommendations.append("âš ï¸ ì•™ìƒë¸”ì´ ë³€ë™ì„± ì¦ê°€")

        return recommendations

    def run_full_comparison(self):
        """ì „ì²´ ë¹„êµ ë¶„ì„ ì‹¤í–‰"""
        logger.info("ğŸš€ V1 + V5 ì•™ìƒë¸” ì „ì²´ ë¹„êµ ë¶„ì„ ì‹œì‘")

        try:
            # 1. ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            v1_perf, v5_perf, v1_data, v5_data = self.test_single_models()

            # 2. ì•™ìƒë¸” ì¡°í•© í…ŒìŠ¤íŠ¸
            ensemble_results = self.test_ensemble_combinations(v1_data, v5_data)

            # 3. ìµœì  ì¡°í•© ì°¾ê¸°
            optimal_result, performance_summary = self.find_optimal_ensemble(ensemble_results)

            # 4. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            report = self.generate_comparison_report(v1_perf, v5_perf, ensemble_results, optimal_result)

            logger.info("ğŸ’¡ ìµœì¢… ê¶Œì¥ì‚¬í•­:")
            for rec in report['recommendations']:
                logger.info(f"   {rec}")

            return report

        except Exception as e:
            logger.error(f"âŒ ì•™ìƒë¸” ë¹„êµ ì‹¤íŒ¨: {str(e)}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logger.info("ğŸ¯ V1 + V5 ì•™ìƒë¸” ì„±ëŠ¥ ë¹„êµ ì‹œì‘")

    comparator = V1V5EnsembleComparator()

    try:
        results = comparator.run_full_comparison()
        return results

    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ë¹„êµ ì‹¤íŒ¨: {str(e)}")
        raise

if __name__ == "__main__":
    main()