#!/usr/bin/env python3
"""
í†µí•© ë³€ë™ì„± ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ v2.1
- í•µì‹¬ ê°œì„ : íƒ€ê²Ÿ ì •ê·œí™”, íŠ¹ì„± ì„ íƒ, ë‹¨ìˆœí™”ëœ íŠ¹ì„±
- ê¸°ì¡´ RÂ² 0.22 â†’ ëª©í‘œ RÂ² 0.30+

ì£¼ìš” ë³€ê²½ì :
1. íƒ€ê²Ÿ: ì›ì‹œ 5ì¼ std (ì—°ê°„í™” X)
2. í•µì‹¬ íŠ¹ì„±ë§Œ ì‚¬ìš© (VIX ì¤‘ì‹¬)
3. íŠ¹ì„± ìƒê´€ê´€ê³„ ê¸°ë°˜ ì„ íƒ
4. ë” ë³´ìˆ˜ì ì¸ ëª¨ë¸ ì„¤ì •
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.linear_model import ElasticNet, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
import joblib
import json
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ì„ íƒì  ì„í¬íŠ¸
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False

try:
    import lightgbm as lgb
    HAS_LGB = True
except ImportError:
    HAS_LGB = False

try:
    from arch import arch_model
    HAS_ARCH = True
except ImportError:
    HAS_ARCH = False

SEED = 42
np.random.seed(SEED)


class ImprovedVolatilityPipeline:
    """ê°œì„ ëœ ë³€ë™ì„± ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ v2.1"""
    
    def __init__(self, start_date='2015-01-01', end_date='2024-12-31'):
        self.start_date = start_date
        self.end_date = end_date
        self.data = None
        self.feature_cols = []
        self.scaler = None
        self.best_model = None
        self.models = {}
        self.results = {}
        
    def load_data(self):
        """ë‹¤ì¤‘ ìì‚° ë°ì´í„° ë¡œë“œ"""
        print("\n" + "="*60)
        print("[1/8] ë°ì´í„° ë¡œë“œ...")
        print("="*60)
        
        tickers = {
            'SPY': 'SPY',
            'VIX': '^VIX',
            'TLT': 'TLT',
            'GLD': 'GLD',
        }
        
        all_data = {}
        for name, ticker in tickers.items():
            try:
                df = yf.download(ticker, start=self.start_date, end=self.end_date,
                               progress=False, auto_adjust=True)
                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = df.columns.get_level_values(0)
                all_data[name] = df
                print(f"  âœ“ {name}: {len(df)} í–‰")
            except Exception as e:
                print(f"  âš ï¸ {name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # SPY ê¸°ë³¸ + ë‹¤ë¥¸ ìì‚° Close ë³‘í•©
        self.data = all_data['SPY'].copy()
        for name in ['VIX', 'TLT', 'GLD']:
            if name in all_data:
                self.data[f'{name}_Close'] = all_data[name]['Close']
        
        self.data = self.data.ffill().dropna()
        print(f"\n  âœ“ ìµœì¢…: {len(self.data)} í–‰")
        return self.data
    
    def create_features(self):
        """í•µì‹¬ íŠ¹ì„±ë§Œ ìƒì„± (ê³¼ì í•© ë°©ì§€)"""
        print("\n" + "="*60)
        print("[2/8] í•µì‹¬ íŠ¹ì„± ìƒì„±...")
        print("="*60)
        
        df = self.data.copy()
        
        # ê¸°ë³¸ ìˆ˜ìµë¥ 
        df['returns'] = df['Close'].pct_change()
        
        # === 1. ê³¼ê±° ë³€ë™ì„± (í•µì‹¬) ===
        print("  â†’ ë³€ë™ì„± íŠ¹ì„±...")
        for w in [5, 10, 20, 50]:
            df[f'rv_{w}'] = df['returns'].rolling(w).std()  # ì›ì‹œ std
        
        # ë³€ë™ì„± ë˜ê·¸ (HAR ìŠ¤íƒ€ì¼)
        for lag in [1, 5, 10, 22]:
            df[f'rv_5_lag_{lag}'] = df['rv_5'].shift(lag)
        
        # ë³€ë™ì„± ë³€í™”
        df['rv_change_1'] = df['rv_5'].pct_change()
        df['rv_change_5'] = df['rv_5'].pct_change(5)
        
        # ë³€ë™ì„± ë¹„ìœ¨
        df['rv_ratio_5_20'] = df['rv_5'] / (df['rv_20'] + 1e-10)
        df['rv_ratio_5_50'] = df['rv_5'] / (df['rv_50'] + 1e-10)
        
        # === 2. VIX (ë³€ë™ì„± ì˜ˆì¸¡ì˜ í•µì‹¬) ===
        print("  â†’ VIX íŠ¹ì„±...")
        if 'VIX_Close' in df.columns:
            df['vix'] = df['VIX_Close'] / 100  # ì •ê·œí™”
            df['vix_lag_1'] = df['vix'].shift(1)
            df['vix_lag_5'] = df['vix'].shift(5)
            df['vix_ma_5'] = df['vix'].rolling(5).mean()
            df['vix_ma_20'] = df['vix'].rolling(20).mean()
            df['vix_change'] = df['vix'].pct_change()
            
            # VIX ìƒëŒ€ ìœ„ì¹˜ (0-1 ì‚¬ì´)
            df['vix_percentile'] = df['vix'].rolling(252).rank(pct=True)
            
            # VIX vs Realized Vol (VRP proxy)
            df['vix_rv_ratio'] = df['vix'] / (df['rv_20'] + 1e-10)
        
        # === 3. ìˆ˜ìµë¥  íŠ¹ì„± (ê°„ì†Œí™”) ===
        print("  â†’ ìˆ˜ìµë¥  íŠ¹ì„±...")
        for w in [5, 10, 20]:
            df[f'return_mean_{w}'] = df['returns'].rolling(w).mean()
            df[f'abs_return_sum_{w}'] = df['returns'].abs().rolling(w).sum()
        
        # ìˆ˜ìµë¥  ë˜ê·¸
        for lag in [1, 2, 3, 5]:
            df[f'return_lag_{lag}'] = df['returns'].shift(lag)
        
        # === 4. í¬ë¡œìŠ¤ì—ì…‹ (ê°„ì†Œí™”) ===
        print("  â†’ í¬ë¡œìŠ¤ì—ì…‹...")
        for asset in ['TLT', 'GLD']:
            col = f'{asset}_Close'
            if col in df.columns:
                df[f'{asset}_return'] = df[col].pct_change()
                df[f'{asset}_return_lag_1'] = df[f'{asset}_return'].shift(1)
                df[f'spy_{asset}_corr'] = df['returns'].rolling(20).corr(df[f'{asset}_return'])
        
        # === 5. ê¸°ìˆ ì  ì§€í‘œ (ìµœì†Œí•œ) ===
        print("  â†’ ê¸°ìˆ ì  ì§€í‘œ...")
        # ATR proxy
        df['range'] = (df['High'] - df['Low']) / df['Close']
        df['atr_5'] = df['range'].rolling(5).mean()
        df['atr_20'] = df['range'].rolling(20).mean()
        
        self.data = df
        print(f"\n  âœ“ ì´ {len(df.columns)} ì»¬ëŸ¼")
        return df
    
    def create_target(self, horizon=5):
        """ë¯¸ë˜ ë³€ë™ì„± íƒ€ê²Ÿ"""
        print("\n" + "="*60)
        print(f"[3/8] íƒ€ê²Ÿ ìƒì„± (horizon={horizon})...")
        print("="*60)
        
        df = self.data.copy()
        
        # ë¯¸ë˜ ë³€ë™ì„±: t+1 ~ t+horizonì˜ ìˆ˜ìµë¥  í‘œì¤€í¸ì°¨
        target = []
        returns = df['returns'].values
        
        for i in range(len(returns)):
            if i + horizon < len(returns):
                future_ret = returns[i+1:i+1+horizon]
                target.append(np.std(future_ret))
            else:
                target.append(np.nan)
        
        df['target'] = target
        
        # íƒ€ê²Ÿ í†µê³„
        print(f"  âœ“ íƒ€ê²Ÿ í‰ê· : {np.nanmean(target):.6f}")
        print(f"  âœ“ íƒ€ê²Ÿ í‘œì¤€í¸ì°¨: {np.nanstd(target):.6f}")
        
        self.data = df
        return df
    
    def select_features(self, method='correlation', top_k=30):
        """íŠ¹ì„± ì„ íƒ"""
        print("\n" + "="*60)
        print(f"[4/8] íŠ¹ì„± ì„ íƒ (method={method}, k={top_k})...")
        print("="*60)
        
        df = self.data.dropna().copy()
        
        # íŠ¹ì„± í›„ë³´
        exclude = ['Open', 'High', 'Low', 'Close', 'Volume', 
                   'VIX_Close', 'TLT_Close', 'GLD_Close',
                   'returns', 'target']
        candidates = [c for c in df.columns if c not in exclude]
        
        X = df[candidates]
        y = df['target']
        
        if method == 'correlation':
            # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ë¡œ ì„ íƒ
            correlations = X.corrwith(y).abs().sort_values(ascending=False)
            selected = correlations.head(top_k).index.tolist()
            
            print("\n  ğŸ“Š ìƒìœ„ íŠ¹ì„± (íƒ€ê²Ÿ ìƒê´€ê´€ê³„):")
            for i, feat in enumerate(selected[:10]):
                print(f"    {i+1}. {feat}: {correlations[feat]:.4f}")
        
        elif method == 'mutual_info':
            # ìƒí˜¸ì •ë³´ëŸ‰ìœ¼ë¡œ ì„ íƒ
            selector = SelectKBest(mutual_info_regression, k=top_k)
            selector.fit(X, y)
            mask = selector.get_support()
            selected = [c for c, m in zip(candidates, mask) if m]
        
        else:
            selected = candidates[:top_k]
        
        self.feature_cols = selected
        print(f"\n  âœ“ {len(selected)}ê°œ íŠ¹ì„± ì„ íƒë¨")
        return selected
    
    def prepare_data(self, test_ratio=0.2):
        """í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• """
        print("\n" + "="*60)
        print("[5/8] ë°ì´í„° ë¶„í• ...")
        print("="*60)
        
        df = self.data.dropna().copy()
        
        split_idx = int(len(df) * (1 - test_ratio))
        train_df = df.iloc[:split_idx]
        test_df = df.iloc[split_idx:]
        
        X_train = train_df[self.feature_cols]
        y_train = train_df['target']
        X_test = test_df[self.feature_cols]
        y_test = test_df['target']
        
        # ìŠ¤ì¼€ì¼ë§
        self.scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            self.scaler.fit_transform(X_train),
            columns=self.feature_cols,
            index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            self.scaler.transform(X_test),
            columns=self.feature_cols,
            index=X_test.index
        )
        
        print(f"  âœ“ Train: {len(X_train)}, Test: {len(X_test)}")
        print(f"  âœ“ íŠ¹ì„±: {len(self.feature_cols)}")
        
        # í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥
        self.X_train = X_train_scaled
        self.X_test = X_test_scaled
        self.y_train = y_train
        self.y_test = y_test
        self.test_df = test_df
        
        return X_train_scaled, X_test_scaled, y_train, y_test, test_df
    
    def train_baseline_models(self):
        """ê¸°ì¤€ ëª¨ë¸ë“¤ í•™ìŠµ"""
        print("\n" + "="*60)
        print("[6/8] ê¸°ì¤€ ëª¨ë¸ í•™ìŠµ...")
        print("="*60)
        
        X_train, X_test = self.X_train, self.X_test
        y_train, y_test = self.y_train, self.y_test
        
        tscv = TimeSeriesSplit(n_splits=5)
        results = {}
        
        # 1. Ridge (ê°•í•œ ì •ê·œí™”)
        print("\n  [1] Ridge Regression...")
        ridge_params = {'alpha': [0.1, 1.0, 10.0, 100.0]}
        ridge = GridSearchCV(Ridge(), ridge_params, cv=tscv, scoring='r2')
        ridge.fit(X_train, y_train)
        self.models['Ridge'] = ridge.best_estimator_
        results['Ridge'] = self._evaluate(ridge.best_estimator_, X_test, y_test)
        print(f"      Best alpha: {ridge.best_params_['alpha']}, Test RÂ²: {results['Ridge']['test_r2']:.4f}")
        
        # 2. ElasticNet
        print("  [2] ElasticNet...")
        en_params = {'alpha': [0.001, 0.01, 0.1, 1.0], 'l1_ratio': [0.1, 0.5, 0.9]}
        en = GridSearchCV(ElasticNet(max_iter=10000), en_params, cv=tscv, scoring='r2')
        en.fit(X_train, y_train)
        self.models['ElasticNet'] = en.best_estimator_
        results['ElasticNet'] = self._evaluate(en.best_estimator_, X_test, y_test)
        print(f"      Best params: {en.best_params_}, Test RÂ²: {results['ElasticNet']['test_r2']:.4f}")
        
        # 3. Random Forest (ë³´ìˆ˜ì  ì„¤ì •)
        print("  [3] Random Forest...")
        rf = RandomForestRegressor(
            n_estimators=100, max_depth=5, min_samples_leaf=10,
            random_state=SEED, n_jobs=-1
        )
        rf.fit(X_train, y_train)
        self.models['RandomForest'] = rf
        results['RandomForest'] = self._evaluate(rf, X_test, y_test)
        print(f"      Test RÂ²: {results['RandomForest']['test_r2']:.4f}")
        
        # 4. Gradient Boosting (ë³´ìˆ˜ì  ì„¤ì •)
        print("  [4] Gradient Boosting...")
        gb = GradientBoostingRegressor(
            n_estimators=100, max_depth=3, learning_rate=0.05,
            min_samples_leaf=10, random_state=SEED
        )
        gb.fit(X_train, y_train)
        self.models['GradientBoosting'] = gb
        results['GradientBoosting'] = self._evaluate(gb, X_test, y_test)
        print(f"      Test RÂ²: {results['GradientBoosting']['test_r2']:.4f}")
        
        # 5. XGBoost
        if HAS_XGB:
            print("  [5] XGBoost...")
            xgb_model = xgb.XGBRegressor(
                n_estimators=100, max_depth=3, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                reg_alpha=1.0, reg_lambda=1.0,
                random_state=SEED, n_jobs=-1, verbosity=0
            )
            xgb_model.fit(X_train, y_train)
            self.models['XGBoost'] = xgb_model
            results['XGBoost'] = self._evaluate(xgb_model, X_test, y_test)
            print(f"      Test RÂ²: {results['XGBoost']['test_r2']:.4f}")
        
        # 6. LightGBM
        if HAS_LGB:
            print("  [6] LightGBM...")
            lgb_model = lgb.LGBMRegressor(
                n_estimators=100, max_depth=3, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                reg_alpha=1.0, reg_lambda=1.0,
                random_state=SEED, n_jobs=-1, verbose=-1
            )
            lgb_model.fit(X_train, y_train)
            self.models['LightGBM'] = lgb_model
            results['LightGBM'] = self._evaluate(lgb_model, X_test, y_test)
            print(f"      Test RÂ²: {results['LightGBM']['test_r2']:.4f}")
        
        self.results = results
        self._print_comparison(results)
        return results
    
    def train_har_model(self):
        """HAR ëª¨ë¸ (Heterogeneous Autoregressive)"""
        print("\n" + "="*60)
        print("[6.5/8] HAR ëª¨ë¸ (ë²¤ì¹˜ë§ˆí¬)...")
        print("="*60)
        
        # HAR íŠ¹ì„±ë§Œ ì‚¬ìš©
        har_features = ['rv_5', 'rv_5_lag_1', 'rv_5_lag_5', 'rv_5_lag_22']
        har_features = [f for f in har_features if f in self.data.columns]
        
        df = self.data.dropna().copy()
        split_idx = int(len(df) * 0.8)
        
        X_train = df.iloc[:split_idx][har_features]
        y_train = df.iloc[:split_idx]['target']
        X_test = df.iloc[split_idx:][har_features]
        y_test = df.iloc[split_idx:]['target']
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_s = scaler.fit_transform(X_train)
        X_test_s = scaler.transform(X_test)
        
        # HARì€ ë‹¨ìˆœ OLS
        har = Ridge(alpha=0.1)
        har.fit(X_train_s, y_train)
        
        y_pred = har.predict(X_test_s)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        
        print(f"\n  âœ“ HAR ëª¨ë¸ ì„±ëŠ¥:")
        print(f"    - Test RÂ²: {r2:.4f}")
        print(f"    - Test RMSE: {rmse:.6f}")
        print(f"    - íŠ¹ì„±: {har_features}")
        
        self.models['HAR'] = har
        self.results['HAR'] = {'test_r2': r2, 'test_rmse': rmse}
        
        return har
    
    def train_simple_vix_model(self):
        """VIX ê¸°ë°˜ ë‹¨ìˆœ ëª¨ë¸ (ë²¤ì¹˜ë§ˆí¬)"""
        print("\n" + "="*60)
        print("[6.6/8] VIX ê¸°ë°˜ ëª¨ë¸ (ë²¤ì¹˜ë§ˆí¬)...")
        print("="*60)
        
        # VIXë§Œ ì‚¬ìš©
        vix_features = ['vix', 'vix_lag_1', 'vix_change', 'vix_rv_ratio']
        vix_features = [f for f in vix_features if f in self.data.columns]
        
        if not vix_features:
            print("  âš ï¸ VIX íŠ¹ì„± ì—†ìŒ")
            return None
        
        df = self.data.dropna().copy()
        split_idx = int(len(df) * 0.8)
        
        X_train = df.iloc[:split_idx][vix_features]
        y_train = df.iloc[:split_idx]['target']
        X_test = df.iloc[split_idx:][vix_features]
        y_test = df.iloc[split_idx:]['target']
        
        scaler = StandardScaler()
        X_train_s = scaler.fit_transform(X_train)
        X_test_s = scaler.transform(X_test)
        
        vix_model = Ridge(alpha=1.0)
        vix_model.fit(X_train_s, y_train)
        
        y_pred = vix_model.predict(X_test_s)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        
        print(f"\n  âœ“ VIX ëª¨ë¸ ì„±ëŠ¥:")
        print(f"    - Test RÂ²: {r2:.4f}")
        print(f"    - Test RMSE: {rmse:.6f}")
        print(f"    - íŠ¹ì„±: {vix_features}")
        
        self.models['VIX_Only'] = vix_model
        self.results['VIX_Only'] = {'test_r2': r2, 'test_rmse': rmse}
        
        return vix_model
    
    def _evaluate(self, model, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        y_pred = model.predict(X_test)
        return {
            'test_r2': r2_score(y_test, y_pred),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'test_mae': mean_absolute_error(y_test, y_pred)
        }
    
    def _print_comparison(self, results):
        """ê²°ê³¼ ë¹„êµ ì¶œë ¥"""
        print("\n" + "-"*60)
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
        print("-"*60)
        print(f"{'ëª¨ë¸':<20} {'Test RÂ²':>12} {'Test RMSE':>12}")
        print("-"*60)
        for name, res in sorted(results.items(), key=lambda x: x[1]['test_r2'], reverse=True):
            print(f"{name:<20} {res['test_r2']:>12.4f} {res['test_rmse']:>12.6f}")
    
    def select_best_model(self):
        """ìµœê³  ëª¨ë¸ ì„ íƒ"""
        print("\n" + "="*60)
        print("[7/8] ìµœì¢… ëª¨ë¸ ì„ íƒ...")
        print("="*60)
        
        best_name = max(self.results, key=lambda x: self.results[x]['test_r2'])
        self.best_model = self.models[best_name]
        
        print(f"\n  ğŸ† ìµœê³  ëª¨ë¸: {best_name}")
        print(f"  ğŸ“Š Test RÂ²: {self.results[best_name]['test_r2']:.4f}")
        
        return self.best_model, best_name
    
    def save_results(self, best_name):
        """ê²°ê³¼ ì €ì¥"""
        print("\n" + "="*60)
        print("[8/8] ê²°ê³¼ ì €ì¥...")
        print("="*60)
        
        model_dir = Path('data/models')
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        joblib.dump(self.best_model, model_dir / 'improved_best_model.pkl')
        joblib.dump(self.scaler, model_dir / 'improved_scaler.pkl')
        
        # ì˜ˆì¸¡ ì €ì¥
        y_pred = self.best_model.predict(self.X_test)
        predictions = pd.DataFrame({
            'Date': self.test_df.index,
            'actual': self.y_test.values,
            'predicted': y_pred
        })
        predictions.to_csv('data/raw/improved_predictions.csv', index=False)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        metrics = {
            'model_name': f'Improved {best_name}',
            'test_r2': float(self.results[best_name]['test_r2']),
            'test_rmse': float(self.results[best_name]['test_rmse']),
            'n_features': len(self.feature_cols),
            'all_results': {k: {'test_r2': float(v['test_r2'])} for k, v in self.results.items()},
            'timestamp': datetime.now().isoformat()
        }
        
        with open('data/raw/improved_model_performance.json', 'w') as f:
            json.dump(metrics, f, indent=2)
        
        print(f"  âœ“ ëª¨ë¸ ì €ì¥ë¨")
        print(f"  âœ“ ì˜ˆì¸¡ ì €ì¥ë¨")
        print(f"  âœ“ ë©”íŠ¸ë¦­ ì €ì¥ë¨")
        
        return metrics
    
    def analyze_feature_importance(self):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        print("\n" + "="*60)
        print("íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„...")
        print("="*60)
        
        # Ridge ê³„ìˆ˜
        if 'Ridge' in self.models:
            ridge = self.models['Ridge']
            importance = pd.Series(
                np.abs(ridge.coef_),
                index=self.feature_cols
            ).sort_values(ascending=False)
            
            print("\n  ğŸ“Š Ridge ê³„ìˆ˜ (ìƒìœ„ 10):")
            for i, (feat, imp) in enumerate(importance.head(10).items()):
                print(f"    {i+1}. {feat}: {imp:.4f}")
        
        # Random Forest ì¤‘ìš”ë„
        if 'RandomForest' in self.models:
            rf = self.models['RandomForest']
            importance = pd.Series(
                rf.feature_importances_,
                index=self.feature_cols
            ).sort_values(ascending=False)
            
            print("\n  ğŸ“Š Random Forest ì¤‘ìš”ë„ (ìƒìœ„ 10):")
            for i, (feat, imp) in enumerate(importance.head(10).items()):
                print(f"    {i+1}. {feat}: {imp:.4f}")
    
    def run(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start = datetime.now()
        print("\n" + "ğŸš€"*30)
        print("ê°œì„ ëœ ë³€ë™ì„± ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ v2.1")
        print("ğŸš€"*30)
        
        # 1. ë°ì´í„° ë¡œë“œ
        self.load_data()
        
        # 2. íŠ¹ì„± ìƒì„±
        self.create_features()
        
        # 3. íƒ€ê²Ÿ ìƒì„±
        self.create_target(horizon=5)
        
        # 4. íŠ¹ì„± ì„ íƒ
        self.select_features(method='correlation', top_k=25)
        
        # 5. ë°ì´í„° ë¶„í• 
        self.prepare_data()
        
        # 6. ê¸°ì¤€ ëª¨ë¸ í•™ìŠµ
        self.train_baseline_models()
        
        # 6.5. HAR ë²¤ì¹˜ë§ˆí¬
        self.train_har_model()
        
        # 6.6. VIX ë²¤ì¹˜ë§ˆí¬
        self.train_simple_vix_model()
        
        # 7. ìµœê³  ëª¨ë¸ ì„ íƒ
        best_model, best_name = self.select_best_model()
        
        # 8. ê²°ê³¼ ì €ì¥
        metrics = self.save_results(best_name)
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        self.analyze_feature_importance()
        
        # ìµœì¢… ë¹„êµ
        self._print_comparison(self.results)
        
        elapsed = datetime.now() - start
        print("\n" + "="*60)
        print("âœ… ì™„ë£Œ!")
        print("="*60)
        print(f"  â±ï¸ ì†Œìš” ì‹œê°„: {elapsed}")
        print(f"  ğŸ† ìµœê³  ëª¨ë¸: {best_name}")
        print(f"  ğŸ“Š Test RÂ²: {metrics['test_r2']:.4f}")
        
        return metrics


def main():
    pipeline = ImprovedVolatilityPipeline(
        start_date='2015-01-01',
        end_date='2024-12-31'
    )
    metrics = pipeline.run()
    return metrics


if __name__ == '__main__':
    metrics = main()
