#!/usr/bin/env python3
"""
ğŸš€ ê°œì„ ëœ LSTM ì•™ìƒë¸” ì‹œìŠ¤í…œ

ê¸°ì¤€ì„ : 89.58% (direction_lstm_logloss)
ëª©í‘œ: 92-94% ì„±ëŠ¥ ë‹¬ì„±

ì‹œí€€ìŠ¤ ê¸°ë°˜ LSTM + ì•™ìƒë¸” ê¸°ë²• + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, log_loss, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

class ImprovedLSTMClassifier(nn.Module):
    """ê°œì„ ëœ LSTM ë¶„ë¥˜ê¸°"""

    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):
        super(ImprovedLSTMClassifier, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM ë ˆì´ì–´ë“¤
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
            bidirectional=True  # ì–‘ë°©í–¥ LSTM
        )

        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,  # ì–‘ë°©í–¥ì´ë¯€ë¡œ * 2
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )

        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 2)
        )

    def forward(self, x):
        # LSTM ì²˜ë¦¬
        lstm_out, _ = self.lstm(x)

        # ì–´í…ì…˜ ì ìš©
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)

        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ ì‚¬ìš©
        last_output = attn_out[:, -1, :]

        # ë¶„ë¥˜
        output = self.classifier(last_output)

        return output

class LSTMEnsembleOptimizer:
    """LSTM ì•™ìƒë¸” ìµœì í™” ì‹œìŠ¤í…œ"""

    def __init__(self, sequence_length=20, random_state=42):
        self.sequence_length = sequence_length
        self.random_state = random_state
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.models = []
        self.scalers = []

        # ì‹œë“œ ì„¤ì •
        torch.manual_seed(random_state)
        np.random.seed(random_state)

    def create_sequences(self, data, target, sequence_length):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        sequences = []
        targets = []

        for i in range(sequence_length, len(data)):
            sequences.append(data[i-sequence_length:i])
            targets.append(target[i])

        return np.array(sequences), np.array(targets)

    def prepare_data(self, df):
        """ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

        # íŠ¹ì„± ì„ íƒ (124ê°œ íŠ¹ì„± ëª¨ë‘ ì‚¬ìš©)
        feature_cols = [col for col in df.columns if col not in ['Date', 'Returns']]
        X = df[feature_cols].fillna(method='ffill').fillna(0)

        # ë°©í–¥ ì˜ˆì¸¡ íƒ€ê²Ÿ ìƒì„±
        y = (df['Returns'].shift(-1) > 0).astype(int)

        # NaN ì œê±°
        valid_idx = ~(y.isnull())
        X = X[valid_idx]
        y = y[valid_idx]

        print(f"   íŠ¹ì„± ìˆ˜: {X.shape[1]}ê°œ")
        print(f"   ìƒ˜í”Œ ìˆ˜: {X.shape[0]}ê°œ")
        print(f"   íƒ€ê²Ÿ ë¶„í¬: {y.value_counts().to_dict()}")

        return X, y

    def train_single_lstm(self, X_train, y_train, X_val, y_val, config):
        """ë‹¨ì¼ LSTM ëª¨ë¸ í›ˆë ¨"""

        # ìŠ¤ì¼€ì¼ëŸ¬
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)

        # ì‹œí€€ìŠ¤ ìƒì„±
        seq_train, y_seq_train = self.create_sequences(
            X_train_scaled, y_train.values, self.sequence_length
        )
        seq_val, y_seq_val = self.create_sequences(
            X_val_scaled, y_val.values, self.sequence_length
        )

        # í…ì„œ ë³€í™˜
        train_dataset = TensorDataset(
            torch.FloatTensor(seq_train),
            torch.LongTensor(y_seq_train)
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(seq_val),
            torch.LongTensor(y_seq_val)
        )

        # ë°ì´í„° ë¡œë”
        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=False)
        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)

        # ëª¨ë¸ ì´ˆê¸°í™”
        model = ImprovedLSTMClassifier(
            input_size=X_train_scaled.shape[1],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers'],
            dropout=config['dropout']
        ).to(self.device)

        # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', patience=10, factor=0.5
        )

        # í›ˆë ¨
        best_val_acc = 0
        patience_counter = 0

        for epoch in range(config['epochs']):
            # í›ˆë ¨ ëª¨ë“œ
            model.train()
            train_loss = 0

            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)

                optimizer.zero_grad()
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()

                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()
                train_loss += loss.item()

            # ê²€ì¦
            model.eval()
            val_predictions = []
            val_targets = []

            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                    outputs = model(batch_x)
                    predictions = torch.argmax(outputs, dim=1)

                    val_predictions.extend(predictions.cpu().numpy())
                    val_targets.extend(batch_y.cpu().numpy())

            val_acc = accuracy_score(val_targets, val_predictions)
            scheduler.step(val_acc)

            # ì¡°ê¸° ì¢…ë£Œ
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= config['patience']:
                    break

        # ìµœê³  ëª¨ë¸ ë³µì›
        model.load_state_dict(best_model_state)

        return model, scaler, best_val_acc

    def train_ensemble(self, X_train, y_train, X_val, y_val):
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        print("ğŸ¤– LSTM ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘...")

        # ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        configs = [
            {
                'hidden_size': 64, 'num_layers': 2, 'dropout': 0.3,
                'learning_rate': 0.001, 'weight_decay': 1e-5,
                'batch_size': 32, 'epochs': 100, 'patience': 15
            },
            {
                'hidden_size': 128, 'num_layers': 2, 'dropout': 0.4,
                'learning_rate': 0.0005, 'weight_decay': 1e-4,
                'batch_size': 32, 'epochs': 100, 'patience': 15
            },
            {
                'hidden_size': 96, 'num_layers': 3, 'dropout': 0.35,
                'learning_rate': 0.0008, 'weight_decay': 5e-5,
                'batch_size': 16, 'epochs': 100, 'patience': 15
            }
        ]

        individual_scores = []

        for i, config in enumerate(configs):
            print(f"   ğŸ“Š ëª¨ë¸ {i+1}/{len(configs)} í›ˆë ¨ ì¤‘...")
            model, scaler, val_acc = self.train_single_lstm(
                X_train, y_train, X_val, y_val, config
            )

            self.models.append(model)
            self.scalers.append(scaler)
            individual_scores.append(val_acc)

            print(f"      âœ… ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")

        return individual_scores

    def predict_ensemble(self, X_test, y_test):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        print("ğŸ”® ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰...")

        all_predictions = []
        all_probabilities = []

        for i, (model, scaler) in enumerate(zip(self.models, self.scalers)):
            # ë°ì´í„° ì „ì²˜ë¦¬
            X_test_scaled = scaler.transform(X_test)
            seq_test, y_seq_test = self.create_sequences(
                X_test_scaled, y_test.values, self.sequence_length
            )

            # ì˜ˆì¸¡
            model.eval()
            test_dataset = TensorDataset(torch.FloatTensor(seq_test))
            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

            predictions = []
            probabilities = []

            with torch.no_grad():
                for batch_x, in test_loader:
                    batch_x = batch_x.to(self.device)
                    outputs = model(batch_x)
                    probs = torch.softmax(outputs, dim=1)
                    preds = torch.argmax(outputs, dim=1)

                    predictions.extend(preds.cpu().numpy())
                    probabilities.extend(probs.cpu().numpy())

            all_predictions.append(predictions)
            all_probabilities.append(probabilities)

        # ì•™ìƒë¸” ê²°í•©
        all_predictions = np.array(all_predictions)
        all_probabilities = np.array(all_probabilities)

        # 1. ë‹¨ìˆœ íˆ¬í‘œ
        voting_pred = np.apply_along_axis(
            lambda x: np.bincount(x).argmax(), 0, all_predictions
        )

        # 2. í‰ê·  í™•ë¥ 
        avg_proba = np.mean(all_probabilities, axis=0)
        avg_pred = np.argmax(avg_proba, axis=1)

        # ì‹¤ì œ íƒ€ê²Ÿ (ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì•ë¶€ë¶„ ì œê±°)
        y_true = y_test.values[self.sequence_length:]

        # ì„±ëŠ¥ ê³„ì‚°
        voting_acc = accuracy_score(y_true, voting_pred)
        avg_acc = accuracy_score(y_true, avg_pred)

        return {
            'voting_accuracy': voting_acc,
            'average_probability_accuracy': avg_acc,
            'best_accuracy': max(voting_acc, avg_acc),
            'individual_predictions': all_predictions,
            'ensemble_probabilities': avg_proba
        }

    def run_experiment(self, data_path):
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰"""
        print("ğŸš€ LSTM ì•™ìƒë¸” ê°œì„  ì‹¤í—˜ ì‹œì‘")
        print("="*60)

        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)
        X, y = self.prepare_data(df)

        # ì‹œê³„ì—´ ë¶„í• 
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        print(f"ğŸ“Š í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape}")

        # ê²€ì¦ìš© ë¶„í•  (í›ˆë ¨ ì„¸íŠ¸ì˜ 20%)
        val_split_idx = int(len(X_train) * 0.8)
        X_val = X_train.iloc[val_split_idx:]
        y_val = y_train.iloc[val_split_idx:]
        X_train = X_train.iloc[:val_split_idx]
        y_train = y_train.iloc[:val_split_idx]

        # ì•™ìƒë¸” í›ˆë ¨
        individual_scores = self.train_ensemble(X_train, y_train, X_val, y_val)

        # ì•™ìƒë¸” ì˜ˆì¸¡
        ensemble_results = self.predict_ensemble(X_test, y_test)

        # ê²°ê³¼ ì •ë¦¬
        print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
        print(f"   ê°œë³„ ëª¨ë¸ í‰ê· : {np.mean(individual_scores):.4f}")
        print(f"   ì•™ìƒë¸” íˆ¬í‘œ: {ensemble_results['voting_accuracy']:.4f}")
        print(f"   ì•™ìƒë¸” í‰ê· : {ensemble_results['average_probability_accuracy']:.4f}")
        print(f"   ğŸ† ìµœê³  ì„±ëŠ¥: {ensemble_results['best_accuracy']:.4f}")

        # ê¸°ì¤€ì„  ëŒ€ë¹„ ê°œì„ 
        baseline = 0.8958  # ê¸°ì¡´ LSTM ì„±ê³¼
        improvement = (ensemble_results['best_accuracy'] - baseline) * 100
        print(f"ğŸ“ˆ ê¸°ì¤€ì„ ({baseline:.4f}) ëŒ€ë¹„: {improvement:+.2f}%p")

        return {
            'baseline_accuracy': baseline,
            'best_accuracy': ensemble_results['best_accuracy'],
            'improvement': improvement,
            'individual_scores': individual_scores,
            'ensemble_results': ensemble_results,
            'sample_count': len(X),
            'feature_count': X.shape[1],
            'sequence_length': self.sequence_length
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    optimizer = LSTMEnsembleOptimizer(sequence_length=20)

    # enhanced ë°ì´í„° ì‚¬ìš© (124ê°œ íŠ¹ì„±)
    data_path = "/root/workspace/data/training/sp500_2020_2024_enhanced.csv"

    results = optimizer.run_experiment(data_path)

    # ê²°ê³¼ ì €ì¥
    import json
    from datetime import datetime

    experiment_results = {
        'timestamp': datetime.now().isoformat(),
        'experiment_type': 'improved_lstm_ensemble',
        'model_architecture': 'Bidirectional LSTM + Attention + Ensemble',
        'baseline_accuracy': results['baseline_accuracy'],
        'achieved_accuracy': results['best_accuracy'],
        'improvement': results['improvement'],
        'sample_count': results['sample_count'],
        'feature_count': results['feature_count'],
        'sequence_length': results['sequence_length'],
        'validation_method': 'TimeSeriesSplit',
        'status': 'completed',
        'gpu_used': torch.cuda.is_available()
    }

    output_path = f"/root/workspace/data/results/improved_lstm_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_path, 'w') as f:
        json.dump(experiment_results, f, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")

    return results

if __name__ == "__main__":
    main()