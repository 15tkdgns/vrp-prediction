#!/usr/bin/env python3
"""
State-of-the-Art Volatility Model (Fixed) - RÂ² 0.25+ ëª©í‘œ
ë°ì´í„° ì²˜ë¦¬ ë¬¸ì œ í•´ê²° ë²„ì „
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import warnings
import os
import json
from datetime import datetime
import matplotlib.pyplot as plt

# ë”¥ëŸ¬ë‹ ëª¨ë¸ (ì„ íƒì  import)
try:
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping
    import tensorflow as tf
    TENSORFLOW_AVAILABLE = True
    print("âœ… TensorFlow ì‚¬ìš© ê°€ëŠ¥ - LSTM ëª¨ë¸ í™œìš©")
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("âš ï¸ TensorFlow ì—†ìŒ - ì „í†µì ì¸ ML ëª¨ë¸ë§Œ ì‚¬ìš©")

warnings.filterwarnings('ignore')

class SOTAVolatilityPredictorFixed:
    """State-of-the-Art ë³€ë™ì„± ì˜ˆì¸¡ê¸° (ìˆ˜ì •ëœ ë²„ì „)"""

    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.performance_history = {}

    def load_comprehensive_data(self):
        """í¬ê´„ì  ë°ì´í„° ë¡œë“œ - VIX, ê²½ì œì§€í‘œ, ê³ ë¹ˆë„ í”„ë¡ì‹œ"""
        print("ğŸ“Š í¬ê´„ì  ë°ì´í„° ë¡œë“œ ì¤‘...")

        # SPY ê¸°ë³¸ ë°ì´í„° (ë” ê¸´ ê¸°ê°„)
        spy = yf.download('SPY', start='2010-01-01', end='2024-12-31', progress=False)
        spy['returns'] = spy['Close'].pct_change()

        # VIX (í•µì‹¬ ì§€í‘œ)
        vix = yf.download('^VIX', start='2010-01-01', end='2024-12-31', progress=False)
        spy['vix'] = vix['Close'].reindex(spy.index, method='ffill')

        # ì¶”ê°€ ì§€í‘œë“¤ (ì•ˆì „í•˜ê²Œ ë¡œë“œ)
        additional_data = {}
        tickers_info = {
            '^TNX': 'treasury_10y',
            'UUP': 'dollar_index',
            'GLD': 'gold',
            'USO': 'oil'
        }

        for ticker, name in tickers_info.items():
            try:
                data = yf.download(ticker, start='2010-01-01', end='2024-12-31', progress=False)
                additional_data[name] = data['Close'].reindex(spy.index, method='ffill')
                print(f"âœ… {name} ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ {name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                additional_data[name] = pd.Series(index=spy.index, dtype=float)

        # ë°ì´í„° ê²°í•©
        for name, series in additional_data.items():
            spy[name] = series

        # ì´ˆê¸° ë°ì´í„° ì •ë¦¬ (ê¸°ë³¸ ì»¬ëŸ¼ë§Œ)
        initial_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'returns', 'vix']
        spy_clean = spy[initial_columns].dropna()

        # ì¶”ê°€ ì§€í‘œë“¤ì€ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
        for name in additional_data.keys():
            if name in spy.columns:
                spy_clean[name] = spy[name].reindex(spy_clean.index, method='ffill')

        print(f"âœ… í¬ê´„ì  ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(spy_clean)} ê´€ì¸¡ì¹˜")
        return spy_clean

    def create_stable_features(self, data):
        """ì•ˆì •ì ì¸ íŠ¹ì„± ìƒì„± (NaN ìµœì†Œí™”)"""
        print("ğŸ”§ ì•ˆì •ì ì¸ íŠ¹ì„± ìƒì„± ì¤‘...")

        features = pd.DataFrame(index=data.index)
        returns = data['returns']
        prices = data['Close']
        vix = data['vix']

        # 1. ê¸°ë³¸ ë³€ë™ì„± íŠ¹ì„± (í™•ì‹¤íˆ ì‘ë™)
        for window in [5, 10, 20]:
            features[f'vol_{window}'] = returns.rolling(window, min_periods=min(window//2, 3)).std()
            features[f'vol_{window}_normalized'] = features[f'vol_{window}'] / features[f'vol_{window}'].rolling(252, min_periods=60).mean()

        # 2. VIX ê¸°ë°˜ íŠ¹ì„± (í•µì‹¬)
        features['vix_level'] = vix
        features['vix_change'] = vix.pct_change()
        features['vix_ma_5'] = vix.rolling(5, min_periods=3).mean()
        features['vix_ma_20'] = vix.rolling(20, min_periods=10).mean()
        features['vix_regime'] = (vix > vix.rolling(60, min_periods=30).quantile(0.7)).astype(int)

        # 3. ìˆ˜ìµë¥  ê¸°ë°˜ íŠ¹ì„±
        for lag in [1, 2, 3, 5]:
            features[f'returns_lag_{lag}'] = returns.shift(lag)
            features[f'abs_returns_lag_{lag}'] = np.abs(returns.shift(lag))

        # 4. íŠ¸ë Œë“œ íŠ¹ì„±
        features['price_ma_5'] = prices.rolling(5, min_periods=3).mean()
        features['price_ma_20'] = prices.rolling(20, min_periods=10).mean()
        features['trend_5_20'] = features['price_ma_5'] / features['price_ma_20'] - 1

        # 5. ê±°ë˜ëŸ‰ íŠ¹ì„± (ìˆëŠ” ê²½ìš°)
        if 'Volume' in data.columns:
            volume = data['Volume']
            volume_ma_5 = volume.rolling(5, min_periods=3).mean()
            features['volume_ma_5'] = volume_ma_5
            volume_ratio = volume / volume_ma_5
            if isinstance(volume_ratio, pd.Series):
                features['volume_ratio'] = volume_ratio
            else:
                features['volume_ratio'] = volume_ratio.iloc[:, 0] if hasattr(volume_ratio, 'iloc') else volume_ratio

        # 6. ì™¸ë¶€ ì§€í‘œ (ì•ˆì „í•˜ê²Œ ì¶”ê°€)
        if 'treasury_10y' in data.columns and data['treasury_10y'].notna().sum() > 100:
            treasury = data['treasury_10y']
            features['treasury_level'] = treasury
            features['vix_treasury_spread'] = vix - treasury

        # 7. ê³ ê¸‰ ë³€ë™ì„± ì¶”ì •ëŸ‰ (ê°„ë‹¨ ë²„ì „)
        if 'High' in data.columns and 'Low' in data.columns:
            high = data['High']
            low = data['Low']
            high_valid = int(high.notna().sum())
            low_valid = int(low.notna().sum())
            if high_valid > 100 and low_valid > 100:
                gk_vol = np.log(high / low) ** 2
                features['gk_vol_simple'] = gk_vol.rolling(5, min_periods=3).mean()

        print(f"âœ… ì•ˆì •ì ì¸ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
        return features

    def create_robust_targets(self, data):
        """ê°•ê±´í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ê³„"""
        print("ğŸ¯ ê°•ê±´í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

        targets = pd.DataFrame(index=data.index)
        returns = data['returns']

        # ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ íƒ€ê²Ÿë“¤ë§Œ
        for horizon in [1, 3, 5]:
            vol_values = []
            for i in range(len(returns)):
                if i + horizon < len(returns):
                    future_returns = returns.iloc[i+1:i+1+horizon]
                    if len(future_returns) >= horizon:  # ì¶©ë¶„í•œ ë°ì´í„° í™•ì¸
                        vol_values.append(future_returns.std() * np.sqrt(252))
                    else:
                        vol_values.append(np.nan)
                else:
                    vol_values.append(np.nan)
            targets[f'target_vol_{horizon}d'] = vol_values

        # ë¡œê·¸ ë³€í™˜ (ì•ˆì „í•˜ê²Œ)
        for col in targets.columns:
            if 'target_vol' in col:
                log_values = np.log(targets[col] + 1e-8)
                log_values = log_values.replace([np.inf, -np.inf], np.nan)
                targets[f'log_{col}'] = log_values

        print(f"âœ… ê°•ê±´í•œ íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ: {len(targets.columns)}ê°œ")
        return targets

    def smart_feature_selection(self, features, targets, target_name, max_features=15):
        """ì§€ëŠ¥ì  íŠ¹ì„± ì„ íƒ (NaN ê³ ë ¤)"""
        print(f"ğŸ¯ ì§€ëŠ¥ì  íŠ¹ì„± ì„ íƒ: {target_name}")

        target_series = targets[target_name]

        # 1. ê° íŠ¹ì„±ë³„ ìœ íš¨ ë°ì´í„° ë¹„ìœ¨ ê³„ì‚°
        feature_stats = {}
        for col in features.columns:
            feature_series = features[col]

            # íŠ¹ì„±ê³¼ íƒ€ê²Ÿì´ ëª¨ë‘ ìœ íš¨í•œ ìƒ˜í”Œ ìˆ˜
            valid_mask = feature_series.notna() & target_series.notna()
            valid_count = valid_mask.sum()
            total_count = len(feature_series)
            valid_ratio = valid_count / total_count

            if valid_count > 100:  # ìµœì†Œ 100ê°œ ìƒ˜í”Œ í•„ìš”
                # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                correlation = feature_series[valid_mask].corr(target_series[valid_mask])
                if not np.isnan(correlation):
                    feature_stats[col] = {
                        'valid_count': valid_count,
                        'valid_ratio': valid_ratio,
                        'correlation': abs(correlation),
                        'score': abs(correlation) * valid_ratio  # ë³µí•© ì ìˆ˜
                    }

        if not feature_stats:
            print(f"âŒ {target_name}: ìœ íš¨í•œ íŠ¹ì„±ì´ ì—†ìŒ")
            return None, None, 0

        # 2. íŠ¹ì„± ì„ íƒ (ë³µí•© ì ìˆ˜ ê¸°ì¤€)
        sorted_features = sorted(feature_stats.items(),
                               key=lambda x: x[1]['score'], reverse=True)

        print(f"íŠ¹ì„± í†µê³„ (ìƒìœ„ 10ê°œ):")
        for i, (feature, stats) in enumerate(sorted_features[:10]):
            print(f"  {i+1:2d}. {feature:25}: ìƒê´€={stats['correlation']:.3f}, "
                  f"ìœ íš¨={stats['valid_ratio']:.2f}, ì ìˆ˜={stats['score']:.3f}")

        # ìµœëŒ€ max_featuresê°œ ì„ íƒ
        selected_features = [f for f, _ in sorted_features[:max_features]]

        # 3. ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±
        X = features[selected_features].copy()
        y = target_series.copy()

        # ëª¨ë“  ì„ íƒëœ íŠ¹ì„±ê³¼ íƒ€ê²Ÿì´ ìœ íš¨í•œ ìƒ˜í”Œë§Œ ì„ íƒ
        valid_mask = X.notna().all(axis=1) & y.notna()
        X_clean = X[valid_mask]
        y_clean = y[valid_mask]

        print(f"ìµœì¢… ë°ì´í„°ì…‹: {len(X_clean)} ìƒ˜í”Œ, {len(selected_features)} íŠ¹ì„±")

        return X_clean, y_clean, len(X_clean)

    def train_focused_models(self, X, y, target_name):
        """ì§‘ì¤‘ëœ ëª¨ë¸ í›ˆë ¨"""
        print(f"ğŸ¤– ì§‘ì¤‘ëœ ëª¨ë¸ í›ˆë ¨: {target_name}")

        if len(X) < 100:
            print(f"âŒ ë°ì´í„° ë¶€ì¡±: {len(X)} ìƒ˜í”Œ")
            return {}

        models = {}

        # 1. Ridge íšŒê·€ (ì—¬ëŸ¬ ì •ê·œí™” ê°•ë„)
        for alpha in [0.1, 1.0, 10.0]:
            models[f'Ridge_a{alpha}'] = Ridge(alpha=alpha)

        # 2. ElasticNet
        models['ElasticNet'] = ElasticNet(alpha=0.01, l1_ratio=0.7, max_iter=3000)

        # 3. ì•™ìƒë¸” ëª¨ë¸ë“¤
        models['RandomForest'] = RandomForestRegressor(
            n_estimators=100, max_depth=8, min_samples_split=10,
            random_state=42, n_jobs=-1)

        models['GradientBoosting'] = GradientBoostingRegressor(
            n_estimators=100, max_depth=4, learning_rate=0.1,
            random_state=42)

        # 4. ì‹ ê²½ë§ (ì ë‹¹í•œ í¬ê¸°)
        models['MLP'] = MLPRegressor(
            hidden_layer_sizes=(50, 25), max_iter=1000,
            random_state=42, early_stopping=True, validation_fraction=0.2)

        # ëª¨ë¸ í›ˆë ¨
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        trained_models = {}
        for name, model in models.items():
            try:
                model.fit(X_scaled, y)
                trained_models[name] = {'model': model, 'scaler': scaler}
                print(f"âœ… {name} í›ˆë ¨ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {name} í›ˆë ¨ ì‹¤íŒ¨: {e}")

        return trained_models

    def walk_forward_validation(self, X, y, trained_models, target_name):
        """Walk-Forward ê²€ì¦"""
        print(f"ğŸ“Š Walk-Forward ê²€ì¦: {target_name}")

        if len(X) < 200:
            print(f"âŒ Walk-Forward ê²€ì¦ì— ë°ì´í„° ë¶€ì¡±: {len(X)} ìƒ˜í”Œ")
            return {}

        # ê²€ì¦ ì„¤ì •
        initial_window = min(504, len(X) // 3)  # ì´ˆê¸° í›ˆë ¨ ìœˆë„ìš°
        refit_frequency = 63  # ì¬í›ˆë ¨ ì£¼ê¸°
        prediction_horizon = 21  # ì˜ˆì¸¡ êµ¬ê°„

        results = {}

        for model_name, model_info in trained_models.items():
            model = model_info['model']
            scaler = model_info['scaler']

            predictions = []
            actuals = []

            start_idx = initial_window
            while start_idx + prediction_horizon < len(X):
                # í›ˆë ¨ ë°ì´í„°
                train_end = start_idx
                train_start = max(0, train_end - initial_window)

                X_train = X.iloc[train_start:train_end]
                y_train = y.iloc[train_start:train_end]

                # í…ŒìŠ¤íŠ¸ ë°ì´í„°
                test_end = min(start_idx + prediction_horizon, len(X))
                X_test = X.iloc[start_idx:test_end]
                y_test = y.iloc[start_idx:test_end]

                if len(X_train) > 50 and len(X_test) > 0:
                    try:
                        # ì¬í›ˆë ¨
                        scaler_fold = StandardScaler()
                        X_train_scaled = scaler_fold.fit_transform(X_train)
                        X_test_scaled = scaler_fold.transform(X_test)

                        # ìƒˆë¡œìš´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                        if 'Ridge' in model_name:
                            alpha = float(model_name.split('_a')[1])
                            model_fold = Ridge(alpha=alpha)
                        elif model_name == 'ElasticNet':
                            model_fold = ElasticNet(alpha=0.01, l1_ratio=0.7, max_iter=3000)
                        elif model_name == 'RandomForest':
                            model_fold = RandomForestRegressor(
                                n_estimators=100, max_depth=8, min_samples_split=10,
                                random_state=42, n_jobs=-1)
                        elif model_name == 'GradientBoosting':
                            model_fold = GradientBoostingRegressor(
                                n_estimators=100, max_depth=4, learning_rate=0.1,
                                random_state=42)
                        elif model_name == 'MLP':
                            model_fold = MLPRegressor(
                                hidden_layer_sizes=(50, 25), max_iter=1000,
                                random_state=42, early_stopping=True, validation_fraction=0.2)
                        else:
                            continue

                        model_fold.fit(X_train_scaled, y_train)
                        y_pred = model_fold.predict(X_test_scaled)

                        predictions.extend(y_pred)
                        actuals.extend(y_test.values)

                    except Exception as e:
                        print(f"âš ï¸ {model_name} Fold ì‹¤íŒ¨: {e}")

                start_idx += refit_frequency

            if len(predictions) > 10:
                r2 = r2_score(actuals, predictions)
                mse = mean_squared_error(actuals, predictions)

                results[model_name] = {
                    'r2': r2,
                    'mse': mse,
                    'predictions_count': len(predictions)
                }

                print(f"  {model_name:15}: RÂ² = {r2:6.3f}, MSE = {mse:6.3f}")

        return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ State-of-the-Art Volatility Model (Fixed) - RÂ² 0.25+ ëª©í‘œ")
    print("="*80)
    print("ë°ì´í„° ì²˜ë¦¬ ë¬¸ì œ í•´ê²° ë²„ì „")
    print("="*80)

    predictor = SOTAVolatilityPredictorFixed()

    # 1. ë°ì´í„° ë¡œë“œ
    data = predictor.load_comprehensive_data()

    # 2. íŠ¹ì„± ìƒì„±
    features = predictor.create_stable_features(data)

    # 3. íƒ€ê²Ÿ ìƒì„±
    targets = predictor.create_robust_targets(data)

    print(f"\në°ì´í„° ìš”ì•½:")
    print(f"  ì „ì²´ ë°ì´í„°: {len(data)} ê´€ì¸¡ì¹˜")
    print(f"  íŠ¹ì„± ìˆ˜: {len(features.columns)}ê°œ")
    print(f"  íƒ€ê²Ÿ ìˆ˜: {len(targets.columns)}ê°œ")

    # 4. ê° íƒ€ê²Ÿì— ëŒ€í•´ ëª¨ë¸ë§
    target_priority = ['target_vol_5d', 'target_vol_3d', 'target_vol_1d', 'log_target_vol_5d']

    best_performance = {'target': None, 'model': None, 'r2': -999}
    all_results = {}

    for target_name in target_priority:
        if target_name not in targets.columns:
            continue

        print(f"\n" + "="*60)
        print(f"ğŸ¯ íƒ€ê²Ÿ: {target_name}")
        print("="*60)

        # íŠ¹ì„± ì„ íƒ
        X, y, sample_count = predictor.smart_feature_selection(features, targets, target_name)

        if sample_count < 100:
            print(f"âŒ {target_name}: ë°ì´í„° ë¶€ì¡± ({sample_count} ìƒ˜í”Œ)")
            continue

        # ëª¨ë¸ í›ˆë ¨
        trained_models = predictor.train_focused_models(X, y, target_name)

        if not trained_models:
            continue

        # Walk-Forward ê²€ì¦
        validation_results = predictor.walk_forward_validation(X, y, trained_models, target_name)

        if validation_results:
            all_results[target_name] = validation_results

            # ìµœê³  ì„±ëŠ¥ ì¶”ì 
            for model_name, stats in validation_results.items():
                if stats['r2'] > best_performance['r2']:
                    best_performance = {
                        'target': target_name,
                        'model': model_name,
                        'r2': stats['r2'],
                        'mse': stats['mse'],
                        'sample_count': sample_count
                    }

    # 5. ìµœì¢… ê²°ê³¼
    print(f"\n" + "="*80)
    print(f"ğŸ† ìµœì¢… ê²°ê³¼")
    print("="*80)

    if best_performance['target']:
        print(f"ìµœê³  ì„±ëŠ¥:")
        print(f"  íƒ€ê²Ÿ: {best_performance['target']}")
        print(f"  ëª¨ë¸: {best_performance['model']}")
        print(f"  RÂ²: {best_performance['r2']:.4f}")
        print(f"  MSE: {best_performance['mse']:.4f}")
        print(f"  ìƒ˜í”Œ ìˆ˜: {best_performance['sample_count']}")

        target_r2 = 0.25
        if best_performance['r2'] >= target_r2:
            print(f"\nğŸ‰ ëª©í‘œ ë‹¬ì„±! RÂ² â‰¥ {target_r2}")
        else:
            print(f"\nğŸ“ˆ ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: RÂ² â‰¥ {target_r2})")
            gap = target_r2 - best_performance['r2']
            print(f"   ëª©í‘œê¹Œì§€ ê°­: {gap:.4f}")
    else:
        print("âŒ ëª¨ë“  ëª¨ë¸ì—ì„œ ìœ íš¨í•œ ì„±ëŠ¥ì„ ì–»ì§€ ëª»í•¨")

    # 6. ê²°ê³¼ ì €ì¥
    os.makedirs('results', exist_ok=True)

    fixed_results = {
        'version': 'SOTA_Volatility_Model_Fixed',
        'timestamp': datetime.now().isoformat(),
        'goal': 'RÂ² â‰¥ 0.25',
        'best_performance': best_performance,
        'all_results': all_results,
        'data_summary': {
            'total_observations': len(data),
            'total_features': len(features.columns),
            'total_targets': len(targets.columns)
        }
    }

    with open('results/sota_volatility_model_fixed.json', 'w') as f:
        json.dump(fixed_results, f, indent=2, default=str)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: results/sota_volatility_model_fixed.json")
    print("="*80)

if __name__ == "__main__":
    main()