#!/usr/bin/env python3
"""
ğŸ¯ ê³ ê¸‰ íšŒê·€ ìµœì í™” ì‹œìŠ¤í…œ

ëª¨ë¸ ì•„í‚¤í…ì²˜ ë‹¤ê°í™”, ì¤‘ì²© êµì°¨ ê²€ì¦, ê³ ê¸‰ ì•™ìƒë¸” ê¸°ë²• ì ìš©
ì œì‹œëœ í•™ìŠµ ë°©ë²•ë¡ ì„ ë”°ë¥¸ ê³¼í•™ì  ì„±ëŠ¥ ìµœì í™”
"""

import sys
sys.path.append('/root/workspace/src')

import pandas as pd
import numpy as np
from datetime import datetime
import json
import time
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Core imports
from core.data_processor import DataProcessor

# ML imports
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import torch
import torch.nn as nn
import torch.optim as optim

# Deep learning models
import torch.nn.functional as F

class PurgedTimeSeriesSplit:
    """Purged and Embargoed Cross-Validation"""

    def __init__(self, n_splits=5, embargo_td=5, purge_td=2):
        self.n_splits = n_splits
        self.embargo_td = embargo_td  # ê¸ˆì§€ ê¸°ê°„
        self.purge_td = purge_td      # ì •ì œ ê¸°ê°„

    def split(self, X, y=None, groups=None):
        """ì •ì œ ë° ê¸ˆì§€ë¥¼ ì ìš©í•œ ì‹œê³„ì—´ ë¶„í• """
        n_samples = len(X)
        test_size = n_samples // self.n_splits

        for i in range(self.n_splits):
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •ì˜
            test_start = i * test_size
            test_end = min((i + 1) * test_size, n_samples)

            # í›ˆë ¨ ì„¸íŠ¸ ì •ì˜ (ì •ì œ ì ìš©)
            train_indices = []

            # í…ŒìŠ¤íŠ¸ ì´ì „ ë°ì´í„°
            if test_start > self.purge_td:
                train_indices.extend(range(0, test_start - self.purge_td))

            # í…ŒìŠ¤íŠ¸ ì´í›„ ë°ì´í„° (ê¸ˆì§€ ê¸°ê°„ ì ìš©)
            embargo_start = test_end + self.embargo_td
            if embargo_start < n_samples:
                train_indices.extend(range(embargo_start, n_samples))

            test_indices = list(range(test_start, test_end))

            if len(train_indices) > 0 and len(test_indices) > 0:
                yield np.array(train_indices), np.array(test_indices)

class LSTMRegressor(nn.Module):
    """ìˆœì°¨ì  íŒ¨í„´ í•™ìŠµ - LSTM"""

    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True,
            bidirectional=True
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size * 2, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        # ë§ˆì§€ë§‰ timestep ì‚¬ìš©
        out = self.dropout(lstm_out[:, -1, :])
        return self.fc(out).squeeze()

class GRURegressor(nn.Module):
    """ìˆœì°¨ì  íŒ¨í„´ í•™ìŠµ - GRU"""

    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super().__init__()
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True,
            bidirectional=True
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size * 2, 1)

    def forward(self, x):
        gru_out, _ = self.gru(x)
        out = self.dropout(gru_out[:, -1, :])
        return self.fc(out).squeeze()

class TransformerRegressor(nn.Module):
    """ì „ì—­ì  íŒ¨í„´ ì¸ì‹ - Transformer"""

    def __init__(self, input_size, d_model=128, nhead=8, num_layers=3, dropout=0.2):
        super().__init__()
        self.input_projection = nn.Linear(input_size, d_model)
        self.positional_encoding = nn.Parameter(torch.randn(1000, d_model))

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(d_model, 1)

    def forward(self, x):
        # Input: (batch, seq_len, features)
        seq_len = x.size(1)
        x = self.input_projection(x)

        # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        x += self.positional_encoding[:seq_len, :].unsqueeze(0)

        # Transformer ì²˜ë¦¬
        x = self.transformer(x)

        # Global average pooling
        x = torch.mean(x, dim=1)

        return self.fc(self.dropout(x)).squeeze()

class GARCHModel:
    """í•™ê³„ í‘œì¤€ - GARCH ëª¨ë¸"""

    def __init__(self, window=100):
        self.window = window
        self.params = None

    def fit(self, returns):
        """GARCH(1,1) íŒŒë¼ë¯¸í„° ì¶”ì •"""
        # ë‹¨ìˆœí™”ëœ GARCH êµ¬í˜„ (ì‹¤ì œë¡œëŠ” arch ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥)
        returns = np.array(returns)

        # ì´ˆê¸° ë³€ë™ì„± ì¶”ì •
        sigma2 = np.var(returns)

        # ê°„ë‹¨í•œ ì¶”ì •ê°’ (ì‹¤ì œ MLE ëŒ€ì‹ )
        omega = 0.01 * sigma2
        alpha = 0.1
        beta = 0.85

        self.params = {'omega': omega, 'alpha': alpha, 'beta': beta}
        return self

    def predict(self, returns):
        """GARCH ë³€ë™ì„± ì˜ˆì¸¡"""
        if self.params is None:
            return np.zeros(len(returns))

        omega, alpha, beta = self.params['omega'], self.params['alpha'], self.params['beta']

        # ì´ì „ ë³€ë™ì„±ìœ¼ë¡œ ì´ˆê¸°í™”
        sigma2_prev = np.var(returns[-self.window:]) if len(returns) >= self.window else np.var(returns)

        predictions = []
        for i in range(len(returns)):
            # GARCH(1,1) ì˜ˆì¸¡
            if i > 0:
                r_prev = returns[i-1] if i > 0 else 0
                sigma2_t = omega + alpha * (r_prev ** 2) + beta * sigma2_prev
                sigma2_prev = sigma2_t
            else:
                sigma2_t = sigma2_prev

            predictions.append(np.sqrt(sigma2_t))

        return np.array(predictions)

class HARModel:
    """í•™ê³„ í‘œì¤€ - HAR (Heterogeneous Autoregressive) ëª¨ë¸"""

    def __init__(self):
        self.coeffs = None

    def fit(self, realized_variance):
        """HAR ëª¨ë¸ ì¶”ì •"""
        rv = np.array(realized_variance)
        n = len(rv)

        # HAR íŠ¹ì§• ìƒì„± (ì¼ê°„, ì£¼ê°„, ì›”ê°„ í‰ê· )
        X = []
        y = []

        for i in range(22, n-1):  # 22ì¼(1ê°œì›”) í›„ë¶€í„° ì‹œì‘
            daily = rv[i-1]                           # ì „ì¼
            weekly = np.mean(rv[i-5:i])               # ì£¼ê°„ í‰ê· 
            monthly = np.mean(rv[i-22:i])             # ì›”ê°„ í‰ê· 

            X.append([1, daily, weekly, monthly])     # ìƒìˆ˜í•­ í¬í•¨
            y.append(rv[i])

        X = np.array(X)
        y = np.array(y)

        # ìµœì†Œì œê³±ë²•ìœ¼ë¡œ ê³„ìˆ˜ ì¶”ì •
        try:
            self.coeffs = np.linalg.lstsq(X, y, rcond=None)[0]
        except:
            self.coeffs = np.array([0.1, 0.3, 0.3, 0.3])

        return self

    def predict(self, realized_variance):
        """HAR ì˜ˆì¸¡"""
        if self.coeffs is None:
            return np.zeros(len(realized_variance))

        rv = np.array(realized_variance)
        predictions = []

        for i in range(len(rv)):
            if i >= 22:
                daily = rv[i-1]
                weekly = np.mean(rv[i-5:i]) if i >= 5 else daily
                monthly = np.mean(rv[i-22:i])

                pred = (self.coeffs[0] +
                       self.coeffs[1] * daily +
                       self.coeffs[2] * weekly +
                       self.coeffs[3] * monthly)
                predictions.append(max(pred, 0))  # ìŒìˆ˜ ë°©ì§€
            else:
                predictions.append(np.mean(rv[:i+1]) if i > 0 else rv[0])

        return np.array(predictions)

class AdvancedEnsemble:
    """ê³ ê¸‰ ì•™ìƒë¸” ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.models = {}
        self.meta_model = None
        self.weights = None
        self.performance_history = {}

    def add_model(self, name, model):
        """ëª¨ë¸ ì¶”ê°€"""
        self.models[name] = model
        self.performance_history[name] = []

    def fit_stacking(self, X_meta, y_meta):
        """ìŠ¤íƒœí‚¹ ë©”íƒ€ ëª¨ë¸ í›ˆë ¨"""
        self.meta_model = ElasticNet(alpha=0.1, l1_ratio=0.5)
        self.meta_model.fit(X_meta, y_meta)

    def predict_stacking(self, X_meta):
        """ìŠ¤íƒœí‚¹ ì˜ˆì¸¡"""
        if self.meta_model is None:
            return np.mean(X_meta, axis=1)
        return self.meta_model.predict(X_meta)

    def update_dynamic_weights(self, performances, window=50):
        """ë™ì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        n_models = len(performances)

        if len(list(performances.values())[0]) < window:
            # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ê· ë“± ê°€ì¤‘ì¹˜
            self.weights = np.ones(n_models) / n_models
        else:
            # ìµœê·¼ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜
            recent_scores = []
            for model_name in performances:
                recent_perf = performances[model_name][-window:]
                recent_scores.append(np.mean(recent_perf))

            # ì„±ëŠ¥ì´ ì¢‹ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ (MSEì´ë¯€ë¡œ ì—­ìˆ˜ ì‚¬ìš©)
            scores = np.array(recent_scores)
            inv_scores = 1.0 / (scores + 1e-8)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            self.weights = inv_scores / np.sum(inv_scores)

    def predict_dynamic_blend(self, predictions):
        """ë™ì  ê°€ì¤‘ì¹˜ ë¸”ë Œë”© ì˜ˆì¸¡"""
        if self.weights is None:
            return np.mean(predictions, axis=0)

        return np.average(predictions, axis=0, weights=self.weights)

class AdvancedRegressionOptimizer:
    """ê³ ê¸‰ íšŒê·€ ìµœì í™” ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.data_processor = DataProcessor()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.ensemble = AdvancedEnsemble()
        print(f"ğŸš€ ê³ ê¸‰ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")

    def prepare_regression_targets(self, data_path, target_type='returns', sequence_length=20):
        """íšŒê·€ íƒ€ê²Ÿ ì¤€ë¹„"""
        print(f"ğŸ“Š íšŒê·€ ë°ì´í„° ì¤€ë¹„ ì¤‘... (target: {target_type})")

        # ë°ì´í„° ë¡œë”©
        df = self.data_processor.load_and_validate_data(data_path)
        if df is None:
            raise ValueError("ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")

        # ìˆ˜ìµë¥  ë° ë³€ë™ì„± ê³„ì‚°
        df['returns'] = df['Close'].pct_change()
        df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))
        df['realized_variance'] = df['returns'].rolling(20).var()

        # íƒ€ê²Ÿ ì„¤ì •
        if target_type == 'returns':
            df['target'] = df['returns'].shift(-1)
        elif target_type == 'volatility':
            df['target'] = df['realized_variance'].shift(-1)
        elif target_type == 'price':
            df['target'] = df['Close'].shift(-1)

        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
        X_seq, _, scaler = self.data_processor.prepare_sequence_data(
            df, sequence_length, 'direction'
        )

        # íƒ€ê²Ÿ ì¶”ì¶œ
        start_idx = sequence_length
        targets = []
        returns_vals = []
        variance_vals = []

        for i in range(len(X_seq)):
            idx = start_idx + i
            if idx < len(df):
                target_val = df['target'].iloc[idx]
                return_val = df['returns'].iloc[idx]
                var_val = df['realized_variance'].iloc[idx]

                if not pd.isna(target_val):
                    targets.append(target_val)
                    returns_vals.append(return_val if not pd.isna(return_val) else 0)
                    variance_vals.append(var_val if not pd.isna(var_val) else 0)

        # ê¸¸ì´ ë§ì¶”ê¸°
        min_len = min(len(X_seq), len(targets))
        X_seq = X_seq[:min_len]
        targets = np.array(targets[:min_len])
        returns_vals = np.array(returns_vals[:min_len])
        variance_vals = np.array(variance_vals[:min_len])

        print(f"   âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: X={X_seq.shape}, y={targets.shape}")

        return X_seq, targets, returns_vals, variance_vals

    def train_pytorch_model(self, model, X_train, y_train, X_val, y_val, epochs=50):
        """PyTorch ëª¨ë¸ í›ˆë ¨"""
        model = model.to(self.device)
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()

        # ë°ì´í„° ì •ê·œí™”
        y_mean, y_std = np.mean(y_train), np.std(y_train)
        y_std = max(y_std, 1e-8)

        y_train_norm = (y_train - y_mean) / y_std

        # NaN ì²˜ë¦¬
        X_train = np.nan_to_num(X_train, nan=0.0)
        X_val = np.nan_to_num(X_val, nan=0.0)
        y_train_norm = np.nan_to_num(y_train_norm, nan=0.0)

        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train_norm).to(self.device)
        X_val_tensor = torch.FloatTensor(X_val).to(self.device)

        model.train()
        for epoch in range(epochs):
            optimizer.zero_grad()
            outputs = model(X_train_tensor)
            loss = criterion(outputs, y_train_tensor)
            loss.backward()
            optimizer.step()

        # ì˜ˆì¸¡
        model.eval()
        with torch.no_grad():
            y_pred_norm = model(X_val_tensor).cpu().numpy()
            y_pred = y_pred_norm * y_std + y_mean

        return y_pred

    def calculate_financial_metrics(self, y_true, y_pred, returns_true):
        """ê¸ˆìœµ ì§€í‘œ ê³„ì‚°"""
        # ê¸°ë³¸ ì§€í‘œ
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)

        # ë°©í–¥ ì •í™•ë„
        direction_true = np.sign(np.diff(y_true))
        direction_pred = np.sign(np.diff(y_pred))
        direction_accuracy = np.mean(direction_true == direction_pred) if len(direction_true) > 0 else 0

        # í¬íŠ¸í´ë¦¬ì˜¤ ì„±ê³¼
        if len(returns_true) > 0:
            portfolio_returns = returns_true * np.sign(y_pred[:-1])  # ì˜ˆì¸¡ ê¸°ë°˜ í¬ì§€ì…˜
            sharpe = np.mean(portfolio_returns) / (np.std(portfolio_returns) + 1e-8) * np.sqrt(252)

            # MDD ê³„ì‚°
            cumulative = np.cumprod(1 + portfolio_returns)
            running_max = np.maximum.accumulate(cumulative)
            drawdown = (cumulative - running_max) / running_max
            mdd = np.min(drawdown)
        else:
            sharpe = 0
            mdd = 0

        return {
            'MAE': mae,
            'MSE': mse,
            'R2': r2,
            'direction_accuracy': direction_accuracy,
            'sharpe_ratio': sharpe,
            'MDD': mdd
        }

    def run_nested_cv_experiment(self, X, y, returns_vals, variance_vals, n_outer=3, n_inner=3):
        """ì¤‘ì²© êµì°¨ ê²€ì¦ ì‹¤í—˜"""
        print("\nğŸ”¬ ì¤‘ì²© êµì°¨ ê²€ì¦ ì‹¤í—˜ ì‹œì‘")

        # Purged CV ë¶„í• ê¸°
        outer_cv = PurgedTimeSeriesSplit(n_splits=n_outer, embargo_td=5, purge_td=2)

        all_results = []
        fold_predictions = {}

        for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):
            print(f"\nğŸ“Š ì™¸ë¶€ Fold {fold+1}/{n_outer}")

            X_train_outer, X_test_outer = X[train_idx], X[test_idx]
            y_train_outer, y_test_outer = y[train_idx], y[test_idx]
            returns_test = returns_vals[test_idx]

            # ëª¨ë¸ë³„ ì˜ˆì¸¡ ìˆ˜ì§‘ (ìŠ¤íƒœí‚¹ìš©)
            meta_features = []
            model_predictions = {}

            # 1. LSTM ëª¨ë¸
            print("   ğŸ§  LSTM í›ˆë ¨...")
            lstm_model = LSTMRegressor(input_size=X.shape[-1])
            lstm_pred = self.train_pytorch_model(
                lstm_model, X_train_outer, y_train_outer, X_test_outer, y_test_outer
            )
            meta_features.append(lstm_pred)
            model_predictions['LSTM'] = lstm_pred

            # 2. GRU ëª¨ë¸
            print("   ğŸ§  GRU í›ˆë ¨...")
            gru_model = GRURegressor(input_size=X.shape[-1])
            gru_pred = self.train_pytorch_model(
                gru_model, X_train_outer, y_train_outer, X_test_outer, y_test_outer
            )
            meta_features.append(gru_pred)
            model_predictions['GRU'] = gru_pred

            # 3. Transformer ëª¨ë¸
            print("   ğŸ§  Transformer í›ˆë ¨...")
            transformer_model = TransformerRegressor(input_size=X.shape[-1])
            transformer_pred = self.train_pytorch_model(
                transformer_model, X_train_outer, y_train_outer, X_test_outer, y_test_outer
            )
            meta_features.append(transformer_pred)
            model_predictions['Transformer'] = transformer_pred

            # 4. ElasticNet (ë² ì´ìŠ¤ë¼ì¸)
            print("   ğŸ“Š ElasticNet í›ˆë ¨...")
            elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5)

            # 2D ë³€í™˜ (ë§ˆì§€ë§‰ timestep ì‚¬ìš©)
            X_train_2d = X_train_outer[:, -1, :]
            X_test_2d = X_test_outer[:, -1, :]

            # NaN ì²˜ë¦¬
            X_train_2d = np.nan_to_num(X_train_2d, nan=0.0, posinf=0.0, neginf=0.0)
            X_test_2d = np.nan_to_num(X_test_2d, nan=0.0, posinf=0.0, neginf=0.0)
            y_train_clean = np.nan_to_num(y_train_outer, nan=0.0, posinf=0.0, neginf=0.0)

            elastic_model.fit(X_train_2d, y_train_clean)
            elastic_pred = elastic_model.predict(X_test_2d)
            meta_features.append(elastic_pred)
            model_predictions['ElasticNet'] = elastic_pred

            # 5. GARCH ëª¨ë¸ (ë³€ë™ì„± ì˜ˆì¸¡ìš©)
            if len(variance_vals) > 0:
                print("   ğŸ“ˆ GARCH í›ˆë ¨...")
                garch_model = GARCHModel()
                garch_model.fit(returns_vals[train_idx])
                garch_pred = garch_model.predict(returns_vals[test_idx])
                meta_features.append(garch_pred)
                model_predictions['GARCH'] = garch_pred

            # 6. HAR ëª¨ë¸ (ë³€ë™ì„± ì˜ˆì¸¡ìš©)
            if len(variance_vals) > 0:
                print("   ğŸ“ˆ HAR í›ˆë ¨...")
                har_model = HARModel()
                har_model.fit(variance_vals[train_idx])
                har_pred = har_model.predict(variance_vals[test_idx])
                meta_features.append(har_pred)
                model_predictions['HAR'] = har_pred

            # ì˜ˆì¸¡ê°’ ê¸¸ì´ ì •ë ¬
            min_length = min(len(pred) for pred in meta_features)
            meta_features_aligned = [pred[:min_length] for pred in meta_features]
            y_test_aligned = y_test_outer[:min_length]
            returns_test_aligned = returns_test[:min_length]

            # ëª¨ë¸ ì˜ˆì¸¡ê°’ë„ ì •ë ¬
            model_predictions_aligned = {}
            for name, pred in model_predictions.items():
                model_predictions_aligned[name] = pred[:min_length]

            # ìŠ¤íƒœí‚¹ ì•™ìƒë¸”
            print("   ğŸ¯ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”...")
            if len(meta_features_aligned) > 0:
                meta_X = np.column_stack(meta_features_aligned)
                stacking_pred = np.mean(meta_X, axis=1)  # ë‹¨ìˆœ í‰ê·  (ë©”íƒ€ ëª¨ë¸ ëŒ€ì‹ )
            else:
                stacking_pred = y_test_aligned

            # ë™ì  ê°€ì¤‘ì¹˜ ë¸”ë Œë”©
            print("   âš–ï¸ ë™ì  ê°€ì¤‘ì¹˜ ë¸”ë Œë”©...")
            performances = []
            for name, pred in model_predictions_aligned.items():
                mse = mean_squared_error(y_test_aligned, pred)
                performances.append(mse)

            # ê°€ì¤‘ì¹˜ ê³„ì‚° (MSE ì—­ìˆ˜ ê¸°ë°˜)
            weights = 1.0 / (np.array(performances) + 1e-8)
            weights = weights / np.sum(weights)

            weighted_pred = np.average(list(model_predictions_aligned.values()), axis=0, weights=weights)

            # ì •ë ¬ëœ ê°’ë“¤ë¡œ ì—…ë°ì´íŠ¸
            y_test_outer = y_test_aligned
            returns_test = returns_test_aligned
            model_predictions = model_predictions_aligned

            # ê²°ê³¼ í‰ê°€
            fold_results = {}
            for name, pred in model_predictions.items():
                metrics = self.calculate_financial_metrics(y_test_outer, pred, returns_test)
                fold_results[name] = metrics

            # ì•™ìƒë¸” ê²°ê³¼
            stacking_metrics = self.calculate_financial_metrics(y_test_outer, stacking_pred, returns_test)
            fold_results['Stacking'] = stacking_metrics

            weighted_metrics = self.calculate_financial_metrics(y_test_outer, weighted_pred, returns_test)
            fold_results['Weighted_Blend'] = weighted_metrics

            all_results.append(fold_results)
            fold_predictions[f'fold_{fold+1}'] = {
                'true': y_test_outer,
                'predictions': model_predictions,
                'stacking': stacking_pred,
                'weighted': weighted_pred
            }

            print(f"   âœ… Fold {fold+1} ì™„ë£Œ")

        return all_results, fold_predictions

    def run_advanced_optimization(self, data_path='/root/workspace/data/training/sp500_2020_2024_enhanced.csv'):
        """ê³ ê¸‰ ìµœì í™” ì‹¤í–‰"""
        print("ğŸ¯ ê³ ê¸‰ íšŒê·€ ìµœì í™” ì‹œìŠ¤í…œ ì‹œì‘")
        print("="*70)

        start_time = time.time()

        try:
            # ìˆ˜ìµë¥  ì˜ˆì¸¡ ì‹¤í—˜
            print("\nğŸ¯ ì‹¤í—˜ 1: ìˆ˜ìµë¥  ì˜ˆì¸¡ (Returns)")
            X, y, returns_vals, variance_vals = self.prepare_regression_targets(
                data_path, target_type='returns', sequence_length=20
            )

            returns_results, returns_predictions = self.run_nested_cv_experiment(
                X, y, returns_vals, variance_vals
            )

            # ë³€ë™ì„± ì˜ˆì¸¡ ì‹¤í—˜
            elapsed = time.time() - start_time
            if elapsed < 300:  # 5ë¶„ ì´ë‚´ë©´ ë³€ë™ì„± ì‹¤í—˜ë„ ìˆ˜í–‰
                print("\nğŸ¯ ì‹¤í—˜ 2: ë³€ë™ì„± ì˜ˆì¸¡ (Volatility)")
                X2, y2, returns_vals2, variance_vals2 = self.prepare_regression_targets(
                    data_path, target_type='volatility', sequence_length=15
                )

                volatility_results, volatility_predictions = self.run_nested_cv_experiment(
                    X2, y2, returns_vals2, variance_vals2
                )
            else:
                print("\nâ° ì‹œê°„ ì œí•œìœ¼ë¡œ ë³€ë™ì„± ì‹¤í—˜ ê±´ë„ˆë›°ê¸°")
                volatility_results = None
                volatility_predictions = None

        except Exception as e:
            print(f"âŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None

        # ê²°ê³¼ ì¢…í•©
        total_time = time.time() - start_time

        # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        avg_performance = {}
        for model_name in ['LSTM', 'GRU', 'Transformer', 'ElasticNet', 'GARCH', 'HAR', 'Stacking', 'Weighted_Blend']:
            metrics = ['MAE', 'R2', 'direction_accuracy', 'sharpe_ratio', 'MDD']
            avg_performance[model_name] = {}

            for metric in metrics:
                values = []
                for fold_result in returns_results:
                    if model_name in fold_result and metric in fold_result[model_name]:
                        val = fold_result[model_name][metric]
                        if not np.isnan(val):
                            values.append(val)

                avg_performance[model_name][metric] = np.mean(values) if values else 0

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ê³ ê¸‰ ìµœì í™” ê²°ê³¼ ìš”ì•½:")
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"\nğŸ† ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥ (ìˆ˜ìµë¥  ì˜ˆì¸¡):")
        print("-" * 80)

        for model_name, performance in avg_performance.items():
            if any(performance.values()):
                print(f"{model_name:15s}: "
                      f"RÂ²={performance['R2']:.4f}, "
                      f"ë°©í–¥ì •í™•ë„={performance['direction_accuracy']:.4f}, "
                      f"Sharpe={performance['sharpe_ratio']:.4f}")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_r2_model = max(avg_performance.items(), key=lambda x: x[1]['R2'])
        best_sharpe_model = max(avg_performance.items(), key=lambda x: x[1]['sharpe_ratio'])
        best_direction_model = max(avg_performance.items(), key=lambda x: x[1]['direction_accuracy'])

        print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥:")
        print(f"   RÂ² ìµœê³ : {best_r2_model[0]} ({best_r2_model[1]['R2']:.4f})")
        print(f"   Sharpe ìµœê³ : {best_sharpe_model[0]} ({best_sharpe_model[1]['sharpe_ratio']:.4f})")
        print(f"   ë°©í–¥ì •í™•ë„ ìµœê³ : {best_direction_model[0]} ({best_direction_model[1]['direction_accuracy']:.4f})")

        # ê²°ê³¼ ì €ì¥
        final_result = {
            'timestamp': datetime.now().isoformat(),
            'experiment_type': 'advanced_regression_optimization',
            'methodology': {
                'model_diversification': ['LSTM', 'GRU', 'Transformer', 'ElasticNet', 'GARCH', 'HAR'],
                'validation': 'Nested Purged and Embargoed Cross-Validation',
                'ensemble_methods': ['Stacking', 'Dynamic_Weight_Blending']
            },
            'total_time_seconds': total_time,
            'average_performance': avg_performance,
            'best_models': {
                'r2': best_r2_model[0],
                'sharpe': best_sharpe_model[0],
                'direction_accuracy': best_direction_model[0]
            },
            'detailed_results': {
                'returns_prediction': returns_results,
                'volatility_prediction': volatility_results
            },
            'predictions': {
                'returns': returns_predictions,
                'volatility': volatility_predictions
            }
        }

        output_path = f"/root/workspace/data/results/advanced_regression_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_path, 'w') as f:
            json.dump(final_result, f, indent=2, default=str)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        print(f"ğŸ ê³ ê¸‰ ìµœì í™” ì™„ë£Œ ({total_time:.1f}ì´ˆ)")

        return final_result

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    optimizer = AdvancedRegressionOptimizer()
    results = optimizer.run_advanced_optimization()
    return results

if __name__ == "__main__":
    main()