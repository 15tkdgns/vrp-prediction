#!/usr/bin/env python3
"""
ğŸš€ ê³ ê¸‰ ëˆ„ì¶œ ë°©ì§€ íšŒê·€ ì‹œìŠ¤í…œ

ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ë°©ì§€ ìƒíƒœì—ì„œ ë” ë§ì€ ê³ ê¸‰ íšŒê·€ ëª¨ë¸ í•™ìŠµ
ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ë²• ì ìš©
"""

import sys
sys.path.append('/root/workspace/src')

import pandas as pd
import numpy as np
from datetime import datetime
import json
import time
import warnings
warnings.filterwarnings('ignore')

# Core imports
from core.data_processor import DataProcessor

# ML imports
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,
                            AdaBoostRegressor, VotingRegressor, BaggingRegressor)
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import (Ridge, BayesianRidge, ElasticNet, Lasso,
                                HuberRegressor, TheilSenRegressor, RANSACRegressor)
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import RobustScaler, StandardScaler, PowerTransformer
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.pipeline import Pipeline
import catboost as cb
import xgboost as xgb
import lightgbm as lgb

# Deep learning
import torch
import torch.nn as nn
import torch.optim as optim

class AdvancedLeakFreeRegression:
    """ê³ ê¸‰ ëˆ„ì¶œ ë°©ì§€ íšŒê·€ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.data_processor = DataProcessor()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.max_allowed_correlation = 0.25  # ë” ì—„ê²©í•œ ê¸°ì¤€

        print(f"ğŸš€ ê³ ê¸‰ ëˆ„ì¶œ ë°©ì§€ íšŒê·€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"   ğŸ’¾ ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   ğŸ”’ ìµœëŒ€ í—ˆìš© ìƒê´€ê´€ê³„: {self.max_allowed_correlation}")

    def create_enhanced_safe_features(self, df):
        """í–¥ìƒëœ ì•ˆì „ íŠ¹ì„± ìƒì„±"""
        print("ğŸ”§ í–¥ìƒëœ ì•ˆì „ íŠ¹ì„± ìƒì„±...")

        safe_df = df.copy()

        # ê¸°ë³¸ ì•ˆì „ íŠ¹ì„±ë“¤
        safe_df['returns'] = safe_df['Close'].pct_change()
        safe_df['log_returns'] = np.log(safe_df['Close'] / safe_df['Close'].shift(1))

        # ê³ ê¸‰ ëª¨ë©˜í…€ íŠ¹ì„± (ë‹¤ì–‘í•œ ê¸°ê°„)
        for period in [3, 5, 7, 10, 14, 20, 30]:
            safe_df[f'momentum_past_{period}'] = (
                safe_df['Close'] / safe_df['Close'].shift(period) - 1
            )
            safe_df[f'returns_mean_past_{period}'] = (
                safe_df['returns'].rolling(period).mean()
            )

        # ê³ ê¸‰ ë³€ë™ì„± íŠ¹ì„±
        for window in [5, 10, 15, 20, 30]:
            safe_df[f'volatility_past_{window}'] = (
                safe_df['returns'].rolling(window).std()
            )
            safe_df[f'volatility_ewm_past_{window}'] = (
                safe_df['returns'].ewm(span=window).std()
            )

        # ê³ ê¸‰ ê¸°ìˆ ì  ì§€í‘œ (ì•ˆì „í•œ ë²„ì „)
        for period in [10, 14, 20, 30]:
            # RSI (ì•ˆì „ ë²„ì „)
            safe_df[f'rsi_past_{period}'] = self._calculate_safe_rsi(safe_df['Close'], period)

            # SMA ë¹„ìœ¨
            safe_df[f'sma_ratio_past_{period}'] = (
                safe_df['Close'] / safe_df['Close'].rolling(period).mean()
            )

            # EMA ë¹„ìœ¨
            safe_df[f'ema_ratio_past_{period}'] = (
                safe_df['Close'] / safe_df['Close'].ewm(span=period).mean()
            )

        # ê³ ê¸‰ ë³¼ë¥¨ íŠ¹ì„± (ì•ˆì „í•œ ë²„ì „)
        for period in [10, 20, 30]:
            safe_df[f'volume_sma_past_{period}'] = safe_df['Volume'].rolling(period).mean()
            safe_df[f'volume_ratio_past_{period}'] = (
                safe_df['Volume'] / safe_df[f'volume_sma_past_{period}']
            )
            safe_df[f'volume_momentum_past_{period}'] = (
                safe_df['Volume'] / safe_df['Volume'].shift(period)
            )

        # ê³ ê¸‰ ê°€ê²© íŠ¹ì„± (í˜„ì¬ ì‹œì ë§Œ)
        safe_df['hl_range_norm'] = (safe_df['High'] - safe_df['Low']) / safe_df['Close']
        safe_df['oc_range_norm'] = abs(safe_df['Open'] - safe_df['Close']) / safe_df['Close']
        safe_df['gap_ratio'] = (safe_df['Open'] - safe_df['Close'].shift(1)) / safe_df['Close'].shift(1)

        # ê³ ê¸‰ í†µê³„ì  íŠ¹ì„± (ê³¼ê±°ë§Œ)
        for window in [10, 20]:
            safe_df[f'skewness_past_{window}'] = safe_df['returns'].rolling(window).skew()
            safe_df[f'kurtosis_past_{window}'] = safe_df['returns'].rolling(window).kurt()
            safe_df[f'quantile_25_past_{window}'] = safe_df['returns'].rolling(window).quantile(0.25)
            safe_df[f'quantile_75_past_{window}'] = safe_df['returns'].rolling(window).quantile(0.75)

        # ê³ ê¸‰ MACD (ì•ˆì „ ë²„ì „)
        for fast_period, slow_period in [(12, 26), (8, 21)]:
            ema_fast = safe_df['Close'].ewm(span=fast_period).mean()
            ema_slow = safe_df['Close'].ewm(span=slow_period).mean()
            safe_df[f'macd_past_{fast_period}_{slow_period}'] = ema_fast - ema_slow

        # íƒ€ê²Ÿ (ìœ ì¼í•œ ë¯¸ë˜ ì •ë³´)
        safe_df['future_return'] = safe_df['Close'].pct_change().shift(-1)
        safe_df['direction_target'] = (safe_df['future_return'] > 0).astype(int)

        # NaN ì²˜ë¦¬
        safe_df = safe_df.fillna(method='ffill').fillna(0)
        safe_df = safe_df.replace([np.inf, -np.inf], 0)

        print(f"   âœ… í–¥ìƒëœ ì•ˆì „ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {safe_df.shape}")
        return safe_df

    def _calculate_safe_rsi(self, prices, window=14):
        """ì•ˆì „í•œ RSI ê³„ì‚° (ê³¼ê±° ë°ì´í„°ë§Œ)"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / (loss + 1e-8)
        return 100 - (100 / (1 + rs))

    def validate_enhanced_safety(self, df):
        """í–¥ìƒëœ ì•ˆì „ì„± ê²€ì¦"""
        print("ğŸ” í–¥ìƒëœ ì•ˆì „ì„± ê²€ì¦ ì‹œìŠ¤í…œ...")

        # ì•ˆì „í•œ íŠ¹ì„±ë§Œ ì„ íƒ
        safe_features = []
        for col in df.columns:
            if col not in ['direction_target', 'future_return', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']:
                # ì¶”ê°€ ì•ˆì „ ê²€ì‚¬: 'next_', 'target', 'future_' í¬í•¨ íŠ¹ì„± ì œì™¸
                if not any(keyword in col.lower() for keyword in ['next_', 'target', 'future_']):
                    safe_features.append(col)

        print(f"   1ì°¨ í•„í„°ë§ í›„ íŠ¹ì„± ìˆ˜: {len(safe_features)}")

        # íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ì—„ê²© ê²€ì‚¬
        suspicious_features = []
        safe_correlations = []

        for feature in safe_features:
            if feature in df.columns:
                corr = abs(df[feature].corr(df['direction_target']))
                if not pd.isna(corr):
                    if corr > self.max_allowed_correlation:
                        suspicious_features.append((feature, corr))
                        print(f"   âš ï¸ ì œê±°: {feature} (ìƒê´€ê´€ê³„: {corr:.4f})")
                    else:
                        safe_correlations.append((feature, corr))

        # ì˜ì‹¬ íŠ¹ì„±ë“¤ ì œê±°
        final_safe_features = [f for f, _ in safe_correlations]

        print(f"   ì œê±°ëœ ì˜ì‹¬ íŠ¹ì„±: {len(suspicious_features)}ê°œ")
        print(f"   ìµœì¢… ì•ˆì „ íŠ¹ì„±: {len(final_safe_features)}ê°œ")
        print(f"   í‰ê·  ìƒê´€ê´€ê³„: {np.mean([c for _, c in safe_correlations]):.4f}")

        return final_safe_features

    def get_advanced_models(self):
        """ê³ ê¸‰ íšŒê·€ ëª¨ë¸ë“¤ ì •ì˜"""
        models = {
            # ì •ê·œí™” íšŒê·€ ëª¨ë¸ë“¤
            'RobustRidge': Ridge(alpha=1.0, random_state=42),
            'TunedBayesianRidge': BayesianRidge(),
            'RobustElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42),
            'RobustLasso': Lasso(alpha=0.1, random_state=42),
            'HuberRegressor': HuberRegressor(epsilon=1.35),
            'TheilSenRegressor': TheilSenRegressor(random_state=42),
            'RANSACRegressor': RANSACRegressor(random_state=42),

            # íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ë“¤ (íŠœë‹ëœ ë²„ì „)
            'TunedRandomForest': RandomForestRegressor(
                n_estimators=100, max_depth=8, min_samples_split=10,
                min_samples_leaf=5, random_state=42
            ),
            'TunedGradientBoosting': GradientBoostingRegressor(
                n_estimators=100, max_depth=6, learning_rate=0.05,
                min_samples_split=10, random_state=42
            ),
            'AdaBoostRegressor': AdaBoostRegressor(
                n_estimators=50, learning_rate=0.1, random_state=42
            ),
            'BaggingRegressor': BaggingRegressor(
                n_estimators=50, random_state=42
            ),

            # ê³ ê¸‰ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…
            'TunedXGBoost': xgb.XGBRegressor(
                n_estimators=100, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8,
                random_state=42, verbosity=0
            ),
            'TunedCatBoost': cb.CatBoostRegressor(
                iterations=100, depth=6, learning_rate=0.05,
                l2_leaf_reg=10, random_seed=42, verbose=False
            ),
            'TunedLightGBM': lgb.LGBMRegressor(
                n_estimators=100, max_depth=6, learning_rate=0.05,
                num_leaves=31, random_state=42, verbosity=-1
            ),

            # ì¸ìŠ¤í„´ìŠ¤ ê¸°ë°˜ ëª¨ë¸ë“¤
            'TunedKNN': KNeighborsRegressor(n_neighbors=7, weights='distance'),
            'SVR_RBF': SVR(kernel='rbf', C=1.0, gamma='scale', epsilon=0.1),
            'SVR_Poly': SVR(kernel='poly', degree=3, C=1.0, epsilon=0.1),
            'SVR_Sigmoid': SVR(kernel='sigmoid', C=1.0, gamma='scale', epsilon=0.1),

            # ì‹ ê²½ë§ ëª¨ë¸ë“¤
            'MLPRegressor_Small': MLPRegressor(
                hidden_layer_sizes=(50, 25), max_iter=500,
                alpha=0.01, random_state=42
            ),
            'MLPRegressor_Medium': MLPRegressor(
                hidden_layer_sizes=(100, 50, 25), max_iter=500,
                alpha=0.01, random_state=42
            ),
        }

        return models

    class AdvancedNeuralNet(nn.Module):
        """ê³ ê¸‰ ì‹ ê²½ë§ ëª¨ë¸"""

        def __init__(self, input_size, hidden_sizes=[128, 64, 32, 16]):
            super().__init__()
            layers = []

            prev_size = input_size
            for i, hidden_size in enumerate(hidden_sizes):
                layers.extend([
                    nn.Linear(prev_size, hidden_size),
                    nn.BatchNorm1d(hidden_size),
                    nn.ReLU(),
                    nn.Dropout(0.3 if i < len(hidden_sizes)-1 else 0.1)
                ])
                prev_size = hidden_size

            layers.append(nn.Linear(prev_size, 1))
            self.network = nn.Sequential(*layers)

        def forward(self, x):
            return self.network(x)

    def train_advanced_neural_net(self, X_train, y_train, X_val, y_val):
        """ê³ ê¸‰ ì‹ ê²½ë§ í›ˆë ¨"""
        model = self.AdvancedNeuralNet(X_train.shape[1]).to(self.device)
        criterion = nn.MSELoss()
        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)

        # í…ì„œ ë³€í™˜
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train).to(self.device)
        X_val_tensor = torch.FloatTensor(X_val).to(self.device)

        best_loss = float('inf')
        patience_counter = 0

        # í›ˆë ¨
        model.train()
        for epoch in range(200):
            optimizer.zero_grad()
            outputs = model(X_train_tensor)
            loss = criterion(outputs.squeeze(), y_train_tensor)
            loss.backward()
            optimizer.step()

            # ê²€ì¦
            if epoch % 10 == 0:
                model.eval()
                with torch.no_grad():
                    val_outputs = model(X_val_tensor)
                    val_loss = criterion(val_outputs.squeeze(), torch.FloatTensor(y_val).to(self.device))

                scheduler.step(val_loss)

                if val_loss < best_loss:
                    best_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1

                if patience_counter >= 20:
                    break

                model.train()

        # ìµœì¢… ì˜ˆì¸¡
        model.eval()
        with torch.no_grad():
            val_pred = model(X_val_tensor).cpu().numpy().squeeze()

        return val_pred

    def create_ensemble_models(self, base_models, X_train, y_train):
        """ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""
        ensemble_models = {}

        try:
            # Voting Regressor
            voting_estimators = [(name, model) for name, model in list(base_models.items())[:5]]
            ensemble_models['VotingRegressor'] = VotingRegressor(
                estimators=voting_estimators
            )
        except Exception as e:
            print(f"   VotingRegressor ìƒì„± ì‹¤íŒ¨: {e}")

        try:
            # Stacking with Ridge meta-learner
            from sklearn.ensemble import StackingRegressor
            stacking_estimators = [(name, model) for name, model in list(base_models.items())[:5]]
            ensemble_models['StackingRegressor'] = StackingRegressor(
                estimators=stacking_estimators,
                final_estimator=Ridge(alpha=1.0),
                cv=3
            )
        except Exception as e:
            print(f"   StackingRegressor ìƒì„± ì‹¤íŒ¨: {e}")

        return ensemble_models

    def run_advanced_leak_free_experiments(self, data_path='/root/workspace/data/training/sp500_2020_2024_enhanced.csv'):
        """ê³ ê¸‰ ëˆ„ì¶œ ë°©ì§€ ì‹¤í—˜ ì‹¤í–‰"""
        print("ğŸš€ ê³ ê¸‰ ëˆ„ì¶œ ë°©ì§€ íšŒê·€ ì‹¤í—˜ ì‹œì‘")
        print("="*70)

        start_time = time.time()

        try:
            # 1. ë°ì´í„° ë¡œë”© ë° í–¥ìƒëœ íŠ¹ì„± ìƒì„±
            df = self.data_processor.load_and_validate_data(data_path)
            enhanced_df = self.create_enhanced_safe_features(df)

            # 2. í–¥ìƒëœ ì•ˆì „ì„± ê²€ì¦
            safe_features = self.validate_enhanced_safety(enhanced_df)

            if len(safe_features) < 10:
                print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì•ˆì „ íŠ¹ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤!")
                return None

            # 3. ê³ ê¸‰ ëª¨ë¸ë“¤ë¡œ ì‹¤í—˜
            results = self._run_advanced_models(enhanced_df, safe_features)

            # 4. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
            total_time = time.time() - start_time
            self._analyze_advanced_results(results, total_time)

            return results

        except Exception as e:
            print(f"âŒ ê³ ê¸‰ ëˆ„ì¶œ ë°©ì§€ ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def _run_advanced_models(self, enhanced_df, safe_features):
        """ê³ ê¸‰ ëª¨ë¸ë“¤ ì‹¤í–‰"""
        print(f"\nğŸ›¡ï¸ ê³ ê¸‰ ëª¨ë¸ë“¤ ì‹¤í–‰ (ì•ˆì „ íŠ¹ì„±: {len(safe_features)}ê°œ)")

        # ë°ì´í„° ì¤€ë¹„
        X = enhanced_df[safe_features].values
        y = enhanced_df['direction_target'].values

        # ì•ˆì „ ì²˜ë¦¬
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        y = np.nan_to_num(y, nan=0.5, posinf=1.0, neginf=0.0).astype(int)

        valid_idx = ~pd.isna(enhanced_df['direction_target'])
        X = X[valid_idx]
        y = y[valid_idx]

        print(f"   ìµœì¢… ë°ì´í„°: X={X.shape}, y=í´ë˜ìŠ¤ë¶„í¬{np.bincount(y)}")

        # ëª¨ë¸ ì •ì˜
        base_models = self.get_advanced_models()
        ensemble_models = self.create_ensemble_models(base_models, X, y)
        all_models = {**base_models, **ensemble_models}

        # êµì°¨ ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=3)
        model_results = {}

        for model_name, model in all_models.items():
            print(f"\n   ğŸ”’ {model_name} ê³ ê¸‰ ì‹¤í—˜...")

            fold_results = []

            for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
                try:
                    X_train, X_val = X[train_idx], X[val_idx]
                    y_train, y_val = y[train_idx], y[val_idx]

                    # ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ë§ ì‹œë„
                    scalers = [
                        ('RobustScaler', RobustScaler()),
                        ('StandardScaler', StandardScaler()),
                        ('PowerTransformer', PowerTransformer())
                    ]

                    best_score = -1
                    best_pred = None

                    for scaler_name, scaler in scalers:
                        try:
                            X_train_scaled = scaler.fit_transform(X_train)
                            X_val_scaled = scaler.transform(X_val)

                            # ëª¨ë¸ í›ˆë ¨
                            model.fit(X_train_scaled, y_train)
                            y_pred = model.predict(X_val_scaled)

                            # ë°©í–¥ ì •í™•ë„
                            y_pred_direction = (y_pred > 0.5).astype(int)
                            direction_acc = np.mean(y_pred_direction == y_val)

                            if direction_acc > best_score:
                                best_score = direction_acc
                                best_pred = y_pred

                        except Exception as e:
                            continue

                    if best_pred is not None:
                        mae = mean_absolute_error(y_val, best_pred)
                        r2 = r2_score(y_val, best_pred)

                        fold_results.append({
                            'direction_accuracy': best_score,
                            'mae': mae,
                            'r2': r2
                        })

                        print(f"      Fold {fold+1}: ë°©í–¥ì •í™•ë„={best_score:.4f}, MAE={mae:.6f}, RÂ²={r2:.4f}")
                    else:
                        print(f"      Fold {fold+1}: ì‹¤íŒ¨")
                        fold_results.append({
                            'direction_accuracy': 0.5,
                            'mae': 1.0,
                            'r2': -1.0
                        })

                except Exception as e:
                    print(f"      Fold {fold+1} ì˜¤ë¥˜: {e}")
                    fold_results.append({
                        'direction_accuracy': 0.5,
                        'mae': 1.0,
                        'r2': -1.0
                    })

            # í‰ê·  ì„±ëŠ¥
            if fold_results:
                avg_accuracy = np.mean([r['direction_accuracy'] for r in fold_results])
                avg_mae = np.mean([r['mae'] for r in fold_results])
                avg_r2 = np.mean([r['r2'] for r in fold_results])

                model_results[model_name] = {
                    'direction_accuracy': avg_accuracy,
                    'mae': avg_mae,
                    'r2': avg_r2,
                    'fold_results': fold_results
                }

                print(f"   âœ… {model_name} í‰ê· : ë°©í–¥ì •í™•ë„={avg_accuracy:.4f}, MAE={avg_mae:.6f}, RÂ²={avg_r2:.4f}")

        # ê³ ê¸‰ ì‹ ê²½ë§ ì¶”ê°€
        print(f"\n   ğŸ§  AdvancedNeuralNet ì‹¤í—˜...")
        neural_results = self._run_advanced_neural_net(X, y)
        if neural_results:
            model_results['AdvancedNeuralNet'] = neural_results

        return model_results

    def _run_advanced_neural_net(self, X, y):
        """ê³ ê¸‰ ì‹ ê²½ë§ ì‹¤í–‰"""
        tscv = TimeSeriesSplit(n_splits=3)
        fold_results = []

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            try:
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                # ìŠ¤ì¼€ì¼ë§
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)

                # ì‹ ê²½ë§ í›ˆë ¨
                y_pred = self.train_advanced_neural_net(X_train_scaled, y_train, X_val_scaled, y_val)

                # ì„±ëŠ¥ ê³„ì‚°
                mae = mean_absolute_error(y_val, y_pred)
                r2 = r2_score(y_val, y_pred)
                y_pred_direction = (y_pred > 0.5).astype(int)
                direction_acc = np.mean(y_pred_direction == y_val)

                fold_results.append({
                    'direction_accuracy': direction_acc,
                    'mae': mae,
                    'r2': r2
                })

                print(f"      Fold {fold+1}: ë°©í–¥ì •í™•ë„={direction_acc:.4f}, MAE={mae:.6f}, RÂ²={r2:.4f}")

            except Exception as e:
                print(f"      Fold {fold+1} ì˜¤ë¥˜: {e}")
                fold_results.append({
                    'direction_accuracy': 0.5,
                    'mae': 1.0,
                    'r2': -1.0
                })

        if fold_results:
            avg_accuracy = np.mean([r['direction_accuracy'] for r in fold_results])
            avg_mae = np.mean([r['mae'] for r in fold_results])
            avg_r2 = np.mean([r['r2'] for r in fold_results])

            return {
                'direction_accuracy': avg_accuracy,
                'mae': avg_mae,
                'r2': avg_r2,
                'fold_results': fold_results
            }

        return None

    def _analyze_advanced_results(self, results, total_time):
        """ê³ ê¸‰ ê²°ê³¼ ë¶„ì„"""
        print(f"\nğŸš€ ê³ ê¸‰ ëˆ„ì¶œ ë°©ì§€ íšŒê·€ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„")
        print("="*70)
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")

        # ì„±ëŠ¥ ìˆœìœ„
        sorted_results = sorted(results.items(),
                              key=lambda x: x[1]['direction_accuracy'],
                              reverse=True)

        print(f"\nğŸ† ì„±ëŠ¥ ìˆœìœ„ (ë°©í–¥ì •í™•ë„ ê¸°ì¤€):")
        print("-"*70)

        for i, (model_name, metrics) in enumerate(sorted_results):
            rank_emoji = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else f"{i+1:2d}."
            acc = metrics['direction_accuracy']
            mae = metrics['mae']
            r2 = metrics['r2']

            # ì„±ëŠ¥ í‰ê°€
            if acc > 0.65:
                status = "ğŸ”¥ ìš°ìˆ˜"
            elif acc > 0.55:
                status = "âœ… ì–‘í˜¸"
            elif acc > 0.5:
                status = "ğŸ“Š ë³´í†µ"
            else:
                status = "âš ï¸ ë‚®ìŒ"

            print(f"   {rank_emoji} {model_name:25s}: {acc:.1%} | MAE={mae:.4f} | RÂ²={r2:+.3f} | {status}")

        # ì•ˆì „ì„± ê²€ì¦
        print(f"\nğŸ”’ ì•ˆì „ì„± ê²€ì¦:")
        best_accuracy = max(r['direction_accuracy'] for r in results.values())

        if best_accuracy > 0.8:
            print(f"   ğŸš¨ ê²½ê³ : ìµœê³  ì„±ëŠ¥ {best_accuracy:.1%} - ëˆ„ì¶œ ì¬ê²€ì¦ í•„ìš”!")
        elif best_accuracy > 0.7:
            print(f"   âš ï¸ ì£¼ì˜: ìµœê³  ì„±ëŠ¥ {best_accuracy:.1%} - ë†’ì€ ì„±ëŠ¥, ëª¨ë‹ˆí„°ë§ í•„ìš”")
        else:
            print(f"   âœ… ì•ˆì „: ìµœê³  ì„±ëŠ¥ {best_accuracy:.1%} - í˜„ì‹¤ì  ì„±ëŠ¥, ëˆ„ì¶œ ì—†ìŒ")

        # ê²°ê³¼ ì €ì¥
        self._save_advanced_results(results, total_time)

    def _save_advanced_results(self, results, total_time):
        """ê³ ê¸‰ ê²°ê³¼ ì €ì¥"""
        output_path = f"/root/workspace/data/results/advanced_leak_free_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        try:
            serializable_results = {}
            for model_name, metrics in results.items():
                serializable_results[model_name] = {
                    'direction_accuracy': float(metrics['direction_accuracy']),
                    'mae': float(metrics['mae']),
                    'r2': float(metrics['r2']),
                    'fold_results': [{k: float(v) for k, v in fold.items()}
                                   for fold in metrics.get('fold_results', [])]
                }

            with open(output_path, 'w') as f:
                json.dump({
                    'timestamp': datetime.now().isoformat(),
                    'experiment_type': 'advanced_leak_free_regression',
                    'max_allowed_correlation': self.max_allowed_correlation,
                    'total_time': total_time,
                    'total_models': len(results),
                    'results': serializable_results
                }, f, indent=2)

            print(f"\nğŸ’¾ ê³ ê¸‰ ê²°ê³¼ ì €ì¥: {output_path}")

        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    system = AdvancedLeakFreeRegression()
    results = system.run_advanced_leak_free_experiments()

    if results:
        print("\nğŸ‰ ê³ ê¸‰ ëˆ„ì¶œ ë°©ì§€ íšŒê·€ ì‹¤í—˜ ì™„ë£Œ!")
        print("âœ… ëª¨ë“  ëª¨ë¸ì´ ì—„ê²©í•œ ëˆ„ì¶œ ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = max(results.keys(), key=lambda k: results[k]['direction_accuracy'])
        best_acc = results[best_model]['direction_accuracy']

        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} ({best_acc:.1%})")
    else:
        print("\nâŒ ê³ ê¸‰ ëˆ„ì¶œ ë°©ì§€ ì‹¤í—˜ ì‹¤íŒ¨!")

    return results

if __name__ == "__main__":
    main()