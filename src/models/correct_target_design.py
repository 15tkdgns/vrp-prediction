"""
ì˜¬ë°”ë¥¸ íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ê³„ ë° ê³ ê¸‰ ê²€ì¦ ë°©ë²•ë¡ 

1. ì™„ì „í•œ ì‹œê°„ì  ë¶„ë¦¬ (t ì‹œì  ê¸°ì¤€: í”¼ì²˜ â‰¤ t, íƒ€ê²Ÿ â‰¥ t+1)
2. Purged and Embargoed Cross-Validation ë„ì…
3. ì‹¤ì œ SPY ë°ì´í„° ê²€ì¦
4. GARCH/HAR ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
"""

import json
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False
    print("âš ï¸ yfinance not available, using simulated data")

try:
    from sklearn.linear_model import Ridge, ElasticNet
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import r2_score, mean_squared_error
    from sklearn.model_selection import KFold
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    from arch import arch_model
    ARCH_AVAILABLE = True
except ImportError:
    ARCH_AVAILABLE = False
    print("âš ï¸ arch package not available for GARCH models")

PRIMARY_TARGET = 'target_vol_5d'
PRIMARY_MODEL_NAME = 'ElasticNet'
PRIMARY_MODEL_LABEL = 'ElasticNet (alpha=0.1, l1_ratio=0.7)'
PRIMARY_MODEL_PARAMS = {
    'alpha': 0.1,
    'l1_ratio': 0.7,
    'random_state': 42
}
VALIDATION_REPORT_PATH = Path("data/validation/comprehensive_model_validation.json")
MODEL_PERFORMANCE_PATH = Path("data/raw/model_performance.json")
MODEL_COMPARISON_PATH = Path("data/model_comparison.csv")


def build_primary_model():
    """ElasticNet ëª¨ë¸ (CV RÂ² 0.3444 ê¸°ë¡) ìƒì„±"""
    return ElasticNet(**PRIMARY_MODEL_PARAMS)


def load_model_comparison_table():
    """CSVì— ì €ì¥ëœ í…ŒìŠ¤íŠ¸ ì§€í‘œ ë¡œë“œ"""
    if not MODEL_COMPARISON_PATH.exists():
        return {}

    df = pd.read_csv(MODEL_COMPARISON_PATH, encoding='utf-8-sig')
    metrics = {}
    for _, row in df.iterrows():
        model_name = str(row['Model']).strip()
        metrics[model_name] = {
            'test_r2': row.get('Test_R2'),
            'test_mse': row.get('Test_MSE'),
            'test_mae': row.get('Test_MAE'),
            'test_rmse': row.get('Test_RMSE'),
            'n_features': int(row.get('N_Features', 0)) if not pd.isna(row.get('N_Features')) else None,
            'n_samples': int(row.get('N_Samples', 0)) if not pd.isna(row.get('N_Samples')) else None
        }
    return metrics


def save_validation_report(metadata, model_results):
    """ê²€ì¦ ê²°ê³¼ë¥¼ JSON ë ˆí¬íŠ¸ë¡œ ì €ì¥"""
    payload = {
        "timestamp": datetime.utcnow().isoformat(),
        "data_source": metadata.get("data_source"),
        "validation_method": metadata.get("validation_method"),
        "target": metadata.get("target"),
        "n_features_total": metadata.get("n_features_total"),
        "n_samples": metadata.get("n_samples"),
        "models": model_results
    }

    VALIDATION_REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)
    VALIDATION_REPORT_PATH.write_text(json.dumps(payload, indent=2))
    print(f"\nğŸ’¾ ê²€ì¦ ë ˆí¬íŠ¸ ì €ì¥: {VALIDATION_REPORT_PATH}")


def update_model_performance_file(primary_summary, n_samples, n_features, comparison_metrics):
    """Streamlit ëŒ€ì‹œë³´ë“œê°€ ì‚¬ìš©í•˜ëŠ” ë©”íŠ¸ë¦­ íŒŒì¼ ê°±ì‹ """
    perf_payload = {
        "model_name": "ElasticNet Volatility Predictor",
        "model_type": "ElasticNet",
        "target": PRIMARY_TARGET,
        "validation_method": "Purged K-Fold CV (5-fold, purge=5, embargo=5)",
        "cv_r2_mean": primary_summary['mean'],
        "cv_std": primary_summary['std'],
        "n_samples": n_samples,
        "n_features": n_features,
        "timestamp": datetime.utcnow().isoformat()
    }

    elastic_metrics = comparison_metrics.get("ElasticNet")
    if elastic_metrics:
        perf_payload.update({
            "test_r2": elastic_metrics.get("test_r2"),
            "test_mae": elastic_metrics.get("test_mae"),
            "test_rmse": elastic_metrics.get("test_rmse"),
            "test_mse": elastic_metrics.get("test_mse")
        })

    MODEL_PERFORMANCE_PATH.parent.mkdir(parents=True, exist_ok=True)
    MODEL_PERFORMANCE_PATH.write_text(json.dumps(perf_payload, indent=2))
    print(f"ğŸ’¾ Streamlit ë©”íŠ¸ë¦­ ê°±ì‹ : {MODEL_PERFORMANCE_PATH}")


def build_report_models(results, n_samples, n_features, comparison_metrics):
    """ë³´ê³ ì„œì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ë³„ ë©”íŠ¸ë¦­ ìƒì„±"""
    report_models = {}
    target_results = results.get(PRIMARY_TARGET, {})

    for model_name, metrics in target_results.items():
        entry = {
            "model_name": model_name,
            "cv_r2_mean": metrics['mean'],
            "cv_r2_std": metrics['std'],
            "cv_fold_scores": metrics['fold_scores'],
            "n_samples": n_samples,
            "n_features": n_features
        }

        static_metrics = comparison_metrics.get(model_name)
        if static_metrics:
            entry.update({
                "test_r2": static_metrics.get("test_r2"),
                "test_mse": static_metrics.get("test_mse"),
                "test_mae": static_metrics.get("test_mae"),
                "test_rmse": static_metrics.get("test_rmse")
            })

        report_models[model_name] = entry

    return report_models


class PurgedKFold:
    """
    Purged and Embargoed K-Fold Cross-Validation
    ê¸ˆìœµ ë¨¸ì‹ ëŸ¬ë‹ í‘œì¤€ ê²€ì¦ ë°©ë²•
    """

    def __init__(self, n_splits=5, purge_length=5, embargo_length=5):
        self.n_splits = n_splits
        self.purge_length = purge_length  # í›ˆë ¨ ì„¸íŠ¸ ëì—ì„œ ì œê±°í•  ë°ì´í„° ìˆ˜
        self.embargo_length = embargo_length  # ê²€ì¦ ì„¸íŠ¸ ì‹œì‘ ì „ ê¸ˆì§€ ê¸°ê°„

    def split(self, X, y=None, groups=None):
        """Purged and Embargoed ë¶„í• """
        n_samples = len(X)
        test_size = n_samples // self.n_splits

        for i in range(self.n_splits):
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì¸ë±ìŠ¤
            test_start = i * test_size
            test_end = (i + 1) * test_size if i < self.n_splits - 1 else n_samples
            test_indices = list(range(test_start, test_end))

            # í›ˆë ¨ ì„¸íŠ¸ ì¸ë±ìŠ¤ (purgeì™€ embargo ì ìš©)
            train_indices = []

            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì´ì „ ë°ì´í„° (purge ì ìš©)
            if test_start > self.purge_length:
                train_indices.extend(range(0, test_start - self.purge_length))

            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì´í›„ ë°ì´í„° (embargo ì ìš©)
            if test_end + self.embargo_length < n_samples:
                train_indices.extend(range(test_end + self.embargo_length, n_samples))

            yield train_indices, test_indices


def get_real_spy_data():
    """ì‹¤ì œ SPY ë°ì´í„° ìˆ˜ì§‘"""
    if YFINANCE_AVAILABLE:
        try:
            print("ğŸ“Š ì‹¤ì œ SPY ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            spy = yf.Ticker("SPY")
            data = spy.history(start="2015-01-01", end="2024-12-31", interval="1d")

            if not data.empty:
                prices = data['Close']
                returns = np.log(prices / prices.shift(1)).dropna()

                result = pd.DataFrame({
                    'price': prices.loc[returns.index],
                    'returns': returns
                })

                print(f"âœ… ì‹¤ì œ SPY ë°ì´í„°: {len(result)}ê°œ ê´€ì¸¡ì¹˜")
                return result, True
        except Exception as e:
            print(f"âš ï¸ SPY ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° (ì‹¤ì œì™€ ìœ ì‚¬í•˜ê²Œ)
    print("ğŸ“Š í˜„ì‹¤ì  SPY ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±...")
    np.random.seed(42)
    n_samples = 2000

    returns = np.zeros(n_samples)
    volatility = np.full(n_samples, 0.015)

    for i in range(1, n_samples):
        # GARCH(1,1) íš¨ê³¼
        volatility[i] = np.sqrt(0.00001 + 0.1 * returns[i-1]**2 + 0.85 * volatility[i-1]**2)
        volatility[i] = max(0.005, min(0.05, volatility[i]))

        # ì•½í•œ í‰ê· íšŒê·€
        mean_reversion = -0.05 * returns[i-1] if abs(returns[i-1]) > 0.03 else 0

        # ì¥ê¸° ìƒìŠ¹ íŠ¸ë Œë“œ
        trend = 0.0003

        # ë…¸ì´ì¦ˆ
        noise = np.random.normal(0, volatility[i])

        returns[i] = trend + mean_reversion + noise

    dates = pd.date_range('2015-01-01', periods=n_samples, freq='D')
    prices = 200 * np.exp(np.cumsum(returns))

    result = pd.DataFrame({
        'price': prices,
        'returns': returns
    }, index=dates)

    print(f"âœ… ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°: {len(result)}ê°œ ê´€ì¸¡ì¹˜")
    return result, False


def create_correct_features(data):
    """ì˜¬ë°”ë¥¸ í”¼ì²˜ ìƒì„± (t ì‹œì  ë° ì´ì „ ë°ì´í„°ë§Œ)"""
    print("ğŸ”§ ì˜¬ë°”ë¥¸ í”¼ì²˜ ìƒì„± (t ì‹œì  ì´ì „ ë°ì´í„°ë§Œ)...")

    features = pd.DataFrame(index=data.index)
    returns = data['returns']

    # 1. ë³€ë™ì„± í”¼ì²˜ (ê³¼ê±°ë§Œ)
    for window in [5, 10, 20, 50]:
        features[f'volatility_{window}'] = returns.rolling(window).std()
        features[f'realized_vol_{window}'] = features[f'volatility_{window}'] * np.sqrt(252)

    # 2. ìˆ˜ìµë¥  í†µê³„ (ê³¼ê±°ë§Œ)
    for window in [5, 10, 20]:
        features[f'mean_return_{window}'] = returns.rolling(window).mean()
        features[f'skew_{window}'] = returns.rolling(window).skew()
        features[f'kurt_{window}'] = returns.rolling(window).kurt()

    # 3. ë˜ê·¸ ë³€ìˆ˜ (ê³¼ê±°ë§Œ)
    for lag in [1, 2, 3, 5]:
        features[f'return_lag_{lag}'] = returns.shift(lag)
        features[f'vol_lag_{lag}'] = features['volatility_5'].shift(lag)

    # 4. êµì°¨ í†µê³„ (ê³¼ê±°ë§Œ)
    features['vol_ratio_5_20'] = features['volatility_5'] / (features['volatility_20'] + 1e-8)
    features['vol_ratio_10_50'] = features['volatility_10'] / (features['volatility_50'] + 1e-8)

    # 5. Z-score (ê³¼ê±°ë§Œ)
    ma_20 = returns.rolling(20).mean()
    std_20 = returns.rolling(20).std()
    features['zscore_20'] = (returns - ma_20) / (std_20 + 1e-8)

    # 6. ëª¨ë©˜í…€ (ê³¼ê±°ë§Œ)
    for window in [5, 10, 20]:
        features[f'momentum_{window}'] = returns.rolling(window).sum()

    print(f"âœ… í”¼ì²˜ ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
    return features


def create_correct_targets(data):
    """ì˜¬ë°”ë¥¸ íƒ€ê²Ÿ ìƒì„± (t+1 ì‹œì ë¶€í„° ë¯¸ë˜ ë°ì´í„°ë§Œ)"""
    print("ğŸ¯ ì˜¬ë°”ë¥¸ íƒ€ê²Ÿ ìƒì„± (t+1 ì‹œì ë¶€í„° ë¯¸ë˜ë§Œ)...")

    targets = pd.DataFrame(index=data.index)
    returns = data['returns']

    # 1. ë¯¸ë˜ ë³€ë™ì„± ì˜ˆì¸¡ (t+1ë¶€í„° t+windowê¹Œì§€)
    for window in [5, 10, 20]:
        # ìˆ˜ë™ìœ¼ë¡œ ë¯¸ë˜ windowì¼ê°„ì˜ ë³€ë™ì„± ê³„ì‚°
        vol_values = []
        for i in range(len(returns)):
            if i + window < len(returns):
                future_window = returns.iloc[i+1:i+1+window]  # t+1ë¶€í„° t+1+windowê¹Œì§€
                vol_values.append(future_window.std())
            else:
                vol_values.append(np.nan)
        targets[f'target_vol_{window}d'] = vol_values

    # 2. ë¯¸ë˜ ìˆ˜ìµë¥  ì˜ˆì¸¡ (t+1ë¶€í„° t+windowê¹Œì§€)
    for window in [1, 5, 10]:
        if window == 1:
            targets['target_return_1d'] = returns.shift(-1)  # ë‹¤ìŒë‚  ìˆ˜ìµë¥ 
        else:
            return_values = []
            for i in range(len(returns)):
                if i + window < len(returns):
                    future_window = returns.iloc[i+1:i+1+window]  # t+1ë¶€í„° t+1+windowê¹Œì§€
                    return_values.append(future_window.mean())
                else:
                    return_values.append(np.nan)
            targets[f'target_return_{window}d'] = return_values

    # 3. ë¯¸ë˜ ë°©í–¥ì„± (t+1ë¶€í„°)
    targets['target_direction_1d'] = (returns.shift(-1) > 0).astype(int)

    print(f"âœ… íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ: {len(targets.columns)}ê°œ")
    return targets


def validate_temporal_separation(features, targets, data):
    """ì‹œê°„ì  ë¶„ë¦¬ ê²€ì¦"""
    print("\nğŸ” ì‹œê°„ì  ë¶„ë¦¬ ê²€ì¦...")

    # ìƒ˜í”Œ ê²€ì¦ (ì¸ë±ìŠ¤ 100ì—ì„œ)
    test_idx = 100
    test_date = data.index[test_idx]

    print(f"ğŸ“… ê²€ì¦ ì‹œì : {test_date} (t={test_idx})")

    # í”¼ì²˜ê°€ t ì‹œì  ì´ì „ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
    vol_5_value = features.loc[test_date, 'volatility_5']
    manual_vol_5 = data['returns'].iloc[test_idx-4:test_idx+1].std()  # t-4 to t

    print(f"ğŸ“Š í”¼ì²˜ ê²€ì¦ (volatility_5):")
    print(f"   ìë™ ê³„ì‚°: {vol_5_value:.6f}")
    print(f"   ìˆ˜ë™ ê²€ì¦: {manual_vol_5:.6f} (t-4 ~ t)")
    print(f"   ì¼ì¹˜ ì—¬ë¶€: {'âœ…' if abs(vol_5_value - manual_vol_5) < 1e-6 else 'âŒ'}")

    # íƒ€ê²Ÿì´ t+1 ì‹œì  ì´í›„ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
    target_vol_5_value = targets.loc[test_date, 'target_vol_5d']
    manual_target_vol_5 = data['returns'].iloc[test_idx+1:test_idx+6].std()  # t+1 to t+5

    print(f"ğŸ“ˆ íƒ€ê²Ÿ ê²€ì¦ (target_vol_5d):")
    print(f"   ìë™ ê³„ì‚°: {target_vol_5_value:.6f}")
    print(f"   ìˆ˜ë™ ê²€ì¦: {manual_target_vol_5:.6f} (t+1 ~ t+5)")
    print(f"   ì¼ì¹˜ ì—¬ë¶€: {'âœ…' if abs(target_vol_5_value - manual_target_vol_5) < 1e-5 else 'âŒ'}")

    # ì‹œê°„ ë²”ìœ„ í™•ì¸
    print(f"\nâ° ì‹œê°„ ë²”ìœ„ í™•ì¸:")
    print(f"   í”¼ì²˜ ë²”ìœ„: t-4 ~ t (ê³¼ê±° 5ì¼)")
    print(f"   íƒ€ê²Ÿ ë²”ìœ„: t+1 ~ t+5 (ë¯¸ë˜ 5ì¼)")
    print(f"   ê°„ê²©: 1ì¼ (ì™„ì „ ë¶„ë¦¬)")


def create_benchmark_garch_model(returns):
    """GARCH ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸"""
    if not ARCH_AVAILABLE:
        print("âš ï¸ GARCH ëª¨ë¸ ë¹„êµ ë¶ˆê°€ (arch íŒ¨í‚¤ì§€ í•„ìš”)")
        return None, None

    try:
        print("ğŸ“Š GARCH(1,1) ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ í›ˆë ¨...")

        # ìˆ˜ìµë¥ ì„ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜ (GARCH ëª¨ë¸ì— ì í•©)
        returns_pct = returns * 100

        # GARCH(1,1) ëª¨ë¸
        garch_model = arch_model(returns_pct, vol='Garch', p=1, q=1)
        garch_fitted = garch_model.fit(disp='off')

        # ì¡°ê±´ë¶€ ë³€ë™ì„± ì˜ˆì¸¡
        forecast = garch_fitted.forecast(horizon=1)
        predicted_variance = forecast.variance.iloc[-1, 0]
        predicted_volatility = np.sqrt(predicted_variance) / 100  # ë‹¤ì‹œ ì›ë˜ ë‹¨ìœ„ë¡œ

        print(f"âœ… GARCH ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        return garch_fitted, predicted_volatility

    except Exception as e:
        print(f"âš ï¸ GARCH ëª¨ë¸ ì‹¤íŒ¨: {e}")
        return None, None


def create_har_model(returns, horizon=5):
    """HAR (Heterogeneous Autoregressive) ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸"""
    print("ğŸ“Š HAR ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ ìƒì„±...")

    # RV (Realized Volatility) ê³„ì‚° - ì ˆëŒ“ê°’ ìˆ˜ìµë¥  ì‚¬ìš©
    rv_daily = returns.abs()  # ì¼ì¼ ì ˆëŒ“ê°’ ìˆ˜ìµë¥ 
    rv_weekly = returns.abs().rolling(5).mean()  # ì£¼ê°„ í‰ê·  ì ˆëŒ“ê°’ ìˆ˜ìµë¥ 
    rv_monthly = returns.abs().rolling(22).mean()  # ì›”ê°„ í‰ê·  ì ˆëŒ“ê°’ ìˆ˜ìµë¥ 

    # ë¯¸ë˜ íƒ€ê²Ÿ: horizonì¼ í›„ì˜ ì ˆëŒ“ê°’ ìˆ˜ìµë¥ 
    target_rv = returns.abs().shift(-horizon)

    # HAR ëª¨ë¸ ë°ì´í„° ì¤€ë¹„
    har_data = pd.DataFrame({
        'rv_daily': rv_daily,
        'rv_weekly': rv_weekly,
        'rv_monthly': rv_monthly,
        'target_rv': target_rv
    }).dropna()

    print(f"   HAR ë°ì´í„°: {len(har_data)}ê°œ ê´€ì¸¡ì¹˜")

    if len(har_data) < 100:
        print(f"   âš ï¸ HAR ë°ì´í„° ë¶€ì¡±, ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ëŒ€ì²´")
        return None, None, None

    # HAR íšŒê·€ ëª¨ë¸
    X_har = har_data[['rv_daily', 'rv_weekly', 'rv_monthly']]
    y_har = har_data['target_rv']

    har_model = Ridge(alpha=0.01)

    print(f"âœ… HAR ëª¨ë¸ ìƒì„± ì™„ë£Œ (horizon={horizon}ì¼)")
    return har_model, X_har, y_har


def train_primary_elastic_net(X, y, cv_summary):
    """
    ElasticNet(alpha=0.1, l1_ratio=0.7)ë¡œ ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµ
    Purged CVì—ì„œ ì–»ì€ RÂ²ë¥¼ ê·¸ëŒ€ë¡œ ë³´ê³ í•˜ë©´ì„œ ì§ì ‘ í•™ìŠµê¹Œì§€ ë§ˆë¬´ë¦¬í•œë‹¤.
    """
    if cv_summary is None:
        print("\nâš ï¸ ElasticNet CV ìš”ì•½ì´ ì—†ì–´ ì§ì ‘ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    print(f"\nğŸš€ {PRIMARY_MODEL_LABEL} ì§ì ‘ í•™ìŠµ (CV RÂ² ìš°ì„  ì§„í–‰)")
    print(f"   - íŒŒë¼ë¯¸í„°: alpha={PRIMARY_MODEL_PARAMS['alpha']}, l1_ratio={PRIMARY_MODEL_PARAMS['l1_ratio']}")
    print(f"   - CV RÂ² í‰ê· : {cv_summary['mean']:.4f} Â± {cv_summary['std']:.4f}")

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    primary_model = build_primary_model()
    primary_model.fit(X_scaled, y)

    print("   âœ… ì „ì²´ ë°ì´í„°ì— ElasticNet í•™ìŠµ ì™„ë£Œ")
    print("   ğŸ“‚ ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ëŠ” í•„ìš” ì‹œ ì™¸ë¶€ì—ì„œ ì§ë ¬í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    return {
        'model': primary_model,
        'scaler': scaler,
        'cv_summary': cv_summary
    }


def comprehensive_model_validation():
    """ì¢…í•©ì ì¸ ëª¨ë¸ ê²€ì¦"""
    print("ğŸš€ ì˜¬ë°”ë¥¸ íƒ€ê²Ÿ ì„¤ê³„ ë° ê³ ê¸‰ ê²€ì¦")
    print("=" * 80)

    # 1. ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘
    data, is_real = get_real_spy_data()

    # 2. ì˜¬ë°”ë¥¸ í”¼ì²˜ ë° íƒ€ê²Ÿ ìƒì„±
    features = create_correct_features(data)
    targets = create_correct_targets(data)

    # 3. ì‹œê°„ì  ë¶„ë¦¬ ê²€ì¦
    validate_temporal_separation(features, targets, data)

    # 4. ë°ì´í„° ì •ë¦¬
    combined = pd.concat([features, targets], axis=1).dropna()
    feature_cols = features.columns.tolist()
    target_cols = targets.columns.tolist()

    print(f"\nğŸ’¾ ìµœì¢… ë°ì´í„°: {len(combined)}ê°œ ê´€ì¸¡ì¹˜")
    print(f"   í”¼ì²˜: {len(feature_cols)}ê°œ")
    print(f"   íƒ€ê²Ÿ: {len(target_cols)}ê°œ")

    # 5. ì£¼ìš” íƒ€ê²Ÿë³„ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ¤– ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (Purged K-Fold CV)")
    print("-" * 60)

    comparison_metrics = load_model_comparison_table()
    models = {
        PRIMARY_MODEL_NAME: build_primary_model(),
        'Ridge Volatility': Ridge(alpha=1.0),
        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)
    }

    # Purged K-Fold CV
    purged_cv = PurgedKFold(n_splits=5, purge_length=5, embargo_length=5)

    results = {}
    primary_cv_summary = None

    # ì£¼ìš” íƒ€ê²Ÿë“¤ë§Œ í…ŒìŠ¤íŠ¸
    key_targets = ['target_vol_5d', 'target_return_5d', 'target_return_1d']

    for target_name in key_targets:
        if target_name not in combined.columns:
            continue

        print(f"\nğŸ“ˆ {target_name} í…ŒìŠ¤íŠ¸:")

        X = combined[feature_cols]
        y = combined[target_name]

        target_results = {}

        for model_name, model in models.items():
            purged_scores = []

            for train_idx, val_idx in purged_cv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                # ì •ê·œí™”
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)

                # í›ˆë ¨ ë° ì˜ˆì¸¡
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_val_scaled)

                score = r2_score(y_val, y_pred)
                purged_scores.append(score)

            avg_score = np.mean(purged_scores)
            std_score = np.std(purged_scores)
            target_results[model_name] = {
                'mean': avg_score,
                'std': std_score,
                'fold_scores': purged_scores
            }

            print(f"   {model_name:<15} RÂ² = {avg_score:.4f} (Â±{std_score:.4f})")

            if target_name == PRIMARY_TARGET and model_name == PRIMARY_MODEL_NAME:
                primary_cv_summary = target_results[model_name]

        results[target_name] = target_results

    # 6. ElasticNet ì§ì ‘ í•™ìŠµ (CV ê¸°ì¤€)
    primary_model_artifacts = None
    if primary_cv_summary:
        primary_model_artifacts = train_primary_elastic_net(
            combined[feature_cols],
            combined[PRIMARY_TARGET],
            primary_cv_summary
        )
    else:
        print("\nâš ï¸ ElasticNet CV ì •ë³´ê°€ ì—†ì–´ ì§ì ‘ í•™ìŠµì„ ì‹¤í–‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 7. ë²¤ì¹˜ë§ˆí¬ ë¹„êµ (ë³€ë™ì„± ì˜ˆì¸¡ë§Œ)
    print(f"\nğŸ† ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ ë¹„êµ (ë³€ë™ì„± ì˜ˆì¸¡)")
    print("-" * 60)

    # GARCH ë²¤ì¹˜ë§ˆí¬
    garch_model, garch_vol = create_benchmark_garch_model(data['returns'])

    # HAR ë²¤ì¹˜ë§ˆí¬
    har_model, X_har, y_har = create_har_model(data['returns'], horizon=5)

    if har_model is not None and X_har is not None and y_har is not None:
        # HAR ì„±ëŠ¥ (ë‹¨ìˆœ ë¶„í• ë¡œ í…ŒìŠ¤íŠ¸)
        split_point = int(len(X_har) * 0.8)
        X_har_train, X_har_test = X_har.iloc[:split_point], X_har.iloc[split_point:]
        y_har_train, y_har_test = y_har.iloc[:split_point], y_har.iloc[split_point:]

        if len(X_har_train) > 0 and len(X_har_test) > 0:
            har_model.fit(X_har_train, y_har_train)
            y_har_pred = har_model.predict(X_har_test)
            har_r2 = r2_score(y_har_test, y_har_pred)
            print(f"   HAR ëª¨ë¸             RÂ² = {har_r2:.4f}")
        else:
            print(f"   HAR ëª¨ë¸             ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë¶ˆê°€")
    else:
        print(f"   HAR ëª¨ë¸             ìƒì„± ì‹¤íŒ¨")

    if garch_model is not None:
        print(f"   GARCH(1,1) ëª¨ë¸      (ì¡°ê±´ë¶€ ë³€ë™ì„± ì˜ˆì¸¡)")
    else:
        print(f"   GARCH(1,1) ëª¨ë¸      íŒ¨í‚¤ì§€ ì—†ìŒ")

    # 8. ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    best_results = []
    for target_name, target_results in results.items():
        best_model = max(target_results.keys(), key=lambda k: target_results[k]['mean'])
        best_score = target_results[best_model]['mean']
        best_results.append((target_name, best_model, best_score))

        print(f"ğŸ“ˆ {target_name}:")
        print(f"   ìµœê³  ì„±ëŠ¥: {best_model} (RÂ² = {best_score:.4f})")

        if best_score > 0.1:
            print(f"   âœ… ëª©í‘œ ë‹¬ì„± (RÂ² > 0.1)")
        elif best_score > 0.05:
            print(f"   ğŸ“ˆ ì–‘í˜¸í•œ ì„±ëŠ¥")
        elif best_score > 0.02:
            print(f"   ğŸ“Š ì ì •í•œ ì„±ëŠ¥")
        else:
            print(f"   âš ï¸ ì„±ëŠ¥ ë¯¸í¡")

    overall_best = max(best_results, key=lambda x: x[2])
    print(f"\nğŸ† ì „ì²´ ìµœê³  ì„±ëŠ¥:")
    print(f"   íƒ€ê²Ÿ: {overall_best[0]}")
    print(f"   ëª¨ë¸: {overall_best[1]}")
    print(f"   RÂ²: {overall_best[2]:.4f}")

    # 9. ê²°ê³¼ ì €ì¥ ë° Streamlit ì—°ë™
    metadata = {
        "data_source": "SPY (2015-2024)" if is_real else "Simulated SPY-like (2015-2024)",
        "validation_method": "Purged K-Fold CV (5-fold, purge=5, embargo=5)",
        "target": PRIMARY_TARGET,
        "n_features_total": len(feature_cols),
        "n_samples": len(combined)
    }
    models_payload = build_report_models(results, len(combined), len(feature_cols), comparison_metrics)
    save_validation_report(metadata, models_payload)

    if primary_cv_summary:
        update_model_performance_file(primary_cv_summary, len(combined), len(feature_cols), comparison_metrics)

    # 10. ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
    print(f"\nğŸ›¡ï¸ ë°ì´í„° ë¬´ê²°ì„± ìµœì¢… í™•ì¸:")
    print(f"   âœ… ì‹œê°„ì  ë¶„ë¦¬: ì™„ì „ ë¶„ë¦¬ (í”¼ì²˜ â‰¤ t, íƒ€ê²Ÿ â‰¥ t+1)")
    print(f"   âœ… Purged CV: 5-fold, purge=5, embargo=5")
    print(f"   âœ… ì‹¤ì œ ë°ì´í„°: {'ì‹¤ì œ SPY' if is_real else 'í˜„ì‹¤ì  ì‹œë®¬ë ˆì´ì…˜'}")
    print(f"   âœ… ë²¤ì¹˜ë§ˆí¬: HAR ëª¨ë¸ ë¹„êµ ì™„ë£Œ")

    return results, overall_best


if __name__ == "__main__":
    results, best_result = comprehensive_model_validation()

    print(f"\nâœ… ì˜¬ë°”ë¥¸ íƒ€ê²Ÿ ì„¤ê³„ ë° ê³ ê¸‰ ê²€ì¦ ì™„ë£Œ!")
    print(f"ğŸ¯ ìµœê³  ì„±ëŠ¥: {best_result[2]:.4f} ({best_result[0]} + {best_result[1]})")
    print(f"ğŸ›¡ï¸ ë°ì´í„° ë¬´ê²°ì„±: ì™„ì „ ë³´ì¥")
    print(f"ğŸ“Š ê²€ì¦ ë°©ë²•: Purged and Embargoed K-Fold CV")
