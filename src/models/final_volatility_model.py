#!/usr/bin/env python3
"""
ìµœì¢… ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ íŒŒì´í”„ë¼ì¸
í›ˆë ¨ëœ ìµœì  ëª¨ë¸ì„ ì €ì¥í•˜ê³  ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡ì„ ìœ„í•œ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸
"""

import numpy as np
import pandas as pd
import yfinance as yf
import pickle
import os
import json
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import warnings

warnings.filterwarnings('ignore')

# ì´ì „ ëª¨ë“ˆì—ì„œ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
import sys
sys.path.append('/root/workspace/src/features')
from improved_volatility_model import load_enhanced_spy_data, create_comprehensive_features, create_future_volatility_targets, create_interaction_features

class VolatilityPredictor:
    """ìµœì¢… ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.is_trained = False

    def prepare_features(self, data):
        """ë°ì´í„°ì—ì„œ íŠ¹ì„± ìƒì„±"""
        print("ğŸ“Š íŠ¹ì„± ìƒì„± ì¤‘...")

        # í¬ê´„ì ì¸ íŠ¹ì„± ìƒì„±
        comprehensive_features = create_comprehensive_features(data)

        # ìƒìœ„ 15ê°œ íŠ¹ì„± (í›ˆë ¨ ì‹œ ë°œê²¬í•œ ê²ƒ)
        top_features = [
            'vix_level', 'intraday_vol_5', 'intraday_vol_10', 'ewm_vol_10',
            'ewm_vol_5', 'vix_ma_5', 'volatility_10', 'realized_vol_10',
            'realized_vol_5', 'volatility_5', 'ewm_vol_20', 'garman_klass_5',
            'garman_klass_10', 'vol_lag_1', 'vix_std_20'
        ]

        # ì¡´ì¬í•˜ëŠ” íŠ¹ì„±ë§Œ ì„ íƒ
        available_features = [f for f in top_features if f in comprehensive_features.columns]
        top_features_df = comprehensive_features[available_features]

        # ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±
        interaction_features = create_interaction_features(top_features_df, n_top=8)

        # ìµœì¢… íŠ¹ì„± ê²°í•©
        final_features = pd.concat([top_features_df, interaction_features], axis=1)

        print(f"âœ… ìƒì„±ëœ íŠ¹ì„± ìˆ˜: {len(final_features.columns)}")
        return final_features

    def train_final_model(self, save_path='models/'):
        """ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° ì €ì¥"""
        print("ğŸ¤– ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        print("=" * 50)

        # 1. ë°ì´í„° ë¡œë“œ
        spy_data = load_enhanced_spy_data()

        # 2. íŠ¹ì„± ë° íƒ€ê²Ÿ ìƒì„±
        features = self.prepare_features(spy_data)
        targets = create_future_volatility_targets(spy_data)

        # 3. ë°ì´í„° ê²°í•© ë° ì •ë¦¬
        combined_data = pd.concat([features, targets[['target_vol_5d']]], axis=1).dropna()
        X = combined_data[features.columns]
        y = combined_data['target_vol_5d']

        print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(X)} ìƒ˜í”Œ, {len(X.columns)} íŠ¹ì„±")

        # 4. ì‹œê°„ ìˆœì„œ ë¶„í• 
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        # 5. ìŠ¤ì¼€ì¼ë§
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # 6. ìµœì  ëª¨ë¸ í›ˆë ¨ (ì´ì „ì— ë°œê²¬í•œ ìµœì  íŒŒë¼ë¯¸í„°)
        self.model = Lasso(alpha=0.0005, max_iter=1000, random_state=42)
        self.model.fit(X_train_scaled, y_train)
        self.feature_names = X.columns.tolist()
        self.is_trained = True

        # 7. ì„±ëŠ¥ í‰ê°€
        y_pred = self.model.predict(X_test_scaled)
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)

        print(f"ğŸ“ˆ ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   RÂ² Score: {r2:.4f}")
        print(f"   MAE:      {mae:.6f}")
        print(f"   RMSE:     {np.sqrt(mse):.6f}")

        # 8. íŠ¹ì„± ì¤‘ìš”ë„
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': np.abs(self.model.coef_)
        }).sort_values('importance', ascending=False)

        print(f"\nğŸ¯ ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
        for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
            print(f"  {i+1:2d}. {row['feature']:25}: {row['importance']:.6f}")

        # 9. ëª¨ë¸ ë° ë©”íƒ€ë°ì´í„° ì €ì¥
        os.makedirs(save_path, exist_ok=True)

        # ëª¨ë¸ ì €ì¥
        with open(f"{save_path}/volatility_model.pkl", "wb") as f:
            pickle.dump(self.model, f)

        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        with open(f"{save_path}/scaler.pkl", "wb") as f:
            pickle.dump(self.scaler, f)

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'Lasso',
            'alpha': 0.0005,
            'max_iter': 1000,
            'feature_names': self.feature_names,
            'feature_count': len(self.feature_names),
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'performance': {
                'r2_score': float(r2),
                'mae': float(mae),
                'mse': float(mse),
                'rmse': float(np.sqrt(mse))
            },
            'feature_importance': feature_importance.to_dict('records'),
            'trained_date': datetime.now().isoformat(),
            'data_period': '2015-01-01 to 2024-12-31'
        }

        with open(f"{save_path}/model_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)

        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        return metadata

    def load_model(self, model_path='models/'):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")

        try:
            # ëª¨ë¸ ë¡œë“œ
            with open(f"{model_path}/volatility_model.pkl", "rb") as f:
                self.model = pickle.load(f)

            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            with open(f"{model_path}/scaler.pkl", "rb") as f:
                self.scaler = pickle.load(f)

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(f"{model_path}/model_metadata.json", "r") as f:
                metadata = json.load(f)
                self.feature_names = metadata['feature_names']

            self.is_trained = True
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return metadata

        except FileNotFoundError as e:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return None

    def predict_volatility(self, data, days_ahead=5):
        """ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ë³€ë™ì„± ì˜ˆì¸¡"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_final_model() ë˜ëŠ” load_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

        print(f"ğŸ”® {days_ahead}ì¼ í›„ ë³€ë™ì„± ì˜ˆì¸¡ ì¤‘...")

        # íŠ¹ì„± ìƒì„±
        features = self.prepare_features(data)

        # íŠ¹ì„± ìˆœì„œ ë§ì¶”ê¸°
        features_ordered = features.reindex(columns=self.feature_names, fill_value=0)

        # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì„ íƒ (NaN ì œê±°)
        valid_data = features_ordered.dropna()

        if len(valid_data) == 0:
            print("âš ï¸ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ìŠ¤ì¼€ì¼ë§
        features_scaled = self.scaler.transform(valid_data)

        # ì˜ˆì¸¡
        predictions = self.model.predict(features_scaled)

        # ê²°ê³¼ DataFrame ìƒì„±
        results = pd.DataFrame({
            'date': valid_data.index,
            'predicted_volatility': predictions
        })

        print(f"âœ… {len(results)}ê°œ ì˜ˆì¸¡ ì™„ë£Œ")
        return results

    def get_latest_prediction(self, symbol='SPY'):
        """ìµœì‹  ë°ì´í„°ë¡œ ë³€ë™ì„± ì˜ˆì¸¡"""
        print(f"ğŸ“Š {symbol} ìµœì‹  ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")

        # ìµœê·¼ 6ê°œì›” ë°ì´í„° ë‹¤ìš´ë¡œë“œ (íŠ¹ì„± ìƒì„±ì„ ìœ„í•´ ì¶©ë¶„í•œ ê¸°ê°„)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=180)

        try:
            data = yf.download(symbol, start=start_date.strftime('%Y-%m-%d'),
                             end=end_date.strftime('%Y-%m-%d'), progress=False)
            data['returns'] = data['Close'].pct_change()

            # VIX ë°ì´í„° ì¶”ê°€
            vix = yf.download('^VIX', start=start_date.strftime('%Y-%m-%d'),
                            end=end_date.strftime('%Y-%m-%d'), progress=False)
            vix_close = vix['Close'].reindex(data.index, method='ffill')
            data['vix'] = vix_close

            data = data.dropna()

            # ì˜ˆì¸¡
            predictions = self.predict_volatility(data)

            if predictions is not None and len(predictions) > 0:
                latest_prediction = predictions.iloc[-1]
                print(f"ğŸ¯ ìµœì‹  ì˜ˆì¸¡ (5ì¼ í›„ ë³€ë™ì„±): {latest_prediction['predicted_volatility']:.6f}")
                print(f"ğŸ“… ì˜ˆì¸¡ ë‚ ì§œ: {latest_prediction['date'].strftime('%Y-%m-%d')}")
                return latest_prediction
            else:
                print("âŒ ì˜ˆì¸¡ ì‹¤íŒ¨")
                return None

        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ìµœì¢… ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ íŒŒì´í”„ë¼ì¸")
    print("=" * 60)

    # 1. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    predictor = VolatilityPredictor()

    # 2. ëª¨ë¸ í›ˆë ¨ ë° ì €ì¥
    metadata = predictor.train_final_model()

    # 3. ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ì¬ë¡œë“œ)
    print(f"\nğŸ§ª ëª¨ë¸ ì¬ë¡œë“œ í…ŒìŠ¤íŠ¸")
    test_predictor = VolatilityPredictor()
    loaded_metadata = test_predictor.load_model()

    if loaded_metadata:
        print(f"âœ… ì¬ë¡œë“œ ì„±ê³µ: {loaded_metadata['model_type']}")

        # 4. ìµœì‹  ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ”® ìµœì‹  SPY ë³€ë™ì„± ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸")
        latest_pred = test_predictor.get_latest_prediction('SPY')

        if latest_pred is not None:
            print(f"âœ… ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì •ìƒ ì‘ë™")
        else:
            print(f"âš ï¸ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜")

    print("=" * 60)
    print("âœ… ìµœì¢… ëª¨ë¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ")
    print("ğŸ“‚ ì €ì¥ëœ íŒŒì¼:")
    print("   - models/volatility_model.pkl")
    print("   - models/scaler.pkl")
    print("   - models/model_metadata.json")

    return predictor

if __name__ == "__main__":
    predictor = main()