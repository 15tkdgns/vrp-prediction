#!/usr/bin/env python3
"""
Enhanced Volatility Model V2 - ì„±ëŠ¥ í–¥ìƒ ë²„ì „
Phase 1: ë°ì´í„° í™•ì¥ ë° ê²½ì œ ì§€í‘œ ì¶”ê°€ë¥¼ í†µí•œ RÂ² ì„±ëŠ¥ ê°œì„ 
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso, ElasticNet, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
import warnings
import os
import json
from datetime import datetime
import pickle

warnings.filterwarnings('ignore')

# ì´ì „ ëª¨ë“ˆì—ì„œ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
import sys
sys.path.append('/root/workspace/src/features')

def load_extended_data(start_date='2010-01-01', end_date='2024-12-31'):
    """í™•ì¥ëœ ê¸°ê°„ì˜ ë°ì´í„° ë¡œë“œ (2010-2024)"""
    print(f"ğŸ“Š í™•ì¥ëœ ë°ì´í„° ë¡œë“œ: {start_date} ~ {end_date}")

    # SPY ë°ì´í„°
    spy = yf.download('SPY', start=start_date, end=end_date, progress=False)
    spy['returns'] = spy['Close'].pct_change()

    # VIX ë°ì´í„°
    vix = yf.download('^VIX', start=start_date, end=end_date, progress=False)
    spy['vix'] = vix['Close'].reindex(spy.index, method='ffill')

    # 10ë…„ êµ­ì±„ ê¸ˆë¦¬ (^TNX)
    try:
        treasury_10y = yf.download('^TNX', start=start_date, end=end_date, progress=False)
        spy['treasury_10y'] = treasury_10y['Close'].reindex(spy.index, method='ffill')
        print("âœ… 10ë…„ êµ­ì±„ ê¸ˆë¦¬ ë°ì´í„° ì¶”ê°€")
    except:
        spy['treasury_10y'] = 2.5  # ê¸°ë³¸ê°’
        print("âš ï¸ 10ë…„ êµ­ì±„ ê¸ˆë¦¬ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")

    # 2ë…„ êµ­ì±„ ê¸ˆë¦¬ (^IRXë¥¼ 2ë…„ í”„ë¡ì‹œë¡œ ì‚¬ìš©)
    try:
        treasury_2y = yf.download('^TNX', start=start_date, end=end_date, progress=False)  # ì„ì‹œë¡œ TNX ì‚¬ìš©
        spy['treasury_2y'] = (treasury_10y['Close'] * 0.7).reindex(spy.index, method='ffill')  # ê·¼ì‚¬ê°’
        print("âœ… 2ë…„ êµ­ì±„ ê¸ˆë¦¬ í”„ë¡ì‹œ ì¶”ê°€")
    except:
        spy['treasury_2y'] = 1.5  # ê¸°ë³¸ê°’
        print("âš ï¸ 2ë…„ êµ­ì±„ ê¸ˆë¦¬ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")

    # ìˆ˜ìµë¥  ê³¡ì„  ê¸°ìš¸ê¸°
    spy['yield_curve_slope'] = spy['treasury_10y'] - spy['treasury_2y']

    spy = spy.dropna()
    print(f"âœ… í™•ì¥ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(spy)} ê´€ì¸¡ì¹˜ ({start_date} ~ {end_date})")
    return spy

def create_enhanced_features_v2(data):
    """V2 í–¥ìƒëœ íŠ¹ì„± ìƒì„± - ê²½ì œ ì§€í‘œ í¬í•¨"""
    print("ğŸ”§ V2 í–¥ìƒëœ íŠ¹ì„± ìƒì„± ì¤‘...")

    features = pd.DataFrame(index=data.index)
    returns = data['returns']
    prices = data['Close']
    high = data['High']
    low = data['Low']
    volume = data['Volume']

    # 1. ê¸°ë³¸ ë³€ë™ì„± íŠ¹ì„± (ê¸°ì¡´)
    for window in [5, 10, 20, 50, 100]:  # 100ì¼ ì¶”ê°€
        features[f'volatility_{window}'] = returns.rolling(window).std()
        features[f'realized_vol_{window}'] = features[f'volatility_{window}'] * np.sqrt(252)

    # 2. ì§€ìˆ˜ ê°€ì¤‘ ë³€ë™ì„± (ê¸°ì¡´ + ì¶”ê°€)
    for span in [5, 10, 20, 50]:  # 50ì¼ ì¶”ê°€
        features[f'ewm_vol_{span}'] = returns.ewm(span=span).std()

    # 3. VIX ê¸°ë°˜ íŠ¹ì„± (ê¸°ì¡´ + ê°•í™”)
    if 'vix' in data.columns:
        vix = data['vix']
        features['vix_level'] = vix
        features['vix_change'] = vix.pct_change()
        for window in [5, 10, 20, 50]:  # 50ì¼ ì¶”ê°€
            features[f'vix_ma_{window}'] = vix.rolling(window).mean()
            features[f'vix_std_{window}'] = vix.rolling(window).std()

        # VIX ê¸°ê°„êµ¬ì¡° í”„ë¡ì‹œ (VIX vs ì´ë™í‰ê·  ë¹„êµ)
        features['vix_term_structure'] = vix / features['vix_ma_20']
        features['vix_contango'] = features['vix_ma_5'] / features['vix_ma_20']
        features['vix_backwardation'] = np.where(features['vix_contango'] < 1, 1, 0)

    # 4. ê²½ì œ ì§€í‘œ ê¸°ë°˜ íŠ¹ì„± (ì‹ ê·œ)
    if 'treasury_10y' in data.columns:
        treasury_10y = data['treasury_10y']
        features['treasury_10y_level'] = treasury_10y
        features['treasury_10y_change'] = treasury_10y.diff()
        for window in [5, 10, 20]:
            features[f'treasury_10y_ma_{window}'] = treasury_10y.rolling(window).mean()
            features[f'treasury_10y_vol_{window}'] = treasury_10y.rolling(window).std()

    if 'yield_curve_slope' in data.columns:
        yield_slope = data['yield_curve_slope']
        features['yield_curve_slope'] = yield_slope
        features['yield_slope_change'] = yield_slope.diff()
        for window in [5, 10, 20]:
            features[f'yield_slope_ma_{window}'] = yield_slope.rolling(window).mean()
            features[f'yield_slope_vol_{window}'] = yield_slope.rolling(window).std()

    # 5. ê³ ê¸‰ ë³€ë™ì„± ì¸¡ì • (ê¸°ì¡´ + ì¶”ê°€)
    for window in [5, 10, 20, 50]:  # 50ì¼ ì¶”ê°€
        # Garman-Klass ë³€ë™ì„±
        gk_vol = np.log(high / low) ** 2
        features[f'garman_klass_{window}'] = gk_vol.rolling(window).mean()

        # ì¼ì¤‘ ë³€ë™ì„±
        intraday_range = (high - low) / prices
        features[f'intraday_vol_{window}'] = intraday_range.rolling(window).mean()

    # 6. ë˜ê·¸ íŠ¹ì„± (ê¸°ì¡´ + ì¶”ê°€)
    for lag in [1, 2, 3, 5, 10]:  # 10ì¼ ë˜ê·¸ ì¶”ê°€
        features[f'return_lag_{lag}'] = returns.shift(lag)
        features[f'vol_lag_{lag}'] = features['volatility_5'].shift(lag)
        if 'vix_level' in features.columns:
            features[f'vix_lag_{lag}'] = features['vix_level'].shift(lag)

    # 7. ë³¼ë¥¨ ê¸°ë°˜ íŠ¹ì„± (ê¸°ì¡´)
    volume_ma_5 = volume.rolling(5).mean()
    volume_ma_20 = volume.rolling(20).mean()
    volume_ma_50 = volume.rolling(50).mean()  # ì¶”ê°€

    features['volume_ma_5'] = volume_ma_5
    features['volume_ma_20'] = volume_ma_20
    features['volume_ma_50'] = volume_ma_50
    features['volume_ratio_5'] = volume / (volume_ma_5 + 1e-8)
    features['volume_ratio_20'] = volume / (volume_ma_20 + 1e-8)
    features['volume_ratio_50'] = volume / (volume_ma_50 + 1e-8)

    # 8. ìˆ˜ìµë¥  í†µê³„ (ê¸°ì¡´ + ì¶”ê°€)
    for window in [5, 10, 20, 50]:  # 50ì¼ ì¶”ê°€
        features[f'return_mean_{window}'] = returns.rolling(window).mean()
        features[f'return_skew_{window}'] = returns.rolling(window).skew()
        features[f'return_kurt_{window}'] = returns.rolling(window).kurt()

    # 9. ë³€ë™ì„± ë¹„ìœ¨ (ê¸°ì¡´ + ì¶”ê°€)
    features['vol_ratio_5_20'] = features['volatility_5'] / (features['volatility_20'] + 1e-8)
    features['vol_ratio_10_50'] = features['volatility_10'] / (features['volatility_50'] + 1e-8)
    features['vol_ratio_20_100'] = features['volatility_20'] / (features['volatility_100'] + 1e-8)

    # 10. Z-score íŠ¹ì„± (ê¸°ì¡´ + ì¶”ê°€)
    for window in [20, 50, 100]:  # 100ì¼ ì¶”ê°€
        # ìˆ˜ìµë¥  Z-score
        mean_ret = returns.rolling(window).mean()
        std_ret = returns.rolling(window).std()
        features[f'return_zscore_{window}'] = (returns - mean_ret) / (std_ret + 1e-8)

        # ë³€ë™ì„± Z-score
        mean_vol = features['volatility_5'].rolling(window).mean()
        std_vol = features['volatility_5'].rolling(window).std()
        features[f'vol_zscore_{window}'] = (features['volatility_5'] - mean_vol) / (std_vol + 1e-8)

    # 11. ëª¨ë©˜í…€ íŠ¹ì„± (ê¸°ì¡´ + ì¶”ê°€)
    for window in [5, 10, 20, 50]:  # 50ì¼ ì¶”ê°€
        features[f'momentum_{window}'] = returns.rolling(window).sum()
        features[f'price_momentum_{window}'] = prices / prices.shift(window) - 1

    # 12. ë³€ë™ì„± ì§€ì†ì„± (ê¸°ì¡´ + ì¶”ê°€)
    vol_5 = features['volatility_5']
    for lag in [1, 2, 3, 5, 10]:  # 10ì¼ ì¶”ê°€
        features[f'vol_autocorr_{lag}'] = vol_5.rolling(50).corr(vol_5.shift(lag))

    # 13. êµì°¨ ìƒê´€ê´€ê³„ (ì‹ ê·œ)
    if 'vix_level' in features.columns and 'treasury_10y_level' in features.columns:
        # VIX-ê¸ˆë¦¬ ìƒê´€ê´€ê³„
        for window in [10, 20, 50]:
            features[f'vix_treasury_corr_{window}'] = features['vix_level'].rolling(window).corr(
                features['treasury_10y_level']
            )

    print(f"âœ… V2 í–¥ìƒëœ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
    return features

def create_enhanced_interaction_features(base_features, n_top=10):
    """í–¥ìƒëœ ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±"""
    print(f"ğŸ”§ í–¥ìƒëœ ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì¤‘ (ìƒìœ„ {n_top}ê°œ)...")

    selected_features = base_features.iloc[:, :n_top]
    interactions = pd.DataFrame(index=base_features.index)

    # ê³±ì…ˆ ìƒí˜¸ì‘ìš©
    for i, col1 in enumerate(selected_features.columns):
        for j, col2 in enumerate(selected_features.columns[i+1:], i+1):
            if j < len(selected_features.columns):
                interactions[f'{col1}_x_{col2}'] = selected_features.iloc[:, i] * selected_features.iloc[:, j]

    # ë¹„ìœ¨ ìƒí˜¸ì‘ìš© (ìƒìœ„ 6ê°œë§Œ)
    for i in range(min(6, len(selected_features.columns))):
        for j in range(i+1, min(7, len(selected_features.columns))):
            col1 = selected_features.columns[i]
            col2 = selected_features.columns[j]
            interactions[f'{col1}_div_{col2}'] = selected_features.iloc[:, i] / (selected_features.iloc[:, j] + 1e-8)

    # ì œê³± íŠ¹ì„± (ìƒìœ„ 5ê°œë§Œ)
    for i in range(min(5, len(selected_features.columns))):
        col = selected_features.columns[i]
        interactions[f'{col}_squared'] = selected_features.iloc[:, i] ** 2

    print(f"âœ… í–¥ìƒëœ ìƒí˜¸ì‘ìš© íŠ¹ì„± {len(interactions.columns)}ê°œ ìƒì„±")
    return interactions

def create_future_volatility_targets(data):
    """ë¯¸ë˜ ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)"""
    print("ğŸ¯ ë¯¸ë˜ ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„± ì¤‘...")

    targets = pd.DataFrame(index=data.index)
    returns = data['returns']

    # ë‹¤ì–‘í•œ ê¸°ê°„ì˜ ë¯¸ë˜ ë³€ë™ì„±
    for window in [1, 3, 5, 10, 20]:
        vol_values = []
        for i in range(len(returns)):
            if i + window < len(returns):
                future_window = returns.iloc[i+1:i+1+window]
                vol_values.append(future_window.std())
            else:
                vol_values.append(np.nan)
        targets[f'target_vol_{window}d'] = vol_values

    print(f"âœ… íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ: {len(targets.columns)}ê°œ")
    return targets

def test_enhanced_models_v2(X, y, model_name='5ì¼ ë³€ë™ì„± V2'):
    """í–¥ìƒëœ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸ V2"""
    print(f"\nğŸ¤– {model_name} ì˜ˆì¸¡ - í–¥ìƒëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # ì™„ì „í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
    combined_data = pd.concat([X, y], axis=1).dropna()
    print(f"ìœ íš¨ ìƒ˜í”Œ ìˆ˜: {len(combined_data)}")

    if len(combined_data) < 300:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ì¦ê°€
        print("âš ï¸ ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±")
        return {}

    X_clean = combined_data[X.columns]
    y_clean = combined_data[y.name]

    # ì‹œê°„ ìˆœì„œ êµì°¨ ê²€ì¦ (ë” ë§ì€ í´ë“œ)
    tscv = TimeSeriesSplit(n_splits=5)  # 3 -> 5ë¡œ ì¦ê°€
    results = {}

    models = {
        'Ridge (Î±=0.01)': Ridge(alpha=0.01),
        'Ridge (Î±=1.0)': Ridge(alpha=1.0),
        'Ridge (Î±=10.0)': Ridge(alpha=10.0),
        'Lasso (Î±=0.0001)': Lasso(alpha=0.0001, max_iter=3000),
        'Lasso (Î±=0.0005)': Lasso(alpha=0.0005, max_iter=3000),
        'Lasso (Î±=0.001)': Lasso(alpha=0.001, max_iter=3000),
        'Lasso (Î±=0.005)': Lasso(alpha=0.005, max_iter=3000),
        'ElasticNet (Î±=0.0005, l1=0.5)': ElasticNet(alpha=0.0005, l1_ratio=0.5, max_iter=3000),
        'ElasticNet (Î±=0.0005, l1=0.7)': ElasticNet(alpha=0.0005, l1_ratio=0.7, max_iter=3000),
        'RandomForest': RandomForestRegressor(n_estimators=200, random_state=42, max_depth=10),
        'GradientBoosting': GradientBoostingRegressor(n_estimators=200, random_state=42, max_depth=6)
    }

    for name, model in models.items():
        scores = []
        mae_scores = []

        for train_idx, test_idx in tscv.split(X_clean):
            X_train, X_test = X_clean.iloc[train_idx], X_clean.iloc[test_idx]
            y_train, y_test = y_clean.iloc[train_idx], y_clean.iloc[test_idx]

            # ìŠ¤ì¼€ì¼ë§ (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ ì œì™¸)
            if 'Forest' not in name and 'Boosting' not in name:
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
            else:
                X_train_scaled = X_train.values
                X_test_scaled = X_test.values

            try:
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)

                score = r2_score(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)

                scores.append(score)
                mae_scores.append(mae)
            except Exception as e:
                print(f"  ëª¨ë¸ {name} ì˜¤ë¥˜: {e}")
                scores.append(-999)
                mae_scores.append(999)

        avg_score = np.mean(scores)
        std_score = np.std(scores)
        avg_mae = np.mean(mae_scores)

        results[name] = {
            'mean_r2': avg_score,
            'std_r2': std_score,
            'mean_mae': avg_mae
        }

        print(f"{name:30}: RÂ² = {avg_score:7.4f} Â± {std_score:.4f}, MAE = {avg_mae:.6f}")

    return results

def main():
    """ë©”ì¸ í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("ğŸš€ Enhanced Volatility Model V2 - ì„±ëŠ¥ í–¥ìƒ ë²„ì „")
    print("=" * 70)

    # 1. í™•ì¥ëœ ë°ì´í„° ë¡œë“œ (2010-2024)
    spy_data = load_extended_data('2010-01-01', '2024-12-31')

    # 2. V2 í–¥ìƒëœ íŠ¹ì„± ìƒì„±
    enhanced_features = create_enhanced_features_v2(spy_data)

    # 3. íƒ€ê²Ÿ ìƒì„±
    targets = create_future_volatility_targets(spy_data)

    # 4. íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„ (ë” ë§ì€ íŠ¹ì„± ê³ ë ¤)
    if 'target_vol_5d' in targets.columns:
        combined_for_selection = pd.concat([enhanced_features, targets[['target_vol_5d']]], axis=1).dropna()

        if len(combined_for_selection) > 200:
            print(f"\nğŸ“Š V2 íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ë¶„ì„ (ìƒ˜í”Œ ìˆ˜: {len(combined_for_selection)})")

            correlations = combined_for_selection[enhanced_features.columns].corrwith(
                combined_for_selection['target_vol_5d']
            ).abs().sort_values(ascending=False)

            print("ìƒìœ„ 20ê°œ íŠ¹ì„±:")
            for i, (feature, corr) in enumerate(correlations.head(20).items()):
                print(f"  {i+1:2d}. {feature:30}: {corr:.4f}")

            # 5. ìƒìœ„ íŠ¹ì„± ì„ ë³„ ë° ìƒí˜¸ì‘ìš© ìƒì„± (ë” ë§ì€ íŠ¹ì„±)
            top_20_features = correlations.head(20).index
            top_features_df = enhanced_features[top_20_features]

            # ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± (ìƒìœ„ 10ê°œë¡œ ì œí•œ)
            interaction_features = create_enhanced_interaction_features(top_features_df, n_top=10)

            # ìµœì¢… íŠ¹ì„± ì„¸íŠ¸
            final_features = pd.concat([top_features_df, interaction_features], axis=1)
            print(f"\nğŸ“Š V2 ìµœì¢… íŠ¹ì„± ìˆ˜: {len(final_features.columns)}")

            # 6. ëª¨ë¸ í…ŒìŠ¤íŠ¸
            results = test_enhanced_models_v2(final_features, targets['target_vol_5d'])

            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
            if results:
                valid_results = {k: v for k, v in results.items() if v['mean_r2'] > -900}
                if valid_results:
                    best_model = max(valid_results.items(), key=lambda x: x[1]['mean_r2'])
                    print(f"\nğŸ† V2 ìµœê³  ì„±ëŠ¥: {best_model[0]}")
                    print(f"   RÂ² = {best_model[1]['mean_r2']:.4f} Â± {best_model[1]['std_r2']:.4f}")
                    print(f"   MAE = {best_model[1]['mean_mae']:.6f}")

                    # ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ
                    print(f"\nğŸ“ˆ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ê°œì„ :")
                    baseline_r2 = 0.0988  # ê¸°ì¡´ ìµœì¢… ê²€ì¦ ì„±ëŠ¥
                    improvement = (best_model[1]['mean_r2'] - baseline_r2) / abs(baseline_r2) * 100
                    print(f"   ê¸°ì¡´ RÂ²: {baseline_r2:.4f}")
                    print(f"   V2 RÂ²:   {best_model[1]['mean_r2']:.4f}")
                    print(f"   ê°œì„ :    {improvement:+.1f}%")

            # 7. ê²°ê³¼ ì €ì¥
            os.makedirs('results', exist_ok=True)

            v2_results = {
                'version': 'V2',
                'timestamp': datetime.now().isoformat(),
                'data_period': '2010-2024 (í™•ì¥)',
                'data_source': 'SPY + VIX + Treasury rates',
                'feature_counts': {
                    'enhanced_total': len(enhanced_features.columns),
                    'top_selected': len(top_features_df.columns),
                    'interactions': len(interaction_features.columns),
                    'final_total': len(final_features.columns)
                },
                'model_results': results,
                'top_features': top_20_features.tolist(),
                'best_model': {
                    'name': best_model[0] if 'best_model' in locals() else None,
                    'performance': best_model[1] if 'best_model' in locals() else None
                }
            }

            with open('results/enhanced_volatility_model_v2.json', 'w') as f:
                json.dump(v2_results, f, indent=2, default=str)

            print(f"\nğŸ’¾ V2 ëª¨ë¸ ê²°ê³¼ ì €ì¥: results/enhanced_volatility_model_v2.json")

    print("=" * 70)

if __name__ == "__main__":
    main()