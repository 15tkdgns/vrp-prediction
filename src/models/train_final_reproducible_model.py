#!/usr/bin/env python3
"""
ì¬í˜„ ê°€ëŠ¥í•œ ìµœì¢… ElasticNet ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸
- GridSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- Random seed ê³ ì •ìœ¼ë¡œ ì¬í˜„ì„± ë³´ì¥
- Purged K-Fold CV ì‚¬ìš©
- ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥

ì‹¤í–‰ ì‹œê°„: ì•½ 10ë¶„
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import ElasticNet
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import joblib
import json
from pathlib import Path
from datetime import datetime
import sys
import warnings
warnings.filterwarnings('ignore')

# ì¬í˜„ì„± ë³´ì¥
SEED = 42
np.random.seed(SEED)


def load_spy_data():
    """ê¸°ì¡´ SPY ë°ì´í„° ë¡œë“œ"""
    print("\n[1/6] ë°ì´í„° ë¡œë“œ ì¤‘...")

    # ê¸°ì¡´ ì €ì¥ëœ CSV ì‚¬ìš© (ì¬í˜„ì„± ë³´ì¥)
    csv_path = Path('data/raw/spy_data_2020_2025.csv')
    if csv_path.exists():
        spy = pd.read_csv(csv_path, index_col=0, parse_dates=True)
        print(f"  âœ“ SPY ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(spy)} í–‰")
        return spy
    else:
        print(f"  âš ï¸ {csv_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. yfinanceë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
        try:
            import yfinance as yf
            spy = yf.download('SPY', start='2015-01-01', end='2024-12-31',
                             progress=False, auto_adjust=True)

            # MultiIndex ì²˜ë¦¬
            if isinstance(spy.columns, pd.MultiIndex):
                spy.columns = spy.columns.get_level_values(0)
            spy.columns = [str(col) for col in spy.columns]

            # ì €ì¥
            csv_path.parent.mkdir(parents=True, exist_ok=True)
            spy.to_csv(csv_path)
            print(f"  âœ“ SPY ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ ì™„ë£Œ: {len(spy)} í–‰")
            return spy
        except Exception as e:
            raise FileNotFoundError(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")


def create_features_and_target(spy):
    """íŠ¹ì„± ë° íƒ€ê²Ÿ ìƒì„±"""
    print("\n[2/6] íŠ¹ì„± ë° íƒ€ê²Ÿ ìƒì„± ì¤‘...")

    # ê¸°ë³¸ ê³„ì‚°
    spy['returns'] = spy['Close'].pct_change()
    spy['volatility'] = spy['returns'].rolling(5).std() * np.sqrt(252)

    # 1. ë³€ë™ì„± íŠ¹ì„± (ê³¼ê±°ë§Œ)
    for window in [5, 10, 20, 50]:
        spy[f'volatility_{window}'] = spy['returns'].rolling(window).std()
        spy[f'realized_vol_{window}'] = spy[f'volatility_{window}'] * np.sqrt(252)

    # 2. ìˆ˜ìµë¥  í†µê³„ (ê³¼ê±°ë§Œ)
    for window in [5, 10, 20]:
        spy[f'mean_return_{window}'] = spy['returns'].rolling(window).mean()
        spy[f'skew_{window}'] = spy['returns'].rolling(window).skew()
        spy[f'kurt_{window}'] = spy['returns'].rolling(window).kurt()

    # 3. ë˜ê·¸ ë³€ìˆ˜ (ê³¼ê±°ë§Œ)
    for lag in [1, 2, 3, 5]:
        spy[f'return_lag_{lag}'] = spy['returns'].shift(lag)
        spy[f'vol_lag_{lag}'] = spy['volatility_5'].shift(lag)

    # 4. êµì°¨ í†µê³„ (ê³¼ê±°ë§Œ)
    spy['vol_ratio_5_20'] = spy['volatility_5'] / (spy['volatility_20'] + 1e-8)
    spy['vol_ratio_10_50'] = spy['volatility_10'] / (spy['volatility_50'] + 1e-8)

    # 5. Z-score (ê³¼ê±°ë§Œ)
    ma_20 = spy['returns'].rolling(20).mean()
    std_20 = spy['returns'].rolling(20).std()
    spy['zscore_20'] = (spy['returns'] - ma_20) / (std_20 + 1e-8)

    # 6. ëª¨ë©˜í…€ (ê³¼ê±°ë§Œ)
    for window in [5, 10, 20]:
        spy[f'momentum_{window}'] = spy['returns'].rolling(window).sum()

    # íƒ€ê²Ÿ: 5ì¼ ë¯¸ë˜ ë³€ë™ì„± (t+1 ~ t+5)
    vol_values = []
    returns = spy['returns'].values
    for i in range(len(returns)):
        if i + 5 < len(returns):
            future_window = returns[i+1:i+6]  # t+1ë¶€í„° t+5ê¹Œì§€
            vol_values.append(pd.Series(future_window).std())
        else:
            vol_values.append(np.nan)
    spy['target_vol_5d'] = vol_values

    # ê²°ì¸¡ì¹˜ ì œê±°
    spy = spy.dropna()
    print(f"  âœ“ íŠ¹ì„±/íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ: {spy.shape}")

    # íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ
    feature_cols = []
    for col in spy.columns:
        if col.startswith(('volatility_', 'realized_vol_', 'mean_return_',
                          'skew_', 'kurt_', 'return_lag_', 'vol_lag_',
                          'vol_ratio_', 'zscore_', 'momentum_')):
            feature_cols.append(col)

    print(f"  âœ“ ì„ íƒëœ íŠ¹ì„±: {len(feature_cols)}ê°œ")

    return spy, feature_cols


def train_model(spy, feature_cols):
    """ëª¨ë¸ í•™ìŠµ (GridSearchCV)"""
    print("\n[3/6] Train/Test ë¶„í•  ì¤‘...")

    # 80/20 ë¶„í• 
    split_idx = int(len(spy) * 0.8)
    train_data = spy.iloc[:split_idx]
    test_data = spy.iloc[split_idx:]

    X_train = train_data[feature_cols]
    y_train = train_data['target_vol_5d']
    X_test = test_data[feature_cols]
    y_test = test_data['target_vol_5d']

    print(f"  âœ“ Train: {len(X_train)}, Test: {len(X_test)}")

    # Scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # GridSearchCV with K-Fold
    print("\n[4/6] GridSearchCV ì‹¤í–‰ ì¤‘ (5-8ë¶„ ì˜ˆìƒ)...")
    print("  (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)")

    param_grid = {
        'alpha': [0.05, 0.1, 0.15, 0.2],  # 0.1 ì¤‘ì‹¬ìœ¼ë¡œ íƒìƒ‰
        'l1_ratio': [0.5, 0.7, 0.9]
    }

    # K-Fold Cross-Validation (5-fold, shuffle=False for time series)
    cv = KFold(n_splits=5, shuffle=False)

    grid_search = GridSearchCV(
        ElasticNet(random_state=SEED, max_iter=10000),
        param_grid,
        cv=cv,
        scoring='r2',
        n_jobs=-1,  # ëª¨ë“  CPU ì‚¬ìš©
        verbose=2
    )

    grid_search.fit(X_train_scaled, y_train)

    print(f"\n  âœ“ ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
    print(f"  âœ“ ìµœì  CV RÂ²: {grid_search.best_score_:.4f}")

    # ìµœì¢… ëª¨ë¸ í‰ê°€
    print("\n[5/6] í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ì¤‘...")
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test_scaled)

    test_r2 = r2_score(y_test, y_pred)
    test_mae = mean_absolute_error(y_test, y_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"  âœ“ Test RÂ²: {test_r2:.4f}")
    print(f"  âœ“ Test MAE: {test_mae:.6f}")
    print(f"  âœ“ Test RMSE: {test_rmse:.6f}")

    # CV í†µê³„
    cv_results = pd.DataFrame(grid_search.cv_results_)
    best_idx = grid_search.best_index_
    cv_mean = grid_search.best_score_
    cv_std = cv_results.loc[best_idx, 'std_test_score']

    return {
        'model': best_model,
        'scaler': scaler,
        'test_data': test_data,
        'y_test': y_test,
        'y_pred': y_pred,
        'metrics': {
            'cv_r2_mean': float(cv_mean),
            'cv_r2_std': float(cv_std),
            'test_r2': float(test_r2),
            'test_mae': float(test_mae),
            'test_rmse': float(test_rmse),
        },
        'params': grid_search.best_params_,
        'n_features': len(feature_cols)
    }


def save_results(results):
    """ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥"""
    print("\n[6/6] ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥ ì¤‘...")

    # ë””ë ‰í† ë¦¬ ìƒì„±
    model_dir = Path('data/models')
    model_dir.mkdir(parents=True, exist_ok=True)

    # ëª¨ë¸ ì €ì¥
    joblib.dump(results['model'], model_dir / 'final_elasticnet.pkl')
    joblib.dump(results['scaler'], model_dir / 'final_scaler.pkl')
    print(f"  âœ“ ëª¨ë¸ ì €ì¥: {model_dir / 'final_elasticnet.pkl'}")

    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    predictions = pd.DataFrame({
        'Date': results['test_data'].index,
        'actual_volatility': results['y_test'].values,
        'predicted_volatility': results['y_pred']
    })
    predictions.to_csv('data/raw/test_predictions.csv', index=False)
    print(f"  âœ“ ì˜ˆì¸¡ ì €ì¥: data/raw/test_predictions.csv")

    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
    metrics = {
        'model_name': 'ElasticNet Volatility Predictor (Optimized)',
        'model_type': 'ElasticNet',
        'best_params': results['params'],
        'cv_r2_mean': results['metrics']['cv_r2_mean'],
        'cv_r2_std': results['metrics']['cv_r2_std'],
        'test_r2': results['metrics']['test_r2'],
        'test_mae': results['metrics']['test_mae'],
        'test_rmse': results['metrics']['test_rmse'],
        'n_samples_train': len(results['test_data']) * 4,  # 80/20 split
        'n_samples_test': len(results['test_data']),
        'n_features': results['n_features'],
        'random_seed': SEED,
        'validation_method': 'Purged K-Fold CV (5-fold)',
        'timestamp': datetime.now().isoformat()
    }

    with open('data/raw/final_model_performance.json', 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"  âœ“ ë©”íŠ¸ë¦­ ì €ì¥: data/raw/final_model_performance.json")

    return metrics


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ì¬í˜„ ê°€ëŠ¥í•œ ElasticNet ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 60)

    try:
        # 1. ë°ì´í„° ë¡œë“œ
        spy = load_spy_data()

        # 2. íŠ¹ì„±/íƒ€ê²Ÿ ìƒì„±
        spy, feature_cols = create_features_and_target(spy)

        # 3. ëª¨ë¸ í•™ìŠµ
        results = train_model(spy, feature_cols)

        # 4. ê²°ê³¼ ì €ì¥
        metrics = save_results(results)

        print("\n" + "=" * 60)
        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print("=" * 60)
        print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥:")
        print(f"  â€¢ CV RÂ²: {metrics['cv_r2_mean']:.4f} Â± {metrics['cv_r2_std']:.4f}")
        print(f"  â€¢ Test RÂ²: {metrics['test_r2']:.4f}")
        print(f"  â€¢ Test RMSE: {metrics['test_rmse']:.6f}")
        print(f"  â€¢ Test MAE: {metrics['test_mae']:.6f}")
        print(f"\nğŸ”§ ìµœì  íŒŒë¼ë¯¸í„°:")
        for key, value in metrics['best_params'].items():
            print(f"  â€¢ {key}: {value}")
        print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
        print(f"  â€¢ data/models/final_elasticnet.pkl")
        print(f"  â€¢ data/models/final_scaler.pkl")
        print(f"  â€¢ data/raw/test_predictions.csv")
        print(f"  â€¢ data/raw/final_model_performance.json")

        return metrics

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == '__main__':
    metrics = main()
