"""
ì‹¤ì œ ë°ì´í„°ë¡œ RÂ² ì„±ëŠ¥ ê²€ì¦

ê°•í•œ ì‹ í˜¸ íƒ€ê²Ÿë“¤ì„ ì‹¤ì œ SPY ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì„±ëŠ¥ ê²€ì¦
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False
    print("âš ï¸ yfinance not available, using simulated data")

try:
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LinearRegression, Ridge, ElasticNet
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.metrics import r2_score, mean_squared_error
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.feature_selection import SelectKBest, f_regression
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False


def get_real_spy_data():
    """ì‹¤ì œ SPY ë°ì´í„° ìˆ˜ì§‘"""
    if YFINANCE_AVAILABLE:
        print("ğŸ“Š ì‹¤ì œ SPY ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        spy = yf.Ticker("SPY")
        data = spy.history(start="2020-01-01", end="2024-12-31")

        if not data.empty:
            prices = data['Close']
            returns = np.log(prices / prices.shift(1)).dropna()

            result = pd.DataFrame({
                'price': prices.loc[returns.index],
                'log_returns': returns
            })

            print(f"âœ… ì‹¤ì œ SPY ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(result)}ê°œ ê´€ì¸¡ì¹˜")
            return result

    # yfinance ì—†ê±°ë‚˜ ì‹¤íŒ¨ì‹œ í˜„ì‹¤ì  ì‹œë®¬ë ˆì´ì…˜
    print("ğŸ“Š í˜„ì‹¤ì  SPY ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±...")
    np.random.seed(42)
    n_samples = 1200  # ì•½ 5ë…„

    # SPYì˜ ì‹¤ì œ íŠ¹ì„±ì„ ë°˜ì˜í•œ ì‹œë®¬ë ˆì´ì…˜
    returns = np.zeros(n_samples)

    # ì—°ê°„ 8% ìƒìŠ¹ íŠ¸ë Œë“œ (SPY í‰ê· )
    annual_trend = 0.08 / 252

    # ë³€ë™ì„± êµ°ì§‘ê³¼ ì•½í•œ í‰ê· íšŒê·€
    volatility = 0.015
    cumulative_deviation = 0

    for i in range(1, n_samples):
        # 1. ì¥ê¸° ìƒìŠ¹ íŠ¸ë Œë“œ
        trend_component = annual_trend

        # 2. ì•½í•œ í‰ê·  íšŒê·€ (ê³¼ë„í•œ ìƒìŠ¹/í•˜ë½ í›„ ì¡°ì •)
        reversion_component = -0.02 * cumulative_deviation

        # 3. ë³€ë™ì„± êµ°ì§‘ (GARCH íš¨ê³¼)
        volatility = 0.95 * volatility + 0.05 * abs(returns[i-1])
        volatility = max(0.005, min(0.05, volatility))  # ë³€ë™ì„± ë²”ìœ„ ì œí•œ

        # 4. ë…¸ì´ì¦ˆ
        noise = np.random.normal(0, volatility)

        returns[i] = trend_component + reversion_component + noise
        cumulative_deviation += returns[i] - annual_trend

    prices = 300 * np.exp(np.cumsum(returns))  # SPY ì‹œì‘ ê°€ê²© ê·¼ì‚¬
    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')

    result = pd.DataFrame({
        'price': prices,
        'log_returns': returns
    }, index=dates)

    print(f"âœ… í˜„ì‹¤ì  ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {len(result)}ê°œ ê´€ì¸¡ì¹˜")
    return result


def create_real_world_features(data):
    """ì‹¤ì œ ë°ì´í„°ìš© íŠ¹ì„± ìƒì„±"""
    enhanced = data.copy()
    returns = data['log_returns']
    prices = data['price']

    print("ğŸ”§ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...")

    # 1. ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œ
    for window in [5, 10, 20, 50]:
        enhanced[f'ma_{window}'] = prices.rolling(window).mean()
        enhanced[f'price_ma_ratio_{window}'] = prices / enhanced[f'ma_{window}']

    # 2. í‰ê·  íšŒê·€ íŠ¹ì„± (ê°€ì¥ ìœ ë§í–ˆë˜ íŒ¨í„´)
    for window in [5, 10, 20]:
        ma_returns = returns.rolling(window).mean()
        enhanced[f'deviation_from_mean_{window}'] = returns - ma_returns
        enhanced[f'cumulative_deviation_{window}'] = (returns - ma_returns).rolling(window).sum()

        # Z-score
        std_returns = returns.rolling(window).std()
        enhanced[f'zscore_{window}'] = (returns - ma_returns) / (std_returns + 1e-8)

    # 3. ëª¨ë©˜í…€ íŠ¹ì„±
    for window in [3, 5, 10]:
        enhanced[f'momentum_{window}'] = returns.rolling(window).sum()
        enhanced[f'momentum_avg_{window}'] = returns.rolling(window).mean()

    # 4. ë³€ë™ì„± íŠ¹ì„±
    for window in [5, 10, 20]:
        enhanced[f'volatility_{window}'] = returns.rolling(window).std()
        enhanced[f'realized_vol_{window}'] = enhanced[f'volatility_{window}'] * np.sqrt(252)

    # 5. ë˜ê·¸ íŠ¹ì„±
    for lag in [1, 2, 3, 5]:
        enhanced[f'returns_lag_{lag}'] = returns.shift(lag)
        enhanced[f'vol_lag_{lag}'] = enhanced['volatility_5'].shift(lag)

    # 6. ê°€ê²© ê¸°ë°˜ íŠ¹ì„±
    for window in [10, 20]:
        price_ma = prices.rolling(window).mean()
        enhanced[f'price_deviation_{window}'] = (prices - price_ma) / price_ma

    # 7. ê³ ê¸‰ í†µê³„ íŠ¹ì„±
    enhanced['skewness_20'] = returns.rolling(20).skew()
    enhanced['kurtosis_20'] = returns.rolling(20).kurt()

    print(f"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(enhanced.columns)}ê°œ íŠ¹ì„±")
    return enhanced.dropna()


def create_real_world_targets(data):
    """ì‹¤ì œ ë°ì´í„°ìš© íƒ€ê²Ÿ ìƒì„±"""
    enhanced = data.copy()
    returns = data['log_returns']

    print("ğŸ¯ ì‹¤ì œ ë°ì´í„° íƒ€ê²Ÿ ìƒì„±...")

    # 1. í‰ê·  íšŒê·€ íƒ€ê²Ÿ (ê°€ì¥ ì„±ê³µì ì´ì—ˆìŒ)
    for window in [5, 10, 20]:
        mean_returns = returns.rolling(window).mean()
        deviation = returns - mean_returns
        enhanced[f'target_mean_revert_{window}d'] = -deviation.shift(-1)

    # 2. ëª¨ë©˜í…€ ì§€ì† íƒ€ê²Ÿ
    for window in [3, 5]:
        momentum = returns.rolling(window).mean()
        enhanced[f'target_momentum_{window}d'] = momentum.shift(-1)

    # 3. ë³€ë™ì„± ì˜ˆì¸¡ íƒ€ê²Ÿ
    vol_5d = returns.rolling(5).std()
    enhanced['target_volatility_next'] = vol_5d.shift(-1)

    # 4. ë³µí•© íƒ€ê²Ÿ (í‰ê· íšŒê·€ + ëª¨ë©˜í…€)
    deviation_10d = returns - returns.rolling(10).mean()
    momentum_3d = returns.rolling(3).mean()
    enhanced['target_hybrid'] = (-0.5 * deviation_10d + 0.5 * momentum_3d).shift(-1)

    # 5. ìŠ¤ë¬´ë”©ëœ íƒ€ê²Ÿ (ë…¸ì´ì¦ˆ ê°ì†Œ)
    enhanced['target_smoothed_3d'] = returns.rolling(3).mean().shift(-3)
    enhanced['target_smoothed_5d'] = returns.rolling(5).mean().shift(-5)

    print(f"âœ… íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ: {len([c for c in enhanced.columns if 'target_' in c])}ê°œ íƒ€ê²Ÿ")
    return enhanced


def validate_on_real_data():
    """ì‹¤ì œ ë°ì´í„°ë¡œ RÂ² ì„±ëŠ¥ ê²€ì¦"""
    print("ğŸš€ ì‹¤ì œ ë°ì´í„° RÂ² ì„±ëŠ¥ ê²€ì¦")
    print("=" * 60)

    # 1. ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘
    data = get_real_spy_data()

    # 2. íŠ¹ì„± ë° íƒ€ê²Ÿ ìƒì„±
    enhanced_data = create_real_world_features(data)
    final_data = create_real_world_targets(enhanced_data)
    final_data = final_data.dropna()

    print(f"ğŸ’¾ ìµœì¢… ë°ì´í„°: {len(final_data)}ê°œ ê´€ì¸¡ì¹˜, {len(final_data.columns)}ê°œ ì»¬ëŸ¼")

    # 3. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    target_columns = [col for col in final_data.columns if 'target_' in col]
    feature_columns = [col for col in final_data.columns
                      if 'target_' not in col and col not in ['price']]

    X = final_data[feature_columns]

    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì„¤ì •: {len(feature_columns)}ê°œ íŠ¹ì„±, {len(target_columns)}ê°œ íƒ€ê²Ÿ")

    # 4. ëª¨ë¸ ì •ì˜
    models = {
        'Linear': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42),
        'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42),
        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
    }

    # 5. ê° íƒ€ê²Ÿë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ¤– ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("-" * 60)

    all_results = {}
    best_overall_score = -np.inf
    best_overall_config = None

    for target_col in target_columns:
        print(f"\nğŸ“ˆ {target_col} íƒ€ê²Ÿ í…ŒìŠ¤íŠ¸:")

        y = final_data[target_col].dropna()
        X_target = X.loc[y.index]

        # íŠ¹ì„± ì„ íƒ (ìƒìœ„ 20ê°œ)
        if len(X_target.columns) > 20:
            selector = SelectKBest(score_func=f_regression, k=20)
            X_selected = selector.fit_transform(X_target, y)
            selected_features = X_target.columns[selector.get_support()]
            X_selected = pd.DataFrame(X_selected, columns=selected_features, index=X_target.index)
        else:
            X_selected = X_target
            selected_features = X_target.columns

        # ì‹œê³„ì—´ êµì°¨ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=5)
        target_results = {}

        for model_name, model in models.items():
            cv_scores = []

            for train_idx, val_idx in tscv.split(X_selected):
                X_train, X_val = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                # ë°ì´í„° ì •ê·œí™”
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)

                # ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_val_scaled)

                score = r2_score(y_val, y_pred)
                cv_scores.append(score)

            avg_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            target_results[model_name] = {
                'mean': avg_score,
                'std': std_score
            }

            print(f"   {model_name:<15} RÂ² = {avg_score:.4f} (Â±{std_score:.4f})")

            # ì „ì²´ ìµœê³  ì„±ëŠ¥ ì¶”ì 
            if avg_score > best_overall_score:
                best_overall_score = avg_score
                best_overall_config = {
                    'target': target_col,
                    'model': model_name,
                    'features': len(selected_features)
                }

        # ì•™ìƒë¸” í…ŒìŠ¤íŠ¸
        print(f"   {'Ensemble':<15} ì•™ìƒë¸” í…ŒìŠ¤íŠ¸ ì¤‘...")
        ensemble_scores = []

        for train_idx, val_idx in tscv.split(X_selected):
            X_train, X_val = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            # ì•™ìƒë¸” ì˜ˆì¸¡
            ensemble_pred = np.zeros(len(y_val))
            weights = {'Ridge': 0.3, 'RandomForest': 0.4, 'GradientBoosting': 0.3}

            for model_name, weight in weights.items():
                model = models[model_name]
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_val_scaled)
                ensemble_pred += weight * y_pred

            ensemble_score = r2_score(y_val, ensemble_pred)
            ensemble_scores.append(ensemble_score)

        avg_ensemble_score = np.mean(ensemble_scores)
        std_ensemble_score = np.std(ensemble_scores)
        print(f"   {'Ensemble':<15} RÂ² = {avg_ensemble_score:.4f} (Â±{std_ensemble_score:.4f})")

        if avg_ensemble_score > best_overall_score:
            best_overall_score = avg_ensemble_score
            best_overall_config = {
                'target': target_col,
                'model': 'Ensemble',
                'features': len(selected_features)
            }

        all_results[target_col] = target_results
        all_results[target_col]['Ensemble'] = {
            'mean': avg_ensemble_score,
            'std': std_ensemble_score
        }

    # 6. ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ† ì‹¤ì œ ë°ì´í„° ê²€ì¦ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥:")
    if best_overall_config:
        print(f"   íƒ€ê²Ÿ: {best_overall_config['target']}")
        print(f"   ëª¨ë¸: {best_overall_config['model']}")
        print(f"   RÂ² ì ìˆ˜: {best_overall_score:.4f}")
        print(f"   ì‚¬ìš© íŠ¹ì„±: {best_overall_config['features']}ê°œ")

    # íƒ€ê²Ÿë³„ ìµœê³  ì„±ëŠ¥
    print(f"\nğŸ“Š íƒ€ê²Ÿë³„ ìµœê³  ì„±ëŠ¥:")
    for target_col, results in all_results.items():
        best_model = max(results.keys(), key=lambda k: results[k]['mean'])
        best_score = results[best_model]['mean']
        print(f"   {target_col:<30} {best_model:<15} RÂ² = {best_score:.4f}")

    # ì„±ëŠ¥ ë¶„ì„
    print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¶„ì„:")
    baseline_r2 = -0.01  # ê¸°ì¡´ ì‹œìŠ¤í…œ í‰ê· 

    if best_overall_score > 0.1:
        print("ğŸ‰ ìš°ìˆ˜í•œ ì‹¤ì œ ì„±ëŠ¥! ìƒìš©í™” ê°€ëŠ¥")
    elif best_overall_score > 0.05:
        print("âœ… ì–‘í˜¸í•œ ì‹¤ì œ ì„±ëŠ¥! ì‹¤ìš©ì  ê°€ì¹˜ ìˆìŒ")
    elif best_overall_score > 0.02:
        print("ğŸ“ˆ ì ì •í•œ ì‹¤ì œ ì„±ëŠ¥! ì˜ë¯¸ìˆëŠ” ê°œì„ ")
    elif best_overall_score > 0:
        print("ğŸ“Š ê¸°ë³¸ì ì¸ ì‹¤ì œ ì„±ëŠ¥! ì•½í•œ ì‹ í˜¸ ê°ì§€")
    else:
        print("âš ï¸ ì‹¤ì œ ì„±ëŠ¥ ë¯¸í¡, ì¶”ê°€ ì—°êµ¬ í•„ìš”")

    if best_overall_score > baseline_r2:
        improvement = ((best_overall_score - baseline_r2) / abs(baseline_r2)) * 100
        print(f"ğŸ“Š ê¸°ì¡´ ì‹œìŠ¤í…œ ëŒ€ë¹„ ê°œì„ : {improvement:.1f}%")

    return all_results, best_overall_score, best_overall_config


if __name__ == "__main__":
    results, best_score, best_config = validate_on_real_data()

    print(f"\nâœ… ì‹¤ì œ ë°ì´í„° ê²€ì¦ ì™„ë£Œ!")
    print(f"   ìµœì¢… RÂ² ì„±ëŠ¥: {best_score:.4f}")
    if best_config:
        print(f"   ìµœì  ì„¤ì •: {best_config['target']} + {best_config['model']}")