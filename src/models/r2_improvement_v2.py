"""
RÂ² ì„±ëŠ¥ ê°œì„  V2 - ë” ê°•ë ¥í•œ ì ‘ê·¼ë²•

ê¸°ì¡´ ë°©ë²•ì—ì„œ í•œ ë‹¨ê³„ ë” ë‚˜ì•„ê°„ ê°œì„  ë°©ë²•ë“¤:
1. íŠ¹ì„± ì„ íƒ ë° ì°¨ì› ì¶•ì†Œ
2. íƒ€ê²Ÿ ë³€ìˆ˜ ë³€í™˜ (ë¡œê·¸, ë°•ìŠ¤ì½•ìŠ¤ ë“±)
3. ì‹œê³„ì—´ íŠ¹í™” íŠ¹ì„± (ë˜ê·¸, ì°¨ë¶„, ê³„ì ˆì„±)
4. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •
5. ë…¸ì´ì¦ˆ ì œê±° ë° ì´ìƒì¹˜ ì²˜ë¦¬
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

try:
    from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer
    from sklearn.feature_selection import SelectKBest, f_regression, RFE
    from sklearn.decomposition import PCA
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
    from sklearn.linear_model import ElasticNet, Ridge, Lasso, BayesianRidge
    from sklearn.metrics import r2_score, mean_squared_error
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.pipeline import Pipeline
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ sklearn not available")

try:
    from scipy import stats
    from scipy.signal import savgol_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("âš ï¸ scipy not available")


class AdvancedTargetEngineering:
    """
    ê³ ê¸‰ íƒ€ê²Ÿ ë³€ìˆ˜ ì—”ì§€ë‹ˆì–´ë§

    RÂ² ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ë³€í™˜:
    - ìˆ˜ìµë¥  ìŠ¤ë¬´ë”©
    - ë¡œê·¸ ë³€í™˜
    - ë°•ìŠ¤ì½•ìŠ¤ ë³€í™˜
    - ëˆ„ì  ìˆ˜ìµë¥ 
    """

    def __init__(self):
        self.transformers = {}

    def create_enhanced_targets(self, data: pd.DataFrame) -> pd.DataFrame:
        """í–¥ìƒëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±"""
        print("ğŸ¯ ê³ ê¸‰ íƒ€ê²Ÿ ì—”ì§€ë‹ˆì–´ë§...")

        enhanced_data = data.copy()

        if 'log_returns' not in data.columns:
            print("âš ï¸ log_returns ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return enhanced_data

        returns = data['log_returns']

        # 1. ìŠ¤ë¬´ë”©ëœ ìˆ˜ìµë¥  (ë…¸ì´ì¦ˆ ì œê±°)
        if SCIPY_AVAILABLE:
            # Savitzky-Golay í•„í„°ë¡œ ìŠ¤ë¬´ë”©
            for window in [5, 11, 21]:
                try:
                    smoothed = savgol_filter(returns, window_length=window, polyorder=2)
                    enhanced_data[f'target_smoothed_{window}d'] = pd.Series(smoothed, index=returns.index).shift(-1)
                except:
                    pass

        # 2. ëˆ„ì  ìˆ˜ìµë¥  ê¸°ë°˜ íƒ€ê²Ÿ
        for days in [2, 3, 5, 7]:
            # í–¥í›„ Nì¼ ëˆ„ì  ìˆ˜ìµë¥ 
            enhanced_data[f'target_cumulative_{days}d'] = returns.rolling(days).sum().shift(-days)

            # í–¥í›„ Nì¼ ê¸°í•˜í‰ê·  ìˆ˜ìµë¥ 
            enhanced_data[f'target_geomean_{days}d'] = np.log(
                (1 + returns).rolling(days).apply(lambda x: np.prod(x) ** (1/len(x)) - 1)
            ).shift(-days)

        # 3. ë³€ë™ì„± ì¡°ì • ìˆ˜ìµë¥ 
        for window in [5, 10, 20]:
            vol = returns.rolling(window).std()
            # ìƒ¤í”„ ë¹„ìœ¨ ìŠ¤íƒ€ì¼ ì¡°ì •
            enhanced_data[f'target_vol_adjusted_{window}d'] = (returns / (vol + 1e-8)).shift(-1)

        # 4. ë¶„ìœ„ìˆ˜ ê¸°ë°˜ íƒ€ê²Ÿ (ê·¹ê°’ì— ëœ ë¯¼ê°)
        for days in [1, 3, 5]:
            future_returns = returns.shift(-days).rolling(days).sum()
            # ë¶„ìœ„ìˆ˜ ë³€í™˜ìœ¼ë¡œ ì •ê·œí™”
            if SKLEARN_AVAILABLE:
                qt = QuantileTransformer(output_distribution='normal')
                try:
                    transformed = qt.fit_transform(future_returns.dropna().values.reshape(-1, 1))
                    enhanced_data[f'target_quantile_{days}d'] = pd.Series(
                        transformed.flatten(),
                        index=future_returns.dropna().index
                    )
                    self.transformers[f'quantile_{days}d'] = qt
                except:
                    pass

        # 5. ë¡œê·¸ ë³€í™˜ íƒ€ê²Ÿ (ê·¹ê°’ ì™„í™”)
        for days in [1, 3, 5]:
            future_returns = returns.shift(-days).rolling(days).sum()
            # ë¡œê·¸ ë³€í™˜ (ìŒìˆ˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ shift)
            log_transformed = np.sign(future_returns) * np.log1p(np.abs(future_returns))
            enhanced_data[f'target_log_{days}d'] = log_transformed

        print(f"âœ… íƒ€ê²Ÿ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {len([c for c in enhanced_data.columns if 'target_' in c])}ê°œ íƒ€ê²Ÿ")
        return enhanced_data


class TimeSeriesFeatureEngineering:
    """
    ì‹œê³„ì—´ íŠ¹í™” íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§

    ì‹œê³„ì—´ ë°ì´í„°ì˜ ê³ ìœ í•œ íŒ¨í„´ì„ í¬ì°©í•˜ëŠ” íŠ¹ì„±ë“¤:
    - ë˜ê·¸ íŠ¹ì„±
    - ì°¨ë¶„ íŠ¹ì„±
    - ê³„ì ˆì„± íŠ¹ì„±
    - íŠ¸ë Œë“œ íŠ¹ì„±
    """

    def __init__(self):
        pass

    def create_timeseries_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ì‹œê³„ì—´ íŠ¹í™” íŠ¹ì„± ìƒì„±"""
        print("â° ì‹œê³„ì—´ íŠ¹í™” íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...")

        enhanced_data = data.copy()

        if 'log_returns' not in data.columns or 'price' not in data.columns:
            print("âš ï¸ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return enhanced_data

        returns = data['log_returns']
        prices = data['price']

        # 1. ë˜ê·¸ íŠ¹ì„± (ê³¼ê±° ì •ë³´)
        for lag in [1, 2, 3, 5, 10, 20]:
            enhanced_data[f'returns_lag_{lag}'] = returns.shift(lag)
            enhanced_data[f'price_lag_{lag}'] = prices.shift(lag)

        # 2. ì°¨ë¶„ íŠ¹ì„± (ë³€í™”ìœ¨)
        for lag in [1, 2, 5, 10]:
            enhanced_data[f'returns_diff_{lag}'] = returns.diff(lag)
            enhanced_data[f'price_diff_{lag}'] = prices.diff(lag)
            enhanced_data[f'price_pct_change_{lag}'] = prices.pct_change(lag)

        # 3. ì´ë™ í†µê³„ëŸ‰ì˜ ë˜ê·¸
        for window in [5, 10, 20]:
            ma = returns.rolling(window).mean()
            std = returns.rolling(window).std()

            for lag in [1, 2, 5]:
                enhanced_data[f'ma_{window}_lag_{lag}'] = ma.shift(lag)
                enhanced_data[f'std_{window}_lag_{lag}'] = std.shift(lag)

        # 4. íŠ¸ë Œë“œ íŠ¹ì„±
        # ì„ í˜• íŠ¸ë Œë“œ
        enhanced_data['price_trend_5d'] = prices.rolling(5).apply(
            lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 5 else np.nan
        )
        enhanced_data['price_trend_20d'] = prices.rolling(20).apply(
            lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 20 else np.nan
        )

        # 5. ìê¸°ìƒê´€ íŠ¹ì„±
        for lag in [1, 5, 10]:
            enhanced_data[f'returns_autocorr_{lag}'] = returns.rolling(20).apply(
                lambda x: x.autocorr(lag=lag) if len(x) > lag + 1 else np.nan
            )

        # 6. íŒ¨í„´ ì¸ì‹ íŠ¹ì„±
        # ì—°ì† ìƒìŠ¹/í•˜ë½ ì¼ìˆ˜
        enhanced_data['consecutive_up'] = (returns > 0).astype(int).groupby(
            (returns <= 0).cumsum()
        ).cumsum()
        enhanced_data['consecutive_down'] = (returns < 0).astype(int).groupby(
            (returns >= 0).cumsum()
        ).cumsum()

        # 7. ìµœê³ /ìµœì €ì ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬
        for window in [10, 20, 50]:
            rolling_max = prices.rolling(window).max()
            rolling_min = prices.rolling(window).min()
            enhanced_data[f'distance_from_high_{window}'] = (rolling_max - prices) / rolling_max
            enhanced_data[f'distance_from_low_{window}'] = (prices - rolling_min) / rolling_min

        print(f"âœ… ì‹œê³„ì—´ íŠ¹ì„± ìƒì„± ì™„ë£Œ")
        return enhanced_data


class IntelligentFeatureSelection:
    """
    ì§€ëŠ¥í˜• íŠ¹ì„± ì„ íƒ

    RÂ² ì„±ëŠ¥ì— ê¸°ì—¬í•˜ëŠ” íŠ¹ì„±ë“¤ë§Œ ì„ ë³„:
    - í†µê³„ì  ìœ ì˜ì„± ê¸°ë°˜ ì„ íƒ
    - ì¬ê·€ì  íŠ¹ì„± ì œê±° (RFE)
    - ìƒí˜¸ ì •ë³´ëŸ‰ ê¸°ë°˜ ì„ íƒ
    - ëª¨ë¸ ê¸°ë°˜ ì¤‘ìš”ë„ ì„ íƒ
    """

    def __init__(self):
        self.selected_features = {}
        self.feature_scores = {}

    def select_best_features(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        n_features: int = 50,
        methods: List[str] = None
    ) -> pd.DataFrame:
        """ìµœì  íŠ¹ì„± ì„ íƒ"""
        print(f"ğŸ¯ ì§€ëŠ¥í˜• íŠ¹ì„± ì„ íƒ (ëª©í‘œ: {n_features}ê°œ)...")

        if methods is None:
            methods = ['f_test', 'mutual_info', 'rfe', 'model_based']

        feature_scores = {}
        selected_features_by_method = {}

        # 1. F-ê²€ì • ê¸°ë°˜ ì„ íƒ
        if 'f_test' in methods:
            print("   F-ê²€ì • ê¸°ë°˜ íŠ¹ì„± ì„ íƒ...")
            selector = SelectKBest(score_func=f_regression, k=min(n_features, len(X.columns)))
            X_selected = selector.fit_transform(X, y)
            selected_features = X.columns[selector.get_support()].tolist()
            scores = selector.scores_

            selected_features_by_method['f_test'] = selected_features
            feature_scores['f_test'] = dict(zip(X.columns, scores))

        # 2. ìƒí˜¸ ì •ë³´ëŸ‰ ê¸°ë°˜ ì„ íƒ
        if 'mutual_info' in methods:
            print("   ìƒí˜¸ ì •ë³´ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ...")
            from sklearn.feature_selection import mutual_info_regression

            mi_scores = mutual_info_regression(X, y, random_state=42)
            mi_ranking = np.argsort(mi_scores)[::-1][:n_features]
            selected_features = X.columns[mi_ranking].tolist()

            selected_features_by_method['mutual_info'] = selected_features
            feature_scores['mutual_info'] = dict(zip(X.columns, mi_scores))

        # 3. ì¬ê·€ì  íŠ¹ì„± ì œê±° (RFE)
        if 'rfe' in methods:
            print("   ì¬ê·€ì  íŠ¹ì„± ì œê±°...")
            estimator = RandomForestRegressor(n_estimators=50, random_state=42)
            selector = RFE(estimator, n_features_to_select=n_features)
            selector.fit(X, y)
            selected_features = X.columns[selector.support_].tolist()

            selected_features_by_method['rfe'] = selected_features

        # 4. ëª¨ë¸ ê¸°ë°˜ ì¤‘ìš”ë„
        if 'model_based' in methods:
            print("   ëª¨ë¸ ê¸°ë°˜ ì¤‘ìš”ë„ ì„ íƒ...")
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
            rf.fit(X, y)

            importance_ranking = np.argsort(rf.feature_importances_)[::-1][:n_features]
            selected_features = X.columns[importance_ranking].tolist()

            selected_features_by_method['model_based'] = selected_features
            feature_scores['model_based'] = dict(zip(X.columns, rf.feature_importances_))

        # 5. ì•™ìƒë¸” íŠ¹ì„± ì„ íƒ (íˆ¬í‘œ ë°©ì‹)
        all_features = []
        for method_features in selected_features_by_method.values():
            all_features.extend(method_features)

        # íŠ¹ì„±ë³„ ì„ íƒ íšŸìˆ˜ ê³„ì‚°
        feature_counts = {}
        for feature in all_features:
            feature_counts[feature] = feature_counts.get(feature, 0) + 1

        # ê°€ì¥ ë§ì´ ì„ íƒëœ íŠ¹ì„±ë“¤ ì„ íƒ
        sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)
        final_selected = [feature for feature, count in sorted_features[:n_features]]

        print(f"âœ… íŠ¹ì„± ì„ íƒ ì™„ë£Œ: {len(final_selected)}ê°œ íŠ¹ì„± ì„ íƒ")
        print(f"   ìƒìœ„ 10ê°œ íŠ¹ì„±: {final_selected[:10]}")

        self.selected_features = final_selected
        self.feature_scores = feature_scores

        return X[final_selected]


class DynamicEnsembleOptimizer:
    """
    ë™ì  ì•™ìƒë¸” ìµœì í™”

    ì‹œê°„ì— ë”°ë¼ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ì•™ìƒë¸”:
    - ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
    - ì‹œê°„ ê°ì‡  ê°€ì¤‘ì¹˜
    - ì ì‘ì  ëª¨ë¸ ì„ íƒ
    """

    def __init__(self):
        self.models = {}
        self.weights_history = []
        self.performance_history = []

    def create_dynamic_ensemble(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        base_models: List = None
    ) -> Dict:
        """ë™ì  ì•™ìƒë¸” ìƒì„±"""
        print("ğŸ­ ë™ì  ì•™ìƒë¸” ìµœì í™”...")

        if base_models is None:
            base_models = [
                ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
                ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),
                ('elastic', ElasticNet(random_state=42)),
                ('ridge', Ridge(random_state=42)),
                ('bayesian', BayesianRidge())
            ]

        # ì‹œê³„ì—´ ë¶„í• 
        tscv = TimeSeriesSplit(n_splits=5)

        # ê° ëª¨ë¸ì˜ ì‹œê°„ë³„ ì„±ëŠ¥ ì¶”ì 
        model_performance = {name: [] for name, _ in base_models}

        fold_predictions = {name: [] for name, _ in base_models}
        fold_actuals = []

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            print(f"   í´ë“œ {fold + 1}/5 ì²˜ë¦¬ ì¤‘...")

            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            fold_actuals.extend(y_val.values)

            for name, model in base_models:
                # ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)

                # ì˜ˆì¸¡
                y_pred = model.predict(X_val)
                fold_predictions[name].extend(y_pred)

                # ì„±ëŠ¥ ê³„ì‚°
                score = r2_score(y_val, y_pred)
                model_performance[name].append(score)

                print(f"     {name}: RÂ² = {score:.4f}")

        # ì „ì²´ ì„±ëŠ¥ì— ê¸°ë°˜í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        final_weights = {}
        for name in model_performance:
            # í‰ê·  ì„±ëŠ¥ì— ê¸°ë°˜í•œ ê¸°ë³¸ ê°€ì¤‘ì¹˜
            avg_performance = np.mean(model_performance[name])

            # ì„±ëŠ¥ì´ ìŒìˆ˜ì¸ ê²½ìš° 0 ê°€ì¤‘ì¹˜, ì–‘ìˆ˜ì¸ ê²½ìš° softmax ì ìš©
            if avg_performance > 0:
                final_weights[name] = avg_performance
            else:
                final_weights[name] = 0.01  # ìµœì†Œ ê°€ì¤‘ì¹˜

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(final_weights.values())
        if total_weight > 0:
            final_weights = {k: v/total_weight for k, v in final_weights.items()}
        else:
            # ëª¨ë“  ëª¨ë¸ì´ ìŒìˆ˜ ì„±ëŠ¥ì¸ ê²½ìš° ê· ë“± ê°€ì¤‘ì¹˜
            final_weights = {name: 1/len(base_models) for name, _ in base_models}

        # ì•™ìƒë¸” ì˜ˆì¸¡ ê³„ì‚°
        ensemble_predictions = np.zeros(len(fold_actuals))
        for name, weight in final_weights.items():
            ensemble_predictions += weight * np.array(fold_predictions[name])

        # ìµœì¢… ì•™ìƒë¸” ì„±ëŠ¥
        ensemble_score = r2_score(fold_actuals, ensemble_predictions)

        print(f"âœ… ë™ì  ì•™ìƒë¸” ì™„ë£Œ:")
        print(f"   ì•™ìƒë¸” RÂ² = {ensemble_score:.4f}")
        print(f"   ê°€ì¤‘ì¹˜: {final_weights}")

        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ë“¤ í›ˆë ¨
        final_models = {}
        for name, model in base_models:
            model.fit(X, y)
            final_models[name] = model

        return {
            'models': final_models,
            'weights': final_weights,
            'ensemble_score': ensemble_score,
            'individual_performance': model_performance
        }


def run_advanced_r2_improvement():
    """ê³ ê¸‰ RÂ² ê°œì„  ì‹¤í—˜"""
    print("ğŸš€ ê³ ê¸‰ RÂ² ì„±ëŠ¥ ê°œì„  ì‹¤í—˜ V2")
    print("=" * 60)

    # 1. í–¥ìƒëœ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
    print("\nğŸ“Š 1ë‹¨ê³„: í–¥ìƒëœ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±")
    np.random.seed(42)

    n_samples = 1000
    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')

    # ë” í˜„ì‹¤ì ì¸ ì‹œê³„ì—´ íŒ¨í„´ ìƒì„±
    # 1. íŠ¸ë Œë“œ ì„±ë¶„
    trend = 0.0001 * np.arange(n_samples)

    # 2. ê³„ì ˆì„± ì„±ë¶„ (ì—°ê°„ ì‚¬ì´í´)
    seasonal = 0.0005 * np.sin(2 * np.pi * np.arange(n_samples) / 252)

    # 3. AR(1) ì„±ë¶„ (ìê¸°ìƒê´€)
    ar_coef = 0.1
    ar_noise = np.random.normal(0, 0.01, n_samples)
    ar_component = np.zeros(n_samples)
    for i in range(1, n_samples):
        ar_component[i] = ar_coef * ar_component[i-1] + ar_noise[i]

    # 4. ë³€ë™ì„± êµ°ì§‘ (GARCH ìŠ¤íƒ€ì¼)
    volatility = np.zeros(n_samples)
    volatility[0] = 0.02
    for i in range(1, n_samples):
        volatility[i] = 0.95 * volatility[i-1] + 0.05 * np.abs(ar_component[i-1])

    # ìµœì¢… ìˆ˜ìµë¥ 
    returns = trend + seasonal + ar_component + np.random.normal(0, volatility)

    # ê°€ê²© ê³„ì‚°
    prices = 100 * np.exp(np.cumsum(returns))

    base_data = pd.DataFrame({
        'price': prices,
        'log_returns': returns
    }, index=dates)

    print(f"   ê¸°ë³¸ ë°ì´í„°: {len(base_data.columns)}ê°œ íŠ¹ì„±, {len(base_data)}ê°œ ê´€ì¸¡ì¹˜")

    # 2. ê³ ê¸‰ íƒ€ê²Ÿ ì—”ì§€ë‹ˆì–´ë§
    print("\nğŸ¯ 2ë‹¨ê³„: ê³ ê¸‰ íƒ€ê²Ÿ ì—”ì§€ë‹ˆì–´ë§")
    target_engineer = AdvancedTargetEngineering()
    enhanced_data = target_engineer.create_enhanced_targets(base_data)

    # 3. ì‹œê³„ì—´ íŠ¹í™” íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
    print("\nâ° 3ë‹¨ê³„: ì‹œê³„ì—´ íŠ¹í™” íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§")
    ts_engineer = TimeSeriesFeatureEngineering()
    enhanced_data = ts_engineer.create_timeseries_features(enhanced_data)

    # ê²°ì¸¡ì¹˜ ì œê±°
    enhanced_data = enhanced_data.dropna()
    print(f"   ìµœì¢… ë°ì´í„°: {len(enhanced_data.columns)}ê°œ íŠ¹ì„±, {len(enhanced_data)}ê°œ ê´€ì¸¡ì¹˜")

    # 4. ì§€ëŠ¥í˜• íŠ¹ì„± ì„ íƒ
    print("\nğŸ¯ 4ë‹¨ê³„: ì§€ëŠ¥í˜• íŠ¹ì„± ì„ íƒ")

    # íƒ€ê²Ÿê³¼ íŠ¹ì„± ë¶„ë¦¬
    target_columns = [col for col in enhanced_data.columns if 'target_' in col]
    feature_columns = [col for col in enhanced_data.columns if 'target_' not in col]

    X = enhanced_data[feature_columns]

    feature_selector = IntelligentFeatureSelection()

    # ê°€ì¥ ìœ ë§í•œ íƒ€ê²Ÿë“¤ì— ëŒ€í•´ íŠ¹ì„± ì„ íƒ ìˆ˜í–‰
    best_targets = ['target_smoothed_5d', 'target_cumulative_3d', 'target_vol_adjusted_10d', 'target_quantile_1d']

    results = {}

    for target_col in best_targets:
        if target_col in enhanced_data.columns:
            print(f"\n   {target_col} íƒ€ê²Ÿ ì²˜ë¦¬:")
            y = enhanced_data[target_col].dropna()
            X_target = X.loc[y.index]

            # íŠ¹ì„± ì„ íƒ
            X_selected = feature_selector.select_best_features(X_target, y, n_features=30)

            # 5. ë™ì  ì•™ìƒë¸” ìµœì í™”
            print(f"\nğŸ­ 5ë‹¨ê³„: ë™ì  ì•™ìƒë¸” ìµœì í™” ({target_col})")
            ensemble_optimizer = DynamicEnsembleOptimizer()
            ensemble_result = ensemble_optimizer.create_dynamic_ensemble(X_selected, y)

            results[target_col] = {
                'ensemble_result': ensemble_result,
                'selected_features': X_selected.columns.tolist()
            }

    # 6. ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½
    print("\nğŸ“ˆ 6ë‹¨ê³„: ê²°ê³¼ ë¶„ì„")
    print("=" * 60)

    print("\nğŸ† íƒ€ê²Ÿë³„ ìµœê³  ì„±ëŠ¥:")
    print("-" * 50)

    best_overall_score = -np.inf
    best_overall_target = None

    for target_col, result in results.items():
        score = result['ensemble_result']['ensemble_score']
        print(f"   {target_col:<25} RÂ² = {score:.4f}")

        if score > best_overall_score:
            best_overall_score = score
            best_overall_target = target_col

    print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ íƒ€ê²Ÿ: {best_overall_target}")
    print(f"   ìµœê³  RÂ² ì ìˆ˜: {best_overall_score:.4f}")

    # ê°œì„  ë¶„ì„
    baseline_r2 = -0.01  # ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ í‰ê·  RÂ²
    if best_overall_score > baseline_r2:
        improvement = ((best_overall_score - baseline_r2) / abs(baseline_r2)) * 100
        print(f"ğŸ“Š ì„±ëŠ¥ ê°œì„ : {improvement:.1f}% í–¥ìƒ")

        if best_overall_score > 0.1:
            print("ğŸ‰ ìƒë‹¹í•œ ì„±ëŠ¥ ê°œì„  ë‹¬ì„±!")
        elif best_overall_score > 0.05:
            print("âœ… ì˜ë¯¸ìˆëŠ” ì„±ëŠ¥ ê°œì„  ë‹¬ì„±!")
        else:
            print("ğŸ“ˆ ì ì§„ì  ì„±ëŠ¥ ê°œì„ ")
    else:
        print("âš ï¸ ì¶”ê°€ ìµœì í™” í•„ìš”")

    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ ìƒì„¸ ë¶„ì„
    if best_overall_target:
        best_result = results[best_overall_target]
        print(f"\nğŸ” ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìƒì„¸ ë¶„ì„ ({best_overall_target}):")
        print("-" * 50)

        individual_perf = best_result['ensemble_result']['individual_performance']
        weights = best_result['ensemble_result']['weights']

        print("   ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:")
        for model_name, scores in individual_perf.items():
            avg_score = np.mean(scores)
            weight = weights[model_name]
            print(f"     {model_name:<12} RÂ² = {avg_score:.4f}, ê°€ì¤‘ì¹˜ = {weight:.3f}")

        print(f"\n   ì„ íƒëœ íŠ¹ì„± ìˆ˜: {len(best_result['selected_features'])}")
        print(f"   ìƒìœ„ 10ê°œ íŠ¹ì„±: {best_result['selected_features'][:10]}")

    print(f"\nâœ… ê³ ê¸‰ RÂ² ê°œì„  ì‹¤í—˜ ì™„ë£Œ!")

    return {
        'results': results,
        'best_target': best_overall_target,
        'best_score': best_overall_score,
        'enhanced_data': enhanced_data
    }


if __name__ == "__main__":
    # ê³ ê¸‰ RÂ² ê°œì„  ì‹¤í—˜ ì‹¤í–‰
    experiment_results = run_advanced_r2_improvement()