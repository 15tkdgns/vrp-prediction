#!/usr/bin/env python3
"""
FinBERT ë‰´ìŠ¤ ì‹¬ë¦¬ ì§€ìˆ˜ í†µí•© íŒŒì´í”„ë¼ì¸
======================================

Hugging Faceì˜ ê¸ˆìœµ ë‰´ìŠ¤ ì‹¬ë¦¬ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬
ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ì— í†µí•©

ë°ì´í„°ì…‹: 
- Kaggle Financial PhraseBank
- Hugging Face zeroshot/twitter-financial-news-sentiment
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import yfinance as yf
from pathlib import Path
from datetime import datetime, timedelta
import json

# Hugging Face ë°ì´í„°ì…‹
try:
    from datasets import load_dataset
    HAS_HF = True
except ImportError:
    HAS_HF = False
    print("âš ï¸ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”: pip install datasets")

# FinBERT
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    HAS_FINBERT = True
except ImportError:
    HAS_FINBERT = False
    print("âš ï¸ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”: pip install transformers")

# sklearn
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error

SEED = 42
np.random.seed(SEED)


# =============================================================================
# 1. ë‰´ìŠ¤ ì‹¬ë¦¬ ë°ì´í„° ë¡œë“œ (Hugging Face)
# =============================================================================

def load_financial_sentiment_dataset():
    """Hugging Faceì—ì„œ ê¸ˆìœµ ë‰´ìŠ¤ ì‹¬ë¦¬ ë°ì´í„° ë¡œë“œ"""
    print("\n" + "="*60)
    print("[1/6] ê¸ˆìœµ ë‰´ìŠ¤ ì‹¬ë¦¬ ë°ì´í„°ì…‹ ë¡œë“œ")
    print("="*60)
    
    if not HAS_HF:
        # ëŒ€ì•ˆ: ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
        print("  âš ï¸ Hugging Face ë¯¸ì„¤ì¹˜, ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì‚¬ìš©")
        return generate_simulated_sentiment()
    
    try:
        # Twitter Financial Sentiment ë°ì´í„°ì…‹
        print("  â†’ Twitter Financial Sentiment ë¡œë“œ ì¤‘...")
        dataset = load_dataset("zeroshot/twitter-financial-news-sentiment", split="train")
        
        df = pd.DataFrame({
            'text': dataset['text'],
            'label': dataset['label']
        })
        
        # ë¼ë²¨ ë³€í™˜ (0=bearish, 1=bullish, 2=neutral)
        label_map = {0: -1, 1: 1, 2: 0}
        df['sentiment'] = df['label'].map(label_map)
        
        print(f"  âœ“ ë¡œë“œ ì™„ë£Œ: {len(df)} ìƒ˜í”Œ")
        print(f"  âœ“ ê°ì„± ë¶„í¬: Bullish={sum(df['sentiment']==1)}, Bearish={sum(df['sentiment']==-1)}, Neutral={sum(df['sentiment']==0)}")
        
        return df
        
    except Exception as e:
        print(f"  âš ï¸ Hugging Face ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("  â†’ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì‚¬ìš©")
        return generate_simulated_sentiment()


def generate_simulated_sentiment():
    """VIX ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ ì‹¬ë¦¬ ì§€ìˆ˜ ìƒì„±"""
    print("  â†’ VIX ê¸°ë°˜ ì‹¬ë¦¬ ì§€ìˆ˜ ì‹œë®¬ë ˆì´ì…˜...")
    
    # VIX ë°ì´í„° ë¡œë“œ
    vix = yf.download('^VIX', start='2020-01-01', end='2025-01-01', 
                     progress=False, auto_adjust=True)
    if isinstance(vix.columns, pd.MultiIndex):
        vix.columns = vix.columns.get_level_values(0)
    
    # VIXë¥¼ ê°ì„± ì§€ìˆ˜ë¡œ ë³€í™˜ (-1 ~ 1)
    # ë†’ì€ VIX = ë¶€ì •ì  ê°ì„±, ë‚®ì€ VIX = ê¸ì •ì  ê°ì„±
    vix_norm = (vix['Close'] - vix['Close'].mean()) / vix['Close'].std()
    sentiment = -np.tanh(vix_norm / 2)  # -1 ~ 1 ë²”ìœ„
    
    # ë…¸ì´ì¦ˆ ì¶”ê°€
    np.random.seed(SEED)
    noise = np.random.normal(0, 0.1, len(sentiment))
    sentiment = np.clip(sentiment + noise, -1, 1)
    
    df = pd.DataFrame({
        'date': vix.index,
        'sentiment': sentiment.values,
        'vix': vix['Close'].values
    })
    
    print(f"  âœ“ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {len(df)} ì¼")
    
    return df


# =============================================================================
# 2. FinBERT ê°ì„± ë¶„ì„ (ì„ íƒì )
# =============================================================================

class FinBERTSentimentAnalyzer:
    """FinBERT ê¸°ë°˜ ê°ì„± ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = "cpu"
        
    def load_model(self):
        """FinBERT ëª¨ë¸ ë¡œë“œ"""
        if not HAS_FINBERT:
            print("  âš ï¸ FinBERT ë¯¸ì„¤ì¹˜")
            return False
            
        try:
            print("  â†’ FinBERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
            self.model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
            self.model.eval()
            print("  âœ“ FinBERT ë¡œë“œ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"  âš ï¸ FinBERT ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def analyze(self, text):
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„"""
        if self.model is None:
            return 0.0
            
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, 
                                max_length=512, padding=True)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # [positive, negative, neutral]
        sentiment_score = probs[0][0].item() - probs[0][1].item()  # pos - neg
        return sentiment_score
    
    def analyze_batch(self, texts, batch_size=32):
        """ë°°ì¹˜ ê°ì„± ë¶„ì„"""
        results = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            for text in batch:
                results.append(self.analyze(text))
        return results


# =============================================================================
# 3. ì¼ë³„ ì‹¬ë¦¬ ì§€ìˆ˜ ìƒì„±
# =============================================================================

def create_daily_sentiment_features(sentiment_df, spy_index):
    """ì¼ë³„ ì‹¬ë¦¬ íŠ¹ì„± ìƒì„±"""
    print("\n" + "="*60)
    print("[2/6] ì¼ë³„ ì‹¬ë¦¬ íŠ¹ì„± ìƒì„±")
    print("="*60)
    
    # ë‚ ì§œë³„ ì§‘ê³„ê°€ í•„ìš”í•œ ê²½ìš°
    if 'date' in sentiment_df.columns:
        # ì´ë¯¸ ì¼ë³„ ë°ì´í„°
        daily = sentiment_df.set_index('date')
    else:
        # ì§‘ê³„ í•„ìš” (í…ìŠ¤íŠ¸ ë°ì´í„°)
        # ì‹œë®¬ë ˆì´ì…˜: ëœë¤ ë‚ ì§œ í• ë‹¹
        dates = pd.date_range(start='2020-01-01', end='2024-12-31', freq='B')
        np.random.seed(SEED)
        
        # ì¼ë³„ í‰ê·  ì‹¬ë¦¬ ìƒì„±
        n_days = len(dates)
        daily_sentiment = np.zeros(n_days)
        
        # VIX ê¸°ë°˜ ì‹¬ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        vix = yf.download('^VIX', start='2020-01-01', end='2025-01-01', 
                         progress=False, auto_adjust=True)
        if isinstance(vix.columns, pd.MultiIndex):
            vix.columns = vix.columns.get_level_values(0)
        
        # ì •ê·œí™”ëœ VIXë¥¼ ì‹¬ë¦¬ë¡œ ë³€í™˜
        vix_aligned = vix['Close'].reindex(dates, method='ffill').fillna(vix['Close'].mean())
        vix_norm = (vix_aligned - vix_aligned.mean()) / vix_aligned.std()
        daily_sentiment = -np.tanh(vix_norm / 2).values
        
        # ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = np.random.normal(0, 0.15, len(daily_sentiment))
        daily_sentiment = np.clip(daily_sentiment + noise, -1, 1)
        
        daily = pd.DataFrame({
            'sentiment_mean': daily_sentiment,
        }, index=dates)
    
    # ì‹¬ë¦¬ íŠ¹ì„± ì¶”ê°€
    if 'sentiment_mean' not in daily.columns and 'sentiment' in daily.columns:
        daily['sentiment_mean'] = daily['sentiment']
    
    # í•µì‹¬ íŠ¹ì„± ìƒì„±
    print("  â†’ ì‹¬ë¦¬ íŠ¹ì„± ìƒì„± ì¤‘...")
    
    # 1. ê¸°ë³¸ ì‹¬ë¦¬
    if 'sentiment_mean' not in daily.columns:
        daily['sentiment_mean'] = daily.iloc[:, 0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
    
    # 2. ë˜ê·¸ íŠ¹ì„±
    daily['sentiment_lag1'] = daily['sentiment_mean'].shift(1)
    daily['sentiment_lag5'] = daily['sentiment_mean'].shift(5)
    
    # 3. ë¡¤ë§ í†µê³„
    daily['sentiment_ma5'] = daily['sentiment_mean'].rolling(5).mean()
    daily['sentiment_ma20'] = daily['sentiment_mean'].rolling(20).mean()
    daily['sentiment_std5'] = daily['sentiment_mean'].rolling(5).std()
    
    # 4. ë³€í™”ìœ¨
    daily['sentiment_change'] = daily['sentiment_mean'].diff()
    daily['sentiment_momentum'] = daily['sentiment_mean'].rolling(5).sum()
    
    # 5. ê·¹ë‹¨ ê°ì„±
    daily['sentiment_extreme_pos'] = (daily['sentiment_mean'] > 0.5).astype(int)
    daily['sentiment_extreme_neg'] = (daily['sentiment_mean'] < -0.5).astype(int)
    
    # SPY ë‚ ì§œì™€ ì •ë ¬
    daily = daily.reindex(spy_index, method='ffill')
    
    print(f"  âœ“ ì‹¬ë¦¬ íŠ¹ì„± 10ê°œ ìƒì„±")
    print(f"  âœ“ ê¸°ê°„: {daily.index[0]} ~ {daily.index[-1]}")
    
    return daily


# =============================================================================
# 4. ë³€ë™ì„± ëª¨ë¸ì— í†µí•©
# =============================================================================

def integrate_sentiment_with_volatility():
    """ì‹¬ë¦¬ íŠ¹ì„±ì„ ë³€ë™ì„± ì˜ˆì¸¡ì— í†µí•©"""
    print("\n" + "="*60)
    print("[3/6] SPY ë°ì´í„° ë° ê¸°ì¡´ íŠ¹ì„± ë¡œë“œ")
    print("="*60)
    
    # SPY ë°ì´í„° ë¡œë“œ
    csv_path = Path('data/raw/spy_data_2020_2025.csv')
    if csv_path.exists():
        spy = pd.read_csv(csv_path, index_col=0, parse_dates=True)
    else:
        spy = yf.download('SPY', start='2020-01-01', end='2025-01-01',
                         progress=False, auto_adjust=True)
        if isinstance(spy.columns, pd.MultiIndex):
            spy.columns = spy.columns.get_level_values(0)
    
    print(f"  âœ“ SPY ë°ì´í„°: {len(spy)} í–‰")
    
    # VIX ë¡œë“œ
    vix = yf.download('^VIX', start='2020-01-01', end='2025-01-01',
                     progress=False, auto_adjust=True)
    if isinstance(vix.columns, pd.MultiIndex):
        vix.columns = vix.columns.get_level_values(0)
    spy['VIX'] = vix['Close']
    
    # ê¸°ë³¸ íŠ¹ì„± ìƒì„±
    print("\n" + "="*60)
    print("[4/6] ê¸°ì¡´ íŠ¹ì„± + ì‹¬ë¦¬ íŠ¹ì„± ìƒì„±")
    print("="*60)
    
    spy['returns'] = spy['Close'].pct_change()
    spy['volatility'] = spy['returns'].rolling(5).std() * np.sqrt(252)
    
    # ë³€ë™ì„± íŠ¹ì„±
    for window in [5, 10, 20, 50]:
        spy[f'volatility_{window}'] = spy['returns'].rolling(window).std()
        spy[f'realized_vol_{window}'] = spy[f'volatility_{window}'] * np.sqrt(252)
    
    # VIX íŠ¹ì„± (ê¸°ì¡´ ìµœê³  ì„±ëŠ¥)
    spy['vix_lag_1'] = spy['VIX'].shift(1)
    spy['vix_lag_5'] = spy['VIX'].shift(5)
    spy['vix_change'] = spy['VIX'].pct_change()
    spy['vix_zscore'] = (spy['VIX'] - spy['VIX'].rolling(20).mean()) / (spy['VIX'].rolling(20).std() + 1e-8)
    
    # Regime íŠ¹ì„± (ê¸°ì¡´ ìµœê³  ì„±ëŠ¥)
    vix_lagged = spy['VIX'].shift(1)
    spy['regime_high_vol'] = (vix_lagged >= 25).astype(int)
    spy['regime_crisis'] = (vix_lagged >= 35).astype(int)
    spy['vol_in_high_regime'] = spy['regime_high_vol'] * spy['volatility_5']
    spy['vol_in_crisis'] = spy['regime_crisis'] * spy['volatility_5']
    spy['vix_excess_25'] = np.maximum(vix_lagged - 25, 0)
    spy['vix_excess_35'] = np.maximum(vix_lagged - 35, 0)
    
    # ìˆ˜ìµë¥  í†µê³„
    for window in [5, 10, 20]:
        spy[f'mean_return_{window}'] = spy['returns'].rolling(window).mean()
        spy[f'skew_{window}'] = spy['returns'].rolling(window).skew()
        spy[f'kurt_{window}'] = spy['returns'].rolling(window).kurt()
    
    # ë˜ê·¸ ë³€ìˆ˜
    for lag in [1, 2, 3, 5]:
        spy[f'return_lag_{lag}'] = spy['returns'].shift(lag)
        spy[f'vol_lag_{lag}'] = spy['volatility_5'].shift(lag)
    
    # ëª¨ë©˜í…€
    for window in [5, 10, 20]:
        spy[f'momentum_{window}'] = spy['returns'].rolling(window).sum()
    
    # ë¹„ìœ¨ íŠ¹ì„±
    spy['vol_ratio_5_20'] = spy['volatility_5'] / (spy['volatility_20'] + 1e-8)
    spy['vol_ratio_10_50'] = spy['volatility_10'] / (spy['volatility_50'] + 1e-8)
    
    # Z-score
    ma_20 = spy['returns'].rolling(20).mean()
    std_20 = spy['returns'].rolling(20).std()
    spy['zscore_20'] = (spy['returns'] - ma_20) / (std_20 + 1e-8)
    
    print(f"  âœ“ ê¸°ì¡´ íŠ¹ì„±: {len([c for c in spy.columns if c.startswith(('volatility', 'vix', 'regime', 'vol_', 'mean_', 'skew', 'kurt', 'return_lag', 'momentum', 'zscore'))])}ê°œ")
    
    # ì‹¬ë¦¬ íŠ¹ì„± ì¶”ê°€
    print("  â†’ ì‹¬ë¦¬ íŠ¹ì„± ì¶”ê°€ ì¤‘...")
    sentiment_df = load_financial_sentiment_dataset()
    daily_sentiment = create_daily_sentiment_features(sentiment_df, spy.index)
    
    # ë³‘í•©
    for col in daily_sentiment.columns:
        spy[col] = daily_sentiment[col]
    
    print(f"  âœ“ ì‹¬ë¦¬ íŠ¹ì„±: {len(daily_sentiment.columns)}ê°œ ì¶”ê°€")
    
    # íƒ€ê²Ÿ ìƒì„± (5ì¼ ë¯¸ë˜ ë³€ë™ì„±)
    vol_values = []
    returns = spy['returns'].values
    for i in range(len(returns)):
        if i + 5 < len(returns):
            future_window = returns[i+1:i+6]
            vol_values.append(pd.Series(future_window).std())
        else:
            vol_values.append(np.nan)
    spy['target_vol_5d'] = vol_values
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    spy = spy.ffill().dropna()
    
    print(f"  âœ“ ìµœì¢… ë°ì´í„°: {len(spy)} í–‰, {len(spy.columns)} ì—´")
    
    return spy


# =============================================================================
# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# =============================================================================

def train_and_evaluate(spy):
    """ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
    print("\n" + "="*60)
    print("[5/6] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")
    print("="*60)
    
    # íŠ¹ì„± ì„ íƒ
    feature_cols = []
    for col in spy.columns:
        if col.startswith(('volatility_', 'realized_vol_', 'mean_return_',
                          'skew_', 'kurt_', 'return_lag_', 'vol_lag_',
                          'vol_ratio_', 'zscore_', 'momentum_', 'vix_', 'regime_',
                          'vol_in_', 'vix_excess_', 'sentiment_')):
            feature_cols.append(col)
    
    print(f"  âœ“ ì´ íŠ¹ì„±: {len(feature_cols)}ê°œ")
    print(f"    - ê¸°ì¡´ íŠ¹ì„±: {len([c for c in feature_cols if not c.startswith('sentiment')])}ê°œ")
    print(f"    - ì‹¬ë¦¬ íŠ¹ì„±: {len([c for c in feature_cols if c.startswith('sentiment')])}ê°œ")
    
    # ë°ì´í„° ë¶„í•  (80/20)
    X = spy[feature_cols].values
    y = spy['target_vol_5d'].values
    
    split_idx = int(len(spy) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print(f"\n  â†’ Train: {len(X_train)}, Test: {len(X_test)}")
    
    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ElasticNet ëª¨ë¸
    model = ElasticNet(alpha=0.0005, l1_ratio=0.5, random_state=SEED, max_iter=10000)
    model.fit(X_train_scaled, y_train)
    
    # í‰ê°€
    y_pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    print(f"\n  ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ (VIX + Regime + Sentiment):")
    print(f"    â€¢ Test RÂ²: {r2:.4f}")
    print(f"    â€¢ Test RMSE: {rmse:.6f}")
    
    # ë¹„êµ: ì‹¬ë¦¬ íŠ¹ì„± ì—†ì´
    print("\n  â†’ ë¹„êµ: ê¸°ì¡´ ëª¨ë¸ (ì‹¬ë¦¬ íŠ¹ì„± ì—†ì´)...")
    baseline_cols = [c for c in feature_cols if not c.startswith('sentiment')]
    X_baseline = spy[baseline_cols].values
    
    X_train_b, X_test_b = X_baseline[:split_idx], X_baseline[split_idx:]
    X_train_b_scaled = scaler.fit_transform(X_train_b)
    X_test_b_scaled = scaler.transform(X_test_b)
    
    model_b = ElasticNet(alpha=0.0005, l1_ratio=0.5, random_state=SEED, max_iter=10000)
    model_b.fit(X_train_b_scaled, y_train)
    
    y_pred_b = model_b.predict(X_test_b_scaled)
    r2_b = r2_score(y_test, y_pred_b)
    
    print(f"    â€¢ ê¸°ì¡´ ëª¨ë¸ RÂ²: {r2_b:.4f}")
    
    # ì„±ëŠ¥ ì°¨ì´
    diff = r2 - r2_b
    print(f"\n  ğŸ“ˆ ì‹¬ë¦¬ íŠ¹ì„± ì¶”ê°€ íš¨ê³¼: {diff:+.4f} ({diff/r2_b*100:+.1f}%)")
    
    # íŠ¹ì„± ì¤‘ìš”ë„
    print("\n  ğŸ” ìƒìœ„ 10 íŠ¹ì„± (ì ˆëŒ€ ê³„ìˆ˜):")
    coef_df = pd.DataFrame({
        'feature': feature_cols,
        'coefficient': np.abs(model.coef_)
    }).sort_values('coefficient', ascending=False)
    
    for i, row in coef_df.head(10).iterrows():
        marker = "ğŸ“°" if row['feature'].startswith('sentiment') else "  "
        print(f"    {marker} {row['feature']}: {row['coefficient']:.6f}")
    
    # ì‹¬ë¦¬ íŠ¹ì„± ì¤‘ìš”ë„
    sentiment_coefs = coef_df[coef_df['feature'].str.startswith('sentiment')]
    print(f"\n  ğŸ“° ì‹¬ë¦¬ íŠ¹ì„± ì¤‘ìš”ë„:")
    for _, row in sentiment_coefs.iterrows():
        print(f"    - {row['feature']}: {row['coefficient']:.6f}")
    
    results = {
        'model_with_sentiment': {
            'r2': float(r2),
            'rmse': float(rmse),
            'n_features': len(feature_cols)
        },
        'model_without_sentiment': {
            'r2': float(r2_b),
            'n_features': len(baseline_cols)
        },
        'sentiment_effect': float(diff),
        'sentiment_features': list(sentiment_coefs['feature'].values),
        'timestamp': datetime.now().isoformat()
    }
    
    return results


# =============================================================================
# 6. ë©”ì¸ íŒŒì´í”„ë¼ì¸
# =============================================================================

def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("\n" + "ğŸš€"*30)
    print("FinBERT ë‰´ìŠ¤ ì‹¬ë¦¬ ì§€ìˆ˜ í†µí•© íŒŒì´í”„ë¼ì¸")
    print("ğŸš€"*30)
    
    try:
        # 1-4. ë°ì´í„° ì¤€ë¹„ ë° íŠ¹ì„± ìƒì„±
        spy = integrate_sentiment_with_volatility()
        
        # 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        results = train_and_evaluate(spy)
        
        # 6. ê²°ê³¼ ì €ì¥
        print("\n" + "="*60)
        print("[6/6] ê²°ê³¼ ì €ì¥")
        print("="*60)
        
        output_path = Path('data/raw/sentiment_integration_results.json')
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"  âœ“ ê²°ê³¼ ì €ì¥: {output_path}")
        
        # ìµœì¢… ìš”ì•½
        print("\n" + "="*60)
        print("âœ… ì‹¬ë¦¬ ì§€ìˆ˜ í†µí•© ì™„ë£Œ!")
        print("="*60)
        
        print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
        print(f"  â€¢ ê¸°ì¡´ ëª¨ë¸ (VIX+Regime):          RÂ² = {results['model_without_sentiment']['r2']:.4f}")
        print(f"  â€¢ ì‹¬ë¦¬ ì¶”ê°€ ëª¨ë¸ (VIX+Regime+Sent): RÂ² = {results['model_with_sentiment']['r2']:.4f}")
        print(f"  â€¢ ì‹¬ë¦¬ íŠ¹ì„± íš¨ê³¼:                   {results['sentiment_effect']:+.4f}")
        
        if results['sentiment_effect'] > 0.005:
            print("\nâœ… ì‹¬ë¦¬ íŠ¹ì„±ì´ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬!")
        elif results['sentiment_effect'] > 0:
            print("\nâš ï¸ ì‹¬ë¦¬ íŠ¹ì„±ì˜ íš¨ê³¼ê°€ ë¯¸ë¯¸í•¨")
        else:
            print("\nâŒ ì‹¬ë¦¬ íŠ¹ì„± ì¶”ê°€ë¡œ ì„±ëŠ¥ ì €í•˜")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == '__main__':
    results = main()
