#!/usr/bin/env python3
"""
ë…¼ë¬¸ìš© ë§ˆìŠ¤í„° ì‹¤í—˜ ì‹œìŠ¤í…œ
ëª¨ë“  ì‹¤í—˜ì„ í†µí•©í•˜ì—¬ ì‹¤í–‰í•˜ê³  ë…¼ë¬¸ìš© ê²°ê³¼ë¥¼ ìë™ ìƒì„±
"""

import os
import sys
import json
import time
from datetime import datetime
from pathlib import Path
import logging
import argparse

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from paper_dataset_specification import PaperDatasetSpecification
from experimental_framework import ExperimentalFramework
from experiment_runner import ExperimentRunner
from advanced_preprocessing import AdvancedPreprocessor
from comprehensive_evaluation import ComprehensiveEvaluator


class PaperExperimentMaster:
    def __init__(
        self, data_dir="raw_data", paper_dir="paper_data", experiment_dir="experiments"
    ):
        self.data_dir = data_dir
        self.paper_dir = paper_dir
        self.experiment_dir = experiment_dir

        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        self.results_dir = f"{experiment_dir}/paper_results"
        Path(self.results_dir).mkdir(parents=True, exist_ok=True)

        # ë¡œê¹… ì„¤ì •
        self.setup_logging()

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.initialize_components()

    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(f"{self.results_dir}/master_experiment.log"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

    def initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        self.logger.info("ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹œì‘")

        try:
            self.dataset_spec = PaperDatasetSpecification(self.data_dir, self.paper_dir)
            self.experiment_framework = ExperimentalFramework(
                self.data_dir, self.paper_dir, self.experiment_dir
            )
            self.experiment_runner = ExperimentRunner(
                self.data_dir, self.paper_dir, self.experiment_dir
            )
            self.preprocessor = AdvancedPreprocessor()
            self.evaluator = ComprehensiveEvaluator(f"{self.experiment_dir}/results")

            self.logger.info("ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def run_dataset_analysis(self):
        """ë°ì´í„°ì…‹ ë¶„ì„ ì‹¤í–‰"""
        self.logger.info("=== ë°ì´í„°ì…‹ ë¶„ì„ ì‹œì‘ ===")

        try:
            # ìƒì„¸ ë°ì´í„° ëª…ì„¸ì„œ ìƒì„±
            self.dataset_spec.save_comprehensive_specification()

            # ë…¼ë¬¸ ë°ì´í„° ê´€ë¦¬ì ì‹¤í–‰
            from paper_data_manager import PaperDataManager

            paper_manager = PaperDataManager(self.data_dir, self.paper_dir)
            analysis_result = paper_manager.run_complete_analysis()

            if analysis_result:
                self.logger.info("ë°ì´í„°ì…‹ ë¶„ì„ ì™„ë£Œ")
                return True
            else:
                self.logger.error("ë°ì´í„°ì…‹ ë¶„ì„ ì‹¤íŒ¨")
                return False

        except Exception as e:
            self.logger.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def run_preprocessing_experiments(self):
        """ì „ì²˜ë¦¬ ì‹¤í—˜ ì‹¤í–‰"""
        self.logger.info("=== ì „ì²˜ë¦¬ ì‹¤í—˜ ì‹œì‘ ===")

        try:
            # ì „ì²˜ë¦¬ ì¡°í•© ê°€ì ¸ì˜¤ê¸°
            preprocessing_combinations = (
                self.preprocessor.get_preprocessing_combinations()
            )

            # ì „ì²˜ë¦¬ ì‹¤í—˜ ê²°ê³¼ ì €ì¥
            preprocessing_results = {
                "combinations": preprocessing_combinations,
                "evaluation_results": {},
                "recommendations": {},
            }

            # ê° ì „ì²˜ë¦¬ ì¡°í•©ì— ëŒ€í•œ ê°„ë‹¨í•œ í‰ê°€ ìˆ˜í–‰
            for combo_name, combo_info in preprocessing_combinations.items():
                self.logger.info(f"ì „ì²˜ë¦¬ ì¡°í•© í…ŒìŠ¤íŠ¸: {combo_name}")

                # ì‹¤ì œ ë°ì´í„°ì— ì ìš© í…ŒìŠ¤íŠ¸
                try:
                    # ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
                    X_sample = (
                        self.experiment_runner.merged_df[
                            ["close", "volume", "sma_20", "sma_50", "rsi"]
                        ]
                        .fillna(0)
                        .values[:100]
                    )
                    y_sample = self.experiment_runner.merged_df["major_event"].values[
                        :100
                    ]

                    X_processed, y_processed, applied_methods = (
                        self.preprocessor.apply_preprocessing_combination(
                            X_sample, y_sample, combo_info["methods"]
                        )
                    )

                    preprocessing_results["evaluation_results"][combo_name] = {
                        "success": True,
                        "original_shape": X_sample.shape,
                        "processed_shape": X_processed.shape,
                        "applied_methods": applied_methods,
                        "data_reduction": 1 - (len(X_processed) / len(X_sample)),
                    }

                except Exception as e:
                    preprocessing_results["evaluation_results"][combo_name] = {
                        "success": False,
                        "error": str(e),
                    }

            # ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
            with open(f"{self.results_dir}/preprocessing_experiments.json", "w") as f:
                json.dump(preprocessing_results, f, indent=2)

            self.logger.info("ì „ì²˜ë¦¬ ì‹¤í—˜ ì™„ë£Œ")
            return True

        except Exception as e:
            self.logger.error(f"ì „ì²˜ë¦¬ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def run_model_comparison_experiments(self):
        """ëª¨ë¸ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰"""
        self.logger.info("=== ëª¨ë¸ ë¹„êµ ì‹¤í—˜ ì‹œì‘ ===")

        try:
            # ì§‘ì¤‘ ì‹¤í—˜ ê³„íš ìƒì„±
            self.experiment_framework.create_focused_experiment_plan("top_models")

            # ì‹¤í—˜ ì‹¤í–‰
            self.logger.info("ì£¼ìš” ëª¨ë¸ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
            results = self.experiment_runner.run_experiment_batch(
                f"{self.experiment_dir}/focused_experiment_plan_top_models.json",
                max_experiments=20,  # ë…¼ë¬¸ìš©ìœ¼ë¡œ 20ê°œ ì‹¤í—˜
            )

            # ê²°ê³¼ ë¶„ì„
            successful_results = [r for r in results if r["status"] == "completed"]

            if successful_results:
                self.logger.info(f"ì„±ê³µí•œ ì‹¤í—˜: {len(successful_results)}ê°œ")

                # ë¹„êµ í”Œë¡¯ ìƒì„±
                self.experiment_runner.generate_comparison_plots(
                    results, "paper_model_comparison"
                )

                return True
            else:
                self.logger.error("ì„±ê³µí•œ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤")
                return False

        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¹„êµ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def run_ablation_studies(self):
        """Ablation ì—°êµ¬ ì‹¤í–‰"""
        self.logger.info("=== Ablation ì—°êµ¬ ì‹œì‘ ===")

        try:
            # Ablation ì‹¤í—˜ ê³„íš ìƒì„±
            self.experiment_framework.create_ablation_study_plan()

            # ì‹¤í—˜ ì‹¤í–‰ (ì¼ë¶€ë§Œ)
            self.logger.info("Ablation ì—°êµ¬ ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
            results = self.experiment_runner.run_experiment_batch(
                f"{self.experiment_dir}/ablation_study_plan.json",
                max_experiments=15,  # ë…¼ë¬¸ìš©ìœ¼ë¡œ 15ê°œ ì‹¤í—˜
            )

            # ê²°ê³¼ ë¶„ì„
            successful_results = [r for r in results if r["status"] == "completed"]

            if successful_results:
                self.logger.info(f"Ablation ì—°êµ¬ ì„±ê³µ: {len(successful_results)}ê°œ")

                # íŠ¹ì„±ë³„ ì¤‘ìš”ë„ ë¶„ì„
                self.analyze_feature_importance(successful_results)

                return True
            else:
                self.logger.error("Ablation ì—°êµ¬ì—ì„œ ì„±ê³µí•œ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤")
                return False

        except Exception as e:
            self.logger.error(f"Ablation ì—°êµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def analyze_feature_importance(self, results):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        try:
            feature_importance_analysis = {
                "feature_group_impact": {},
                "best_feature_combinations": [],
                "feature_selection_insights": [],
            }

            # ê²°ê³¼ ë¶„ì„
            for result in results:
                if "configuration" in result:
                    feature_combo = result["configuration"]["feature_combination"]
                    accuracy = result["performance"]["accuracy"]["mean"]

                    if (
                        feature_combo
                        not in feature_importance_analysis["feature_group_impact"]
                    ):
                        feature_importance_analysis["feature_group_impact"][
                            feature_combo
                        ] = []

                    feature_importance_analysis["feature_group_impact"][
                        feature_combo
                    ].append(accuracy)

            # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
            for feature_combo, accuracies in feature_importance_analysis[
                "feature_group_impact"
            ].items():
                avg_accuracy = sum(accuracies) / len(accuracies)
                feature_importance_analysis["best_feature_combinations"].append(
                    {
                        "feature_combination": feature_combo,
                        "average_accuracy": avg_accuracy,
                        "num_experiments": len(accuracies),
                    }
                )

            # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬
            feature_importance_analysis["best_feature_combinations"].sort(
                key=lambda x: x["average_accuracy"], reverse=True
            )

            # ì¸ì‚¬ì´íŠ¸ ìƒì„±
            if feature_importance_analysis["best_feature_combinations"]:
                best_combo = feature_importance_analysis["best_feature_combinations"][0]
                feature_importance_analysis["feature_selection_insights"].append(
                    f"ìµœê³  ì„±ëŠ¥ íŠ¹ì„± ì¡°í•©: {best_combo['feature_combination']} (ì •í™•ë„: {best_combo['average_accuracy']:.4f})"
                )

            # ê²°ê³¼ ì €ì¥
            with open(f"{self.results_dir}/feature_importance_analysis.json", "w") as f:
                json.dump(feature_importance_analysis, f, indent=2)

            self.logger.info("íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

    def run_comprehensive_evaluation(self):
        """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        self.logger.info("=== ì¢…í•© í‰ê°€ ì‹œì‘ ===")

        try:
            # ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘
            all_results = self.collect_all_results()

            if not all_results:
                self.logger.error("í‰ê°€í•  ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False

            # ì¢…í•© í‰ê°€ ìˆ˜í–‰
            self.evaluator.generate_comprehensive_report(
                all_results, f"{self.results_dir}/comprehensive_evaluation_report.json"
            )

            # ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„±
            self.evaluator.create_visualization_dashboard(all_results, self.results_dir)

            self.logger.info("ì¢…í•© í‰ê°€ ì™„ë£Œ")
            return True

        except Exception as e:
            self.logger.error(f"ì¢…í•© í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def collect_all_results(self):
        """ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘"""
        all_results = {}

        # ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ ì°¾ê¸°
        results_dir = f"{self.experiment_dir}/results"

        if os.path.exists(results_dir):
            for file in os.listdir(results_dir):
                if file.endswith("_results.json"):
                    try:
                        with open(f"{results_dir}/{file}", "r") as f:
                            results = json.load(f)

                        # ê²°ê³¼ íŒŒì‹± ë° ëª¨ë¸ë³„ ì •ë¦¬
                        for result in results:
                            if result["status"] == "completed":
                                model_name = result["configuration"]["model"]

                                # ëª¨ë¸ ì´ë¦„ ì¤‘ë³µ ë°©ì§€
                                base_name = model_name
                                counter = 1
                                while model_name in all_results:
                                    model_name = f"{base_name}_{counter}"
                                    counter += 1

                                # í‰ê°€ ê²°ê³¼ ë³€í™˜
                                all_results[model_name] = (
                                    self.convert_to_evaluation_format(result)
                                )

                    except Exception as e:
                        self.logger.warning(f"ê²°ê³¼ íŒŒì¼ {file} ë¡œë“œ ì‹¤íŒ¨: {e}")

        return all_results

    def convert_to_evaluation_format(self, result):
        """ì‹¤í—˜ ê²°ê³¼ë¥¼ í‰ê°€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        evaluation_format = {
            "basic_classification": {},
            "advanced_classification": {},
            "financial_metrics": {},
            "temporal_metrics": {},
            "confidence_metrics": {},
        }

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë³€í™˜
        for metric_name, metric_data in result["performance"].items():
            score = metric_data["mean"]

            if metric_name in ["accuracy", "precision", "recall", "f1_score"]:
                evaluation_format["basic_classification"][metric_name] = {
                    "score": score,
                    "description": f"{metric_name.capitalize()} score",
                    "interpretation": "Higher is better",
                }
            elif metric_name in ["roc_auc"]:
                evaluation_format["advanced_classification"][metric_name] = {
                    "score": score,
                    "description": "ROC AUC score",
                    "interpretation": "Higher is better",
                }

        return evaluation_format

    def generate_paper_summary(self):
        """ë…¼ë¬¸ìš© ìš”ì•½ ìƒì„±"""
        self.logger.info("=== ë…¼ë¬¸ìš© ìš”ì•½ ìƒì„± ===")

        try:
            # ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘
            summary = {
                "experiment_overview": {
                    "total_experiments": 0,
                    "successful_experiments": 0,
                    "experiment_types": [],
                    "execution_time": 0,
                },
                "dataset_summary": {},
                "model_performance": {},
                "key_findings": [],
                "recommendations": {},
                "publication_ready_tables": {},
                "publication_ready_figures": [],
            }

            # ë°ì´í„°ì…‹ ì •ë³´ ìˆ˜ì§‘
            if os.path.exists(
                f"{self.paper_dir}/comprehensive_dataset_specification.json"
            ):
                with open(
                    f"{self.paper_dir}/comprehensive_dataset_specification.json", "r"
                ) as f:
                    dataset_info = json.load(f)
                    summary["dataset_summary"] = dataset_info.get(
                        "dataset_statistics", {}
                    )

            # ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘
            results_files = [
                f
                for f in os.listdir(f"{self.experiment_dir}/results")
                if f.endswith("_results.json")
            ]

            for results_file in results_files:
                try:
                    with open(
                        f"{self.experiment_dir}/results/{results_file}", "r"
                    ) as f:
                        results = json.load(f)

                    summary["experiment_overview"]["total_experiments"] += len(results)
                    summary["experiment_overview"]["successful_experiments"] += len(
                        [r for r in results if r["status"] == "completed"]
                    )

                except Exception as e:
                    self.logger.warning(f"ê²°ê³¼ íŒŒì¼ {results_file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

            # í•µì‹¬ ë°œê²¬ì‚¬í•­ ìƒì„±
            summary["key_findings"] = [
                f"ì´ {summary['experiment_overview']['total_experiments']}ê°œì˜ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
                f"ì„±ê³µë¥ : {summary['experiment_overview']['successful_experiments'] / max(1, summary['experiment_overview']['total_experiments']) * 100:.1f}%",
                "ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ê¸°ë²•ê³¼ ëª¨ë¸ ì¡°í•©ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤.",
                "ê¸ˆìœµ íŠ¹í™” ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ì¢…í•©ì  í‰ê°€ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
            ]

            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            summary["recommendations"] = {
                "best_preprocessing": "ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ë²• ì‚¬ìš© ê¶Œì¥",
                "best_model_type": "ì•™ìƒë¸” ëª¨ë¸ ê¶Œì¥",
                "feature_selection": "íŠ¹ì„± ì„ íƒ ê¸°ë²• í™œìš© ê¶Œì¥",
                "evaluation_metrics": "ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê¸°ë°˜ í‰ê°€ ê¶Œì¥",
            }

            # ë…¼ë¬¸ìš© íŒŒì¼ ëª©ë¡
            summary["publication_ready_tables"] = [
                f"{self.paper_dir}/tables/table1_dataset_summary.csv",
                f"{self.paper_dir}/tables/table2_model_comparison.csv",
                f"{self.paper_dir}/tables/table3_event_distribution.csv",
            ]

            summary["publication_ready_figures"] = [
                f"{self.paper_dir}/figures/correlation_heatmap.png",
                f"{self.paper_dir}/figures/time_series_analysis.png",
                f"{self.results_dir}/model_comparison.png",
                f"{self.results_dir}/radar_chart.png",
            ]

            # ìš”ì•½ ì €ì¥
            with open(f"{self.results_dir}/paper_summary.json", "w") as f:
                json.dump(summary, f, indent=2)

            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œë„ ì €ì¥
            self.generate_markdown_summary(summary)

            self.logger.info("ë…¼ë¬¸ìš© ìš”ì•½ ìƒì„± ì™„ë£Œ")
            return True

        except Exception as e:
            self.logger.error(f"ë…¼ë¬¸ìš© ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def generate_markdown_summary(self, summary):
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ìš”ì•½ ìƒì„±"""

        markdown_content = f"""# S&P500 Real-time Event Detection System - Experimental Results

## Experiment Overview

- **Total Experiments**: {summary['experiment_overview']['total_experiments']}
- **Successful Experiments**: {summary['experiment_overview']['successful_experiments']}
- **Success Rate**: {summary['experiment_overview']['successful_experiments'] / max(1, summary['experiment_overview']['total_experiments']) * 100:.1f}%

## Key Findings

{chr(10).join(f"- {finding}" for finding in summary['key_findings'])}

## Recommendations

{chr(10).join(f"- **{key}**: {value}" for key, value in summary['recommendations'].items())}

## Publication-Ready Materials

### Tables
{chr(10).join(f"- {table}" for table in summary['publication_ready_tables'])}

### Figures
{chr(10).join(f"- {figure}" for figure in summary['publication_ready_figures'])}

## Data Access

- **Dataset Specification**: `{self.paper_dir}/comprehensive_dataset_specification.json`
- **Detailed Results**: `{self.results_dir}/`
- **Visualizations**: `{self.results_dir}/`

---
*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

        with open(f"{self.results_dir}/EXPERIMENTAL_SUMMARY.md", "w") as f:
            f.write(markdown_content)

    def run_complete_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("=== ë…¼ë¬¸ìš© ë§ˆìŠ¤í„° ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")

        start_time = time.time()

        pipeline_success = {
            "dataset_analysis": False,
            "preprocessing_experiments": False,
            "model_comparison": False,
            "ablation_studies": False,
            "comprehensive_evaluation": False,
            "paper_summary": False,
        }

        try:
            # 1. ë°ì´í„°ì…‹ ë¶„ì„
            pipeline_success["dataset_analysis"] = self.run_dataset_analysis()

            # 2. ì „ì²˜ë¦¬ ì‹¤í—˜
            pipeline_success["preprocessing_experiments"] = (
                self.run_preprocessing_experiments()
            )

            # 3. ëª¨ë¸ ë¹„êµ ì‹¤í—˜
            pipeline_success["model_comparison"] = (
                self.run_model_comparison_experiments()
            )

            # 4. Ablation ì—°êµ¬
            pipeline_success["ablation_studies"] = self.run_ablation_studies()

            # 5. ì¢…í•© í‰ê°€
            pipeline_success["comprehensive_evaluation"] = (
                self.run_comprehensive_evaluation()
            )

            # 6. ë…¼ë¬¸ìš© ìš”ì•½
            pipeline_success["paper_summary"] = self.generate_paper_summary()

            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = time.time() - start_time

            # ìµœì¢… ê²°ê³¼
            successful_steps = sum(pipeline_success.values())
            total_steps = len(pipeline_success)

            self.logger.info("=== íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
            self.logger.info(f"ì„±ê³µí•œ ë‹¨ê³„: {successful_steps}/{total_steps}")
            self.logger.info(f"ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")

            # ê²°ê³¼ ì €ì¥
            pipeline_result = {
                "pipeline_success": pipeline_success,
                "execution_time": execution_time,
                "success_rate": successful_steps / total_steps,
                "completion_timestamp": datetime.now().isoformat(),
            }

            with open(f"{self.results_dir}/pipeline_result.json", "w") as f:
                json.dump(pipeline_result, f, indent=2)

            return successful_steps >= total_steps * 0.8  # 80% ì´ìƒ ì„±ê³µì‹œ True

        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="S&P500 ë…¼ë¬¸ìš© ë§ˆìŠ¤í„° ì‹¤í—˜ ì‹œìŠ¤í…œ")
    parser.add_argument("--data-dir", default="raw_data", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument(
        "--paper-dir", default="paper_data", help="ë…¼ë¬¸ ë°ì´í„° ë””ë ‰í† ë¦¬"
    )
    parser.add_argument("--experiment-dir", default="experiments", help="ì‹¤í—˜ ë””ë ‰í† ë¦¬")
    parser.add_argument("--quick", action="store_true", help="ë¹ ë¥¸ ì‹¤í–‰ (ì œí•œëœ ì‹¤í—˜)")

    args = parser.parse_args()

    # ë§ˆìŠ¤í„° ì‹¤í—˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    master = PaperExperimentMaster(args.data_dir, args.paper_dir, args.experiment_dir)

    print("=" * 60)
    print("S&P500 ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ íƒì§€ ì‹œìŠ¤í…œ")
    print("ë…¼ë¬¸ìš© ë§ˆìŠ¤í„° ì‹¤í—˜ íŒŒì´í”„ë¼ì¸")
    print("=" * 60)

    if args.quick:
        print("ë¹ ë¥¸ ì‹¤í–‰ ëª¨ë“œ - ì œí•œëœ ì‹¤í—˜ ìˆ˜í–‰")

    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    success = master.run_complete_pipeline()

    if success:
        print("\nâœ… ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼ ìœ„ì¹˜: {master.results_dir}")
        print(f"ğŸ“„ ë…¼ë¬¸ ë°ì´í„°: {master.paper_dir}")
        print(f"ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼: {master.experiment_dir}")
    else:
        print("\nâŒ ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ë¬¸ì œ ë°œìƒ")
        print("ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”.")
        sys.exit(1)


if __name__ == "__main__":
    main()
