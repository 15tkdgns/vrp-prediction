import pandas as pd
import numpy as np
import json
import os
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
import joblib
import matplotlib.pyplot as plt
import logging
from ..utils.directory_manager import DirectoryManager
from ..core.config_manager import get_config_manager


class SP500EventDetectionModel:
    """
    S&P500 ì£¼ì‹ ë°ì´í„° ê¸°ë°˜ ì´ë²¤íŠ¸ íƒì§€ ëª¨ë¸ì„ í›ˆë ¨, í‰ê°€, ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤.

    ì´ í´ë˜ìŠ¤ëŠ” ë°ì´í„° ë¡œë”©, íŠ¹ì„± ì „ì²˜ë¦¬, ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨,
    ì„±ëŠ¥ í‰ê°€, íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”, ëª¨ë¸ ì €ì¥ ë“±ì˜ ê¸°ëŠ¥ì„ í¬í•¨í•˜ëŠ”
    ì „ì²´ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(self, data_dir="data", models_dir="data/models"):
        """
        SP500EventDetectionModel ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            data_dir (str): ì›ë³¸ ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
            models_dir (str): í›ˆë ¨ëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        """
        # ë””ë ‰í† ë¦¬ ê´€ë¦¬ì ë° ì„¤ì • ê´€ë¦¬ì ì´ˆê¸°í™”
        self.directory_manager = DirectoryManager()
        self.config_manager = get_config_manager()
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ ìë™ ìƒì„±
        additional_dirs = [
            'results/analysis',
            'results/training', 
            'results/visualizations',
            'logs/models'
        ]
        self.directory_manager.ensure_directories(additional_dirs)
        
        self.data_dir = data_dir
        self.models_dir = models_dir
        self.scaler = StandardScaler()
        self.models = {}
        
        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger(__name__)

        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (ê¸°ì¡´ ë°©ì‹ ìœ ì§€í•˜ë˜ ë¡œê¹… ì¶”ê°€)
        if not os.path.exists(self.models_dir):
            os.makedirs(self.models_dir)
            self.logger.info(f"ğŸ“ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±: {self.models_dir}")
        else:
            self.logger.info(f"ğŸ“ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ í™•ì¸: {self.models_dir}")

    def load_training_data(self):
        """
        í›ˆë ¨ì— í•„ìš”í•œ íŠ¹ì„±(features)ê³¼ ë¼ë²¨(labels) ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë³‘í•©í•©ë‹ˆë‹¤.

        Returns:
            pd.DataFrame: íŠ¹ì„±ê³¼ ë¼ë²¨ì´ ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„.
        """
        # í•™ìŠµ íŠ¹ì„± ë° ì´ë²¤íŠ¸ ë¼ë²¨ ë¡œë“œ
        features_df = pd.read_csv(f"{self.data_dir}/raw/training_features.csv")
        labels_df = pd.read_csv(f"{self.data_dir}/raw/event_labels.csv")

        # LLM ê°•í™” íŠ¹ì§• ë¡œë“œ
        try:
            llm_features_df = pd.read_csv(
                f"{self.data_dir}/processed/llm_enhanced_features.csv"
            )
            llm_features_df = llm_features_df.dropna(
                subset=["date"]
            )  # date ì»¬ëŸ¼ì— NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±°
            llm_features_df["date"] = pd.to_datetime(llm_features_df["date"])
        except FileNotFoundError:
            print(
                "LLM enhanced features file not found. Proceeding without LLM features."
            )
            llm_features_df = pd.DataFrame()

        # ë‚ ì§œ í˜•ì‹ í†µì¼
        features_df["Date"] = pd.to_datetime(features_df["Date"])
        labels_df["Date"] = pd.to_datetime(labels_df["Date"])

        # 'ticker'ì™€ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
        merged_df = pd.merge(
            features_df,
            labels_df,
            left_on=["ticker", "Date"],
            right_on=["ticker", "Date"],
            how="inner",
        )

        # LLM íŠ¹ì§• ë³‘í•© (ë‰´ìŠ¤ ì œëª©ê³¼ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ)
        if not llm_features_df.empty:
            merged_df = pd.merge(
                merged_df,
                llm_features_df,
                left_on=["date", "title"],  # news_data.csvì˜ titleê³¼ ë§¤í•‘
                right_on=["date", "title"],
                how="left",
                suffixes=("", "_llm"),
            )
            # ë³‘í•© í›„ ì¤‘ë³µ ì»¬ëŸ¼ ì œê±° (ì˜ˆ: title_llm, date_llm ë“±)
            merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()].copy()

        return merged_df

    def prepare_features(self, df):
        """
        ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•´ íŠ¹ì„±ì„ ì„ íƒ, ì •ì œí•˜ê³  ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤.

        Args:
            df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„.

        Returns:
            tuple: ìŠ¤ì¼€ì¼ë§ëœ íŠ¹ì„±(X_scaled)ê³¼ íŠ¹ì„± ì´ë¦„ ëª©ë¡(numeric_features).
        """
        # ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ì¹˜í˜• íŠ¹ì„± ëª©ë¡
        numeric_features = [
            "Open",
            "High",
            "Low",
            "Close",
            "Volume",
            "sma_20",
            "sma_50",
            "rsi",
            "macd",
            "bb_upper",
            "bb_lower",
            "atr",
            "volatility",
            "obv",
            "price_change",
            "volume_change",
            "unusual_volume",
            "price_spike",
            "news_sentiment",
            "news_polarity",
            "news_count",
            "llm_sentiment_score",
            "uncertainty_score",  # LLM ìˆ˜ì¹˜í˜• íŠ¹ì§• ì¶”ê°€
        ]

        # ë²”ì£¼í˜• LLM íŠ¹ì§• ì›-í•« ì¸ì½”ë”©
        categorical_llm_features = ["market_sentiment", "event_category"]
        for col in categorical_llm_features:
            if col in df.columns:
                df[col] = df[col].fillna("Unknown")  # ê²°ì¸¡ê°’ ì²˜ë¦¬
                df = pd.get_dummies(df, columns=[col], prefix=col)

        # ì›-í•« ì¸ì½”ë”© í›„ ì¶”ê°€ëœ ì»¬ëŸ¼ë“¤ì„ numeric_featuresì— í¬í•¨
        # ê¸°ì¡´ numeric_featuresì— ì—†ëŠ” ìƒˆë¡œìš´ ë”ë¯¸ ë³€ìˆ˜ ì»¬ëŸ¼ë“¤ì„ ì°¾ì•„ì„œ ì¶”ê°€
        current_features = set(df.columns)
        new_dummy_features = [
            col
            for col in current_features
            if col.startswith("market_sentiment_") or col.startswith("event_category_")
        ]
        numeric_features.extend(new_dummy_features)

        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë“¤ë§Œ í•„í„°ë§
        available_features = [col for col in numeric_features if col in df.columns]
        print(f"Available features: {available_features}")

        # íŠ¹ì„± ë°ì´í„° ì„ íƒ ë° ê²°ì¸¡ê°’ ì²˜ë¦¬ (0ìœ¼ë¡œ ì±„ì›€)
        X = df[available_features].fillna(0)

        # StandardScalerë¥¼ ì´ìš©í•œ íŠ¹ì„± ì •ê·œí™”
        X_scaled = self.scaler.fit_transform(X)

        return X_scaled, available_features

    def train_random_forest(self, X_train, y_train, X_test, y_test):
        """
        Random Forest ë¶„ë¥˜ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

        Args:
            X_train, y_train: í›ˆë ¨ ë°ì´í„° (íŠ¹ì„± ë° ë¼ë²¨).
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„° (íŠ¹ì„± ë° ë¼ë²¨).
        """
        rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight="balanced",  # ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°€ì¤‘ì¹˜ ì¡°ì •
        )
        rf_model.fit(X_train, y_train)

        # í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì •í™•ë„ í‰ê°€
        train_score = rf_model.score(X_train, y_train)
        test_score = rf_model.score(X_test, y_test)
        print(f"Random Forest - Train: {train_score:.4f}, Test: {test_score:.4f}")

        # ëª¨ë¸ ë° ê´€ë ¨ ì •ë³´ ì €ì¥
        self.models["random_forest"] = {
            "model": rf_model,
            "train_score": train_score,
            "test_score": test_score,
            "feature_importance": rf_model.feature_importances_,
        }

    def train_gradient_boosting(self, X_train, y_train, X_test, y_test):
        """
        Gradient Boosting ë¶„ë¥˜ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
        """
        gb_model = GradientBoostingClassifier(
            n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42
        )
        gb_model.fit(X_train, y_train)

        train_score = gb_model.score(X_train, y_train)
        test_score = gb_model.score(X_test, y_test)
        print(f"Gradient Boosting - Train: {train_score:.4f}, Test: {test_score:.4f}")

        self.models["gradient_boosting"] = {
            "model": gb_model,
            "train_score": train_score,
            "test_score": test_score,
        }

    def train_xgboost(self, X_train, y_train, X_test, y_test):
        """
        XGBoost ë¶„ë¥˜ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            X_train, y_train: í›ˆë ¨ ë°ì´í„° (íŠ¹ì„± ë° ë¼ë²¨).
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„° (íŠ¹ì„± ë° ë¼ë²¨).
        """
        # XGBoost ë¶„ë¥˜ê¸° ì„¤ì •
        xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            random_state=42,
            eval_metric='logloss',  # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ í‰ê°€ ë©”íŠ¸ë¦­
            use_label_encoder=False  # ìµœì‹  ë²„ì „ì—ì„œ ê¶Œì¥
        )
        
        # ëª¨ë¸ í›ˆë ¨
        xgb_model.fit(X_train, y_train)
        
        # í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì •í™•ë„ í‰ê°€
        train_score = xgb_model.score(X_train, y_train)
        test_score = xgb_model.score(X_test, y_test)
        print(f"XGBoost - Train: {train_score:.4f}, Test: {test_score:.4f}")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ (XGBoostëŠ” ë‚´ì¥ íŠ¹ì„± ì¤‘ìš”ë„ ì œê³µ)
        feature_importance = xgb_model.feature_importances_
        
        # ëª¨ë¸ ë° í‰ê°€ ê²°ê³¼ ì €ì¥
        self.models["xgboost"] = {
            "model": xgb_model,
            "train_score": train_score,
            "test_score": test_score,
            "feature_importance": feature_importance,
        }

    def train_lstm(self, X_train, y_train, X_test, y_test):
        """
        LSTM(Long Short-Term Memory) ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
        ì‹œê³„ì—´ íŠ¹ì„±ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.
        """
        # LSTM ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°ì´í„° í˜•íƒœ ë³€í™˜: (samples, timesteps, features)
        X_train_lstm = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
        X_test_lstm = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

        # Keras Sequential APIë¥¼ ì‚¬ìš©í•œ LSTM ëª¨ë¸ êµ¬ì„±
        lstm_model = Sequential(
            [
                LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[1])),
                Dropout(0.2),  # ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ Dropout
                LSTM(50, return_sequences=False),
                Dropout(0.2),
                Dense(25, activation="relu"),
                Dense(1, activation="sigmoid"),  # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ Sigmoid í™œì„±í™” í•¨ìˆ˜
            ]
        )

        # ëª¨ë¸ ì»´íŒŒì¼
        lstm_model.compile(
            optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"]
        )

        # ëª¨ë¸ í›ˆë ¨
        history = lstm_model.fit(
            X_train_lstm,
            y_train,
            epochs=50,
            batch_size=32,
            validation_data=(X_test_lstm, y_test),
            verbose=0,  # í›ˆë ¨ ê³¼ì • ì¶œë ¥ ìƒëµ
        )

        # ì„±ëŠ¥ í‰ê°€
        _, train_score = lstm_model.evaluate(X_train_lstm, y_train, verbose=0)
        _, test_score = lstm_model.evaluate(X_test_lstm, y_test, verbose=0)
        print(f"LSTM - Train: {train_score:.4f}, Test: {test_score:.4f}")

        self.models["lstm"] = {
            "model": lstm_model,
            "train_score": train_score,
            "test_score": test_score,
            "history": history.history,
        }

    def plot_feature_importance(self, feature_names):
        """
        Random Forest ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ì‹œê°í™”í•˜ê³  ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        if "random_forest" in self.models:
            importance = self.models["random_forest"]["feature_importance"]

            # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            indices = np.argsort(importance)[::-1]

            plt.figure(figsize=(12, 8))
            plt.title("Feature Importance (Random Forest)")
            plt.bar(range(len(importance)), importance[indices])
            plt.xticks(
                range(len(importance)),
                [feature_names[i] for i in indices],
                rotation=45,
                ha="right",
            )
            plt.tight_layout()
            # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs("results/analysis", exist_ok=True)
            plt.savefig("results/analysis/feature_importance.png")
            plt.close()
            print(
                "íŠ¹ì„± ì¤‘ìš”ë„ ê·¸ë˜í”„ê°€ 'results/analysis/feature_importance.png'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
            )

    def save_models(self):
        """
        í›ˆë ¨ëœ ëª¨ë“  ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ì— ì‚¬ìš©ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        - Scikit-learn ëª¨ë¸: .pkl (joblib)
        - Keras ëª¨ë¸: .h5
        - ëª¨ë¸ ì„±ëŠ¥: .json
        """
        for name, model_info in self.models.items():
            if name == "lstm":
                model_info["model"].save(f"{self.models_dir}/{name}_model.h5")
            else:
                joblib.dump(model_info["model"], f"{self.models_dir}/{name}_model.pkl")

        joblib.dump(self.scaler, f"{self.models_dir}/scaler.pkl")
        print(f"ëª¨ë“  ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ê°€ '{self.models_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ëª¨ë¸ë³„ ì„±ëŠ¥ ì§€í‘œë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        performance = {
            name: {
                "train_accuracy": info["train_score"],
                "test_accuracy": info["test_score"],
            }
            for name, info in self.models.items()
        }
        with open(f"{self.data_dir}/raw/model_performance.json", "w") as f:
            json.dump(performance, f, indent=4)

    def run_training_pipeline(self):
        """
        ì „ì²´ ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        print("--- ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ---")

        print("\n[1/6] í›ˆë ¨ ë°ì´í„° ë¡œë“œ...")
        df = self.load_training_data()

        print("\n[2/6] íŠ¹ì„± ë°ì´í„° ì „ì²˜ë¦¬...")
        X, feature_names = self.prepare_features(df)
        y = df["major_event"].values  # íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •

        print("\n[3/6] í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• ...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y  # ë¼ë²¨ ë¹„ìœ¨ ìœ ì§€
        )

        print("\n[4/6] ëª¨ë¸ í›ˆë ¨...")
        self.train_random_forest(X_train, y_train, X_test, y_test)
        self.train_gradient_boosting(X_train, y_train, X_test, y_test)
        self.train_xgboost(X_train, y_train, X_test, y_test)
        self.train_lstm(X_train, y_train, X_test, y_test)

        print("\n[5/6] íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”...")
        self.plot_feature_importance(feature_names)

        print("\n[6/6] ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥...")
        self.save_models()

        print("\n--- ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ---")


if __name__ == "__main__":
    # í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    model_trainer = SP500EventDetectionModel()
    model_trainer.run_training_pipeline()
