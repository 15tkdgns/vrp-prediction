"""
ë°ì´í„° ëˆ„ì¶œ ì—†ì´ RÂ² > 0.1 ë‹¬ì„±ì„ ìœ„í•œ ê³ ê¸‰ ì ‘ê·¼ë²•

ì—„ê²©í•œ ì‹œê°„ì  ë¶„ë¦¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê°•í•œ ì˜ˆì¸¡ ì‹ í˜¸ ë°œêµ´
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False

try:
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.linear_model import Ridge, ElasticNet
    from sklearn.preprocessing import StandardScaler, RobustScaler
    from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import r2_score, mean_absolute_error
    from sklearn.decomposition import PCA
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False


class AdvancedLeakFreeFeatureEngineering:
    """ë°ì´í„° ëˆ„ì¶œ ì—†ëŠ” ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"""

    def __init__(self):
        self.feature_columns = []

    def create_advanced_features(self, data):
        """ê³ ê¸‰ íŠ¹ì„± ìƒì„± (ì—„ê²©í•œ ì‹œê°„ì  ë¶„ë¦¬)"""
        enhanced = data.copy()
        returns = data['returns']

        print("ğŸ”§ ê³ ê¸‰ ëˆ„ì¶œ ì—†ëŠ” íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...")

        # 1. ë‹¤ì¤‘ ì‹œê°„í”„ë ˆì„ ê¸°ìˆ ì  ì§€í‘œ (ê³¼ê±°ë§Œ)
        timeframes = [3, 5, 10, 20, 50]

        for window in timeframes:
            # ì´ë™í‰ê· ê³¼ í¸ì°¨
            ma = returns.rolling(window).mean()
            enhanced[f'ma_{window}'] = ma
            enhanced[f'price_ma_ratio_{window}'] = returns / (ma + 1e-8)
            enhanced[f'deviation_pct_{window}'] = (returns - ma) / (ma.rolling(window).std() + 1e-8)

            # ëª¨ë©˜í…€ (ê³¼ê±° ëˆ„ì )
            enhanced[f'momentum_{window}'] = returns.rolling(window).sum()
            enhanced[f'momentum_avg_{window}'] = returns.rolling(window).mean()

            # ë³€ë™ì„± ì§€í‘œ
            enhanced[f'volatility_{window}'] = returns.rolling(window).std()
            enhanced[f'vol_percentile_{window}'] = enhanced[f'volatility_{window}'].rolling(window*2).rank(pct=True)

            # RSIë¥˜ ì§€í‘œ (ê³¼ê±°ë§Œ)
            gains = returns.where(returns > 0, 0).rolling(window).mean()
            losses = (-returns.where(returns < 0, 0)).rolling(window).mean()
            enhanced[f'rsi_{window}'] = 100 - (100 / (1 + gains / (losses + 1e-8)))

        # 2. ê³ ê¸‰ í†µê³„ì  íŠ¹ì„±
        for window in [10, 20, 50]:
            # ê³ ì°¨ ëª¨ë©˜íŠ¸
            enhanced[f'skewness_{window}'] = returns.rolling(window).skew()
            enhanced[f'kurtosis_{window}'] = returns.rolling(window).kurt()

            # ë³€ë™ì„±ì˜ ë³€ë™ì„±
            vol = returns.rolling(window).std()
            enhanced[f'vol_of_vol_{window}'] = vol.rolling(window).std()

            # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ íŠ¹ì„±
            enhanced[f'percentile_rank_{window}'] = returns.rolling(window*2).rank(pct=True)

            # Z-score ë³€í˜• (MAD ëŒ€ì‹  í‘œì¤€í¸ì°¨ ì‚¬ìš©)
            enhanced[f'zscore_robust_{window}'] = (
                (returns - returns.rolling(window).median()) /
                (returns.rolling(window).std() + 1e-8)
            )

        # 3. êµì°¨ ì‹œê°„í”„ë ˆì„ íŠ¹ì„±
        # ë‹¨ê¸° vs ì¥ê¸° ë¹„êµ (ê³¼ê±°ë§Œ)
        short_ma = returns.rolling(5).mean()
        long_ma = returns.rolling(20).mean()
        enhanced['ma_cross_signal'] = short_ma - long_ma
        enhanced['ma_cross_ratio'] = short_ma / (long_ma + 1e-8)

        # ë³€ë™ì„± ë¹„ìœ¨
        short_vol = returns.rolling(5).std()
        long_vol = returns.rolling(20).std()
        enhanced['vol_ratio'] = short_vol / (long_vol + 1e-8)
        enhanced['vol_expansion'] = (short_vol > long_vol * 1.5).astype(int)

        # 4. ë˜ê·¸ íŠ¹ì„± (ë‹¤ì–‘í•œ ì‹œì )
        for lag in [1, 2, 3, 5, 10, 20]:
            enhanced[f'return_lag_{lag}'] = returns.shift(lag)
            enhanced[f'vol_lag_{lag}'] = enhanced['volatility_5'].shift(lag)
            enhanced[f'momentum_lag_{lag}'] = enhanced['momentum_5'].shift(lag)

        # 5. íŒ¨í„´ ì¸ì‹ íŠ¹ì„±
        # ì—°ì† ìƒìŠ¹/í•˜ë½ ì¼ìˆ˜
        enhanced['consecutive_up'] = (returns > 0).groupby((returns <= 0).cumsum()).cumsum()
        enhanced['consecutive_down'] = (returns < 0).groupby((returns >= 0).cumsum()).cumsum()

        # ê·¹ê°’ í›„ ë°˜ì „ íŒ¨í„´
        enhanced['extreme_high'] = (enhanced['percentile_rank_20'] > 0.9).fillna(False).astype(int)
        enhanced['extreme_low'] = (enhanced['percentile_rank_20'] < 0.1).fillna(False).astype(int)

        # 6. ê²½ì œì  ì˜ë¯¸ ìˆëŠ” íŠ¹ì„±
        # í° ì›€ì§ì„ í›„ ì•ˆì •í™” íŒ¨í„´
        big_moves = (abs(returns) > returns.rolling(20).std() * 2)
        enhanced['post_big_move'] = big_moves.shift(1).fillna(False).astype(int)

        # ë³€ë™ì„± êµ°ì§‘ íŒ¨í„´
        high_vol = (enhanced['volatility_5'] > enhanced['volatility_5'].rolling(20).quantile(0.8))
        enhanced['vol_cluster'] = high_vol.rolling(3).sum().fillna(0)

        # 7. ë§ˆí¬ë¡œ ì‚¬ì´í´ íŠ¹ì„±
        # ì›”ë³„ íš¨ê³¼ (ìš”ì¼, ì›” ë“±)
        if hasattr(enhanced.index, 'dayofweek'):
            enhanced['day_of_week'] = enhanced.index.dayofweek
            enhanced['month'] = enhanced.index.month
            enhanced['quarter'] = enhanced.index.quarter

        # ë°ì´í„° ì •ë¦¬
        enhanced = enhanced.dropna()
        feature_cols = [col for col in enhanced.columns if col != 'returns']
        self.feature_columns = feature_cols

        print(f"âœ… ê³ ê¸‰ íŠ¹ì„± {len(feature_cols)}ê°œ ìƒì„±")
        return enhanced


class AdvancedLeakFreeTargets:
    """ëˆ„ì¶œ ì—†ëŠ” ê°•í•œ ì‹ í˜¸ íƒ€ê²Ÿ ì„¤ê³„"""

    def create_strong_leak_free_targets(self, data):
        """ê°•í•œ ì‹ í˜¸ì´ë©´ì„œ ëˆ„ì¶œ ì—†ëŠ” íƒ€ê²Ÿë“¤"""
        enhanced = data.copy()
        returns = data['returns']

        print("ğŸ¯ ê°•í•œ ì‹ í˜¸ ëˆ„ì¶œ ì—†ëŠ” íƒ€ê²Ÿ ì„¤ê³„...")

        # 1. ë³€ë™ì„± ì˜ˆì¸¡ (ì™„ì „ ë¶„ë¦¬ëœ ê¸°ê°„)
        # í˜„ì¬: ìµœê·¼ 20ì¼ ë³€ë™ì„±
        # íƒ€ê²Ÿ: 5ì¼ í›„ë¶€í„° 10ì¼ê°„ì˜ ë³€ë™ì„± (ì™„ì „ ë¶„ë¦¬)
        future_vol_separated = returns.shift(-5).rolling(10).std()
        enhanced['target_volatility_separated'] = future_vol_separated

        # 2. ì¥ê¸° íŠ¸ë Œë“œ ì˜ˆì¸¡ (10ì¼ í›„ 5ì¼ê°„ í‰ê· )
        future_trend = returns.shift(-10).rolling(5).mean()
        enhanced['target_future_trend'] = future_trend

        # 3. ê·¹ê°’ í›„ ë°˜ì „ ì˜ˆì¸¡ (ì‹œê°„ ê°„ê²© ë‘ê¸°)
        # í˜„ì¬ ê·¹ê°’ ì—¬ë¶€ â†’ 7ì¼ í›„ ë°˜ì „ ì—¬ë¶€
        current_extreme = (abs(returns) > returns.rolling(20).std() * 2)
        future_opposite = (
            (current_extreme & (returns > 0) & (returns.shift(-7) < -returns.rolling(20).std())) |
            (current_extreme & (returns < 0) & (returns.shift(-7) > returns.rolling(20).std()))
        ).astype(int)
        enhanced['target_extreme_reversal'] = future_opposite

        # 4. ë³€ë™ì„± ì²´ì œ ë³€í™” ì˜ˆì¸¡
        # í˜„ì¬ ë‚®ì€ ë³€ë™ì„± â†’ ë¯¸ë˜ ë†’ì€ ë³€ë™ì„±
        current_low_vol = (returns.rolling(10).std() < returns.rolling(50).std().quantile(0.3))
        future_high_vol = (returns.shift(-5).rolling(10).std() > returns.rolling(50).std().quantile(0.7))
        enhanced['target_vol_regime_change'] = (current_low_vol & future_high_vol).astype(int)

        # 5. ì£¼ê°„/ì›”ê°„ ì‚¬ì´í´ ì˜ˆì¸¡ (ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°)
        if len(returns) > 100:
            # 5ì¼ í›„ ì£¼ê°„ ìˆ˜ìµë¥ 
            weekly_return = returns.shift(-5).rolling(5).sum()
            enhanced['target_weekly_return'] = weekly_return

            # ì›”ë§ íš¨ê³¼ ì˜ˆì¸¡ (20ì¼ í›„)
            if hasattr(enhanced.index, 'day'):
                month_end_return = returns.shift(-20)
                enhanced['target_month_end'] = month_end_return

        # 6. ë³µí•© ì‹ í˜¸ íƒ€ê²Ÿ
        # ë³€ë™ì„± + íŠ¸ë Œë“œ ë³µí•© ì˜ˆì¸¡
        vol_component = future_vol_separated
        trend_component = future_trend

        # ì •ê·œí™” í›„ ê²°í•©
        vol_norm = (vol_component - vol_component.mean()) / (vol_component.std() + 1e-8)
        trend_norm = (trend_component - trend_component.mean()) / (trend_component.std() + 1e-8)
        enhanced['target_vol_trend_combo'] = 0.6 * vol_norm + 0.4 * trend_norm

        print(f"âœ… ê°•í•œ ì‹ í˜¸ íƒ€ê²Ÿ {len([c for c in enhanced.columns if 'target_' in c])}ê°œ ìƒì„±")
        return enhanced


class AdvancedLeakFreeModeling:
    """ê³ ê¸‰ ëª¨ë¸ë§ ê¸°ë²•"""

    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.best_model = None
        self.best_score = -np.inf

    def create_advanced_models(self):
        """ê³ ê¸‰ ëª¨ë¸ ì„¸íŠ¸ ìƒì„±"""
        models = {}

        # 1. ì •ê·œí™” ê¸°ë°˜ ëª¨ë¸ë“¤
        models['ridge_advanced'] = Ridge(alpha=0.1)
        models['elasticnet_advanced'] = ElasticNet(alpha=0.1, l1_ratio=0.7, random_state=42)

        # 2. íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ë“¤ (ê³¼ì í•© ë°©ì§€)
        models['rf_conservative'] = RandomForestRegressor(
            n_estimators=100, max_depth=8, min_samples_split=10,
            min_samples_leaf=5, random_state=42
        )
        models['gbm_conservative'] = GradientBoostingRegressor(
            n_estimators=100, max_depth=6, learning_rate=0.05,
            min_samples_split=10, random_state=42
        )

        # 3. XGBoost (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if XGBOOST_AVAILABLE:
            models['xgb_conservative'] = xgb.XGBRegressor(
                n_estimators=100, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8, random_state=42
            )

        return models

    def advanced_feature_selection(self, X, y, method='mutual_info', k=30):
        """ê³ ê¸‰ íŠ¹ì„± ì„ íƒ"""
        if method == 'mutual_info':
            selector = SelectKBest(score_func=mutual_info_regression, k=k)
        else:
            selector = SelectKBest(score_func=f_regression, k=k)

        X_selected = selector.fit_transform(X, y)
        selected_features = X.columns[selector.get_support()]

        return pd.DataFrame(X_selected, columns=selected_features, index=X.index), selected_features

    def robust_cross_validation(self, X, y, models, cv_splits=5):
        """ê°•ê±´í•œ êµì°¨ê²€ì¦"""
        results = {}

        # ì‹œê³„ì—´ êµì°¨ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=cv_splits)

        for model_name, model in models.items():
            print(f"   ğŸ¤– {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...")

            cv_scores = []
            cv_mae = []

            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                # ì •ê·œí™” (Robust Scaler ì‚¬ìš©)
                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)

                # í›ˆë ¨ ë° ì˜ˆì¸¡
                try:
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_val_scaled)

                    score = r2_score(y_val, y_pred)
                    mae = mean_absolute_error(y_val, y_pred)

                    cv_scores.append(score)
                    cv_mae.append(mae)
                except Exception as e:
                    print(f"      âš ï¸ {model_name} ì˜¤ë¥˜: {e}")
                    cv_scores.append(-1.0)
                    cv_mae.append(1.0)

            avg_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            avg_mae = np.mean(cv_mae)

            results[model_name] = {
                'r2_mean': avg_score,
                'r2_std': std_score,
                'mae': avg_mae,
                'scores': cv_scores
            }

            print(f"      RÂ² = {avg_score:.4f} (Â±{std_score:.4f}), MAE = {avg_mae:.4f}")

            # ìµœê³  ì„±ëŠ¥ ì¶”ì 
            if avg_score > self.best_score:
                self.best_score = avg_score
                self.best_model = model_name

        return results


def comprehensive_leak_free_r2_test():
    """ì¢…í•©ì ì¸ ëˆ„ì¶œ ì—†ëŠ” RÂ² > 0.1 ì‹œë„"""
    print("ğŸš€ ë°ì´í„° ëˆ„ì¶œ ì—†ëŠ” RÂ² > 0.1 ë‹¬ì„± ì‹œë„")
    print("=" * 60)

    # 1. í˜„ì‹¤ì ì¸ SPY ë°ì´í„° ìƒì„±
    print("\nğŸ“Š ê³ í’ˆì§ˆ SPY ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±...")

    np.random.seed(42)
    n_samples = 1500  # ë” ë§ì€ ë°ì´í„°

    # ë” í˜„ì‹¤ì ì¸ ì‹œê³„ì—´ (GARCH íš¨ê³¼, í‰ê· íšŒê·€, íŠ¸ë Œë“œ)
    returns = np.zeros(n_samples)
    volatility = np.full(n_samples, 0.02)  # ì´ˆê¸° ë³€ë™ì„±

    # ê±°ì‹œê²½ì œ ì‚¬ì´í´ ì‹œë®¬ë ˆì´ì…˜
    macro_cycle = np.sin(np.arange(n_samples) * 2 * np.pi / 252) * 0.001  # ì—°ê°„ ì‚¬ì´í´

    for i in range(1, n_samples):
        # 1. ë³€ë™ì„± êµ°ì§‘ (GARCH íš¨ê³¼)
        volatility[i] = 0.9 * volatility[i-1] + 0.1 * abs(returns[i-1]) + 0.001 * np.random.normal()
        volatility[i] = max(0.005, min(0.05, volatility[i]))  # ë³€ë™ì„± ë²”ìœ„

        # 2. ì•½í•œ í‰ê· íšŒê·€
        mean_reversion = -0.1 * returns[i-1] if abs(returns[i-1]) > 0.03 else 0

        # 3. ì¥ê¸° ìƒìŠ¹ íŠ¸ë Œë“œ
        trend = 0.0003 + macro_cycle[i]

        # 4. ë³€ë™ì„± ì²´ì œ ë³€í™”
        regime_change = 0
        if i > 50:
            recent_vol = np.std(returns[i-20:i])
            if recent_vol < 0.01 and np.random.random() < 0.02:  # ë‚®ì€ ë³€ë™ì„± í›„ ê¸‰ë³€
                regime_change = np.random.normal(0, 0.04)

        # 5. ë…¸ì´ì¦ˆ
        noise = np.random.normal(0, volatility[i])

        returns[i] = trend + mean_reversion + regime_change + noise

    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')
    data = pd.DataFrame({'returns': returns}, index=dates)

    print(f"âœ… ê³ í’ˆì§ˆ ë°ì´í„° ìƒì„±: {len(data)}ê°œ ê´€ì¸¡ì¹˜")

    # 2. ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
    feature_engineer = AdvancedLeakFreeFeatureEngineering()
    enhanced_data = feature_engineer.create_advanced_features(data)

    # 3. ê°•í•œ ì‹ í˜¸ íƒ€ê²Ÿ ìƒì„±
    target_engineer = AdvancedLeakFreeTargets()
    final_data = target_engineer.create_strong_leak_free_targets(enhanced_data)
    final_data = final_data.dropna()

    print(f"ğŸ’¾ ìµœì¢… ë°ì´í„°: {len(final_data)}ê°œ ê´€ì¸¡ì¹˜")

    # 4. íƒ€ê²Ÿë³„ ê³ ê¸‰ ëª¨ë¸ë§
    target_columns = [col for col in final_data.columns if 'target_' in col]
    feature_columns = feature_engineer.feature_columns

    print(f"\nğŸ¯ {len(target_columns)}ê°œ íƒ€ê²Ÿ, {len(feature_columns)}ê°œ íŠ¹ì„±ìœ¼ë¡œ í…ŒìŠ¤íŠ¸")

    modeling = AdvancedLeakFreeModeling()
    models = modeling.create_advanced_models()

    all_results = {}

    for target_col in target_columns:
        print(f"\nğŸ“ˆ {target_col} íƒ€ê²Ÿ í…ŒìŠ¤íŠ¸:")
        print("-" * 50)

        # ë°ì´í„° ì¤€ë¹„
        y = final_data[target_col].dropna()
        X = final_data[feature_columns].loc[y.index]

        if len(y) < 500:  # ë°ì´í„° ë¶€ì¡±
            print(f"   ë°ì´í„° ë¶€ì¡± ({len(y)}ê°œ), ìŠ¤í‚µ")
            continue

        # ê³ ê¸‰ íŠ¹ì„± ì„ íƒ
        try:
            X_selected, selected_features = modeling.advanced_feature_selection(
                X, y, method='mutual_info', k=min(25, len(X.columns))
            )
            print(f"   ì„ ë³„ëœ íŠ¹ì„±: {len(selected_features)}ê°œ")
        except:
            X_selected = X.iloc[:, :25]  # ì²˜ìŒ 25ê°œë§Œ
            selected_features = X_selected.columns
            print(f"   ê¸°ë³¸ íŠ¹ì„± ì‚¬ìš©: {len(selected_features)}ê°œ")

        # ê³ ê¸‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        results = modeling.robust_cross_validation(X_selected, y, models)
        all_results[target_col] = results

        # ìµœê³  ì„±ëŠ¥ ì¶œë ¥
        best_model_for_target = max(results.keys(), key=lambda k: results[k]['r2_mean'])
        best_score_for_target = results[best_model_for_target]['r2_mean']

        print(f"   ğŸ† ìµœê³ : {best_model_for_target} - RÂ² = {best_score_for_target:.4f}")

        if best_score_for_target > 0.1:
            print(f"   âœ… RÂ² > 0.1 ë‹¬ì„±! ğŸ‰")
        elif best_score_for_target > 0.05:
            print(f"   ğŸ“ˆ ì–‘í˜¸í•œ ì„±ëŠ¥!")
        elif best_score_for_target > 0.02:
            print(f"   ğŸ“Š ì ì •í•œ ì„±ëŠ¥")
        else:
            print(f"   âš ï¸ ì„±ëŠ¥ ë¯¸í¡")

    # 5. ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ† ì¢…í•© ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    all_scores = []
    r2_over_01_count = 0

    for target, results in all_results.items():
        best_model = max(results.keys(), key=lambda k: results[k]['r2_mean'])
        best_score = results[best_model]['r2_mean']
        all_scores.append(best_score)

        if best_score > 0.1:
            r2_over_01_count += 1
            print(f"âœ… {target:<35} RÂ² = {best_score:.4f} ({best_model})")
        elif best_score > 0.05:
            print(f"ğŸ“ˆ {target:<35} RÂ² = {best_score:.4f} ({best_model})")
        else:
            print(f"ğŸ“Š {target:<35} RÂ² = {best_score:.4f} ({best_model})")

    if all_scores:
        max_score = max(all_scores)
        avg_score = np.mean(all_scores)

        print(f"\nğŸ“Š ì„±ëŠ¥ í†µê³„:")
        print(f"   ìµœê³  RÂ²: {max_score:.4f}")
        print(f"   í‰ê·  RÂ²: {avg_score:.4f}")
        print(f"   RÂ² > 0.1 ë‹¬ì„±: {r2_over_01_count}ê°œ íƒ€ê²Ÿ")
        print(f"   RÂ² > 0.05 ë‹¬ì„±: {sum(1 for s in all_scores if s > 0.05)}ê°œ íƒ€ê²Ÿ")

        if max_score > 0.1:
            print(f"\nğŸ‰ ëª©í‘œ ë‹¬ì„±! RÂ² > 0.1 ì„±ê³µ!")
            print(f"   ë°ì´í„° ëˆ„ì¶œ ì—†ì´ {max_score:.4f} ë‹¬ì„±")
        elif max_score > 0.08:
            print(f"\nğŸ“ˆ ê±°ì˜ ë‹¬ì„±! RÂ² = {max_score:.4f}")
            print(f"   ì¶”ê°€ ê°œì„ ìœ¼ë¡œ 0.1 ë‹¬ì„± ê°€ëŠ¥")
        else:
            print(f"\nğŸ“Š í˜„ì‹¤ì  ì„±ëŠ¥: RÂ² = {max_score:.4f}")
            print(f"   ê¸ˆìœµ ë°ì´í„°ì˜ í•œê³„ ë‚´ì—ì„œ ì–‘í˜¸í•œ ê²°ê³¼")

    return all_results, max_score if all_scores else 0


if __name__ == "__main__":
    results, best_r2 = comprehensive_leak_free_r2_test()

    print(f"\nâœ… ê³ ê¸‰ RÂ² ê°œì„  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"   ìµœê³  ì„±ëŠ¥: RÂ² = {best_r2:.4f}")
    print(f"   ë°ì´í„° ëˆ„ì¶œ: ì™„ì „íˆ ë°©ì§€ë¨")
    print(f"   ëª©í‘œ ë‹¬ì„± ì—¬ë¶€: {'âœ… ì„±ê³µ' if best_r2 > 0.1 else 'ğŸ“ˆ ì§€ì† ê°œì„  í•„ìš”'}")