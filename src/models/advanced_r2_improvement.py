"""
RÂ² ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ê³ ê¸‰ ëª¨ë¸ë§ ì‹œìŠ¤í…œ

ë°ì´í„° ëˆ„ì¶œ ì—†ì´ RÂ² ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ë“¤:
1. ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
2. ë‹¤ì¤‘ ì‹œê°„ í”„ë ˆì„ íƒ€ê²Ÿ
3. ê³ ê¸‰ ì‹œê³„ì—´ ëª¨ë¸ (LSTM, XGBoost ì‹œê³„ì—´ ìµœì í™”)
4. ì ì‘ì  ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§
5. ë™ì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

try:
    from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer
    from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.linear_model import ElasticNet, Ridge, Lasso
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    from sklearn.model_selection import TimeSeriesSplit
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ sklearn not available")

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoost not available, using sklearn alternatives")

try:
    from tensorflow import keras
    from tensorflow.keras import layers
    import tensorflow as tf
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("âš ï¸ TensorFlow not available, using sklearn alternatives")


class AdvancedFeatureEngineer:
    """
    ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ

    RÂ² ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ í¬ê´„ì  íŠ¹ì„± ìƒì„±:
    - ê¸°ìˆ ì  ì§€í‘œ í™•ì¥
    - í†µê³„ì  íŠ¹ì„±
    - ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
    - ìƒí˜¸ì‘ìš© íŠ¹ì„±
    """

    def __init__(self):
        self.feature_names = []
        self.scaler = None

    def create_advanced_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ê³ ê¸‰ íŠ¹ì„± ìƒì„±"""
        print("ğŸ”§ ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘...")

        enhanced_data = data.copy()

        # 1. í™•ì¥ëœ ê¸°ìˆ ì  ì§€í‘œ
        enhanced_data = self._add_technical_indicators(enhanced_data)

        # 2. í†µê³„ì  íŠ¹ì„±
        enhanced_data = self._add_statistical_features(enhanced_data)

        # 3. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
        enhanced_data = self._add_temporal_features(enhanced_data)

        # 4. ìƒí˜¸ì‘ìš© íŠ¹ì„±
        enhanced_data = self._add_interaction_features(enhanced_data)

        # 5. ë³€ë™ì„± ê´€ë ¨ íŠ¹ì„±
        enhanced_data = self._add_volatility_features(enhanced_data)

        print(f"âœ… íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {len(enhanced_data.columns)}ê°œ íŠ¹ì„±")
        return enhanced_data.dropna()

    def _add_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """í™•ì¥ëœ ê¸°ìˆ ì  ì§€í‘œ"""
        if 'price' not in data.columns:
            return data

        prices = data['price']

        # ë‹¤ì–‘í•œ ê¸°ê°„ì˜ ì´ë™í‰ê· 
        for window in [3, 7, 14, 21, 50, 100]:
            data[f'ma_{window}'] = prices.rolling(window).mean()
            data[f'price_ma_ratio_{window}'] = prices / data[f'ma_{window}']

        # ì§€ìˆ˜ì´ë™í‰ê· 
        for alpha in [0.1, 0.2, 0.3]:
            data[f'ema_alpha_{alpha:.1f}'] = prices.ewm(alpha=alpha).mean()

        # MACD ì§€í‘œ
        ema_12 = prices.ewm(span=12).mean()
        ema_26 = prices.ewm(span=26).mean()
        data['macd'] = ema_12 - ema_26
        data['macd_signal'] = data['macd'].ewm(span=9).mean()
        data['macd_histogram'] = data['macd'] - data['macd_signal']

        # Stochastic Oscillator
        high_14 = prices.rolling(14).max()
        low_14 = prices.rolling(14).min()
        data['stoch_k'] = 100 * (prices - low_14) / (high_14 - low_14)
        data['stoch_d'] = data['stoch_k'].rolling(3).mean()

        # Williams %R
        data['williams_r'] = -100 * (high_14 - prices) / (high_14 - low_14)

        # Commodity Channel Index (CCI)
        typical_price = prices  # ë‹¨ìˆœí™” (Closeë§Œ ì‚¬ìš©)
        sma_tp = typical_price.rolling(20).mean()
        mad = typical_price.rolling(20).apply(lambda x: np.mean(np.abs(x - x.mean())))
        data['cci'] = (typical_price - sma_tp) / (0.015 * mad)

        return data

    def _add_statistical_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """í†µê³„ì  íŠ¹ì„±"""
        if 'log_returns' not in data.columns:
            return data

        returns = data['log_returns']

        # ë¡¤ë§ í†µê³„
        for window in [5, 10, 20, 50]:
            data[f'returns_mean_{window}'] = returns.rolling(window).mean()
            data[f'returns_std_{window}'] = returns.rolling(window).std()
            data[f'returns_skew_{window}'] = returns.rolling(window).skew()
            data[f'returns_kurt_{window}'] = returns.rolling(window).kurt()

            # ë¶„ìœ„ìˆ˜
            data[f'returns_q25_{window}'] = returns.rolling(window).quantile(0.25)
            data[f'returns_q75_{window}'] = returns.rolling(window).quantile(0.75)
            data[f'returns_iqr_{window}'] = data[f'returns_q75_{window}'] - data[f'returns_q25_{window}']

        # Z-score (ì •ê·œí™”ëœ ìˆ˜ìµë¥ )
        for window in [10, 20, 50]:
            mean = returns.rolling(window).mean()
            std = returns.rolling(window).std()
            data[f'returns_zscore_{window}'] = (returns - mean) / std

        # ëˆ„ì  í†µê³„
        data['returns_cumsum'] = returns.cumsum()
        data['returns_cummax'] = returns.cummax()
        data['returns_cummin'] = returns.cummin()

        return data

    def _add_temporal_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±"""
        # ì¸ë±ìŠ¤ê°€ DatetimeIndexì¸ì§€ í™•ì¸
        if isinstance(data.index, pd.DatetimeIndex):
            dates = data.index
        else:
            # ì„ì˜ì˜ ë‚ ì§œ íŠ¹ì„± ìƒì„±
            dates = pd.date_range(start='2020-01-01', periods=len(data), freq='D')

        # ìš”ì¼ íš¨ê³¼
        data['day_of_week'] = dates.dayofweek
        data['is_monday'] = (dates.dayofweek == 0).astype(int)
        data['is_friday'] = (dates.dayofweek == 4).astype(int)

        # ì›” íš¨ê³¼
        data['month'] = dates.month
        data['quarter'] = dates.quarter
        data['is_january'] = (dates.month == 1).astype(int)
        data['is_december'] = (dates.month == 12).astype(int)

        # ì—°ë§/ì—°ì´ˆ íš¨ê³¼
        data['days_from_year_start'] = dates.dayofyear
        data['days_to_year_end'] = 365 - dates.dayofyear

        # ì‹œê³„ì—´ íŠ¸ë Œë“œ
        data['time_trend'] = np.arange(len(data))
        data['time_trend_squared'] = data['time_trend'] ** 2

        return data

    def _add_interaction_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ìƒí˜¸ì‘ìš© íŠ¹ì„±"""
        # ê°€ê²©ê³¼ ë³€ë™ì„± ìƒí˜¸ì‘ìš©
        if 'price' in data.columns and 'volatility_20d' in data.columns:
            data['price_vol_interaction'] = data['price'] * data['volatility_20d']
            data['price_vol_ratio'] = data['price'] / (data['volatility_20d'] + 1e-8)

        # ëª¨ë©˜í…€ê³¼ ë³€ë™ì„± ìƒí˜¸ì‘ìš©
        if 'price_momentum_5d' in data.columns and 'volatility_5d' in data.columns:
            data['momentum_vol_interaction'] = data['price_momentum_5d'] * data['volatility_5d']

        # ì´ë™í‰ê· ë“¤ ê°„ì˜ ê´€ê³„
        if 'ma_5' in data.columns and 'ma_20' in data.columns:
            data['ma_spread_5_20'] = data['ma_5'] - data['ma_20']
            data['ma_ratio_5_20'] = data['ma_5'] / (data['ma_20'] + 1e-8)

        if 'ma_20' in data.columns and 'ma_50' in data.columns:
            data['ma_spread_20_50'] = data['ma_20'] - data['ma_50']
            data['ma_ratio_20_50'] = data['ma_20'] / (data['ma_50'] + 1e-8)

        return data

    def _add_volatility_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ë³€ë™ì„± ê´€ë ¨ íŠ¹ì„±"""
        if 'log_returns' not in data.columns:
            return data

        returns = data['log_returns']

        # ì‹¤í˜„ ë³€ë™ì„± (ë‹¤ì–‘í•œ ê¸°ê°„)
        for window in [3, 5, 10, 20, 50]:
            data[f'realized_vol_{window}'] = returns.rolling(window).std() * np.sqrt(252)

        # GARCH ìŠ¤íƒ€ì¼ ë³€ë™ì„± (ê°„ë‹¨í•œ ë²„ì „)
        squared_returns = returns ** 2
        for window in [5, 10, 20]:
            data[f'garch_vol_{window}'] = squared_returns.rolling(window).mean()

        # ë³€ë™ì„±ì˜ ë³€ë™ì„±
        for window in [10, 20]:
            vol = returns.rolling(window).std()
            data[f'vol_of_vol_{window}'] = vol.rolling(window).std()

        # ê·¹ê°’ ê¸°ë°˜ ë³€ë™ì„± (Parkinson ì¶”ì •ëŸ‰ ê·¼ì‚¬)
        if 'price' in data.columns:
            for window in [5, 10, 20]:
                high = data['price'].rolling(window).max()
                low = data['price'].rolling(window).min()
                data[f'parkinson_vol_{window}'] = np.sqrt(0.361 * np.log(high / low) ** 2)

        return data


class MultiTimeframeTargets:
    """
    ë‹¤ì¤‘ ì‹œê°„ í”„ë ˆì„ íƒ€ê²Ÿ ìƒì„±

    1ì¼, 3ì¼, 5ì¼, 1ì£¼ì¼ ìˆ˜ìµë¥  ë“± ë‹¤ì–‘í•œ ì˜ˆì¸¡ íƒ€ê²Ÿìœ¼ë¡œ
    ëª¨ë¸ì˜ ì„¤ëª…ë ¥ í–¥ìƒ
    """

    def __init__(self):
        self.target_configs = {
            '1d': 1,
            '3d': 3,
            '5d': 5,
            '1w': 7,
            '2w': 14
        }

    def create_multiple_targets(self, data: pd.DataFrame) -> pd.DataFrame:
        """ë‹¤ì¤‘ ì‹œê°„ í”„ë ˆì„ íƒ€ê²Ÿ ìƒì„±"""
        print("ğŸ¯ ë‹¤ì¤‘ ì‹œê°„ í”„ë ˆì„ íƒ€ê²Ÿ ìƒì„±...")

        if 'log_returns' not in data.columns:
            print("âš ï¸ log_returns ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return data

        enhanced_data = data.copy()
        returns = data['log_returns']

        for name, days in self.target_configs.items():
            # í–¥í›„ Nì¼ ìˆ˜ìµë¥  í•©ê³„
            enhanced_data[f'target_return_{name}'] = returns.shift(-days).rolling(days).sum()

            # í–¥í›„ Nì¼ ìˆ˜ìµë¥  í‰ê· 
            enhanced_data[f'target_return_avg_{name}'] = returns.shift(-days).rolling(days).mean()

            # í–¥í›„ Nì¼ ì¤‘ ìµœëŒ€/ìµœì†Œ ìˆ˜ìµë¥ 
            enhanced_data[f'target_return_max_{name}'] = returns.shift(-days).rolling(days).max()
            enhanced_data[f'target_return_min_{name}'] = returns.shift(-days).rolling(days).min()

            # í–¥í›„ Nì¼ ë°©í–¥ (ìƒìŠ¹/í•˜ë½)
            future_cumsum = returns.shift(-days).rolling(days).sum()
            enhanced_data[f'target_direction_{name}'] = (future_cumsum > 0).astype(int)

        print(f"âœ… ë‹¤ì¤‘ íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ: {len([c for c in enhanced_data.columns if 'target_' in c])}ê°œ íƒ€ê²Ÿ")
        return enhanced_data


class AdvancedTimeSeriesModels:
    """
    ê³ ê¸‰ ì‹œê³„ì—´ ëª¨ë¸ êµ¬í˜„

    RÂ² ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ë“¤:
    - XGBoost ì‹œê³„ì—´ ìµœì í™”
    - LSTM ë”¥ëŸ¬ë‹
    - ì ì‘ì  ì•™ìƒë¸”
    """

    def __init__(self):
        self.models = {}
        self.scalers = {}

    def train_xgboost_timeseries(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        target_name: str = "1d"
    ) -> Dict:
        """XGBoost ì‹œê³„ì—´ ìµœì í™” ëª¨ë¸"""
        print(f"ğŸš€ XGBoost ì‹œê³„ì—´ ëª¨ë¸ í›ˆë ¨ ({target_name})...")

        if not XGBOOST_AVAILABLE:
            print("âš ï¸ XGBoost ì—†ìŒ, RandomForest ì‚¬ìš©")
            return self._train_random_forest_alternative(X, y, target_name)

        # ì‹œê³„ì—´ ë¶„í• 
        tscv = TimeSeriesSplit(n_splits=5)

        best_score = -np.inf
        best_model = None
        best_params = None

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
        param_grid = [
            {
                'n_estimators': [100, 200, 300],
                'max_depth': [3, 5, 7],
                'learning_rate': [0.01, 0.05, 0.1],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0]
            }
        ]

        for params in param_grid:
            for n_est in params['n_estimators']:
                for depth in params['max_depth']:
                    for lr in params['learning_rate']:
                        model_params = {
                            'n_estimators': n_est,
                            'max_depth': depth,
                            'learning_rate': lr,
                            'subsample': 0.9,
                            'colsample_bytree': 0.9,
                            'random_state': 42,
                            'objective': 'reg:squarederror',
                            'eval_metric': 'rmse'
                        }

                        # ì‹œê³„ì—´ êµì°¨ê²€ì¦
                        cv_scores = []
                        for train_idx, val_idx in tscv.split(X):
                            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                            model = xgb.XGBRegressor(**model_params)
                            model.fit(X_train, y_train)

                            y_pred = model.predict(X_val)
                            score = r2_score(y_val, y_pred)
                            cv_scores.append(score)

                        avg_score = np.mean(cv_scores)
                        if avg_score > best_score:
                            best_score = avg_score
                            best_params = model_params
                            best_model = xgb.XGBRegressor(**model_params)

        # ìµœì¢… ëª¨ë¸ í›ˆë ¨
        if best_model is not None:
            best_model.fit(X, y)
            self.models[f'xgboost_{target_name}'] = best_model

            print(f"âœ… XGBoost í›ˆë ¨ ì™„ë£Œ: CV RÂ² = {best_score:.4f}")

            return {
                'model': best_model,
                'cv_score': best_score,
                'best_params': best_params,
                'model_type': 'XGBoost'
            }
        else:
            return self._train_random_forest_alternative(X, y, target_name)

    def _train_random_forest_alternative(self, X: pd.DataFrame, y: pd.Series, target_name: str) -> Dict:
        """XGBoost ëŒ€ì²´ìš© RandomForest"""
        print(f"ğŸŒ² RandomForest ëŒ€ì²´ ëª¨ë¸ í›ˆë ¨ ({target_name})...")

        model = RandomForestRegressor(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )

        # ì‹œê³„ì—´ êµì°¨ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=5)
        cv_scores = []

        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)
            score = r2_score(y_val, y_pred)
            cv_scores.append(score)

        avg_score = np.mean(cv_scores)

        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í›ˆë ¨
        model.fit(X, y)
        self.models[f'rf_{target_name}'] = model

        print(f"âœ… RandomForest í›ˆë ¨ ì™„ë£Œ: CV RÂ² = {avg_score:.4f}")

        return {
            'model': model,
            'cv_score': avg_score,
            'best_params': model.get_params(),
            'model_type': 'RandomForest'
        }

    def train_lstm_model(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        target_name: str = "1d",
        sequence_length: int = 20
    ) -> Dict:
        """LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸"""
        print(f"ğŸ§  LSTM ëª¨ë¸ í›ˆë ¨ ({target_name})...")

        if not TENSORFLOW_AVAILABLE:
            print("âš ï¸ TensorFlow ì—†ìŒ, ì „í†µì  ëª¨ë¸ ì‚¬ìš©")
            return self._train_random_forest_alternative(X, y, target_name)

        # ë°ì´í„° ì „ì²˜ë¦¬
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()

        X_scaled = scaler_X.fit_transform(X)
        y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        X_sequences, y_sequences = self._create_sequences(X_scaled, y_scaled, sequence_length)

        if len(X_sequences) < 50:  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í¬ê¸°
            print("âš ï¸ ì‹œí€€ìŠ¤ ë°ì´í„° ë¶€ì¡±, ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©")
            return self._train_random_forest_alternative(X, y, target_name)

        # ì‹œê³„ì—´ ë¶„í• 
        train_size = int(len(X_sequences) * 0.8)
        X_train, X_test = X_sequences[:train_size], X_sequences[train_size:]
        y_train, y_test = y_sequences[:train_size], y_sequences[train_size:]

        # LSTM ëª¨ë¸ êµ¬ì¶•
        model = keras.Sequential([
            layers.LSTM(50, return_sequences=True, input_shape=(sequence_length, X_scaled.shape[1])),
            layers.Dropout(0.2),
            layers.LSTM(50, return_sequences=False),
            layers.Dropout(0.2),
            layers.Dense(25),
            layers.Dense(1)
        ])

        model.compile(optimizer='adam', loss='mse', metrics=['mae'])

        # ì¡°ê¸° ì¢…ë£Œ ì½œë°±
        early_stopping = keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=10, restore_best_weights=True
        )

        # ëª¨ë¸ í›ˆë ¨
        history = model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_test, y_test),
            callbacks=[early_stopping],
            verbose=0
        )

        # ì„±ëŠ¥ í‰ê°€
        y_pred_scaled = model.predict(X_test)
        y_pred = scaler_y.inverse_transform(y_pred_scaled)
        y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()

        score = r2_score(y_test_original, y_pred.flatten())

        # ëª¨ë¸ ì €ì¥
        self.models[f'lstm_{target_name}'] = model
        self.scalers[f'lstm_{target_name}'] = {'X': scaler_X, 'y': scaler_y}

        print(f"âœ… LSTM í›ˆë ¨ ì™„ë£Œ: Test RÂ² = {score:.4f}")

        return {
            'model': model,
            'cv_score': score,
            'scalers': {'X': scaler_X, 'y': scaler_y},
            'model_type': 'LSTM',
            'sequence_length': sequence_length
        }

    def _create_sequences(self, X: np.ndarray, y: np.ndarray, sequence_length: int) -> Tuple[np.ndarray, np.ndarray]:
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        X_sequences, y_sequences = [], []

        for i in range(len(X) - sequence_length):
            X_sequences.append(X[i:(i + sequence_length)])
            y_sequences.append(y[i + sequence_length])

        return np.array(X_sequences), np.array(y_sequences)


class AdaptiveNormalization:
    """
    ì ì‘ì  ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§

    RÂ² ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ë‹¤ì–‘í•œ ì •ê·œí™” ê¸°ë²•:
    - ì‹œê°„ ì ì‘ì  ìŠ¤ì¼€ì¼ë§
    - ë¶„ìœ„ìˆ˜ ë³€í™˜
    - ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§
    """

    def __init__(self):
        self.scalers = {}
        self.best_scaler = None

    def find_best_normalization(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        test_scalers: List[str] = None
    ) -> Dict:
        """ìµœì  ì •ê·œí™” ë°©ë²• ì°¾ê¸°"""
        print("ğŸ“Š ìµœì  ì •ê·œí™” ë°©ë²• íƒìƒ‰...")

        if test_scalers is None:
            test_scalers = ['standard', 'robust', 'quantile', 'minmax']

        results = {}

        # ê¸°ë³¸ ëª¨ë¸ (ì •ê·œí™” ë¹„êµìš©)
        base_model = RandomForestRegressor(n_estimators=100, random_state=42)

        # ì‹œê³„ì—´ êµì°¨ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=3)

        for scaler_name in test_scalers:
            print(f"   {scaler_name} ìŠ¤ì¼€ì¼ëŸ¬ í…ŒìŠ¤íŠ¸...")

            # ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ
            if scaler_name == 'standard':
                scaler = StandardScaler()
            elif scaler_name == 'robust':
                scaler = RobustScaler()
            elif scaler_name == 'quantile':
                scaler = QuantileTransformer(output_distribution='normal', random_state=42)
            elif scaler_name == 'minmax':
                from sklearn.preprocessing import MinMaxScaler
                scaler = MinMaxScaler()
            else:
                continue

            cv_scores = []

            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
                X_train_scaled = pd.DataFrame(
                    scaler.fit_transform(X_train),
                    columns=X_train.columns,
                    index=X_train.index
                )
                X_val_scaled = pd.DataFrame(
                    scaler.transform(X_val),
                    columns=X_val.columns,
                    index=X_val.index
                )

                # ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
                base_model.fit(X_train_scaled, y_train)
                y_pred = base_model.predict(X_val_scaled)
                score = r2_score(y_val, y_pred)
                cv_scores.append(score)

            avg_score = np.mean(cv_scores)
            results[scaler_name] = {
                'cv_score': avg_score,
                'cv_std': np.std(cv_scores),
                'scaler': scaler
            }

            print(f"     RÂ² = {avg_score:.4f} (Â±{np.std(cv_scores):.4f})")

        # ìµœê³  ì„±ëŠ¥ ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ
        best_scaler_name = max(results.keys(), key=lambda k: results[k]['cv_score'])
        self.best_scaler = results[best_scaler_name]['scaler']

        print(f"âœ… ìµœì  ìŠ¤ì¼€ì¼ëŸ¬: {best_scaler_name} (RÂ² = {results[best_scaler_name]['cv_score']:.4f})")

        return results


def run_r2_improvement_experiment():
    """RÂ² ì„±ëŠ¥ ê°œì„  ì‹¤í—˜ ì‹¤í–‰"""
    print("ğŸš€ RÂ² ì„±ëŠ¥ ê°œì„  ì‹¤í—˜ ì‹œì‘")
    print("=" * 60)

    # 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‹¤ì œ ë°ì´í„° ì‚¬ìš©)
    print("\nğŸ“Š 1ë‹¨ê³„: ìƒ˜í”Œ ë°ì´í„° ìƒì„±")
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=1000, freq='D')

    # ë” í˜„ì‹¤ì ì¸ ê¸ˆìœµ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
    returns = np.random.normal(0.0005, 0.02, 1000)  # ì¼ì¼ ìˆ˜ìµë¥ 
    prices = 100 * np.exp(np.cumsum(returns))  # ê°€ê²©

    base_data = pd.DataFrame({
        'price': prices,
        'log_returns': returns
    }, index=dates)

    # ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€
    base_data['ma_5'] = base_data['price'].rolling(5).mean()
    base_data['ma_20'] = base_data['price'].rolling(20).mean()
    base_data['ma_50'] = base_data['price'].rolling(50).mean()
    base_data['volatility_5d'] = base_data['log_returns'].rolling(5).std()
    base_data['volatility_20d'] = base_data['log_returns'].rolling(20).std()
    base_data['rsi'] = 50 + np.random.normal(0, 15, 1000)  # ê°„ë‹¨í•œ RSI ì‹œë®¬ë ˆì´ì…˜
    base_data['price_momentum_5d'] = base_data['price'].pct_change(5)

    print(f"   ê¸°ë³¸ ë°ì´í„°: {len(base_data.columns)}ê°œ íŠ¹ì„±, {len(base_data)}ê°œ ê´€ì¸¡ì¹˜")

    # 2. ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
    print("\nğŸ”§ 2ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§")
    feature_engineer = AdvancedFeatureEngineer()
    enhanced_data = feature_engineer.create_advanced_features(base_data)

    # 3. ë‹¤ì¤‘ ì‹œê°„ í”„ë ˆì„ íƒ€ê²Ÿ ìƒì„±
    print("\nğŸ¯ 3ë‹¨ê³„: ë‹¤ì¤‘ ì‹œê°„ í”„ë ˆì„ íƒ€ê²Ÿ ìƒì„±")
    target_generator = MultiTimeframeTargets()
    enhanced_data = target_generator.create_multiple_targets(enhanced_data)

    # ê²°ì¸¡ì¹˜ ì œê±°
    enhanced_data = enhanced_data.dropna()
    print(f"   ìµœì¢… ë°ì´í„°: {len(enhanced_data.columns)}ê°œ íŠ¹ì„±, {len(enhanced_data)}ê°œ ê´€ì¸¡ì¹˜")

    # 4. ì ì‘ì  ì •ê·œí™” ìµœì í™”
    print("\nğŸ“Š 4ë‹¨ê³„: ì ì‘ì  ì •ê·œí™” ìµœì í™”")

    # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    target_columns = [col for col in enhanced_data.columns if 'target_' in col]
    feature_columns = [col for col in enhanced_data.columns if 'target_' not in col]

    X = enhanced_data[feature_columns]

    normalizer = AdaptiveNormalization()
    normalization_results = normalizer.find_best_normalization(X, enhanced_data['target_return_1d'])

    # 5. ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨
    print("\nğŸ¤– 5ë‹¨ê³„: ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨")
    model_trainer = AdvancedTimeSeriesModels()

    results = {}

    # ê° íƒ€ê²Ÿì— ëŒ€í•´ ëª¨ë¸ í›ˆë ¨
    for target_col in ['target_return_1d', 'target_return_3d', 'target_return_5d']:
        if target_col in enhanced_data.columns:
            print(f"\n   {target_col} íƒ€ê²Ÿ ëª¨ë¸ë“¤ í›ˆë ¨:")
            y = enhanced_data[target_col].dropna()
            X_target = X.loc[y.index]  # íƒ€ê²Ÿê³¼ ì¸ë±ìŠ¤ ë§ì¶¤

            # XGBoost ëª¨ë¸
            xgb_result = model_trainer.train_xgboost_timeseries(X_target, y, target_col)
            results[f'xgboost_{target_col}'] = xgb_result

            # LSTM ëª¨ë¸ (ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°ì—ë§Œ)
            if len(X_target) > 100:
                lstm_result = model_trainer.train_lstm_model(X_target, y, target_col)
                results[f'lstm_{target_col}'] = lstm_result

    # 6. ê²°ê³¼ ìš”ì•½
    print("\nğŸ“ˆ 6ë‹¨ê³„: ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    print("\nğŸ† ëª¨ë¸ë³„ RÂ² ì„±ëŠ¥:")
    print("-" * 50)

    best_score = -np.inf
    best_model = None

    for model_name, result in results.items():
        score = result['cv_score']
        model_type = result['model_type']
        print(f"   {model_name:<25} {model_type:<15} RÂ² = {score:.4f}")

        if score > best_score:
            best_score = score
            best_model = model_name

    print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (RÂ² = {best_score:.4f})")

    # ê°œì„  ì •ë„ ë¶„ì„
    baseline_score = -0.01  # ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ í‰ê·  RÂ²
    if best_score > baseline_score:
        improvement = ((best_score - baseline_score) / abs(baseline_score)) * 100
        print(f"ğŸ“Š ì„±ëŠ¥ ê°œì„ : {improvement:.1f}% í–¥ìƒ")
    else:
        print("âš ï¸ ì¶”ê°€ ìµœì í™” í•„ìš”")

    print(f"\nâœ… RÂ² ê°œì„  ì‹¤í—˜ ì™„ë£Œ!")

    return {
        'enhanced_data': enhanced_data,
        'results': results,
        'best_model': best_model,
        'best_score': best_score,
        'normalization_results': normalization_results
    }


if __name__ == "__main__":
    # RÂ² ê°œì„  ì‹¤í—˜ ì‹¤í–‰
    experiment_results = run_r2_improvement_experiment()