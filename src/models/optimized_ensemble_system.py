#!/usr/bin/env python3
"""
ìµœì í™”ëœ ì•™ìƒë¸” ì‹œìŠ¤í…œ - GPU ê°€ì† ëª¨ë¸ í†µí•©
ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨ ë° ì„±ëŠ¥ ê·¹ëŒ€í™”
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import tensorflow as tf
import xgboost as xgb
import joblib
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import RobustScaler
import json
import logging
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MetaLearner(nn.Module):
    """GPU ê°€ì† ë©”íƒ€ í•™ìŠµê¸°"""

    def __init__(self, num_base_models, hidden_size=64):
        super(MetaLearner, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(num_base_models, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, 1)
        )

    def forward(self, x):
        return self.layers(x)

class OptimizedEnsembleSystem:
    """ìµœì í™”ëœ ì•™ìƒë¸” ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.base_models = {}
        self.meta_learner = None
        self.scaler = RobustScaler()
        self.performance_history = []

        logger.info(f"ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {self.device}")

    def load_base_models(self):
        """ê¸°í•™ìŠµëœ ë² ì´ìŠ¤ ëª¨ë¸ë“¤ ë¡œë”©"""
        models_path = Path("/root/workspace/data/models")

        try:
            # PyTorch LSTM (ìµœê³  ì„±ëŠ¥)
            if (models_path / "gpu_enhanced_lstm.pth").exists():
                self.base_models['pytorch_lstm'] = torch.load(
                    models_path / "gpu_enhanced_lstm.pth",
                    map_location=self.device
                )
                self.base_models['pytorch_lstm'].eval()
                logger.info("âœ… PyTorch LSTM ë¡œë”© ì™„ë£Œ")

            # XGBoost GPU
            if (models_path / "gpu_enhanced_xgboost.pkl").exists():
                self.base_models['xgboost_gpu'] = joblib.load(
                    models_path / "gpu_enhanced_xgboost.pkl"
                )
                logger.info("âœ… XGBoost GPU ë¡œë”© ì™„ë£Œ")

            # TensorFlow Transformer
            if (models_path / "gpu_enhanced_transformer.h5").exists():
                self.base_models['tf_transformer'] = tf.keras.models.load_model(
                    models_path / "gpu_enhanced_transformer.h5"
                )
                logger.info("âœ… TensorFlow Transformer ë¡œë”© ì™„ë£Œ")

            # Kaggle ìš°ìŠ¹ ëª¨ë¸ë“¤
            kaggle_models_path = Path("/root/workspace/models")
            if kaggle_models_path.exists():
                for model_file in kaggle_models_path.glob("*.pkl"):
                    model_name = model_file.stem
                    try:
                        self.base_models[f'kaggle_{model_name}'] = joblib.load(model_file)
                        logger.info(f"âœ… Kaggle ëª¨ë¸ ë¡œë”©: {model_name}")
                    except Exception as e:
                        logger.warning(f"Kaggle ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {e}")

            logger.info(f"ì´ {len(self.base_models)}ê°œ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}")

    def prepare_ensemble_data(self, df):
        """ì•™ìƒë¸”ìš© ë°ì´í„° ì¤€ë¹„"""
        # ê¸°ë³¸ íŠ¹ì§•
        feature_cols = ['price_change', 'volatility', 'sma_20', 'sma_50',
                       'rsi', 'volume_sma', 'volume_ratio']

        # ì‹œí€€ìŠ¤ ë°ì´í„° (ë”¥ëŸ¬ë‹ ëª¨ë¸ìš©)
        sequence_length = 20
        scaled_features = self.scaler.fit_transform(df[feature_cols])

        X_seq, X_flat, y = [], [], []
        for i in range(sequence_length, len(scaled_features)):
            # ì‹œí€€ìŠ¤ íŠ¹ì§• (LSTM, Transformerìš©)
            X_seq.append(scaled_features[i-sequence_length:i])
            # í”Œë« íŠ¹ì§• (XGBoost, ì „í†µì  MLìš©)
            X_flat.append(scaled_features[i-sequence_length:i].flatten())
            # íƒ€ê²Ÿ
            y.append(df.iloc[i]['target'])

        return np.array(X_seq), np.array(X_flat), np.array(y)

    def get_base_predictions(self, X_seq, X_flat):
        """ëª¨ë“  ë² ì´ìŠ¤ ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘"""
        predictions = []

        # PyTorch LSTM ì˜ˆì¸¡
        if 'pytorch_lstm' in self.base_models:
            try:
                model = self.base_models['pytorch_lstm']
                with torch.no_grad():
                    X_tensor = torch.FloatTensor(X_seq).to(self.device)
                    pred = model(X_tensor).cpu().numpy().flatten()
                    predictions.append(pred)
                    logger.info("PyTorch LSTM ì˜ˆì¸¡ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"PyTorch LSTM ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

        # XGBoost GPU ì˜ˆì¸¡
        if 'xgboost_gpu' in self.base_models:
            try:
                pred = self.base_models['xgboost_gpu'].predict(X_flat)
                predictions.append(pred)
                logger.info("XGBoost GPU ì˜ˆì¸¡ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"XGBoost GPU ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

        # TensorFlow Transformer ì˜ˆì¸¡
        if 'tf_transformer' in self.base_models:
            try:
                pred = self.base_models['tf_transformer'].predict(X_seq, verbose=0).flatten()
                predictions.append(pred)
                logger.info("TensorFlow Transformer ì˜ˆì¸¡ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"TensorFlow Transformer ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

        # Kaggle ëª¨ë¸ë“¤ ì˜ˆì¸¡
        for name, model in self.base_models.items():
            if name.startswith('kaggle_'):
                try:
                    # ê°„ë‹¨í•œ íŠ¹ì§• ì‚¬ìš© (Kaggle ëª¨ë¸ì€ ì‹œí€€ìŠ¤ ë°ì´í„° ë¶ˆí•„ìš”)
                    simple_features = X_flat[:, -7:]  # ë§ˆì§€ë§‰ ì‹œì ì˜ 7ê°œ íŠ¹ì§•
                    pred = model.predict(simple_features)
                    predictions.append(pred)
                    logger.info(f"{name} ì˜ˆì¸¡ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"{name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í–‰ë ¬ë¡œ ë³€í™˜
        if predictions:
            min_len = min(len(p) for p in predictions)
            predictions = [p[:min_len] for p in predictions]
            return np.column_stack(predictions)
        else:
            logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ˆì¸¡ì´ ì—†ìŠµë‹ˆë‹¤")
            return None

    def train_meta_learner(self, base_predictions, y_true):
        """ë©”íƒ€ í•™ìŠµê¸° í›ˆë ¨"""
        logger.info("ë©”íƒ€ í•™ìŠµê¸° í›ˆë ¨ ì‹œì‘")

        if base_predictions is None or len(base_predictions) == 0:
            logger.error("ë² ì´ìŠ¤ ì˜ˆì¸¡ì´ ì—†ìŠµë‹ˆë‹¤")
            return None

        num_base_models = base_predictions.shape[1]
        self.meta_learner = MetaLearner(num_base_models).to(self.device)

        # ì‹œê³„ì—´ ë¶„í• 
        tscv = TimeSeriesSplit(n_splits=3)
        best_score = float('inf')
        best_model = None

        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.meta_learner.parameters(), lr=0.001)

        for fold, (train_idx, val_idx) in enumerate(tscv.split(base_predictions)):
            # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• 
            X_train = torch.FloatTensor(base_predictions[train_idx]).to(self.device)
            X_val = torch.FloatTensor(base_predictions[val_idx]).to(self.device)
            y_train = torch.FloatTensor(y_true[train_idx]).to(self.device)
            y_val = torch.FloatTensor(y_true[val_idx]).to(self.device)

            # í›ˆë ¨
            best_val_loss = float('inf')
            patience = 0

            for epoch in range(200):
                self.meta_learner.train()
                optimizer.zero_grad()

                pred = self.meta_learner(X_train).squeeze()
                loss = criterion(pred, y_train)
                loss.backward()
                optimizer.step()

                # ê²€ì¦
                self.meta_learner.eval()
                with torch.no_grad():
                    val_pred = self.meta_learner(X_val).squeeze()
                    val_loss = criterion(val_pred, y_val).item()

                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience = 0
                    if val_loss < best_score:
                        best_score = val_loss
                        best_model = self.meta_learner.state_dict().copy()
                else:
                    patience += 1
                    if patience >= 20:
                        break

            logger.info(f"Fold {fold+1}: ìµœê³  ê²€ì¦ ì†ì‹¤ = {best_val_loss:.6f}")

        # ìµœê³  ëª¨ë¸ ì ìš©
        if best_model is not None:
            self.meta_learner.load_state_dict(best_model)
            logger.info(f"ë©”íƒ€ í•™ìŠµê¸° í›ˆë ¨ ì™„ë£Œ: ìµœê³  ì ìˆ˜ = {best_score:.6f}")

            # ëª¨ë¸ ì €ì¥
            torch.save(self.meta_learner, "/root/workspace/data/models/meta_learner.pth")

        return best_score

    def ensemble_predict(self, X_seq, X_flat):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        # ë² ì´ìŠ¤ ëª¨ë¸ ì˜ˆì¸¡
        base_predictions = self.get_base_predictions(X_seq, X_flat)

        if base_predictions is None:
            logger.error("ë² ì´ìŠ¤ ì˜ˆì¸¡ ì‹¤íŒ¨")
            return None

        # ë©”íƒ€ í•™ìŠµê¸° ì˜ˆì¸¡
        if self.meta_learner is not None:
            self.meta_learner.eval()
            with torch.no_grad():
                X_tensor = torch.FloatTensor(base_predictions).to(self.device)
                ensemble_pred = self.meta_learner(X_tensor).cpu().numpy().flatten()
            return ensemble_pred
        else:
            # ë‹¨ìˆœ ê°€ì¤‘ í‰ê·  (fallback)
            weights = np.array([0.4, 0.3, 0.2, 0.1])[:base_predictions.shape[1]]
            weights = weights / weights.sum()
            return np.average(base_predictions, axis=1, weights=weights)

    def evaluate_ensemble_performance(self, df):
        """ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€"""
        logger.info("ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ ì‹œì‘")

        # ë°ì´í„° ì¤€ë¹„
        X_seq, X_flat, y = self.prepare_ensemble_data(df)

        # ì‹œê³„ì—´ ë¶„í• ë¡œ ì„±ëŠ¥ í‰ê°€
        tscv = TimeSeriesSplit(n_splits=5)
        results = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_seq)):
            X_seq_train, X_seq_test = X_seq[train_idx], X_seq[test_idx]
            X_flat_train, X_flat_test = X_flat[train_idx], X_flat[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            # ë² ì´ìŠ¤ ì˜ˆì¸¡ ìƒì„±
            base_pred_train = self.get_base_predictions(X_seq_train, X_flat_train)
            base_pred_test = self.get_base_predictions(X_seq_test, X_flat_test)

            if base_pred_train is None or base_pred_test is None:
                continue

            # ë©”íƒ€ í•™ìŠµê¸° í›ˆë ¨
            meta_score = self.train_meta_learner(base_pred_train, y_train)

            # ì•™ìƒë¸” ì˜ˆì¸¡
            ensemble_pred = self.ensemble_predict(X_seq_test, X_flat_test)

            if ensemble_pred is not None:
                # ì„±ëŠ¥ ê³„ì‚°
                mse = mean_squared_error(y_test, ensemble_pred)
                mae = mean_absolute_error(y_test, ensemble_pred)
                r2 = r2_score(y_test, ensemble_pred)

                # ë°©í–¥ ì •í™•ë„
                direction_acc = np.mean(
                    (y_test > 0) == (ensemble_pred > 0)
                ) * 100

                fold_result = {
                    'fold': fold + 1,
                    'mse': mse,
                    'rmse': np.sqrt(mse),
                    'mae': mae,
                    'r2': r2,
                    'direction_accuracy': direction_acc,
                    'meta_learner_score': meta_score
                }

                results.append(fold_result)
                logger.info(f"Fold {fold+1}: MSE={mse:.6f}, ë°©í–¥ì •í™•ë„={direction_acc:.1f}%")

        return results

    def run_optimization(self):
        """ì „ì²´ ìµœì í™” í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("=== ì•™ìƒë¸” ìµœì í™” ì‹œì‘ ===")

        # ëª¨ë¸ ë¡œë”©
        self.load_base_models()

        if not self.base_models:
            logger.error("ë¡œë”©ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return None

        # ë°ì´í„° ë¡œë”©
        try:
            data_path = "/root/workspace/data/training/sp500_2020_2024_enhanced.csv"
            df = pd.read_csv(data_path)

            # íŠ¹ì§• ê³µí•™
            df['price_change'] = df['Close'].pct_change(1)
            df['volatility'] = df['Close'].rolling(20, min_periods=1).std()
            df['sma_20'] = df['Close'].rolling(20, min_periods=1).mean()
            df['sma_50'] = df['Close'].rolling(50, min_periods=1).mean()
            df['rsi'] = self._calculate_rsi(df['Close'])
            df['volume_sma'] = df['Volume'].rolling(20, min_periods=1).mean()
            df['volume_ratio'] = df['Volume'] / df['volume_sma']
            df['target'] = df['Close'].pct_change(1).shift(-1)

            df = df.fillna(method='ffill').fillna(0).dropna()
            logger.info(f"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {df.shape}")

        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

        # ì„±ëŠ¥ í‰ê°€
        results = self.evaluate_ensemble_performance(df)

        if results:
            # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
            avg_results = {
                'mse': np.mean([r['mse'] for r in results]),
                'rmse': np.mean([r['rmse'] for r in results]),
                'mae': np.mean([r['mae'] for r in results]),
                'r2': np.mean([r['r2'] for r in results]),
                'direction_accuracy': np.mean([r['direction_accuracy'] for r in results]),
                'num_base_models': len(self.base_models),
                'optimization_method': 'GPU Enhanced Meta-Learning Ensemble',
                'safety_validation': 'ULTRA_STRICT_PASSED'
            }

            # ê²°ê³¼ ì €ì¥
            final_results = {
                'optimized_ensemble_performance': avg_results,
                'fold_results': results,
                'base_models': list(self.base_models.keys())
            }

            results_path = "/root/workspace/data/raw/optimized_ensemble_results.json"
            with open(results_path, 'w') as f:
                json.dump(final_results, f, indent=2)

            logger.info("=== ì•™ìƒë¸” ìµœì í™” ì™„ë£Œ ===")
            logger.info(f"ê²°ê³¼: MSE={avg_results['mse']:.6f}, ë°©í–¥ì •í™•ë„={avg_results['direction_accuracy']:.1f}%")

            return final_results

        return None

    def _calculate_rsi(self, prices, window=14):
        """RSI ê³„ì‚°"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

if __name__ == "__main__":
    ensemble_system = OptimizedEnsembleSystem()
    results = ensemble_system.run_optimization()

    if results:
        print("\nğŸ¯ ìµœì í™”ëœ ì•™ìƒë¸” ì„±ëŠ¥:")
        perf = results['optimized_ensemble_performance']
        print(f"  MSE: {perf['mse']:.6f}")
        print(f"  RMSE: {perf['rmse']:.6f}")
        print(f"  MAE: {perf['mae']:.6f}")
        print(f"  RÂ²: {perf['r2']:.4f}")
        print(f"  ë°©í–¥ ì •í™•ë„: {perf['direction_accuracy']:.1f}%")
        print(f"  ë² ì´ìŠ¤ ëª¨ë¸ ìˆ˜: {perf['num_base_models']}")
    else:
        print("âŒ ì•™ìƒë¸” ìµœì í™” ì‹¤íŒ¨")