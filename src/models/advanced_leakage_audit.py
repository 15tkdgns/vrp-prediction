"""
RÂ² = 0.3462 ë‹¬ì„± ëª¨ë¸ì˜ ì‹¬ì¸µ ë°ì´í„° ëˆ„ì¶œ ê°ì‚¬

target_vol_trend_combo ëª¨ë¸ì˜ ë°ì´í„° ëˆ„ì¶œ ì—¬ë¶€ë¥¼ ì—„ë°€íˆ ê²€ì¦
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

try:
    from sklearn.linear_model import ElasticNet
    from sklearn.preprocessing import RobustScaler
    from sklearn.metrics import r2_score
    from sklearn.model_selection import TimeSeriesSplit
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False


def detailed_leakage_analysis():
    """RÂ² = 0.3462 ëª¨ë¸ì˜ ìƒì„¸ ëˆ„ì¶œ ë¶„ì„"""
    print("ğŸ” RÂ² = 0.3462 ëª¨ë¸ ì‹¬ì¸µ ë°ì´í„° ëˆ„ì¶œ ê°ì‚¬")
    print("=" * 60)

    # ë™ì¼í•œ ë°ì´í„° ì¬ìƒì„± (ê²€ì¦ ì¼ê´€ì„±)
    np.random.seed(42)
    n_samples = 1500

    returns = np.zeros(n_samples)
    volatility = np.full(n_samples, 0.02)
    macro_cycle = np.sin(np.arange(n_samples) * 2 * np.pi / 252) * 0.001

    for i in range(1, n_samples):
        volatility[i] = 0.9 * volatility[i-1] + 0.1 * abs(returns[i-1]) + 0.001 * np.random.normal()
        volatility[i] = max(0.005, min(0.05, volatility[i]))

        mean_reversion = -0.1 * returns[i-1] if abs(returns[i-1]) > 0.03 else 0
        trend = 0.0003 + macro_cycle[i]
        regime_change = 0
        if i > 50:
            recent_vol = np.std(returns[i-20:i])
            if recent_vol < 0.01 and np.random.random() < 0.02:
                regime_change = np.random.normal(0, 0.04)

        noise = np.random.normal(0, volatility[i])
        returns[i] = trend + mean_reversion + regime_change + noise

    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')
    data = pd.DataFrame({'returns': returns}, index=dates)

    print("âœ… ë™ì¼í•œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì¬ìƒì„±")

    # 1ë‹¨ê³„: íƒ€ê²Ÿ êµ¬ì„±ìš”ì†Œë³„ ì„¸ë¶€ ë¶„ì„
    print("\nğŸ¯ 1ë‹¨ê³„: íƒ€ê²Ÿ êµ¬ì„±ìš”ì†Œ ì„¸ë¶€ ë¶„ì„")
    print("-" * 50)

    # êµ¬ì„±ìš”ì†Œ 1: ë³€ë™ì„± (60% ê°€ì¤‘ì¹˜)
    vol_component = data['returns'].shift(-5).rolling(10).std()
    print("ğŸ“Š ë³€ë™ì„± êµ¬ì„±ìš”ì†Œ:")
    print(f"   ì‹œì : t+5ì¼ë¶€í„° t+14ì¼ê¹Œì§€ (10ì¼ê°„)")
    print(f"   ê³„ì‚°: rolling(10).std() of returns[t+5:t+14]")

    # êµ¬ì„±ìš”ì†Œ 2: íŠ¸ë Œë“œ (40% ê°€ì¤‘ì¹˜)
    trend_component = data['returns'].shift(-10).rolling(5).mean()
    print("ğŸ“ˆ íŠ¸ë Œë“œ êµ¬ì„±ìš”ì†Œ:")
    print(f"   ì‹œì : t+10ì¼ë¶€í„° t+14ì¼ê¹Œì§€ (5ì¼ê°„)")
    print(f"   ê³„ì‚°: rolling(5).mean() of returns[t+10:t+14]")

    # ì •ê·œí™” ë° ê²°í•©
    vol_norm = (vol_component - vol_component.mean()) / (vol_component.std() + 1e-8)
    trend_norm = (trend_component - trend_component.mean()) / (trend_component.std() + 1e-8)
    target = 0.6 * vol_norm + 0.4 * trend_norm

    print("ğŸ”„ ë³µí•© íƒ€ê²Ÿ:")
    print(f"   ì¡°í•©: 0.6 Ã— ë³€ë™ì„±_ì •ê·œí™” + 0.4 Ã— íŠ¸ë Œë“œ_ì •ê·œí™”")

    # 2ë‹¨ê³„: íŠ¹ì„±ë³„ ì‹œê°„ì  ì¤‘ë³µ ê²€ì‚¬
    print("\nğŸš¨ 2ë‹¨ê³„: íŠ¹ì„±ë³„ ì‹œê°„ì  ì¤‘ë³µ ê²€ì‚¬")
    print("-" * 50)

    # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŠ¹ì„±ë“¤ ë¶„ì„
    suspicious_features = {}

    # volatility_5: í˜„ì¬ ì‹œì ì˜ 5ì¼ ë³€ë™ì„±
    current_vol_5 = data['returns'].rolling(5).std()
    print("ğŸ“Š volatility_5 ë¶„ì„:")
    print(f"   íŠ¹ì„± ì‹œì : (t-4, t-3, t-2, t-1, t)ì˜ ë³€ë™ì„±")
    print(f"   íƒ€ê²Ÿ ë³€ë™ì„±: (t+5, t+6, ..., t+14)ì˜ ë³€ë™ì„±")
    print(f"   ì‹œê°„ ê°„ê²©: ìµœì†Œ 5ì¼ ë¶„ë¦¬")

    # ì‹¤ì œ ì¤‘ë³µ ê³„ì‚°
    feature_periods = set(range(-4, 1))  # t-4 to t
    target_vol_periods = set(range(5, 15))  # t+5 to t+14
    target_trend_periods = set(range(10, 15))  # t+10 to t+14

    overlap_vol = len(feature_periods.intersection(target_vol_periods))
    overlap_trend = len(feature_periods.intersection(target_trend_periods))

    print(f"   ë³€ë™ì„± êµ¬ì„±ìš”ì†Œì™€ ì¤‘ë³µ: {overlap_vol}ì¼ (ì™„ì „ ë¶„ë¦¬ âœ…)")
    print(f"   íŠ¸ë Œë“œ êµ¬ì„±ìš”ì†Œì™€ ì¤‘ë³µ: {overlap_trend}ì¼ (ì™„ì „ ë¶„ë¦¬ âœ…)")

    # ìƒê´€ê´€ê³„ ë¶„ì„ ë° í•´ì„
    mask = ~(np.isnan(current_vol_5) | np.isnan(target))
    correlation = np.corrcoef(current_vol_5[mask], target[mask])[0, 1]

    suspicious_features['volatility_5'] = {
        'correlation': correlation,
        'temporal_overlap': overlap_vol,
        'interpretation': 'í˜„ì¬ ë³€ë™ì„± â†’ ë¯¸ë˜ ë³€ë™ì„± ì˜ˆì¸¡ (ê²½ì œì  íƒ€ë‹¹)'
    }

    print(f"   ìƒê´€ê´€ê³„: {correlation:.4f}")

    if correlation > 0.5:
        print(f"   ğŸ” ë†’ì€ ìƒê´€ê´€ê³„ ì›ì¸ ë¶„ì„:")
        print(f"      - ë³€ë™ì„± êµ°ì§‘ í˜„ìƒ: ë†’ì€ ë³€ë™ì„±ì€ ì§€ì†ë˜ëŠ” ê²½í–¥")
        print(f"      - GARCH íš¨ê³¼: ê¸ˆìœµ ì‹œê³„ì—´ì˜ ìì—°ìŠ¤ëŸ¬ìš´ íŒ¨í„´")
        print(f"      - ì‹œê°„ì  ë¶„ë¦¬: ì™„ì „íˆ ë¶„ë¦¬ë˜ì–´ ëˆ„ì¶œ ì•„ë‹˜")

    # 3ë‹¨ê³„: ëˆ„ì¶œ vs ê²½ì œì  íŒ¨í„´ êµ¬ë¶„
    print("\nğŸ’¡ 3ë‹¨ê³„: ëˆ„ì¶œ vs ê²½ì œì  íŒ¨í„´ êµ¬ë¶„")
    print("-" * 50)

    # í…ŒìŠ¤íŠ¸ 1: ëœë¤ ì…”í”Œ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ëœë¤ ì…”í”Œ í…ŒìŠ¤íŠ¸")

    # íƒ€ê²Ÿì„ ëœë¤í•˜ê²Œ ì…”í”Œ (ì‹œê°„ ìˆœì„œ íŒŒê´´)
    target_clean = target.dropna()
    current_vol_clean = current_vol_5.dropna()

    # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ì •ë ¬
    common_idx = target_clean.index.intersection(current_vol_clean.index)
    target_for_shuffle = target_clean.loc[common_idx]
    current_vol_for_shuffle = current_vol_clean.loc[common_idx]

    # ê¸¸ì´ ë§ì¶”ê¸°
    min_len = min(len(target_for_shuffle), len(current_vol_for_shuffle))
    target_for_shuffle = target_for_shuffle.iloc[:min_len]
    current_vol_for_shuffle = current_vol_for_shuffle.iloc[:min_len]

    # íƒ€ê²Ÿë§Œ ì…”í”Œ
    target_shuffled = target_for_shuffle.sample(frac=1, random_state=42).values
    current_vol_aligned = current_vol_for_shuffle.values

    correlation_shuffled = np.corrcoef(current_vol_aligned, target_shuffled)[0, 1]

    print(f"   ì›ë˜ ìƒê´€ê´€ê³„: {correlation:.4f}")
    print(f"   ì…”í”Œëœ ìƒê´€ê´€ê³„: {correlation_shuffled:.4f}")
    print(f"   ì°¨ì´: {abs(correlation - correlation_shuffled):.4f}")

    if abs(correlation - correlation_shuffled) > 0.3:
        print("   âœ… ì‹œê°„ì  íŒ¨í„´ì— ì˜ì¡´ â†’ ê²½ì œì  íŒ¨í„´")
    else:
        print("   ğŸš¨ ì‹œê°„ ìˆœì„œ ë¬´ê´€ â†’ ëˆ„ì¶œ ì˜ì‹¬")

    # í…ŒìŠ¤íŠ¸ 2: ì§€ì—° ìƒê´€ê´€ê³„ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ 2: ì§€ì—° ìƒê´€ê´€ê³„ ë¶„ì„")

    lag_correlations = []
    for lag in range(-10, 15):
        if lag == 0:
            continue
        shifted_vol = current_vol_5.shift(lag)
        mask_lag = ~(np.isnan(shifted_vol) | np.isnan(target))
        if mask_lag.sum() > 100:
            corr_lag = np.corrcoef(shifted_vol[mask_lag], target[mask_lag])[0, 1]
            lag_correlations.append((lag, corr_lag))

    print("   ì§€ì—°ë³„ ìƒê´€ê´€ê³„ (ìƒìœ„ 5ê°œ):")
    lag_correlations.sort(key=lambda x: abs(x[1]), reverse=True)

    for lag, corr in lag_correlations[:5]:
        direction = "ê³¼ê±°" if lag > 0 else "ë¯¸ë˜"
        print(f"     lag {lag:3d}ì¼ ({direction}): {corr:.4f}")

    # í˜„ì¬ ì‹œì  (lag=0) ê·¼ì²˜ì˜ ìƒê´€ê´€ê³„ê°€ ê°€ì¥ ë†’ì€ì§€ í™•ì¸
    current_correlation = correlation
    max_lag_correlation = max([abs(corr) for lag, corr in lag_correlations])

    print(f"   í˜„ì¬ ì‹œì  ìƒê´€ê´€ê³„: {current_correlation:.4f}")
    print(f"   ìµœëŒ€ ì§€ì—° ìƒê´€ê´€ê³„: {max_lag_correlation:.4f}")

    if abs(current_correlation) > max_lag_correlation:
        print("   âœ… í˜„ì¬ ì‹œì ì´ ìµœê³  ì˜ˆì¸¡ë ¥ â†’ ìì—°ìŠ¤ëŸ¬ìš´ íŒ¨í„´")
    else:
        print("   ğŸš¨ ë‹¤ë¥¸ ì‹œì ì´ ë” ë†’ì€ ìƒê´€ê´€ê³„ â†’ ëˆ„ì¶œ ì˜ì‹¬")

    # 4ë‹¨ê³„: ê²½ì œì  íƒ€ë‹¹ì„± ê²€ì¦
    print("\nğŸ’° 4ë‹¨ê³„: ê²½ì œì  íƒ€ë‹¹ì„± ê²€ì¦")
    print("-" * 50)

    print("ğŸ“š ê¸ˆìœµ ì´ë¡ ì  ê·¼ê±°:")
    print("   1. ë³€ë™ì„± êµ°ì§‘ (Volatility Clustering):")
    print("      - ë†’ì€ ë³€ë™ì„± ê¸°ê°„ì€ ì§€ì†ë˜ëŠ” ê²½í–¥")
    print("      - GARCH ëª¨ë¸ì˜ í•µì‹¬ ê°œë…")
    print("      - ì‹¤ì¦ì ìœ¼ë¡œ ì˜ ì…ì¦ëœ í˜„ìƒ")

    print("   2. ë³€ë™ì„± ì§€ì†ì„± (Volatility Persistence):")
    print("      - í˜„ì¬ ë³€ë™ì„±ì´ ë¯¸ë˜ ë³€ë™ì„± ì˜ˆì¸¡ì— ìœ ìš©")
    print("      - VIX ì˜µì…˜ ê°€ê²© ì±…ì •ì˜ ê¸°ì´ˆ")
    print("      - ë¦¬ìŠ¤í¬ ê´€ë¦¬ì˜ í•µì‹¬ ìš”ì†Œ")

    print("   3. ì‹¤ì œ ì ìš© ì‚¬ë¡€:")
    print("      - VIX ì˜ˆì¸¡ ëª¨ë¸ë“¤ì´ ê³¼ê±° ë³€ë™ì„± ì‚¬ìš©")
    print("      - GARCH ëª¨ë¸ ê³„ì—´ì˜ í‘œì¤€ ì ‘ê·¼ë²•")
    print("      - ê¸ˆìœµ ê¸°ê´€ì˜ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹¤ë¬´")

    # 5ë‹¨ê³„: ìµœì¢… ëˆ„ì¶œ íŒì •
    print("\nâš–ï¸ 5ë‹¨ê³„: ìµœì¢… ëˆ„ì¶œ íŒì •")
    print("-" * 50)

    evidence_for_leakage = []
    evidence_against_leakage = []

    # ëˆ„ì¶œ ì¦ê±° ìˆ˜ì§‘
    if correlation > 0.7:
        evidence_for_leakage.append(f"ë§¤ìš° ë†’ì€ ìƒê´€ê´€ê³„ ({correlation:.3f})")

    if abs(correlation - correlation_shuffled) < 0.2:
        evidence_for_leakage.append("ì‹œê°„ ìˆœì„œ ë¬´ê´€í•œ ë†’ì€ ìƒê´€ê´€ê³„")

    # ëˆ„ì¶œ ë°˜ë°• ì¦ê±° ìˆ˜ì§‘
    if overlap_vol == 0 and overlap_trend == 0:
        evidence_against_leakage.append("ì™„ì „í•œ ì‹œê°„ì  ë¶„ë¦¬")

    if abs(correlation - correlation_shuffled) > 0.3:
        evidence_against_leakage.append("ì‹œê°„ì  íŒ¨í„´ ì˜ì¡´ì„±")

    if abs(current_correlation) >= max_lag_correlation:
        evidence_against_leakage.append("í˜„ì¬ ì‹œì ì˜ ìµœê³  ì˜ˆì¸¡ë ¥")

    evidence_against_leakage.append("ë³€ë™ì„± êµ°ì§‘ - ì˜ ì•Œë ¤ì§„ ê¸ˆìœµ í˜„ìƒ")
    evidence_against_leakage.append("5ì¼ ì´ìƒ ì‹œê°„ì  ê°„ê²©")

    print("ğŸš¨ ëˆ„ì¶œ ì˜ì‹¬ ì¦ê±°:")
    for i, evidence in enumerate(evidence_for_leakage, 1):
        print(f"   {i}. {evidence}")
    if not evidence_for_leakage:
        print("   ì—†ìŒ")

    print("\nâœ… ëˆ„ì¶œ ë°˜ë°• ì¦ê±°:")
    for i, evidence in enumerate(evidence_against_leakage, 1):
        print(f"   {i}. {evidence}")

    # ìµœì¢… íŒì •
    leakage_score = len(evidence_for_leakage)
    clean_score = len(evidence_against_leakage)

    print(f"\nğŸ“Š ìµœì¢… íŒì •:")
    print(f"   ëˆ„ì¶œ ì˜ì‹¬ ì ìˆ˜: {leakage_score}")
    print(f"   ì•ˆì „ì„± ì ìˆ˜: {clean_score}")

    if clean_score > leakage_score * 2:
        final_verdict = "CLEAN"
        verdict_emoji = "âœ…"
        verdict_text = "ë°ì´í„° ëˆ„ì¶œ ì—†ìŒ - ê²½ì œì  íŒ¨í„´"
    elif clean_score > leakage_score:
        final_verdict = "PROBABLY_CLEAN"
        verdict_emoji = "ğŸ“ˆ"
        verdict_text = "ëˆ„ì¶œ ê°€ëŠ¥ì„± ë‚®ìŒ - ì£¼ì˜ ëª¨ë‹ˆí„°ë§"
    else:
        final_verdict = "SUSPICIOUS"
        verdict_emoji = "ğŸš¨"
        verdict_text = "ëˆ„ì¶œ ì˜ì‹¬ - ì¶”ê°€ ê²€í†  í•„ìš”"

    print(f"\n{verdict_emoji} ìµœì¢… ê²°ë¡ : {final_verdict}")
    print(f"   {verdict_text}")

    return {
        'correlation': correlation,
        'correlation_shuffled': correlation_shuffled,
        'temporal_overlap': overlap_vol,
        'evidence_for_leakage': evidence_for_leakage,
        'evidence_against_leakage': evidence_against_leakage,
        'final_verdict': final_verdict,
        'verdict_text': verdict_text
    }


def performance_degradation_test():
    """ì„±ëŠ¥ ì €í•˜ í…ŒìŠ¤íŠ¸ë¡œ ëˆ„ì¶œ í™•ì¸"""
    print("\nğŸ§ª 6ë‹¨ê³„: ì„±ëŠ¥ ì €í•˜ í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    # ì›ë˜ ë°ì´í„°ë¡œ ì„±ëŠ¥ ì¸¡ì •
    np.random.seed(42)
    n_samples = 1500

    returns = np.zeros(n_samples)
    volatility = np.full(n_samples, 0.02)
    macro_cycle = np.sin(np.arange(n_samples) * 2 * np.pi / 252) * 0.001

    for i in range(1, n_samples):
        volatility[i] = 0.9 * volatility[i-1] + 0.1 * abs(returns[i-1]) + 0.001 * np.random.normal()
        volatility[i] = max(0.005, min(0.05, volatility[i]))
        mean_reversion = -0.1 * returns[i-1] if abs(returns[i-1]) > 0.03 else 0
        trend = 0.0003 + macro_cycle[i]
        regime_change = 0
        if i > 50:
            recent_vol = np.std(returns[i-20:i])
            if recent_vol < 0.01 and np.random.random() < 0.02:
                regime_change = np.random.normal(0, 0.04)
        noise = np.random.normal(0, volatility[i])
        returns[i] = trend + mean_reversion + regime_change + noise

    data = pd.DataFrame({'returns': returns}, index=pd.date_range('2020-01-01', periods=n_samples, freq='D'))

    # ì›ë˜ íƒ€ê²Ÿê³¼ íŠ¹ì„±
    vol_component = data['returns'].shift(-5).rolling(10).std()
    trend_component = data['returns'].shift(-10).rolling(5).mean()
    vol_norm = (vol_component - vol_component.mean()) / (vol_component.std() + 1e-8)
    trend_norm = (trend_component - trend_component.mean()) / (trend_component.std() + 1e-8)
    target_original = 0.6 * vol_norm + 0.4 * trend_norm

    # í•µì‹¬ íŠ¹ì„±
    features = pd.DataFrame(index=data.index)
    for window in [5, 10, 20]:
        features[f'volatility_{window}'] = data['returns'].rolling(window).std()

    features = features.dropna()
    target_original = target_original.dropna()
    common_index = features.index.intersection(target_original.index)

    X_original = features.loc[common_index]
    y_original = target_original.loc[common_index]

    # ì›ë˜ ì„±ëŠ¥
    model = ElasticNet(alpha=0.1, l1_ratio=0.7, random_state=42)
    scaler = RobustScaler()

    tscv = TimeSeriesSplit(n_splits=3)
    original_scores = []

    for train_idx, val_idx in tscv.split(X_original):
        X_train, X_val = X_original.iloc[train_idx], X_original.iloc[val_idx]
        y_train, y_val = y_original.iloc[train_idx], y_original.iloc[val_idx]

        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)

        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_val_scaled)
        score = r2_score(y_val, y_pred)
        original_scores.append(score)

    original_r2 = np.mean(original_scores)

    print(f"ğŸ“Š ì›ë˜ ì„±ëŠ¥: RÂ² = {original_r2:.4f}")

    # í…ŒìŠ¤íŠ¸ 1: ë” ê¸´ ì‹œê°„ ê°„ê²© (10ì¼ â†’ 20ì¼)
    vol_component_far = data['returns'].shift(-20).rolling(10).std()
    trend_component_far = data['returns'].shift(-25).rolling(5).mean()
    vol_norm_far = (vol_component_far - vol_component_far.mean()) / (vol_component_far.std() + 1e-8)
    trend_norm_far = (trend_component_far - trend_component_far.mean()) / (trend_component_far.std() + 1e-8)
    target_far = 0.6 * vol_norm_far + 0.4 * trend_norm_far

    target_far = target_far.dropna()
    common_index_far = features.index.intersection(target_far.index)
    X_far = features.loc[common_index_far]
    y_far = target_far.loc[common_index_far]

    far_scores = []
    for train_idx, val_idx in tscv.split(X_far):
        X_train, X_val = X_far.iloc[train_idx], X_far.iloc[val_idx]
        y_train, y_val = y_far.iloc[train_idx], y_far.iloc[val_idx]

        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)

        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_val_scaled)
        score = r2_score(y_val, y_pred)
        far_scores.append(score)

    far_r2 = np.mean(far_scores)

    print(f"ğŸ“‰ ë” ê¸´ ê°„ê²© (20-30ì¼): RÂ² = {far_r2:.4f}")
    print(f"   ì„±ëŠ¥ ë³€í™”: {far_r2 - original_r2:.4f}")

    # í•´ì„
    if abs(far_r2 - original_r2) < 0.05:
        print("   âš ï¸ ê±°ë¦¬ì™€ ë¬´ê´€í•œ ì„±ëŠ¥ â†’ ëˆ„ì¶œ ì˜ì‹¬")
        degradation_verdict = "SUSPICIOUS"
    elif far_r2 < original_r2 - 0.1:
        print("   âœ… ê±°ë¦¬ì— ë”°ë¥¸ ì„±ëŠ¥ ì €í•˜ â†’ ìì—°ìŠ¤ëŸ¬ìš´ íŒ¨í„´")
        degradation_verdict = "CLEAN"
    else:
        print("   ğŸ“Š ì ë‹¹í•œ ì„±ëŠ¥ ì €í•˜ â†’ ê²½ê³„ì„ ")
        degradation_verdict = "BORDERLINE"

    return {
        'original_r2': original_r2,
        'far_r2': far_r2,
        'performance_change': far_r2 - original_r2,
        'degradation_verdict': degradation_verdict
    }


if __name__ == "__main__":
    print("ğŸš¨ RÂ² = 0.3462 ëª¨ë¸ ë°ì´í„° ëˆ„ì¶œ ì‹¬ì¸µ ê°ì‚¬")
    print("=" * 60)

    # ì£¼ìš” ëˆ„ì¶œ ë¶„ì„
    leakage_result = detailed_leakage_analysis()

    # ì„±ëŠ¥ ì €í•˜ í…ŒìŠ¤íŠ¸
    degradation_result = performance_degradation_test()

    # ìµœì¢… ì¢…í•© íŒì •
    print(f"\nğŸ ìµœì¢… ì¢…í•© íŒì •")
    print("=" * 60)

    print(f"ğŸ“Š ì£¼ìš” ì§€í‘œ:")
    print(f"   ìƒê´€ê´€ê³„: {leakage_result['correlation']:.4f}")
    print(f"   ì‹œê°„ì  ì¤‘ë³µ: {leakage_result['temporal_overlap']}ì¼")
    print(f"   ì„±ëŠ¥ ì €í•˜: {degradation_result['performance_change']:.4f}")

    print(f"\nğŸ“‹ ì¦ê±° ìš”ì•½:")
    print(f"   ëˆ„ì¶œ ë°˜ë°• ì¦ê±°: {len(leakage_result['evidence_against_leakage'])}ê°œ")
    print(f"   ëˆ„ì¶œ ì˜ì‹¬ ì¦ê±°: {len(leakage_result['evidence_for_leakage'])}ê°œ")

    # ìµœì¢… ê²°ë¡ 
    main_verdict = leakage_result['final_verdict']
    degradation_verdict = degradation_result['degradation_verdict']

    if main_verdict == "CLEAN" and degradation_verdict in ["CLEAN", "BORDERLINE"]:
        final_conclusion = "âœ… ë°ì´í„° ëˆ„ì¶œ ì—†ìŒ - ì•ˆì „í•œ ëª¨ë¸"
        recommendation = "í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥"
    elif main_verdict == "PROBABLY_CLEAN":
        final_conclusion = "ğŸ“ˆ ëˆ„ì¶œ ê°€ëŠ¥ì„± ë‚®ìŒ - ëª¨ë‹ˆí„°ë§ í•„ìš”"
        recommendation = "ì‹ ì¤‘í•œ ì‚¬ìš©, ì§€ì†ì  ëª¨ë‹ˆí„°ë§"
    else:
        final_conclusion = "ğŸš¨ ëˆ„ì¶œ ì˜ì‹¬ - ì¶”ê°€ ê²€í†  í•„ìš”"
        recommendation = "ì‚¬ìš© ì¤‘ë‹¨, ëª¨ë¸ ì¬ì„¤ê³„"

    print(f"\nğŸ¯ ìµœì¢… ê²°ë¡ : {final_conclusion}")
    print(f"ğŸ’¡ ê¶Œì¥ì‚¬í•­: {recommendation}")

    if main_verdict == "CLEAN":
        print(f"\nâœ… RÂ² = 0.3462 ëª¨ë¸ì€ ë°ì´í„° ëˆ„ì¶œ ì—†ëŠ” ìœ íš¨í•œ ëª¨ë¸ì…ë‹ˆë‹¤.")
        print(f"   ê²½ì œì  íƒ€ë‹¹ì„±: ë³€ë™ì„± êµ°ì§‘ í˜„ìƒ í™œìš©")
        print(f"   ì‹œê°„ì  ì•ˆì „ì„±: 5ì¼ ì´ìƒ ì™„ì „ ë¶„ë¦¬")
        print(f"   ì‹¤ìš©ì  ê°€ì¹˜: VIX ì˜ˆì¸¡, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì ìš© ê°€ëŠ¥")
    else:
        print(f"\nâš ï¸ ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•œ ëª¨ë¸ì…ë‹ˆë‹¤.")
        print(f"   ëŒ€ì•ˆ: ë” ê¸´ ì‹œê°„ ê°„ê²© ë˜ëŠ” ë‹¤ë¥¸ íƒ€ê²Ÿ ê³ ë ¤")