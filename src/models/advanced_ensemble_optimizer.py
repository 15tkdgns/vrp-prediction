#!/usr/bin/env python3
"""
ğŸš€ ê³ ë„í™”ëœ ì•™ìƒë¸” ëª¨ë¸ ìµœì í™” ì‹œìŠ¤í…œ

í˜„ì¬ 89.5% ì„±ëŠ¥ì„ 92-94%ë¡œ ê°œì„ í•˜ê¸° ìœ„í•œ ê³ ê¸‰ ì•™ìƒë¸” ê¸°ë²•
- Multi-Level Stacking
- Dynamic Weighted Blending
- Uncertainty-Based Voting
- Time-Aware Ensemble
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

class AdvancedEnsembleOptimizer:
    """ê³ ë„í™”ëœ ì•™ìƒë¸” ìµœì í™” ì‹œìŠ¤í…œ"""

    def __init__(self, random_state=42):
        self.random_state = random_state
        self.base_models = {}
        self.meta_learners = {}
        self.ensemble_weights = {}
        self.performance_history = []

    def initialize_base_models(self):
        """ë‹¤ì–‘í•œ ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™”"""
        self.base_models = {
            'rf_conservative': RandomForestClassifier(
                n_estimators=200, max_depth=8, min_samples_split=20,
                random_state=self.random_state, n_jobs=-1
            ),
            'rf_aggressive': RandomForestClassifier(
                n_estimators=300, max_depth=15, min_samples_split=5,
                random_state=self.random_state, n_jobs=-1
            ),
            'gb_conservative': GradientBoostingClassifier(
                n_estimators=150, learning_rate=0.05, max_depth=6,
                random_state=self.random_state
            ),
            'gb_aggressive': GradientBoostingClassifier(
                n_estimators=200, learning_rate=0.1, max_depth=8,
                random_state=self.random_state
            ),
            'xgb_conservative': xgb.XGBClassifier(
                n_estimators=150, learning_rate=0.05, max_depth=6,
                random_state=self.random_state, eval_metric='logloss'
            ),
            'xgb_aggressive': xgb.XGBClassifier(
                n_estimators=200, learning_rate=0.1, max_depth=8,
                random_state=self.random_state, eval_metric='logloss'
            ),
            'mlp_small': MLPClassifier(
                hidden_layer_sizes=(50, 25), learning_rate_init=0.001,
                max_iter=500, random_state=self.random_state
            ),
            'mlp_large': MLPClassifier(
                hidden_layer_sizes=(100, 50, 25), learning_rate_init=0.001,
                max_iter=500, random_state=self.random_state
            )
        }

        # ë©”íƒ€ ëŸ¬ë„ˆë“¤
        self.meta_learners = {
            'logistic': LogisticRegression(random_state=self.random_state),
            'ridge': RidgeClassifier(random_state=self.random_state),
            'rf_meta': RandomForestClassifier(
                n_estimators=50, max_depth=3, random_state=self.random_state
            )
        }

    def train_base_models(self, X_train, y_train, cv_folds=5):
        """ê¸°ë³¸ ëª¨ë¸ë“¤ í›ˆë ¨ ë° êµì°¨ê²€ì¦"""
        print("ğŸ¤– ê¸°ë³¸ ëª¨ë¸ë“¤ í›ˆë ¨ ì‹œì‘...")

        tscv = TimeSeriesSplit(n_splits=cv_folds)
        base_scores = {}

        for name, model in self.base_models.items():
            print(f"   ğŸ“Š í›ˆë ¨ ì¤‘: {name}")

            # êµì°¨ê²€ì¦ ì ìˆ˜
            cv_scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='accuracy')
            base_scores[name] = {
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'cv_scores': cv_scores
            }

            # ì „ì²´ ë°ì´í„°ë¡œ í›ˆë ¨
            model.fit(X_train, y_train)

            print(f"      âœ… CV ì ìˆ˜: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

        return base_scores

    def generate_meta_features(self, X, y, cv_folds=5):
        """ë©”íƒ€ íŠ¹ì„± ìƒì„± (ìŠ¤íƒœí‚¹ìš©)"""
        print("ğŸ”§ ë©”íƒ€ íŠ¹ì„± ìƒì„± ì¤‘...")

        tscv = TimeSeriesSplit(n_splits=cv_folds)
        meta_features = np.zeros((len(X), len(self.base_models)))

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            print(f"   ğŸ“Š Fold {fold + 1}/{cv_folds}")

            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
            y_fold_train = y.iloc[train_idx]

            for i, (name, model) in enumerate(self.base_models.items()):
                # í´ë“œë³„ë¡œ ìƒˆë¡œìš´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                fold_model = model.__class__(**model.get_params())
                fold_model.fit(X_fold_train, y_fold_train)

                # ê²€ì¦ ì„¸íŠ¸ ì˜ˆì¸¡
                pred_proba = fold_model.predict_proba(X_fold_val)[:, 1]
                meta_features[val_idx, i] = pred_proba

        meta_df = pd.DataFrame(
            meta_features,
            columns=[f'base_{name}' for name in self.base_models.keys()],
            index=X.index
        )

        print(f"   âœ… ë©”íƒ€ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {meta_df.shape}")
        return meta_df

    def train_stacking_ensemble(self, X_train, y_train):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨"""
        print("ğŸ—ï¸ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨...")

        # ë©”íƒ€ íŠ¹ì„± ìƒì„±
        meta_features = self.generate_meta_features(X_train, y_train)

        # ë©”íƒ€ ëŸ¬ë„ˆë“¤ í›ˆë ¨
        stacking_scores = {}
        for name, meta_model in self.meta_learners.items():
            meta_model.fit(meta_features, y_train)

            # ë©”íƒ€ ëª¨ë¸ êµì°¨ê²€ì¦
            tscv = TimeSeriesSplit(n_splits=5)
            cv_scores = cross_val_score(meta_model, meta_features, y_train, cv=tscv, scoring='accuracy')

            stacking_scores[name] = {
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }

            print(f"   ğŸ“Š {name} ë©”íƒ€ëŸ¬ë„ˆ: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

        return stacking_scores

    def dynamic_weighted_blending(self, X_train, y_train, X_test):
        """ë™ì  ê°€ì¤‘ ë¸”ë Œë”©"""
        print("âš–ï¸ ë™ì  ê°€ì¤‘ ë¸”ë Œë”© ìˆ˜í–‰...")

        # ê° ëª¨ë¸ì˜ ìµœê·¼ ì„±ëŠ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        tscv = TimeSeriesSplit(n_splits=5)
        recent_scores = {}

        for name, model in self.base_models.items():
            scores = []
            for train_idx, val_idx in tscv.split(X_train):
                X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
                y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

                fold_model = model.__class__(**model.get_params())
                fold_model.fit(X_fold_train, y_fold_train)
                pred = fold_model.predict(X_fold_val)
                score = accuracy_score(y_fold_val, pred)
                scores.append(score)

            # ìµœê·¼ í´ë“œì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
            weighted_score = np.average(scores, weights=[0.1, 0.15, 0.2, 0.25, 0.3])
            recent_scores[name] = weighted_score

        # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚° (ì†Œí”„íŠ¸ë§¥ìŠ¤)
        scores_array = np.array(list(recent_scores.values()))
        weights = np.exp(scores_array * 10) / np.sum(np.exp(scores_array * 10))

        self.ensemble_weights = dict(zip(recent_scores.keys(), weights))

        print("   ğŸ“Š ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜:")
        for name, weight in self.ensemble_weights.items():
            print(f"      {name}: {weight:.4f}")

        return self.ensemble_weights

    def uncertainty_based_voting(self, X_test):
        """ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ íˆ¬í‘œ"""
        print("ğŸ—³ï¸ ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ íˆ¬í‘œ ìˆ˜í–‰...")

        predictions = {}
        uncertainties = {}

        for name, model in self.base_models.items():
            pred_proba = model.predict_proba(X_test)
            predictions[name] = pred_proba

            # ë¶ˆí™•ì‹¤ì„± ê³„ì‚° (ì—”íŠ¸ë¡œí”¼)
            uncertainty = -np.sum(pred_proba * np.log(pred_proba + 1e-8), axis=1)
            uncertainties[name] = uncertainty

        # ë¶ˆí™•ì‹¤ì„±ì´ ë‚®ì€ ëª¨ë¸ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        final_predictions = np.zeros((len(X_test), 2))

        for i in range(len(X_test)):
            weights = []
            for name in self.base_models.keys():
                # ë¶ˆí™•ì‹¤ì„±ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
                weight = 1.0 / (1.0 + uncertainties[name][i])
                weights.append(weight)

            weights = np.array(weights)
            weights = weights / np.sum(weights)

            # ê°€ì¤‘ í‰ê· 
            for j, name in enumerate(self.base_models.keys()):
                final_predictions[i] += weights[j] * predictions[name][i]

        return final_predictions

    def evaluate_ensemble_methods(self, X_train, y_train, X_test, y_test):
        """ë‹¤ì–‘í•œ ì•™ìƒë¸” ë°©ë²• í‰ê°€"""
        print("ğŸ“Š ì•™ìƒë¸” ë°©ë²•ë“¤ í‰ê°€ ì¤‘...")

        results = {}

        # 1. ë‹¨ìˆœ íˆ¬í‘œ (Voting)
        voting_preds = np.zeros(len(X_test))
        for model in self.base_models.values():
            voting_preds += model.predict(X_test)
        voting_preds = (voting_preds > len(self.base_models) / 2).astype(int)
        results['voting'] = accuracy_score(y_test, voting_preds)

        # 2. í‰ê·  í™•ë¥  (Average Probability)
        avg_proba = np.zeros((len(X_test), 2))
        for model in self.base_models.values():
            avg_proba += model.predict_proba(X_test)
        avg_proba /= len(self.base_models)
        avg_preds = np.argmax(avg_proba, axis=1)
        results['average_probability'] = accuracy_score(y_test, avg_preds)

        # 3. ê°€ì¤‘ ë¸”ë Œë”©
        if self.ensemble_weights:
            weighted_proba = np.zeros((len(X_test), 2))
            for name, model in self.base_models.items():
                weight = self.ensemble_weights.get(name, 1/len(self.base_models))
                weighted_proba += weight * model.predict_proba(X_test)
            weighted_preds = np.argmax(weighted_proba, axis=1)
            results['weighted_blending'] = accuracy_score(y_test, weighted_preds)

        # 4. ìŠ¤íƒœí‚¹ (ìµœê³  ì„±ëŠ¥ ë©”íƒ€ëŸ¬ë„ˆ ì‚¬ìš©)
        meta_features_test = np.zeros((len(X_test), len(self.base_models)))
        for i, model in enumerate(self.base_models.values()):
            meta_features_test[:, i] = model.predict_proba(X_test)[:, 1]

        best_meta_name = max(
            self.meta_learners.keys(),
            key=lambda x: cross_val_score(
                self.meta_learners[x],
                self.generate_meta_features(X_train, y_train),
                y_train, cv=3
            ).mean()
        )

        meta_features_test_df = pd.DataFrame(meta_features_test)
        stacking_preds = self.meta_learners[best_meta_name].predict(meta_features_test_df)
        results['stacking'] = accuracy_score(y_test, stacking_preds)

        # 5. ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ íˆ¬í‘œ
        uncertainty_proba = self.uncertainty_based_voting(X_test)
        uncertainty_preds = np.argmax(uncertainty_proba, axis=1)
        results['uncertainty_voting'] = accuracy_score(y_test, uncertainty_preds)

        return results

    def run_advanced_ensemble_experiment(self, data_path):
        """ê³ ê¸‰ ì•™ìƒë¸” ì‹¤í—˜ ì „ì²´ ì‹¤í–‰"""
        print("ğŸš€ ê³ ê¸‰ ì•™ìƒë¸” ì‹¤í—˜ ì‹œì‘")
        print("="*60)

        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)

        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_cols = [col for col in df.columns if col not in ['Date', 'Returns']]
        X = df[feature_cols].dropna()

        # ë°©í–¥ ì˜ˆì¸¡ì„ ìœ„í•œ íƒ€ê²Ÿ ìƒì„±
        y = (df['Returns'].shift(-1) > 0).astype(int)
        y = y.loc[X.index]  # Xì™€ ì¸ë±ìŠ¤ ë§ì¶¤

        # ìµœì¢… NaN ì œê±°
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask]
        y = y[mask]

        print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {X.shape}")
        print(f"ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {y.value_counts().to_dict()}")

        # ì‹œê³„ì—´ ë¶„í• 
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        print(f"ğŸ“Š í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape}")

        # íŠ¹ì„± ì •ê·œí™”
        scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )

        # ëª¨ë¸ ì´ˆê¸°í™” ë° í›ˆë ¨
        self.initialize_base_models()
        base_scores = self.train_base_models(X_train_scaled, y_train)

        # ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨
        stacking_scores = self.train_stacking_ensemble(X_train_scaled, y_train)

        # ë™ì  ê°€ì¤‘ ë¸”ë Œë”©
        self.dynamic_weighted_blending(X_train_scaled, y_train, X_test_scaled)

        # ì•™ìƒë¸” ë°©ë²•ë“¤ í‰ê°€
        ensemble_results = self.evaluate_ensemble_methods(
            X_train_scaled, y_train, X_test_scaled, y_test
        )

        # ê²°ê³¼ ì •ë¦¬
        print(f"\nğŸ“Š ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥:")
        for name, scores in base_scores.items():
            print(f"   {name}: {scores['cv_mean']:.4f} Â± {scores['cv_std']:.4f}")

        print(f"\nğŸ—ï¸ ì•™ìƒë¸” ë°©ë²• ì„±ëŠ¥:")
        for method, accuracy in ensemble_results.items():
            print(f"   {method}: {accuracy:.4f}")

        # ìµœê³  ì„±ëŠ¥ ë°©ë²• ì°¾ê¸°
        best_method = max(ensemble_results.items(), key=lambda x: x[1])
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_method[0]} = {best_method[1]:.4f}")

        # ê¸°ì¤€ì„  ëŒ€ë¹„ ê°œì„ ëŸ‰
        baseline_accuracy = 0.895  # í˜„ì¬ ê²€ì¦ëœ ìµœê³  ì„±ê³¼
        improvement = (best_method[1] - baseline_accuracy) * 100
        print(f"ğŸ“ˆ ê¸°ì¤€ì„  ëŒ€ë¹„ ê°œì„ : {improvement:+.2f}%p")

        return {
            'base_scores': base_scores,
            'ensemble_results': ensemble_results,
            'best_method': best_method,
            'improvement': improvement,
            'test_accuracy': best_method[1]
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    optimizer = AdvancedEnsembleOptimizer()

    # SPY ë°ì´í„°ë¡œ ì‹¤í—˜
    data_path = "/root/workspace/data/training/sp500_leak_free_dataset.csv"

    results = optimizer.run_advanced_ensemble_experiment(data_path)

    # ê²°ê³¼ ì €ì¥
    import json
    from datetime import datetime

    results_with_metadata = {
        'timestamp': datetime.now().isoformat(),
        'experiment_type': 'advanced_ensemble_optimization',
        'data_source': data_path,
        'baseline_accuracy': 0.895,
        'target_improvement': "92-94%",
        'results': results,
        'validation_status': 'pending_verification'
    }

    output_path = f"/root/workspace/data/results/advanced_ensemble_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_path, 'w') as f:
        json.dump(results_with_metadata, f, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")

    return results

if __name__ == "__main__":
    main()