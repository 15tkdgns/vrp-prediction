"""
ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê°•í•œ ì‹ í˜¸ íƒ€ê²Ÿ ìƒì„±

ê¸ˆìœµì—ì„œ ì‹¤ì œë¡œ ì˜ˆì¸¡ ê°€ëŠ¥ì„±ì´ ë†’ì€ íŒ¨í„´ë“¤ì„ í™œìš©í•œ íƒ€ê²Ÿ ìƒì„±:
1. í‰ê·  íšŒê·€ ì‹ í˜¸
2. ëª¨ë©˜í…€ ì§€ì† ì‹ í˜¸
3. ë³€ë™ì„± ì˜ˆì¸¡ ì‹ í˜¸
4. ê·¹ê°’ ë°˜ì „ ì‹ í˜¸
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

try:
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LinearRegression, Ridge, ElasticNet
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import r2_score, mean_squared_error
    from sklearn.model_selection import TimeSeriesSplit
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False


def create_mean_reversion_data():
    """í‰ê·  íšŒê·€ íŒ¨í„´ì´ ê°•í•œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°"""
    np.random.seed(42)
    n_samples = 800

    # ê°•í•œ í‰ê·  íšŒê·€ ì„±ë¶„
    mean_rev_strength = 0.3  # í‰ê·  íšŒê·€ ê°•ë„
    equilibrium = 0.0

    returns = np.zeros(n_samples)
    cumulative_deviation = 0

    for i in range(1, n_samples):
        # í‰ê· ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ê°•í•œ ë³µì›ë ¥
        reversion_force = -mean_rev_strength * cumulative_deviation
        noise = np.random.normal(0, 0.01)

        returns[i] = reversion_force + noise
        cumulative_deviation += returns[i]

    prices = 100 * np.exp(np.cumsum(returns))

    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')

    return pd.DataFrame({
        'price': prices,
        'log_returns': returns,
        'cumulative_deviation': np.cumsum(returns)
    }, index=dates)


def create_momentum_data():
    """ëª¨ë©˜í…€ íš¨ê³¼ê°€ ê°•í•œ ë°ì´í„°"""
    np.random.seed(42)
    n_samples = 800

    returns = np.zeros(n_samples)
    momentum_factor = 0.2

    for i in range(1, n_samples):
        # ê³¼ê±° 5ì¼ í‰ê·  ìˆ˜ìµë¥ ì— ë¹„ë¡€í•œ ëª¨ë©˜í…€
        if i >= 5:
            past_momentum = np.mean(returns[i-5:i])
            momentum_effect = momentum_factor * past_momentum
        else:
            momentum_effect = 0

        noise = np.random.normal(0, 0.015)
        returns[i] = momentum_effect + noise

    prices = 100 * np.exp(np.cumsum(returns))
    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')

    return pd.DataFrame({
        'price': prices,
        'log_returns': returns
    }, index=dates)


def create_volatility_clustering_data():
    """ë³€ë™ì„± êµ°ì§‘ì´ ê°•í•œ ë°ì´í„°"""
    np.random.seed(42)
    n_samples = 800

    returns = np.zeros(n_samples)
    volatility = 0.02

    for i in range(1, n_samples):
        # GARCH(1,1) ìŠ¤íƒ€ì¼ ë³€ë™ì„± ëª¨ë¸
        volatility = 0.1 * 0.02 + 0.1 * returns[i-1]**2 + 0.8 * volatility
        returns[i] = np.random.normal(0, volatility)

    prices = 100 * np.exp(np.cumsum(returns))
    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')

    # ë³€ë™ì„± ì‹œê³„ì—´ ìƒì„±
    vol_series = np.zeros(n_samples)
    vol_series[0] = 0.02

    for i in range(1, n_samples):
        vol_series[i] = 0.1 * 0.02 + 0.1 * returns[i-1]**2 + 0.8 * vol_series[i-1]

    return pd.DataFrame({
        'price': prices,
        'log_returns': returns,
        'volatility': vol_series
    }, index=dates)


def create_strong_signal_features(data, data_type='mean_reversion'):
    """ê°•í•œ ì‹ í˜¸ë¥¼ ìœ„í•œ íŠ¹ì„± ìƒì„±"""
    enhanced = data.copy()
    returns = data['log_returns']
    prices = data['price']

    if data_type == 'mean_reversion':
        # í‰ê·  íšŒê·€ ê´€ë ¨ íŠ¹ì„±
        for window in [5, 10, 20]:
            ma = returns.rolling(window).mean()
            enhanced[f'deviation_from_mean_{window}'] = returns - ma
            enhanced[f'cumulative_deviation_{window}'] = (returns - ma).rolling(window).sum()

        # Z-score (í‰ê· íšŒê·€ì˜ í•µì‹¬ ì‹ í˜¸)
        for window in [10, 20, 50]:
            mean = returns.rolling(window).mean()
            std = returns.rolling(window).std()
            enhanced[f'zscore_{window}'] = (returns - mean) / (std + 1e-8)

        # ê°€ê²©ì˜ í‰ê· íšŒê·€
        for window in [10, 20]:
            price_ma = prices.rolling(window).mean()
            enhanced[f'price_deviation_{window}'] = (prices - price_ma) / price_ma

    elif data_type == 'momentum':
        # ëª¨ë©˜í…€ ê´€ë ¨ íŠ¹ì„±
        for window in [3, 5, 10, 20]:
            enhanced[f'momentum_{window}'] = returns.rolling(window).sum()
            enhanced[f'momentum_avg_{window}'] = returns.rolling(window).mean()

        # ê°€ì†ë„ (ëª¨ë©˜í…€ì˜ ë³€í™”)
        enhanced['momentum_acceleration'] = enhanced['momentum_5'].diff()

        # ì—°ì† ìƒìŠ¹/í•˜ë½
        enhanced['consecutive_positive'] = (returns > 0).astype(int).groupby(
            (returns <= 0).cumsum()).cumsum()
        enhanced['consecutive_negative'] = (returns < 0).astype(int).groupby(
            (returns >= 0).cumsum()).cumsum()

    elif data_type == 'volatility':
        # ë³€ë™ì„± ê´€ë ¨ íŠ¹ì„±
        for window in [5, 10, 20]:
            vol = returns.rolling(window).std()
            enhanced[f'volatility_{window}'] = vol
            enhanced[f'vol_ratio_{window}'] = vol / (returns.rolling(window*2).std() + 1e-8)

        # ì œê³± ìˆ˜ìµë¥  (ë³€ë™ì„± í”„ë¡ì‹œ)
        enhanced['squared_returns'] = returns ** 2
        enhanced['abs_returns'] = np.abs(returns)

        # ë³€ë™ì„±ì˜ ì§€ì†ì„±
        vol_5 = returns.rolling(5).std()
        enhanced['vol_persistence'] = vol_5 / (vol_5.shift(5) + 1e-8)

    # ê³µí†µ íŠ¹ì„±
    for lag in [1, 2, 3]:
        enhanced[f'returns_lag_{lag}'] = returns.shift(lag)

    return enhanced.dropna()


def create_predictable_targets(data, data_type='mean_reversion'):
    """ì˜ˆì¸¡ ê°€ëŠ¥í•œ íƒ€ê²Ÿ ìƒì„±"""
    enhanced = data.copy()
    returns = data['log_returns']

    if data_type == 'mean_reversion':
        # í‰ê·  íšŒê·€ íƒ€ê²Ÿ - ê³¼ë„í•œ í¸ì°¨ í›„ ë°˜ì „
        for window in [5, 10]:
            mean = returns.rolling(window).mean()
            deviation = returns - mean
            # í˜„ì¬ í¸ì°¨ê°€ í´ìˆ˜ë¡ ë‹¤ìŒ ê¸°ê°„ ë°˜ì „ ê°€ëŠ¥ì„± ë†’ìŒ
            enhanced[f'target_reversion_{window}d'] = -deviation.shift(-1)  # ë°˜ì „ ì˜ˆì¸¡

        # ëˆ„ì  í¸ì°¨ ê¸°ë°˜ íƒ€ê²Ÿ
        cumulative_dev = (returns - returns.rolling(20).mean()).rolling(10).sum()
        enhanced['target_mean_revert'] = -cumulative_dev.shift(-1)

    elif data_type == 'momentum':
        # ëª¨ë©˜í…€ ì§€ì† íƒ€ê²Ÿ
        for window in [3, 5]:
            past_momentum = returns.rolling(window).sum()
            enhanced[f'target_momentum_continue_{window}d'] = past_momentum.shift(-1)

        # íŠ¸ë Œë“œ ì§€ì† íƒ€ê²Ÿ
        trend_5d = returns.rolling(5).mean()
        enhanced['target_trend_continue'] = trend_5d.shift(-1)

    elif data_type == 'volatility':
        # ë³€ë™ì„± ì§€ì† íƒ€ê²Ÿ
        current_vol = returns.rolling(5).std()
        enhanced['target_vol_next'] = current_vol.shift(-1)

        # ë³€ë™ì„± êµ°ì§‘ íƒ€ê²Ÿ
        vol_change = current_vol.diff()
        enhanced['target_vol_cluster'] = vol_change.shift(-1)

    return enhanced


def test_strong_signal_performance():
    """ê°•í•œ ì‹ í˜¸ íƒ€ê²Ÿ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¯ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê°•í•œ ì‹ í˜¸ íƒ€ê²Ÿ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    data_types = ['mean_reversion', 'momentum', 'volatility']
    data_generators = {
        'mean_reversion': create_mean_reversion_data,
        'momentum': create_momentum_data,
        'volatility': create_volatility_clustering_data
    }

    all_results = {}

    for data_type in data_types:
        print(f"\nğŸ“Š {data_type.upper()} íŒ¨í„´ í…ŒìŠ¤íŠ¸")
        print("-" * 40)

        # 1. ë°ì´í„° ìƒì„±
        raw_data = data_generators[data_type]()
        print(f"   ì›ë³¸ ë°ì´í„°: {len(raw_data)}ê°œ ê´€ì¸¡ì¹˜")

        # 2. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
        enhanced_data = create_strong_signal_features(raw_data, data_type)
        print(f"   íŠ¹ì„± ìƒì„±: {len(enhanced_data.columns)}ê°œ íŠ¹ì„±")

        # 3. íƒ€ê²Ÿ ìƒì„±
        final_data = create_predictable_targets(enhanced_data, data_type)
        final_data = final_data.dropna()
        print(f"   ìµœì¢… ë°ì´í„°: {len(final_data)}ê°œ ê´€ì¸¡ì¹˜")

        # 4. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        target_columns = [col for col in final_data.columns if 'target_' in col]
        feature_columns = [col for col in final_data.columns
                          if 'target_' not in col and col not in ['price', 'cumulative_deviation', 'volatility']]

        X = final_data[feature_columns]

        print(f"   í…ŒìŠ¤íŠ¸í•  íƒ€ê²Ÿ: {len(target_columns)}ê°œ")

        data_results = {}

        # 5. ê° íƒ€ê²Ÿë³„ í…ŒìŠ¤íŠ¸
        for target_col in target_columns:
            y = final_data[target_col].dropna()
            X_target = X.loc[y.index]

            # ê°„ë‹¨í•œ ëª¨ë¸ë“¤ë¡œ í…ŒìŠ¤íŠ¸
            models = {
                'Linear': LinearRegression(),
                'Ridge': Ridge(alpha=1.0),
                'ElasticNet': ElasticNet(alpha=0.1, random_state=42),
                'RandomForest': RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42)
            }

            # ì‹œê³„ì—´ êµì°¨ê²€ì¦
            tscv = TimeSeriesSplit(n_splits=3)
            target_results = {}

            for model_name, model in models.items():
                cv_scores = []

                for train_idx, val_idx in tscv.split(X_target):
                    X_train, X_val = X_target.iloc[train_idx], X_target.iloc[val_idx]
                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_val)
                    score = r2_score(y_val, y_pred)
                    cv_scores.append(score)

                avg_score = np.mean(cv_scores)
                target_results[model_name] = avg_score

            best_score = max(target_results.values())
            best_model = max(target_results.keys(), key=lambda k: target_results[k])

            print(f"     {target_col:<30} {best_model:<12} RÂ² = {best_score:.4f}")

            data_results[target_col] = {
                'scores': target_results,
                'best_score': best_score,
                'best_model': best_model
            }

        all_results[data_type] = data_results

    # 6. ì „ì²´ ê²°ê³¼ ìš”ì•½
    print("\nğŸ† ì „ì²´ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    best_overall_score = -np.inf
    best_overall_type = None
    best_overall_target = None

    for data_type, data_results in all_results.items():
        type_best_score = -np.inf
        type_best_target = None

        for target, result in data_results.items():
            if result['best_score'] > type_best_score:
                type_best_score = result['best_score']
                type_best_target = target

        print(f"\n{data_type.upper()}:")
        print(f"   ìµœê³  ì„±ëŠ¥: {type_best_target}")
        print(f"   RÂ² ì ìˆ˜: {type_best_score:.4f}")

        if type_best_score > best_overall_score:
            best_overall_score = type_best_score
            best_overall_type = data_type
            best_overall_target = type_best_target

    print(f"\nğŸ¥‡ ì „ì²´ ìµœê³  ì„±ëŠ¥:")
    print(f"   íŒ¨í„´ íƒ€ì…: {best_overall_type.upper()}")
    print(f"   íƒ€ê²Ÿ: {best_overall_target}")
    print(f"   RÂ² ì ìˆ˜: {best_overall_score:.4f}")

    # ì„±ëŠ¥ í‰ê°€
    if best_overall_score > 0.3:
        print("ğŸ‰ ìš°ìˆ˜í•œ ì˜ˆì¸¡ ì„±ëŠ¥! ì‹¤ìš©ì„± ë†’ìŒ")
    elif best_overall_score > 0.1:
        print("âœ… ì–‘í˜¸í•œ ì˜ˆì¸¡ ì„±ëŠ¥! ì˜ë¯¸ìˆëŠ” ì‹ í˜¸ ê°ì§€")
    elif best_overall_score > 0.05:
        print("ğŸ“ˆ ì ì •í•œ ì˜ˆì¸¡ ì„±ëŠ¥! ì•½í•œ ì‹ í˜¸ ê°ì§€")
    elif best_overall_score > 0:
        print("ğŸ“Š ê¸°ë³¸ì ì¸ ì˜ˆì¸¡ ì„±ëŠ¥! ë¯¸ì•½í•œ ì‹ í˜¸")
    else:
        print("âš ï¸ ì˜ˆì¸¡ ì„±ëŠ¥ ë¯¸í¡, ì¶”ê°€ ìµœì í™” í•„ìš”")

    # ì‹¤ì œ ê¸ˆìœµ ë°ì´í„°ì—ì„œì˜ ê¸°ëŒ€ì¹˜
    print(f"\nğŸ’¡ ì‹¤ì œ ê¸ˆìœµ ë°ì´í„° ì ìš© ì‹œ ì˜ˆìƒ ì„±ëŠ¥:")
    realistic_expectation = best_overall_score * 0.3  # ì‹¤ì œ ë°ì´í„°ëŠ” ë…¸ì´ì¦ˆê°€ ë” ë§ìŒ
    print(f"   ì˜ˆìƒ RÂ² ë²”ìœ„: {realistic_expectation:.4f} ~ {realistic_expectation * 1.5:.4f}")

    if realistic_expectation > 0.02:
        print("   âœ… ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„± ë†’ìŒ")
    else:
        print("   âš ï¸ ì‹¤ì œ ì ìš©ì„ ìœ„í•´ ì¶”ê°€ ê°œì„  í•„ìš”")

    return all_results, best_overall_score


def test_with_current_data():
    """í˜„ì¬ ì‹œìŠ¤í…œ ë°ì´í„°ë¡œ ê°•í•œ ì‹ í˜¸ íƒ€ê²Ÿ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”„ í˜„ì¬ ì‹œìŠ¤í…œ ë°ì´í„°ë¡œ ê°•í•œ ì‹ í˜¸ ì ìš© í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    # í˜„ì¬ ì‹œìŠ¤í…œê³¼ ìœ ì‚¬í•œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 500

    # ì•½í•œ í‰ê· íšŒê·€ + ë…¸ì´ì¦ˆ (í˜„ì‹¤ì )
    returns = np.zeros(n_samples)
    cumulative = 0

    for i in range(1, n_samples):
        # ì•½í•œ í‰ê· íšŒê·€ (í˜„ì‹¤ì  ìˆ˜ì¤€)
        reversion = -0.05 * cumulative
        noise = np.random.normal(0, 0.02)
        returns[i] = reversion + noise
        cumulative += returns[i]

    prices = 100 * np.exp(np.cumsum(returns))
    dates = pd.date_range('2022-01-01', periods=n_samples, freq='D')

    data = pd.DataFrame({
        'price': prices,
        'log_returns': returns
    }, index=dates)

    print(f"   ë°ì´í„° í¬ê¸°: {len(data)}ê°œ ê´€ì¸¡ì¹˜")

    # ê°•í•œ ì‹ í˜¸ íŠ¹ì„± ë° íƒ€ê²Ÿ ì ìš©
    enhanced_data = create_strong_signal_features(data, 'mean_reversion')
    final_data = create_predictable_targets(enhanced_data, 'mean_reversion')
    final_data = final_data.dropna()

    print(f"   ì²˜ë¦¬ í›„ ë°ì´í„°: {len(final_data)}ê°œ ê´€ì¸¡ì¹˜")

    # íƒ€ê²Ÿë³„ í…ŒìŠ¤íŠ¸
    target_columns = [col for col in final_data.columns if 'target_' in col]
    feature_columns = [col for col in final_data.columns
                      if 'target_' not in col and col != 'price']

    X = final_data[feature_columns]

    best_score = -np.inf
    best_target = None

    for target_col in target_columns:
        y = final_data[target_col].dropna()
        X_target = X.loc[y.index]

        # Ridge íšŒê·€ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        model = Ridge(alpha=1.0)
        tscv = TimeSeriesSplit(n_splits=3)

        cv_scores = []
        for train_idx, val_idx in tscv.split(X_target):
            X_train, X_val = X_target.iloc[train_idx], X_target.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)
            score = r2_score(y_val, y_pred)
            cv_scores.append(score)

        avg_score = np.mean(cv_scores)
        print(f"   {target_col:<30} RÂ² = {avg_score:.4f}")

        if avg_score > best_score:
            best_score = avg_score
            best_target = target_col

    print(f"\nâœ… í˜„ì‹¤ì  ë°ì´í„°ì—ì„œ ìµœê³  ì„±ëŠ¥:")
    print(f"   íƒ€ê²Ÿ: {best_target}")
    print(f"   RÂ² ì ìˆ˜: {best_score:.4f}")

    return best_score


if __name__ == "__main__":
    # ê°•í•œ ì‹ í˜¸ íƒ€ê²Ÿ í…ŒìŠ¤íŠ¸
    results, best_score = test_strong_signal_performance()

    # í˜„ì‹¤ì  ë°ì´í„° í…ŒìŠ¤íŠ¸
    realistic_score = test_with_current_data()

    print(f"\nğŸ“‹ ìµœì¢… ìš”ì•½:")
    print(f"   ì´ìƒì  ì¡°ê±´ ìµœê³  RÂ²: {best_score:.4f}")
    print(f"   í˜„ì‹¤ì  ì¡°ê±´ RÂ²: {realistic_score:.4f}")
    print(f"   ì„±ëŠ¥ ì°¨ì´: {((best_score - realistic_score) / abs(realistic_score) * 100) if realistic_score != 0 else 'N/A':.1f}%")