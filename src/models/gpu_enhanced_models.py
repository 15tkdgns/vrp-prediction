#!/usr/bin/env python3
"""
GPU ê°€ì† ê³ ì„±ëŠ¥ ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ
ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì°¨ë‹¨ ë° ì—„ê²©í•œ ì‹œê³„ì—´ ê²€ì¦ ì ìš©
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
import json
import logging
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# GPU ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì • (TensorFlow)
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(f"GPU ì„¤ì • ì˜¤ë¥˜: {e}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataLeakageValidator:
    """ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ê¸° - ì—„ê²©í•œ ì‹œê³„ì—´ ê²€ì¦"""

    @staticmethod
    def validate_temporal_order(df, date_col='Date'):
        """ì‹œê³„ì—´ ìˆœì„œ ê²€ì¦"""
        if date_col in df.columns:
            return df[date_col].is_monotonic_increasing
        return True

    @staticmethod
    def validate_feature_independence(X, y, threshold=0.99):
        """íŠ¹ì§•ê³¼ íƒ€ê²Ÿì˜ ê³¼ë„í•œ ìƒê´€ê´€ê³„ ê²€ì‚¬"""
        correlations = []
        for i in range(X.shape[1]):
            corr = np.corrcoef(X[:, i], y)[0, 1]
            if not np.isnan(corr):
                correlations.append(abs(corr))

        max_corr = max(correlations) if correlations else 0
        return max_corr < threshold, max_corr

    @staticmethod
    def validate_time_series_split(train_idx, test_idx):
        """ì‹œê³„ì—´ ë¶„í•  ê²€ì¦ - í›ˆë ¨ ë°ì´í„°ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë³´ë‹¤ í•­ìƒ ê³¼ê±°ì—¬ì•¼ í•¨"""
        return max(train_idx) < min(test_idx)

class SafeFeatureEngineer:
    """ì•ˆì „í•œ íŠ¹ì§• ê³µí•™ - ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ë°©ì§€"""

    def __init__(self, lookback_window=20):
        self.lookback_window = lookback_window
        self.scalers = {}

    def create_safe_features(self, df):
        """ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ì—†ëŠ” ì•ˆì „í•œ íŠ¹ì§• ìƒì„±"""
        df = df.copy()

        # ê¸°ë³¸ ê°€ê²© íŠ¹ì§• (ê³¼ê±° ì •ë³´ë§Œ ì‚¬ìš©)
        df['price_change'] = df['Close'].pct_change(1)
        df['volatility'] = df['Close'].rolling(self.lookback_window, min_periods=1).std()

        # ê¸°ìˆ ì  ì§€í‘œ (ê³¼ê±° ì •ë³´ë§Œ ì‚¬ìš©)
        df['sma_20'] = df['Close'].rolling(20, min_periods=1).mean()
        df['sma_50'] = df['Close'].rolling(50, min_periods=1).mean()
        df['rsi'] = self._calculate_rsi(df['Close'])

        # ë³¼ë¥¨ íŠ¹ì§•
        df['volume_sma'] = df['Volume'].rolling(20, min_periods=1).mean()
        df['volume_ratio'] = df['Volume'] / df['volume_sma']

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.fillna(method='ffill').fillna(0)

        return df

    def _calculate_rsi(self, prices, window=14):
        """RSI ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

class GPUEnhancedLSTM(nn.Module):
    """GPU ê°€ì† LSTM ëª¨ë¸"""

    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.2):
        super(GPUEnhancedLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                           batch_first=True, dropout=dropout)

        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8,
                                             dropout=dropout, batch_first=True)

        # ì¶œë ¥ ë ˆì´ì–´
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 4, 1)
        )

    def forward(self, x):
        # LSTM
        lstm_out, _ = self.lstm(x)

        # ì–´í…ì…˜
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)

        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥
        out = attn_out[:, -1, :]

        # ìµœì¢… ì˜ˆì¸¡
        out = self.fc_layers(out)
        return out

class TensorFlowTransformer:
    """TensorFlow ê¸°ë°˜ Transformer ëª¨ë¸"""

    def __init__(self, sequence_length, num_features, d_model=128):
        self.sequence_length = sequence_length
        self.num_features = num_features
        self.d_model = d_model
        self.model = self._build_model()

    def _build_model(self):
        """Transformer ëª¨ë¸ êµ¬ì¶•"""
        inputs = tf.keras.Input(shape=(self.sequence_length, self.num_features))

        # ìœ„ì¹˜ ì¸ì½”ë”©
        x = tf.keras.layers.Dense(self.d_model)(inputs)

        # Multi-Head Attention
        attention = tf.keras.layers.MultiHeadAttention(
            num_heads=8, key_dim=self.d_model//8)(x, x)
        x = tf.keras.layers.Add()([x, attention])
        x = tf.keras.layers.LayerNormalization()(x)

        # Feed Forward
        ff = tf.keras.layers.Dense(self.d_model*4, activation='relu')(x)
        ff = tf.keras.layers.Dense(self.d_model)(ff)
        x = tf.keras.layers.Add()([x, ff])
        x = tf.keras.layers.LayerNormalization()(x)

        # Global Average Pooling
        x = tf.keras.layers.GlobalAveragePooling1D()(x)

        # ì¶œë ¥ ë ˆì´ì–´
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        x = tf.keras.layers.Dropout(0.2)(x)
        outputs = tf.keras.layers.Dense(1)(x)

        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )

        return model

class GPUModelTrainer:
    """GPU ê°€ì† ëª¨ë¸ í›ˆë ¨ê¸°"""

    def __init__(self, data_path=None):
        self.data_path = data_path or Path("/root/workspace/data/training/sp500_2020_2024_enhanced.csv")
        self.validator = DataLeakageValidator()
        self.feature_engineer = SafeFeatureEngineer()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")

    def load_safe_data(self):
        """ì•ˆì „í•œ ë°ì´í„° ë¡œë”© (ëˆ„ì¶œ ê²€ì¦)"""
        try:
            df = pd.read_csv(self.data_path)
            logger.info(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: {df.shape}")

            # ì‹œê³„ì—´ ìˆœì„œ ê²€ì¦
            if not self.validator.validate_temporal_order(df):
                raise ValueError("ì‹œê³„ì—´ ìˆœì„œê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤!")

            # íŠ¹ì§• ê³µí•™
            df = self.feature_engineer.create_safe_features(df)

            # íƒ€ê²Ÿ ë³€ìˆ˜ (ë‹¤ìŒ ë‚  ìˆ˜ìµë¥ )
            df['target'] = df['Close'].pct_change(1).shift(-1)

            # ê²°ì¸¡ê°’ ì œê±°
            df = df.dropna()

            logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {df.shape}")
            return df

        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def prepare_sequences(self, df, sequence_length=20):
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ì¤€ë¹„"""
        feature_cols = ['price_change', 'volatility', 'sma_20', 'sma_50',
                       'rsi', 'volume_sma', 'volume_ratio']

        # íŠ¹ì§• ì •ê·œí™”
        scaler = RobustScaler()
        scaled_features = scaler.fit_transform(df[feature_cols])

        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = [], []
        for i in range(sequence_length, len(scaled_features)):
            X.append(scaled_features[i-sequence_length:i])
            y.append(df.iloc[i]['target'])

        return np.array(X), np.array(y), scaler

    def train_pytorch_lstm(self, X, y):
        """PyTorch LSTM ëª¨ë¸ í›ˆë ¨"""
        logger.info("PyTorch LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

        # ì‹œê³„ì—´ ë¶„í• 
        tscv = TimeSeriesSplit(n_splits=5)
        best_model = None
        best_score = float('inf')

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            # ë°ì´í„° ëˆ„ì¶œ ê²€ì¦
            if not self.validator.validate_time_series_split(train_idx, val_idx):
                raise ValueError(f"Fold {fold}: ì‹œê³„ì—´ ë¶„í•  ì˜¤ë¥˜!")

            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # ë°ì´í„°ì…‹ ìƒì„±
            train_dataset = TensorDataset(
                torch.FloatTensor(X_train),
                torch.FloatTensor(y_train)
            )
            val_dataset = TensorDataset(
                torch.FloatTensor(X_val),
                torch.FloatTensor(y_val)
            )

            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)
            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

            # ëª¨ë¸ ì´ˆê¸°í™”
            model = GPUEnhancedLSTM(
                input_size=X.shape[2],
                hidden_size=128,
                num_layers=3
            ).to(self.device)

            criterion = nn.MSELoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)

            # í›ˆë ¨
            best_val_loss = float('inf')
            patience_counter = 0

            for epoch in range(100):
                model.train()
                train_loss = 0

                for batch_X, batch_y in train_loader:
                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)

                    optimizer.zero_grad()
                    outputs = model(batch_X).squeeze()
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()

                    train_loss += loss.item()

                # ê²€ì¦
                model.eval()
                val_loss = 0
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                        outputs = model(batch_X).squeeze()
                        val_loss += criterion(outputs, batch_y).item()

                val_loss /= len(val_loader)
                scheduler.step(val_loss)

                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    if val_loss < best_score:
                        best_score = val_loss
                        best_model = model.state_dict().copy()
                else:
                    patience_counter += 1
                    if patience_counter >= 20:
                        break

            logger.info(f"Fold {fold+1}: ìµœê³  ê²€ì¦ ì†ì‹¤ = {best_val_loss:.6f}")

        # ìµœê³  ëª¨ë¸ ì €ì¥
        if best_model is not None:
            final_model = GPUEnhancedLSTM(
                input_size=X.shape[2],
                hidden_size=128,
                num_layers=3
            )
            final_model.load_state_dict(best_model)
            torch.save(final_model, "/root/workspace/data/models/gpu_enhanced_lstm.pth")
            logger.info(f"PyTorch LSTM í›ˆë ¨ ì™„ë£Œ: ìµœê³  ì ìˆ˜ = {best_score:.6f}")

        return best_score

    def train_tensorflow_transformer(self, X, y):
        """TensorFlow Transformer ëª¨ë¸ í›ˆë ¨"""
        logger.info("TensorFlow Transformer ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

        tscv = TimeSeriesSplit(n_splits=5)
        best_score = float('inf')
        best_model = None

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            # ë°ì´í„° ëˆ„ì¶œ ê²€ì¦
            if not self.validator.validate_time_series_split(train_idx, val_idx):
                raise ValueError(f"Fold {fold}: ì‹œê³„ì—´ ë¶„í•  ì˜¤ë¥˜!")

            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # ëª¨ë¸ ìƒì„±
            transformer = TensorFlowTransformer(
                sequence_length=X.shape[1],
                num_features=X.shape[2]
            )

            # ì½œë°± ì„¤ì •
            callbacks = [
                tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),
                tf.keras.callbacks.ReduceLROnPlateau(patience=10)
            ]

            # í›ˆë ¨
            history = transformer.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=100,
                batch_size=32,
                callbacks=callbacks,
                verbose=0
            )

            val_loss = min(history.history['val_loss'])
            logger.info(f"Fold {fold+1}: ìµœê³  ê²€ì¦ ì†ì‹¤ = {val_loss:.6f}")

            if val_loss < best_score:
                best_score = val_loss
                best_model = transformer.model

        # ìµœê³  ëª¨ë¸ ì €ì¥
        if best_model is not None:
            best_model.save("/root/workspace/data/models/gpu_enhanced_transformer.h5")
            logger.info(f"TensorFlow Transformer í›ˆë ¨ ì™„ë£Œ: ìµœê³  ì ìˆ˜ = {best_score:.6f}")

        return best_score

    def train_xgboost_gpu(self, X, y):
        """XGBoost GPU ê°€ì† ëª¨ë¸ í›ˆë ¨"""
        logger.info("XGBoost GPU ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

        # ì‹œí€€ìŠ¤ë¥¼ í”Œë« íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜
        X_flat = X.reshape(X.shape[0], -1)

        tscv = TimeSeriesSplit(n_splits=5)
        best_score = float('inf')
        best_model = None

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_flat)):
            # ë°ì´í„° ëˆ„ì¶œ ê²€ì¦
            if not self.validator.validate_time_series_split(train_idx, val_idx):
                raise ValueError(f"Fold {fold}: ì‹œê³„ì—´ ë¶„í•  ì˜¤ë¥˜!")

            X_train, X_val = X_flat[train_idx], X_flat[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # XGBoost ëª¨ë¸ (GPU ê°€ì†)
            model = xgb.XGBRegressor(
                tree_method='gpu_hist',
                gpu_id=0,
                n_estimators=1000,
                max_depth=6,
                learning_rate=0.01,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                early_stopping_rounds=50
            )

            # í›ˆë ¨
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=False
            )

            # ê²€ì¦
            y_pred = model.predict(X_val)
            val_score = mean_squared_error(y_val, y_pred)

            logger.info(f"Fold {fold+1}: ê²€ì¦ MSE = {val_score:.6f}")

            if val_score < best_score:
                best_score = val_score
                best_model = model

        # ìµœê³  ëª¨ë¸ ì €ì¥
        if best_model is not None:
            joblib.dump(best_model, "/root/workspace/data/models/gpu_enhanced_xgboost.pkl")
            logger.info(f"XGBoost GPU í›ˆë ¨ ì™„ë£Œ: ìµœê³  ì ìˆ˜ = {best_score:.6f}")

        return best_score

    def run_gpu_training(self):
        """ì „ì²´ GPU í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("=== GPU ê°€ì† ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")

        # ì•ˆì „í•œ ë°ì´í„° ë¡œë”©
        df = self.load_safe_data()
        if df is None:
            return None

        # ì‹œí€€ìŠ¤ ì¤€ë¹„
        X, y, scaler = self.prepare_sequences(df)

        # íŠ¹ì§•-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ê²€ì¦
        is_safe, max_corr = self.validator.validate_feature_independence(
            X.reshape(X.shape[0], -1), y
        )
        if not is_safe:
            logger.warning(f"ë†’ì€ íŠ¹ì§•-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ê°ì§€: {max_corr:.3f}")

        results = {}

        # PyTorch LSTM í›ˆë ¨
        try:
            pytorch_score = self.train_pytorch_lstm(X, y)
            results['pytorch_lstm'] = pytorch_score
        except Exception as e:
            logger.error(f"PyTorch LSTM í›ˆë ¨ ì‹¤íŒ¨: {e}")

        # TensorFlow Transformer í›ˆë ¨
        try:
            tf_score = self.train_tensorflow_transformer(X, y)
            results['tensorflow_transformer'] = tf_score
        except Exception as e:
            logger.error(f"TensorFlow Transformer í›ˆë ¨ ì‹¤íŒ¨: {e}")

        # XGBoost GPU í›ˆë ¨
        try:
            xgb_score = self.train_xgboost_gpu(X, y)
            results['xgboost_gpu'] = xgb_score
        except Exception as e:
            logger.error(f"XGBoost GPU í›ˆë ¨ ì‹¤íŒ¨: {e}")

        # ê²°ê³¼ ì €ì¥
        results_path = "/root/workspace/data/raw/gpu_enhanced_results.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)

        logger.info("=== GPU í›ˆë ¨ ì™„ë£Œ ===")
        logger.info(f"ê²°ê³¼ ì €ì¥: {results_path}")

        return results

if __name__ == "__main__":
    trainer = GPUModelTrainer()
    results = trainer.run_gpu_training()

    if results:
        print("\nğŸš€ GPU ê°€ì† ëª¨ë¸ í›ˆë ¨ ê²°ê³¼:")
        for model_name, score in results.items():
            print(f"  {model_name}: MSE = {score:.6f}")
    else:
        print("âŒ í›ˆë ¨ ì‹¤íŒ¨")