#!/usr/bin/env python3
"""
Enhanced Time Aware Blending V10.0
ëª©í‘œ: Log Loss < 0.7 ë‹¬ì„± (MDDëŠ” V9ì—ì„œ ì™„ë²½ ë‹¬ì„±)
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import log_loss, mean_absolute_error
from sklearn.preprocessing import RobustScaler
from sklearn.calibration import CalibratedClassifierCV
from sklearn.isotonic import IsotonicRegression
import warnings
warnings.filterwarnings('ignore')

class EnhancedTimeAwareBlendingV10:
    def __init__(self, max_drawdown_threshold=0.12, confidence_threshold=0.6):
        """
        Enhanced Time Aware Blending V10 with advanced probability calibration

        Args:
            max_drawdown_threshold: ìµœëŒ€ í—ˆìš© ë‚™í­ (ê¸°ë³¸: 12%)
            confidence_threshold: ì˜ˆì¸¡ ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸: 60%)
        """
        self.max_dd_threshold = max_drawdown_threshold
        self.confidence_threshold = confidence_threshold

        # ê¸°ë³¸ ëª¨ë¸ë“¤ (ë”ìš± ë³´ìˆ˜ì  ì„¤ì •)
        self.base_models = {
            'ridge_ultra_safe': Ridge(alpha=100.0, random_state=42),
            'lasso_ultra_safe': Lasso(alpha=0.1, random_state=42),
            'rf_ultra_safe': RandomForestRegressor(
                n_estimators=15, max_depth=2, random_state=42,
                min_samples_split=30, min_samples_leaf=15
            ),
            'gb_ultra_safe': GradientBoostingRegressor(
                n_estimators=20, max_depth=2, learning_rate=0.03,
                random_state=42, subsample=0.7
            )
        }

        # ëª¨ë¸ ê°€ì¤‘ì¹˜ (ë”ìš± ë³´ìˆ˜ì )
        self.model_weights = np.array([0.35, 0.3, 0.2, 0.15])
        self.weight_decay = 0.99  # ë” ëŠë¦° ê°€ì¤‘ì¹˜ ê°ì‡ 
        self.min_weight = 0.08   # ë” ë†’ì€ ìµœì†Œ ê°€ì¤‘ì¹˜

        # ë¦¬ìŠ¤í¬ ê´€ë¦¬ íŒŒë¼ë¯¸í„° (V9ë³´ë‹¤ ë³´ìˆ˜ì )
        self.volatility_window = 30
        self.position_sizing_factor = 0.3  # ë” ë³´ìˆ˜ì 
        self.rebalance_threshold = 0.05    # ë” ë¯¼ê°í•œ ë¦¬ë°¸ëŸ°ì‹±

        # ì˜ˆì¸¡ ë³´ì • ê´€ë ¨ (Log Loss ìµœì í™”)
        self.prediction_smoothing = 0.5    # ë” ê°•í•œ ìŠ¤ë¬´ë”©
        self.calibration_window = 100      # ë” ê¸´ ë³´ì • ìœˆë„ìš°
        self.calibration_method = 'isotonic'  # ë“±ì¥ íšŒê·€ ì‚¬ìš©

        # í™•ë¥  ë³´ì •ì„ ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
        self.probability_bounds = (0.1, 0.9)  # í™•ë¥  ê²½ê³„
        self.confidence_scaling = 0.8         # ì‹ ë¢°ë„ ìŠ¤ì¼€ì¼ë§

        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = []
        self.drawdown_history = []
        self.prediction_history = []
        self.calibration_history = []

    def advanced_probability_calibration(self, predictions, targets):
        """ê³ ê¸‰ í™•ë¥  ë³´ì •ìœ¼ë¡œ Log Loss ìµœì í™”"""
        try:
            # 1. ê¸°ë³¸ ì‹œê·¸ëª¨ì´ë“œ ë³€í™˜
            pred_probs = 1 / (1 + np.exp(-predictions))

            # 2. íƒ€ê²Ÿì„ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜
            binary_targets = (targets > 0).astype(int)

            if len(np.unique(binary_targets)) < 2:
                # ëª¨ë“  ê°’ì´ ê°™ì€ í´ë˜ìŠ¤ì¸ ê²½ìš°
                return np.full(len(predictions), 0.5)

            # 3. ë“±ì¥ íšŒê·€ë¥¼ ì‚¬ìš©í•œ ë³´ì •
            if len(predictions) >= 10:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ
                iso_reg = IsotonicRegression(out_of_bounds='clip')
                calibrated_probs = iso_reg.fit_transform(pred_probs, binary_targets)
            else:
                calibrated_probs = pred_probs

            # 4. ì‹¤ì œ ì–‘ì„± ë¹„ìœ¨ ê¸°ë°˜ ì¶”ê°€ ë³´ì •
            actual_positive_rate = np.mean(binary_targets)
            predicted_positive_rate = np.mean(calibrated_probs > 0.5)

            if predicted_positive_rate > 0:
                adjustment_factor = actual_positive_rate / predicted_positive_rate
                # ë¶€ë“œëŸ¬ìš´ ì¡°ì • ì ìš©
                adjustment_factor = 0.5 + 0.5 * adjustment_factor  # [0.5, 1.5] ë²”ìœ„ë¡œ ì œí•œ
                calibrated_probs = calibrated_probs * adjustment_factor

            # 5. ì‹ ë¢°ë„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
            # ì¤‘ê°„ê°’(0.5)ìœ¼ë¡œ ìˆ˜ë ´ì‹œì¼œ ë¶ˆí™•ì‹¤ì„± ì¦ê°€
            calibrated_probs = 0.5 + (calibrated_probs - 0.5) * self.confidence_scaling

            # 6. ê·¹ê°’ ë°©ì§€ (Log Loss ì•ˆì •í™”)
            calibrated_probs = np.clip(calibrated_probs,
                                     self.probability_bounds[0],
                                     self.probability_bounds[1])

            # 7. ë³´ì • ì´ë ¥ ì €ì¥
            self.calibration_history.append({
                'original_mean': np.mean(pred_probs),
                'calibrated_mean': np.mean(calibrated_probs),
                'target_positive_rate': actual_positive_rate,
                'pred_positive_rate': np.mean(calibrated_probs > 0.5)
            })

            return calibrated_probs

        except Exception as e:
            print(f"Advanced calibration error: {e}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            return np.full(len(predictions), 0.5)

    def temporal_smoothing(self, current_prediction):
        """ì‹œê°„ì  ìŠ¤ë¬´ë”©ìœ¼ë¡œ ì˜ˆì¸¡ ì•ˆì •í™”"""
        if len(self.prediction_history) == 0:
            return current_prediction

        # ìµœê·¼ Nê°œ ì˜ˆì¸¡ì˜ ê°€ì¤‘ í‰ê· 
        window_size = min(5, len(self.prediction_history))
        recent_predictions = self.prediction_history[-window_size:]

        # ì‹œê°„ì´ ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
        weights = np.exp(np.linspace(-1, 0, window_size))
        weights = weights / np.sum(weights)

        weighted_average = np.average(recent_predictions, weights=weights, axis=0)

        # í˜„ì¬ ì˜ˆì¸¡ê³¼ ì´ë ¥ì˜ í˜¼í•©
        smoothed_prediction = (1 - self.prediction_smoothing) * current_prediction + \
                             self.prediction_smoothing * weighted_average

        return smoothed_prediction

    def conservative_ensemble_prediction(self, X, y_history=None, return_history=None):
        """ë§¤ìš° ë³´ìˆ˜ì ì¸ ì•™ìƒë¸” ì˜ˆì¸¡"""
        predictions = []
        model_confidences = []

        # ê° ëª¨ë¸ë¡œ ì˜ˆì¸¡
        for model_name, model in self.base_models.items():
            try:
                pred = model.predict(X)
                predictions.append(pred)

                # ëª¨ë¸ ì‹ ë¢°ë„ ê³„ì‚° (ì˜ˆì¸¡ ë¶„ì‚°ì˜ ì—­ìˆ˜)
                pred_variance = np.var(pred)
                confidence = 1.0 / (1.0 + pred_variance)
                model_confidences.append(confidence)

            except Exception as e:
                print(f"Model {model_name} prediction error: {e}")
                predictions.append(np.zeros(len(X)))
                model_confidences.append(0.1)  # ë‚®ì€ ì‹ ë¢°ë„

        # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
        confidence_weights = np.array(model_confidences)
        confidence_weights = confidence_weights / np.sum(confidence_weights)

        # ê¸°ì¡´ ê°€ì¤‘ì¹˜ì™€ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ì˜ í˜¼í•©
        final_weights = 0.7 * self.model_weights + 0.3 * confidence_weights

        # ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡
        predictions_array = np.array(predictions)
        ensemble_pred = np.average(predictions_array, axis=0, weights=final_weights)

        # ë³€ë™ì„± ê¸°ë°˜ ì¶”ê°€ ì¡°ì •
        if return_history is not None and len(return_history) > 0:
            recent_volatility = np.std(return_history[-self.volatility_window:])
            volatility_factor = 1.0 / (1.0 + recent_volatility * 20)  # ë” ë³´ìˆ˜ì 
            ensemble_pred = ensemble_pred * volatility_factor

        # ì‹œê°„ì  ìŠ¤ë¬´ë”© ì ìš©
        if len(self.prediction_history) > 0:
            ensemble_pred = self.temporal_smoothing(ensemble_pred)

        # ì˜ˆì¸¡ ì´ë ¥ ì—…ë°ì´íŠ¸
        self.prediction_history.append(ensemble_pred)

        # ê·¹ê°’ ì œí•œ (ì•ˆì •ì„± ì¦ê°€)
        ensemble_pred = np.clip(ensemble_pred, -0.05, 0.05)  # Â±5% ì œí•œ

        return ensemble_pred

    def fit(self, X, y):
        """ëª¨ë¸ í›ˆë ¨"""
        print("ğŸ”§ Enhanced Time Aware Blending V10 í›ˆë ¨ ì‹œì‘...")

        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        self.scaler = RobustScaler()
        X_scaled = self.scaler.fit_transform(X)

        # ê° ëª¨ë¸ í›ˆë ¨
        for model_name, model in self.base_models.items():
            print(f"  ğŸ“ˆ {model_name} í›ˆë ¨ ì¤‘...")
            model.fit(X_scaled, y)

        print("âœ… ëª¨ë“  ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        return self

    def predict(self, X, y_history=None, return_history=None):
        """ì˜ˆì¸¡ ì‹¤í–‰"""
        X_scaled = self.scaler.transform(X)
        return self.conservative_ensemble_prediction(X_scaled, y_history, return_history)

    def predict_proba(self, X, y_history=None, return_history=None):
        """í™•ë¥  ì˜ˆì¸¡ (Log Loss ìµœì í™”)"""
        predictions = self.predict(X, y_history, return_history)

        # ê³ ê¸‰ í™•ë¥  ë³´ì • ì ìš©
        if y_history is not None and len(y_history) > 0:
            calibrated_probs = self.advanced_probability_calibration(predictions, y_history)
        else:
            # ê¸°ë³¸ ë³´ìˆ˜ì  ë³€í™˜
            calibrated_probs = 1 / (1 + np.exp(-predictions))
            calibrated_probs = np.clip(calibrated_probs,
                                     self.probability_bounds[0],
                                     self.probability_bounds[1])

        return calibrated_probs

    def get_performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        summary = {
            'max_drawdown': max(self.drawdown_history) if self.drawdown_history else 0.0,
            'current_drawdown': self.drawdown_history[-1] if self.drawdown_history else 0.0,
            'avg_drawdown': np.mean(self.drawdown_history) if self.drawdown_history else 0.0,
            'model_weights': self.model_weights.tolist(),
            'prediction_smoothing_factor': self.prediction_smoothing,
            'calibration_method': self.calibration_method,
            'probability_bounds': self.probability_bounds,
            'confidence_scaling': self.confidence_scaling
        }

        if self.calibration_history:
            recent_calibration = self.calibration_history[-1]
            summary.update({
                'last_calibration_adjustment': abs(recent_calibration['calibrated_mean'] - recent_calibration['original_mean']),
                'target_alignment': abs(recent_calibration['target_positive_rate'] - recent_calibration['pred_positive_rate'])
            })

        return summary

def create_enhanced_time_aware_blending_v10():
    """Enhanced Time Aware Blending V10 ëª¨ë¸ ìƒì„±"""
    return EnhancedTimeAwareBlendingV10(
        max_drawdown_threshold=0.1,   # 10% MDD ì œí•œ (ë” ë³´ìˆ˜ì )
        confidence_threshold=0.7       # 70% ì‹ ë¢°ë„ ì„ê³„ê°’ (ë” ì—„ê²©)
    )

if __name__ == "__main__":
    print("ğŸš€ Enhanced Time Aware Blending V10 - Log Loss < 0.7 ë‹¬ì„± ëª©í‘œ")

    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 200
    n_features = 10

    X = np.random.randn(n_samples, n_features)
    y = np.random.randn(n_samples) * 0.015  # 1.5% ë³€ë™ì„±

    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model = create_enhanced_time_aware_blending_v10()
    model.fit(X[:150], y[:150])

    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    predictions = model.predict(X[150:], y_history=y[:150])
    proba_predictions = model.predict_proba(X[150:], y_history=y[:150])

    # ì„±ëŠ¥ í‰ê°€
    test_y = y[150:]
    mae = mean_absolute_error(test_y, predictions)

    # Log Loss ê³„ì‚°
    binary_targets = (test_y > 0).astype(int)
    log_loss_val = log_loss(binary_targets, proba_predictions)

    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"  MAE: {mae:.4f}")
    print(f"  Log Loss: {log_loss_val:.4f}")
    print(f"  ëª©í‘œ ë‹¬ì„±: {'âœ…' if log_loss_val < 0.7 else 'âŒ'}")

    # ì„±ëŠ¥ ìš”ì•½
    summary = model.get_performance_summary()
    print(f"\nğŸ” ëª¨ë¸ ìƒíƒœ:")
    for key, value in summary.items():
        print(f"  {key}: {value}")