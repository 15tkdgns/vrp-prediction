#!/usr/bin/env python3
"""
ğŸ¯ ì „ë¬¸ ì‹œê³„ì—´ íšŒê·€ ëª¨ë¸ë“¤

ARIMA, VAR, State Space Models ë“± ì „ë¬¸ ì‹œê³„ì—´ ì•Œê³ ë¦¬ì¦˜ ì ìš©
"""

import sys
sys.path.append('/root/workspace/src')

import pandas as pd
import numpy as np
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# Core imports
from core.data_processor import DataProcessor

# Standard ML imports
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error

# Specialized time series imports
try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.vector_ar.var_model import VAR
    from statsmodels.tsa.holtwinters import ExponentialSmoothing
    from statsmodels.tsa.seasonal import STL
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    print("âš ï¸ Statsmodels ë¯¸ì„¤ì¹˜ - ì¼ë¶€ ì‹œê³„ì—´ ëª¨ë¸ ì œì™¸")

# Advanced time series imports
try:
    from sklearn.gaussian_process import GaussianProcessRegressor
    from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared
    GP_AVAILABLE = True
except ImportError:
    GP_AVAILABLE = False

# Facebook Prophet
try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    print("âš ï¸ Prophet ë¯¸ì„¤ì¹˜ - Prophet ëª¨ë¸ ì œì™¸")

class SpecializedTimeSeriesModels:
    """ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.data_processor = DataProcessor()
        self.max_allowed_correlation = 0.15  # ê·¹ë„ë¡œ ì—„ê²©í•œ ê¸°ì¤€
        self.target_r2_threshold = 0.15      # ë” ë†’ì€ RÂ² ëª©í‘œ
        self.target_mae_threshold = 0.008    # ë” ë‚®ì€ MAE ëª©í‘œ

        print(f"ğŸ¯ ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ ì—°êµ¬ ì‹œìŠ¤í…œ")
        print(f"   ğŸ¯ ëª©í‘œ: RÂ²>{self.target_r2_threshold}, MAE<{self.target_mae_threshold}")
        print(f"   ğŸ”’ ìµœëŒ€ í—ˆìš© ìƒê´€ê´€ê³„: {self.max_allowed_correlation}")

    def create_specialized_features(self, df):
        """ì „ë¬¸ ì‹œê³„ì—´ íŠ¹ì„± ìƒì„±"""
        print("ğŸ”§ ì „ë¬¸ ì‹œê³„ì—´ íŠ¹ì„± ìƒì„±...")

        ts_df = df.copy()

        # ê¸°ë³¸ ìˆ˜ìµë¥ 
        ts_df['returns'] = ts_df['Close'].pct_change()
        ts_df['log_returns'] = np.log(ts_df['Close'] / ts_df['Close'].shift(1))

        # 1. ì‹œê³„ì—´ ë¶„í•´ ê¸°ë°˜ íŠ¹ì„±
        if len(ts_df) > 50:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ
            try:
                # STL ë¶„í•´ (ê³„ì ˆì„±, ì¶”ì„¸, ì”ì°¨)
                if STATSMODELS_AVAILABLE:
                    stl = STL(ts_df['Close'].fillna(method='ffill'), seasonal=13)
                    result = stl.fit()

                    ts_df['trend'] = result.trend
                    ts_df['seasonal'] = result.seasonal
                    ts_df['residual'] = result.resid

                    # ë¶„í•´ ì„±ë¶„ ê¸°ë°˜ íŠ¹ì„±
                    ts_df['trend_strength'] = ts_df['trend'] / ts_df['Close']
                    ts_df['seasonal_strength'] = ts_df['seasonal'] / ts_df['Close']
                    ts_df['residual_strength'] = ts_df['residual'] / ts_df['Close']
            except:
                pass

        # 2. ê³ ê¸‰ ë˜ê·¸ íŠ¹ì„± (ìê¸°ìƒê´€ ê¸°ë°˜)
        optimal_lags = [1, 2, 3, 5, 8, 13, 21]  # í”¼ë³´ë‚˜ì¹˜ ê¸°ë°˜ ë˜ê·¸
        for lag in optimal_lags:
            ts_df[f'returns_lag_{lag}'] = ts_df['returns'].shift(lag)
            ts_df[f'log_returns_lag_{lag}'] = ts_df['log_returns'].shift(lag)

            # ë˜ê·¸ê°„ ì°¨ë¶„
            if lag > 1:
                ts_df[f'returns_diff_{lag}'] = ts_df['returns'] - ts_df['returns'].shift(lag)

        # 3. ì´ë™í†µê³„ íŠ¹ì„± (ë‹¤ì–‘í•œ ìœˆë„ìš°)
        windows = [5, 10, 21, 50, 100]
        for window in windows:
            # ì´ë™í‰ê·  ê¸°ë°˜
            ma = ts_df['returns'].rolling(window).mean()
            ts_df[f'ma_{window}'] = ma
            ts_df[f'returns_vs_ma_{window}'] = ts_df['returns'] - ma

            # ì´ë™ë¶„ì‚° ê¸°ë°˜
            ts_df[f'rolling_vol_{window}'] = ts_df['returns'].rolling(window).std()

            # ì´ë™ì²¨ë„/ì™œë„ (scipy.stats ì‚¬ìš©)
            from scipy.stats import skew, kurtosis
            ts_df[f'rolling_skew_{window}'] = ts_df['returns'].rolling(window).apply(lambda x: skew(x, nan_policy='omit'))
            ts_df[f'rolling_kurt_{window}'] = ts_df['returns'].rolling(window).apply(lambda x: kurtosis(x, nan_policy='omit'))

            # ì´ë™ ìµœëŒ€/ìµœì†Œ
            ts_df[f'rolling_max_{window}'] = ts_df['returns'].rolling(window).max()
            ts_df[f'rolling_min_{window}'] = ts_df['returns'].rolling(window).min()

        # 4. ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§ íŠ¹ì„±
        for window in [10, 20, 50]:
            vol = ts_df['returns'].rolling(window).std()
            ts_df[f'vol_regime_{window}'] = (vol > vol.rolling(100).quantile(0.75)).astype(int)

            # GARCH-like íŠ¹ì„±
            ts_df[f'vol_persistence_{window}'] = vol / vol.shift(1)

        # 5. ëª¨ë©˜í…€ ë° ë°˜ì „ íŠ¹ì„±
        for period in [5, 10, 20]:
            # ê°€ê²© ëª¨ë©˜í…€
            ts_df[f'price_momentum_{period}'] = (
                ts_df['Close'] / ts_df['Close'].shift(period) - 1
            )

            # ìˆ˜ìµë¥  ëª¨ë©˜í…€
            ts_df[f'return_momentum_{period}'] = (
                ts_df['returns'].rolling(period).sum()
            )

            # ë°˜ì „ ì‹ í˜¸
            ts_df[f'reversal_{period}'] = -ts_df[f'return_momentum_{period}'].shift(1)

        # 6. ë³¼ë¥¨ ê¸°ë°˜ ê³ ê¸‰ íŠ¹ì„±
        for period in [5, 10, 20]:
            # ë³¼ë¥¨ ê°€ì¤‘ ìˆ˜ìµë¥ 
            vol_weight = ts_df['Volume'] / ts_df['Volume'].rolling(period).sum()
            ts_df[f'vol_weighted_return_{period}'] = ts_df['returns'] * vol_weight

            # ë³¼ë¥¨ íŠ¸ë Œë“œ
            ts_df[f'volume_trend_{period}'] = (
                ts_df['Volume'] / ts_df['Volume'].rolling(period).mean() - 1
            )

        # 7. í”„ë™íƒˆ ë° ë³µì¡ì„± íŠ¹ì„±
        for window in [20, 50]:
            returns_window = ts_df['returns'].rolling(window)

            # í—ˆìŠ¤íŠ¸ ì§€ìˆ˜ ê·¼ì‚¬
            ts_df[f'hurst_approx_{window}'] = self._estimate_hurst(returns_window)

            # ì—”íŠ¸ë¡œí”¼ ê·¼ì‚¬
            ts_df[f'entropy_approx_{window}'] = returns_window.apply(self._estimate_entropy)

        # 8. íƒ€ê²Ÿ ë³€ìˆ˜ë“¤
        ts_df['target_return_1d'] = ts_df['returns'].shift(-1)
        ts_df['target_return_3d'] = ts_df['Close'].pct_change(3).shift(-3)
        ts_df['target_return_5d'] = ts_df['Close'].pct_change(5).shift(-5)

        # NaN ì²˜ë¦¬
        ts_df = ts_df.fillna(method='ffill').fillna(method='bfill').fillna(0)
        ts_df = ts_df.replace([np.inf, -np.inf], 0)

        print(f"   âœ… ì „ë¬¸ ì‹œê³„ì—´ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {ts_df.shape}")
        return ts_df

    def _estimate_hurst(self, series):
        """í—ˆìŠ¤íŠ¸ ì§€ìˆ˜ ì¶”ì •"""
        try:
            if len(series) < 20:
                return 0.5
            lags = range(2, min(20, len(series)//2))
            tau = [np.sqrt(np.std(np.subtract(series[lag:], series[:-lag]))) for lag in lags]
            poly = np.polyfit(np.log(lags), np.log(tau), 1)
            return poly[0] * 2.0
        except:
            return 0.5

    def _estimate_entropy(self, series):
        """ì—”íŠ¸ë¡œí”¼ ì¶”ì •"""
        try:
            _, counts = np.unique(series.round(6), return_counts=True)
            probabilities = counts / len(series)
            return -np.sum(probabilities * np.log2(probabilities + 1e-10))
        except:
            return 0.0

    def create_specialized_models(self):
        """ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ë“¤ ìƒì„±"""
        print("ğŸ¯ ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ë“¤ ìƒì„±...")

        models = {}

        # 1. ARIMA ëª¨ë¸ë“¤ (statsmodels)
        if STATSMODELS_AVAILABLE:
            models['ARIMA_101'] = {
                'type': 'arima',
                'params': {'order': (1, 0, 1)}
            }

            models['ARIMA_111'] = {
                'type': 'arima',
                'params': {'order': (1, 1, 1)}
            }

            models['ARIMA_212'] = {
                'type': 'arima',
                'params': {'order': (2, 1, 2)}
            }

            # SARIMAX ëª¨ë¸
            models['SARIMAX'] = {
                'type': 'sarimax',
                'params': {'order': (1, 1, 1), 'seasonal_order': (1, 1, 1, 5)}
            }

        # 2. ì§€ìˆ˜í‰í™œë²• ëª¨ë¸ë“¤
        if STATSMODELS_AVAILABLE:
            models['ExponentialSmoothing'] = {
                'type': 'exp_smoothing',
                'params': {'trend': 'add', 'seasonal': None}
            }

        # 3. VAR ëª¨ë¸ (ë‹¤ë³€ëŸ‰)
        if STATSMODELS_AVAILABLE:
            models['VAR_2'] = {
                'type': 'var',
                'params': {'maxlags': 2}
            }

        # 4. ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ì‹œê³„ì—´ ëª¨ë¸ë“¤
        if GP_AVAILABLE:
            # RBF ì»¤ë„ (í‰í™œí•¨)
            rbf_kernel = RBF(length_scale=1.0)
            models['GP_RBF'] = {
                'type': 'gp',
                'kernel': rbf_kernel
            }

            # Matern ì»¤ë„ (ì¼ë°˜ì )
            matern_kernel = Matern(length_scale=1.0, nu=1.5)
            models['GP_Matern'] = {
                'type': 'gp',
                'kernel': matern_kernel
            }

            # ì£¼ê¸°ì  ì»¤ë„ (ê³„ì ˆì„±)
            periodic_kernel = ExpSineSquared(length_scale=1.0, periodicity=1.0)
            models['GP_Periodic'] = {
                'type': 'gp',
                'kernel': periodic_kernel
            }

            # ë³µí•© ì»¤ë„
            composite_kernel = RBF(length_scale=1.0) + Matern(length_scale=1.0, nu=2.5)
            models['GP_Composite'] = {
                'type': 'gp',
                'kernel': composite_kernel
            }

        # 5. Prophet ëª¨ë¸
        if PROPHET_AVAILABLE:
            models['Prophet_Basic'] = {
                'type': 'prophet',
                'params': {'daily_seasonality': False, 'weekly_seasonality': True}
            }

        # 6. ì‚¬ìš©ì ì •ì˜ ì‹œê³„ì—´ ëª¨ë¸ë“¤
        models['AutoRegressive_Custom'] = {
            'type': 'custom_ar',
            'params': {'lags': [1, 2, 3, 5]}
        }

        models['MovingAverage_Adaptive'] = {
            'type': 'adaptive_ma',
            'params': {'windows': [5, 10, 20]}
        }

        print(f"   âœ… ìƒì„±ëœ ì „ë¬¸ ëª¨ë¸: {len(models)}ê°œ")
        return models

    def fit_specialized_model(self, model_name, model_config, X_train, y_train, X_val, y_val):
        """ì „ë¬¸ ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡"""

        model_type = model_config.get('type', 'unknown')

        try:
            if model_type == 'arima' and STATSMODELS_AVAILABLE:
                order = model_config['params']['order']
                model = ARIMA(y_train, order=order)
                fitted_model = model.fit()
                y_pred = fitted_model.forecast(steps=len(y_val))

            elif model_type == 'sarimax' and STATSMODELS_AVAILABLE:
                order = model_config['params']['order']
                seasonal_order = model_config['params']['seasonal_order']
                model = SARIMAX(y_train, order=order, seasonal_order=seasonal_order)
                fitted_model = model.fit(disp=False)
                y_pred = fitted_model.forecast(steps=len(y_val))

            elif model_type == 'exp_smoothing' and STATSMODELS_AVAILABLE:
                model = ExponentialSmoothing(y_train, **model_config['params'])
                fitted_model = model.fit()
                y_pred = fitted_model.forecast(steps=len(y_val))

            elif model_type == 'var' and STATSMODELS_AVAILABLE:
                # VARì€ ë‹¤ë³€ëŸ‰ì´ë¯€ë¡œ ì—¬ëŸ¬ íŠ¹ì„± ì‚¬ìš©
                if X_train.shape[1] >= 2:
                    data = np.column_stack([y_train, X_train[:, :min(3, X_train.shape[1])]])
                    model = VAR(data)
                    fitted_model = model.fit(maxlags=model_config['params']['maxlags'])
                    forecast = fitted_model.forecast(data[-fitted_model.k_ar:], steps=len(y_val))
                    y_pred = forecast[:, 0]  # ì²« ë²ˆì§¸ ë³€ìˆ˜ (íƒ€ê²Ÿ) ì˜ˆì¸¡
                else:
                    y_pred = np.full(len(y_val), np.mean(y_train))

            elif model_type == 'gp' and GP_AVAILABLE:
                kernel = model_config['kernel']
                model = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, random_state=42)

                # ì‹œê³„ì—´ìš© ì¸ë±ìŠ¤ ìƒì„±
                X_train_idx = np.arange(len(X_train)).reshape(-1, 1)
                X_val_idx = np.arange(len(X_train), len(X_train) + len(X_val)).reshape(-1, 1)

                model.fit(X_train_idx, y_train)
                y_pred, _ = model.predict(X_val_idx, return_std=True)

            elif model_type == 'prophet' and PROPHET_AVAILABLE:
                # Prophetìš© ë°ì´í„° ì¤€ë¹„
                train_df = pd.DataFrame({
                    'ds': pd.date_range(start='2020-01-01', periods=len(y_train), freq='D'),
                    'y': y_train
                })

                model = Prophet(**model_config['params'])
                model.fit(train_df)

                future = pd.DataFrame({
                    'ds': pd.date_range(start=train_df['ds'].iloc[-1] + pd.Timedelta(days=1),
                                       periods=len(y_val), freq='D')
                })
                forecast = model.predict(future)
                y_pred = forecast['yhat'].values

            elif model_type == 'custom_ar':
                # ì‚¬ìš©ì ì •ì˜ ìê¸°íšŒê·€ ëª¨ë¸
                lags = model_config['params']['lags']
                y_pred = self._custom_autoregressive_predict(y_train, lags, len(y_val))

            elif model_type == 'adaptive_ma':
                # ì ì‘í˜• ì´ë™í‰ê· 
                windows = model_config['params']['windows']
                y_pred = self._adaptive_moving_average_predict(y_train, windows, len(y_val))

            else:
                # ê¸°ë³¸ ì˜ˆì¸¡ (í‰ê· )
                y_pred = np.full(len(y_val), np.mean(y_train))

            return y_pred

        except Exception as e:
            print(f"      ëª¨ë¸ {model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return np.full(len(y_val), np.mean(y_train))

    def _custom_autoregressive_predict(self, y_train, lags, n_pred):
        """ì‚¬ìš©ì ì •ì˜ ìê¸°íšŒê·€ ì˜ˆì¸¡"""
        try:
            # ê°„ë‹¨í•œ ì„ í˜• ìê¸°íšŒê·€
            X_ar = []
            y_ar = []

            max_lag = max(lags)
            for i in range(max_lag, len(y_train)):
                X_ar.append([y_train[i-lag] for lag in lags])
                y_ar.append(y_train[i])

            X_ar = np.array(X_ar)
            y_ar = np.array(y_ar)

            # ì„ í˜• íšŒê·€ë¡œ ê³„ìˆ˜ ì¶”ì •
            from sklearn.linear_model import LinearRegression
            lr = LinearRegression()
            lr.fit(X_ar, y_ar)

            # ì˜ˆì¸¡
            predictions = []
            current = list(y_train[-max_lag:])

            for _ in range(n_pred):
                X_pred = np.array([[current[-lag] for lag in lags]])
                pred = lr.predict(X_pred)[0]
                predictions.append(pred)
                current.append(pred)

            return np.array(predictions)

        except:
            return np.full(n_pred, np.mean(y_train))

    def _adaptive_moving_average_predict(self, y_train, windows, n_pred):
        """ì ì‘í˜• ì´ë™í‰ê·  ì˜ˆì¸¡"""
        try:
            # ì—¬ëŸ¬ ìœˆë„ìš°ì˜ ê°€ì¤‘í‰ê· 
            predictions = []

            for _ in range(n_pred):
                window_preds = []
                weights = []

                for window in windows:
                    if len(y_train) >= window:
                        ma_pred = np.mean(y_train[-window:])
                        # ì§§ì€ ìœˆë„ìš°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
                        weight = 1.0 / window
                        window_preds.append(ma_pred)
                        weights.append(weight)

                if window_preds:
                    weights = np.array(weights)
                    weights = weights / weights.sum()
                    pred = np.average(window_preds, weights=weights)
                else:
                    pred = np.mean(y_train[-5:]) if len(y_train) >= 5 else np.mean(y_train)

                predictions.append(pred)
                y_train = np.append(y_train, pred)  # ì˜ˆì¸¡ê°’ì„ ë‹¤ìŒ ì˜ˆì¸¡ì— ì‚¬ìš©

            return np.array(predictions)

        except:
            return np.full(n_pred, np.mean(y_train))

    def evaluate_specialized_models(self, models, X, y, safe_features):
        """ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ í‰ê°€"""
        print(f"\nğŸ“Š ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ í‰ê°€ (íŠ¹ì„± ìˆ˜: {len(safe_features)})")

        # ë°ì´í„° ì¤€ë¹„
        X_features = X[safe_features].values if len(safe_features) > 0 else np.zeros((len(X), 1))
        y_values = y.values

        # ì•ˆì „ ì²˜ë¦¬
        X_features = np.nan_to_num(X_features, nan=0.0, posinf=0.0, neginf=0.0)
        y_values = np.nan_to_num(y_values, nan=0.0, posinf=0.0, neginf=0.0)

        # ìœ íš¨ ë°ì´í„°ë§Œ ì„ íƒ
        valid_idx = ~(pd.isna(y) | (y == 0))
        X_features = X_features[valid_idx]
        y_values = y_values[valid_idx]

        print(f"   ìµœì¢… ë°ì´í„°: X={X_features.shape}, y={y_values.shape}")
        print(f"   íƒ€ê²Ÿ í†µê³„: í‰ê· ={np.mean(y_values):.6f}, í‘œì¤€í¸ì°¨={np.std(y_values):.6f}")

        # ì‹œê°„ ìˆœì„œ ë¶„í•  (80-20)
        split_idx = int(len(y_values) * 0.8)
        X_train, X_val = X_features[:split_idx], X_features[split_idx:]
        y_train, y_val = y_values[:split_idx], y_values[split_idx:]

        results = {}

        for model_name, model_config in models.items():
            print(f"\n   ğŸ”¬ {model_name} í‰ê°€...")

            try:
                # ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
                y_pred = self.fit_specialized_model(model_name, model_config, X_train, y_train, X_val, y_val)

                # ì„±ëŠ¥ ê³„ì‚°
                r2 = r2_score(y_val, y_pred)
                mae = mean_absolute_error(y_val, y_pred)
                mse = mean_squared_error(y_val, y_pred)
                rmse = np.sqrt(mse)

                results[model_name] = {
                    'r2': r2,
                    'mae': mae,
                    'mse': mse,
                    'rmse': rmse,
                    'model_type': model_config.get('type', 'unknown'),
                    'target_achieved': {
                        'r2_target': r2 > self.target_r2_threshold,
                        'mae_target': mae < self.target_mae_threshold
                    }
                }

                # ì„±ëŠ¥ í‰ê°€
                status = "ğŸ¯ ëª©í‘œ ë‹¬ì„±!" if (r2 > self.target_r2_threshold and mae < self.target_mae_threshold) else "ğŸ“Š ê¸°ì¤€ì¹˜"
                print(f"      âœ… RÂ²={r2:.4f}, MAE={mae:.6f}, RMSE={rmse:.6f} - {status}")

            except Exception as e:
                print(f"      âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
                results[model_name] = {
                    'r2': -1.0,
                    'mae': 1.0,
                    'mse': 1.0,
                    'rmse': 1.0,
                    'model_type': model_config.get('type', 'unknown'),
                    'target_achieved': {'r2_target': False, 'mae_target': False},
                    'error': str(e)
                }

        return results

    def run_specialized_research(self, data_path='/root/workspace/data/training/sp500_2020_2024_enhanced.csv'):
        """ì „ë¬¸ ì‹œê³„ì—´ ì—°êµ¬ ì‹¤í–‰"""
        print("ğŸ¯ ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ ì—°êµ¬ ì‹œì‘")
        print("="*80)

        try:
            # 1. ë°ì´í„° ë¡œë”© ë° ì „ë¬¸ íŠ¹ì„± ìƒì„±
            df = self.data_processor.load_and_validate_data(data_path)
            ts_df = self.create_specialized_features(df)

            # 2. ê·¹ì—„ê²© ëˆ„ì¶œ ê²€ì¦
            safe_features = self.validate_ultra_strict_leakage(ts_df, 'target_return_1d')

            if len(safe_features) < 5:
                print("âš ï¸ ì•ˆì „ íŠ¹ì„±ì´ ë¶€ì¡±í•˜ì§€ë§Œ ì‹œê³„ì—´ ëª¨ë¸ì€ ì ì€ íŠ¹ì„±ìœ¼ë¡œë„ ê°€ëŠ¥")

            print(f"âœ… ì•ˆì „ íŠ¹ì„± {len(safe_features)}ê°œ í™•ë³´")

            # 3. ì „ë¬¸ ëª¨ë¸ë“¤ ìƒì„±
            models = self.create_specialized_models()

            # 4. ì„±ëŠ¥ í‰ê°€
            X = ts_df[safe_features] if safe_features else ts_df[['returns']].fillna(0)
            y = ts_df['target_return_1d']

            results = self.evaluate_specialized_models(models, X, y, safe_features)

            # 5. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
            self._analyze_specialized_results(results, safe_features)

            return results

        except Exception as e:
            print(f"âŒ ì „ë¬¸ ì‹œê³„ì—´ ì—°êµ¬ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def validate_ultra_strict_leakage(self, df, target_col='target_return_1d'):
        """ê·¹ì—„ê²© ë°ì´í„° ëˆ„ì¶œ ê²€ì¦"""
        print("ğŸ” ê·¹ì—„ê²© ë°ì´í„° ëˆ„ì¶œ ê²€ì¦...")

        exclude_cols = [
            target_col, 'target_return_3d', 'target_return_5d',
            'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'returns', 'log_returns'
        ]

        feature_cols = [col for col in df.columns if col not in exclude_cols]
        print(f"   ê²€ì¦í•  íŠ¹ì„± ìˆ˜: {len(feature_cols)}")

        safe_features = []
        suspicious_features = []

        for feature in feature_cols:
            if feature in df.columns and target_col in df.columns:
                corr = abs(df[feature].corr(df[target_col]))
                if not pd.isna(corr):
                    if corr > self.max_allowed_correlation:
                        suspicious_features.append((feature, corr))
                        print(f"   âš ï¸ ì˜ì‹¬ íŠ¹ì„±: {feature} (ìƒê´€ê´€ê³„: {corr:.4f})")
                    else:
                        safe_features.append(feature)

        if suspicious_features:
            print(f"   ğŸš¨ ì˜ì‹¬ íŠ¹ì„± {len(suspicious_features)}ê°œ ì œê±°!")
        else:
            print("   âœ… ëª¨ë“  íŠ¹ì„±ì´ ê·¹ì—„ê²© ê¸°ì¤€ í†µê³¼")

        return safe_features

    def _analyze_specialized_results(self, results, safe_features):
        """ì „ë¬¸ ì‹œê³„ì—´ ê²°ê³¼ ë¶„ì„"""
        print("\nğŸ“‹ ì „ë¬¸ ì‹œê³„ì—´ ì—°êµ¬ ê²°ê³¼ ë¶„ì„")
        print("="*60)

        # RÂ² ìˆœìœ„
        sorted_by_r2 = sorted(results.items(), key=lambda x: x[1]['r2'], reverse=True)

        print(f"\nğŸ† RÂ² ì„±ëŠ¥ ìˆœìœ„ (ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸):")
        for rank, (model_name, metrics) in enumerate(sorted_by_r2[:5], 1):
            r2 = metrics['r2']
            mae = metrics['mae']
            model_type = metrics['model_type']
            target_status = "ğŸ¯" if metrics['target_achieved']['r2_target'] and metrics['target_achieved']['mae_target'] else "ğŸ“Š"

            print(f"   {rank}. {model_name} ({model_type}): RÂ²={r2:.4f}, MAE={mae:.6f} {target_status}")

        # ëª¨ë¸ íƒ€ì…ë³„ ìµœê³  ì„±ëŠ¥
        type_best = {}
        for model_name, metrics in results.items():
            model_type = metrics['model_type']
            if model_type not in type_best or metrics['r2'] > type_best[model_type]['r2']:
                type_best[model_type] = {**metrics, 'name': model_name}

        print(f"\nğŸ“Š ëª¨ë¸ íƒ€ì…ë³„ ìµœê³  ì„±ëŠ¥:")
        for model_type, best_metrics in type_best.items():
            print(f"   {model_type}: {best_metrics['name']} - RÂ²={best_metrics['r2']:.4f}, MAE={best_metrics['mae']:.6f}")

        # ëª©í‘œ ë‹¬ì„± ëª¨ë¸ë“¤
        target_achieved = [(name, metrics) for name, metrics in results.items()
                          if metrics['target_achieved']['r2_target'] and metrics['target_achieved']['mae_target']]

        if target_achieved:
            print(f"\nğŸ‰ ëª©í‘œ ë‹¬ì„± ëª¨ë¸ë“¤ ({len(target_achieved)}ê°œ):")
            for name, metrics in target_achieved:
                print(f"   âœ… {name}: RÂ²={metrics['r2']:.4f}, MAE={metrics['mae']:.6f}")
        else:
            print(f"\nâš ï¸ ëª©í‘œ ë‹¬ì„± ëª¨ë¸ ì—†ìŒ - ì¶”ê°€ ì—°êµ¬ í•„ìš”")

        # ê²°ê³¼ ì €ì¥
        output_path = f"/root/workspace/data/results/specialized_time_series_models_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        try:
            with open(output_path, 'w') as f:
                json.dump({
                    'timestamp': datetime.now().isoformat(),
                    'experiment_type': 'specialized_time_series_models',
                    'target_thresholds': {
                        'r2_threshold': self.target_r2_threshold,
                        'mae_threshold': self.target_mae_threshold
                    },
                    'safe_features_count': len(safe_features),
                    'max_allowed_correlation': self.max_allowed_correlation,
                    'results': results
                }, f, indent=2)
            print(f"\nğŸ’¾ ì „ë¬¸ ì‹œê³„ì—´ ê²°ê³¼ ì €ì¥: {output_path}")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    research = SpecializedTimeSeriesModels()
    results = research.run_specialized_research()

    if results:
        print("\nğŸ‰ ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ ì—°êµ¬ ì™„ë£Œ!")
    else:
        print("\nâŒ ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ ì—°êµ¬ ì‹¤íŒ¨!")

    return results

if __name__ == "__main__":
    main()