#!/usr/bin/env python3
"""
Extended Data Integration Module
5ë…„ SPY ë°ì´í„°ë¥¼ í™œìš©í•œ í™•ì¥ ë°ì´í„°ì…‹ ìƒì„± ë° ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging
import warnings
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class ExtendedDataIntegrator:
    """5ë…„ SPY ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  ë°ì´í„° ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: str = "data", output_dir: str = "data/processed"):
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.raw_data = None
        self.processed_data = None
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•œ íŠ¹ì„± ë¶„ë¥˜
        self.safe_features = []
        self.leakage_features = []
        
    def load_extended_spy_data(self) -> pd.DataFrame:
        """5ë…„ SPY ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
        logger.info("Loading extended SPY data (2020-2025)...")
        
        spy_file = f"{self.data_dir}/raw/spy_data_2020_2025.csv"
        
        if not os.path.exists(spy_file):
            raise FileNotFoundError(f"Extended SPY data not found: {spy_file}")
            
        # ë°ì´í„° ë¡œë“œ
        self.raw_data = pd.read_csv(spy_file)
        
        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë¹ˆ ì´ë¦„ì¸ ê²½ìš° ì¸ë±ìŠ¤ë¡œ ì²˜ë¦¬
        if self.raw_data.columns[0] == '':
            self.raw_data = self.raw_data.drop(self.raw_data.columns[0], axis=1)
        
        # Date ì»¬ëŸ¼ ìƒì„± (ì¸ë±ìŠ¤ë¡œë¶€í„°)
        if 'Date' not in self.raw_data.columns:
            # ì¸ë±ìŠ¤ë¥¼ ë‚ ì§œë¡œ ë³€í™˜
            date_range = pd.date_range(start='2020-01-02', periods=len(self.raw_data), freq='D')
            # ì£¼ë§ ì œê±°
            business_days = pd.bdate_range(start='2020-01-02', periods=len(self.raw_data))
            
            # ì‹¤ì œë¡œëŠ” CSVì˜ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            # ì²« ë²ˆì§¸ í–‰ì„ í™•ì¸í•´ì„œ ë‚ ì§œ í˜•ì‹ì¸ì§€ íŒë‹¨
            try:
                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ ê°’ë“¤ì„ ë‚ ì§œë¡œ íŒŒì‹± ì‹œë„
                first_col_values = self.raw_data.iloc[:5, 0].astype(str)
                parsed_dates = pd.to_datetime(first_col_values, errors='coerce')
                
                if not parsed_dates.isna().all():
                    # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œ
                    self.raw_data['Date'] = pd.to_datetime(self.raw_data.iloc[:, 0])
                    self.raw_data = self.raw_data.drop(self.raw_data.columns[0], axis=1)
                else:
                    # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±
                    self.raw_data['Date'] = business_days[:len(self.raw_data)]
            except:
                # ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ ë‚ ì§œ ë²”ìœ„ ì‚¬ìš©
                self.raw_data['Date'] = business_days[:len(self.raw_data)]
        
        # ë‚ ì§œ ì •ë ¬
        self.raw_data = self.raw_data.sort_values('Date').reset_index(drop=True)
        
        # ê¸°ë³¸ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        missing_columns = [col for col in required_columns if col not in self.raw_data.columns]
        
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        logger.info(f"Loaded {len(self.raw_data)} data points from {self.raw_data['Date'].min()} to {self.raw_data['Date'].max()}")
        
        return self.raw_data
        
    def calculate_safe_technical_indicators(self, data: pd.DataFrame, lookback_only: bool = True) -> pd.DataFrame:
        """ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        
        Args:
            data: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            lookback_only: Trueë©´ ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©, Falseë©´ ë¯¸ë˜ ë°ì´í„°ë„ í—ˆìš©
        """
        logger.info("Calculating safe technical indicators (no data leakage)...")
        
        df = data.copy()
        
        # ê¸°ë³¸ ê°€ê²© ë³€í™”ìœ¨ (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        df['Returns'] = df['Close'].pct_change()
        df['Price_Change'] = df['Returns'].abs()
        
        # ê°€ê²© ëŒ€ë¹„ ê±°ë˜ëŸ‰ (ì•ˆì „)
        df['Volume_Price_Ratio'] = df['Volume'] / df['Close']
        
        # ì´ë™í‰ê·  (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        windows = [5, 10, 20, 50]
        for window in windows:
            df[f'SMA_{window}'] = df['Close'].rolling(window=window, min_periods=1).mean()
            df[f'Volume_SMA_{window}'] = df['Volume'].rolling(window=window, min_periods=1).mean()
            
            # ê°€ê²©ì´ ì´ë™í‰ê· ë³´ë‹¤ ë†’ì€ì§€ (ì•ˆì „)
            df[f'Price_Above_SMA_{window}'] = (df['Close'] > df[f'SMA_{window}']).astype(int)
            
        # ë³€ë™ì„± (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        volatility_windows = [5, 10, 20]
        for window in volatility_windows:
            df[f'Volatility_{window}'] = df['Returns'].rolling(window=window, min_periods=1).std()
            
        # RSI (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        df['RSI_14'] = self.calculate_rsi(df['Close'])
        
        # MACD (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        df['MACD'], df['MACD_Signal'] = self.calculate_macd(df['Close'])
        df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']
        
        # ë³¼ë¦°ì € ë°´ë“œ (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        df['BB_Upper'], df['BB_Lower'], df['BB_Middle'] = self.calculate_bollinger_bands(df['Close'])
        df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']
        df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])
        
        # ATR (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
        df['ATR_14'] = self.calculate_atr(df)
        
        # ê±°ë˜ëŸ‰ ì§€í‘œ (ì•ˆì „)
        for window in [5, 10, 20]:
            volume_sma = df[f'Volume_SMA_{window}']
            df[f'Volume_Ratio_{window}'] = df['Volume'] / volume_sma
            df[f'Volume_Spike_{window}'] = (df[f'Volume_Ratio_{window}'] > 2.0).astype(int)
            
        # ê°€ê²© ê°­ (ì•ˆì „)
        df['Price_Gap'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1)
        df['Gap_Up'] = (df['Price_Gap'] > 0.02).astype(int)  # 2% ì´ìƒ ê°­ì—…
        df['Gap_Down'] = (df['Price_Gap'] < -0.02).astype(int)  # 2% ì´ìƒ ê°­ë‹¤ìš´
        
        # ì¼ì¤‘ ë³€ë™ì„± (ì•ˆì „)
        df['Intraday_Range'] = (df['High'] - df['Low']) / df['Close']
        df['High_Low_Ratio'] = df['High'] / df['Low']
        
        # ì¢…ê°€ ìœ„ì¹˜ (ì•ˆì „)
        df['Close_Position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])
        
        # ê±°ë˜ëŸ‰ ê°€ì¤‘í‰ê· ê°€ê²© ê·¼ì‚¬ì¹˜ (ì•ˆì „)
        df['VWAP_Approx'] = (df['High'] + df['Low'] + df['Close']) / 3
        
        self.safe_features = [col for col in df.columns if col not in ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']]
        
        logger.info(f"Generated {len(self.safe_features)} safe technical indicators")
        
        return df
        
    def calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """RSI ê³„ì‚° (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()
        
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        
        return rsi
        
    def calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series]:
        """MACD ê³„ì‚° (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)"""
        exp1 = prices.ewm(span=fast, min_periods=1).mean()
        exp2 = prices.ewm(span=slow, min_periods=1).mean()
        macd = exp1 - exp2
        signal_line = macd.ewm(span=signal, min_periods=1).mean()
        
        return macd, signal_line
        
    def calculate_bollinger_bands(self, prices: pd.Series, window: int = 20, num_std: int = 2) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """ë³¼ë¦°ì € ë°´ë“œ ê³„ì‚° (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)"""
        rolling_mean = prices.rolling(window=window, min_periods=1).mean()
        rolling_std = prices.rolling(window=window, min_periods=1).std()
        
        upper_band = rolling_mean + (rolling_std * num_std)
        lower_band = rolling_mean - (rolling_std * num_std)
        
        return upper_band, lower_band, rolling_mean
        
    def calculate_atr(self, data: pd.DataFrame, window: int = 14) -> pd.Series:
        """ATR ê³„ì‚° (ì•ˆì „ - ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)"""
        high_low = data['High'] - data['Low']
        high_close = np.abs(data['High'] - data['Close'].shift())
        low_close = np.abs(data['Low'] - data['Close'].shift())
        
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        atr = true_range.rolling(window=window, min_periods=1).mean()
        
        return atr
        
    def create_safe_event_labels(self, data: pd.DataFrame, price_threshold: float = 0.02,
                                volume_threshold: float = 2.0) -> pd.DataFrame:
        """ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ ì´ë²¤íŠ¸ ë¼ë²¨ ìƒì„±
        
        Args:
            data: ì…ë ¥ ë°ì´í„°
            price_threshold: ê°€ê²© ë³€ë™ ì„ê³„ê°’ (2%)
            volume_threshold: ê±°ë˜ëŸ‰ ë¹„ìœ¨ ì„ê³„ê°’ (í‰ê· ì˜ 2ë°°)
        """
        logger.info("Creating safe event labels (no future data leakage)...")
        
        df = data.copy()
        
        # í˜„ì¬ ì‹œì ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ë¼ë²¨ ìƒì„±
        # ë¯¸ë˜ì˜ ê°€ê²© ë³€ë™ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë¯€ë¡œ, í˜„ì¬ê¹Œì§€ì˜ ì •ë³´ë§Œ ì‚¬ìš©
        
        # 1. ê°€ê²© ê¸‰ë³€ ì´ë²¤íŠ¸ (í˜„ì¬ ëŒ€ë¹„ ë‹¤ìŒë‚  ë³€ë™)
        df['Next_Return'] = df['Returns'].shift(-1)  # ì´ê²ƒì€ ì˜ˆì¸¡ ëŒ€ìƒì´ë¯€ë¡œ í—ˆìš©
        df['Price_Event'] = (np.abs(df['Next_Return']) > price_threshold).astype(int)
        
        # 2. ê±°ë˜ëŸ‰ ê¸‰ì¦ ì´ë²¤íŠ¸ (í˜„ì¬ ì‹œì  ê¸°ì¤€)
        df['Volume_20MA'] = df['Volume'].rolling(window=20, min_periods=1).mean()
        df['Volume_Event'] = (df['Volume'] > df['Volume_20MA'] * volume_threshold).astype(int)
        
        # 3. ë³µí•© ì´ë²¤íŠ¸ (ê°€ê²© + ê±°ë˜ëŸ‰)
        df['Major_Event'] = ((df['Price_Event'] == 1) & (df['Volume_Event'] == 1)).astype(int)
        
        # 4. íŠ¸ë Œë“œ ë³€í™” ì´ë²¤íŠ¸
        df['SMA_5'] = df['Close'].rolling(window=5, min_periods=1).mean()
        df['SMA_20'] = df['Close'].rolling(window=20, min_periods=1).mean()
        
        # ê³¨ë“ /ë°ë“œ í¬ë¡œìŠ¤ ê°ì§€ (í˜„ì¬ ì‹œì  ê¸°ì¤€)
        df['Golden_Cross'] = ((df['SMA_5'] > df['SMA_20']) & 
                             (df['SMA_5'].shift(1) <= df['SMA_20'].shift(1))).astype(int)
        df['Death_Cross'] = ((df['SMA_5'] < df['SMA_20']) & 
                            (df['SMA_5'].shift(1) >= df['SMA_20'].shift(1))).astype(int)
        
        df['Trend_Change'] = (df['Golden_Cross'] | df['Death_Cross']).astype(int)
        
        # 5. ìµœì¢… íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ì´ë²¤íŠ¸)
        # ë‹¤ìŒë‚  í° ê°€ê²© ë³€ë™ì´ ìˆì„ì§€ ì˜ˆì¸¡
        df['Target'] = df['Price_Event']
        
        # ë¯¸ë˜ ë°ì´í„° ì œê±° (Next_Returnì€ ì˜ˆì¸¡ ëŒ€ìƒì´ë¯€ë¡œ íŠ¹ì„±ì—ì„œ ì œì™¸)
        feature_columns = [col for col in df.columns 
                          if col not in ['Next_Return', 'Target'] and 'Event' not in col and 'Cross' not in col]
        
        logger.info(f"Created event labels. Target event rate: {df['Target'].mean():.4f}")
        
        return df
        
    def detect_data_leakage_features(self, data: pd.DataFrame) -> Dict[str, List[str]]:
        """ë°ì´í„° ëˆ„ìˆ˜ ê°€ëŠ¥ì„±ì´ ìˆëŠ” íŠ¹ì„±ë“¤ì„ ê°ì§€"""
        logger.info("Detecting potential data leakage features...")
        
        leakage_indicators = {
            'future_keywords': ['next', 'future', 'ahead', 'forward'],
            'perfect_correlation': [],
            'impossible_accuracy': []
        }
        
        safe_features = []
        risky_features = []
        
        target = data['Target']
        
        for col in data.columns:
            if col in ['Target', 'Date']:
                continue
                
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ì‚¬
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in leakage_indicators['future_keywords']):
                risky_features.append(col)
                continue
                
            # ì™„ë²½í•œ ìƒê´€ê´€ê³„ ê²€ì‚¬
            if pd.api.types.is_numeric_dtype(data[col]):
                try:
                    correlation = np.abs(np.corrcoef(data[col].fillna(0), target)[0, 1])
                    if correlation > 0.99:
                        leakage_indicators['perfect_correlation'].append(col)
                        risky_features.append(col)
                        continue
                except:
                    pass
                    
            # ì•ˆì „í•œ íŠ¹ì„±ìœ¼ë¡œ ë¶„ë¥˜
            safe_features.append(col)
        
        self.safe_features = safe_features
        self.leakage_features = risky_features
        
        logger.info(f"Safe features: {len(safe_features)}")
        logger.info(f"Risky features: {len(risky_features)}")
        
        if risky_features:
            logger.warning(f"Potential leakage features detected: {risky_features}")
            
        return {
            'safe_features': safe_features,
            'leakage_features': risky_features,
            'leakage_indicators': leakage_indicators
        }
        
    def save_processed_data(self, data: pd.DataFrame, filename: str = "extended_spy_features_safe.csv") -> str:
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        output_path = f"{self.output_dir}/{filename}"
        
        # ì•ˆì „í•œ íŠ¹ì„±ë§Œ ì €ì¥
        safe_columns = ['Date', 'Target'] + self.safe_features
        safe_data = data[safe_columns].copy()
        
        # NaN ê°’ ì²˜ë¦¬
        safe_data = safe_data.fillna(0)
        
        # ì €ì¥
        safe_data.to_csv(output_path, index=False)
        
        logger.info(f"Safe processed data saved: {output_path}")
        logger.info(f"Data shape: {safe_data.shape}")
        logger.info(f"Date range: {safe_data['Date'].min()} to {safe_data['Date'].max()}")
        logger.info(f"Target distribution: {safe_data['Target'].value_counts().to_dict()}")
        
        return output_path
        
    def create_data_report(self, data: pd.DataFrame) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'data_summary': {
                'total_samples': len(data),
                'date_range': {
                    'start': data['Date'].min().isoformat(),
                    'end': data['Date'].max().isoformat()
                },
                'target_distribution': data['Target'].value_counts().to_dict(),
                'target_rate': float(data['Target'].mean()),
                'total_features': len(self.safe_features),
                'safe_features_count': len(self.safe_features),
                'leakage_features_count': len(self.leakage_features)
            },
            'feature_summary': {
                'safe_features': self.safe_features[:10],  # ì²˜ìŒ 10ê°œë§Œ
                'leakage_features': self.leakage_features,
                'feature_categories': {
                    'price_features': [f for f in self.safe_features if 'price' in f.lower() or 'close' in f.lower()],
                    'volume_features': [f for f in self.safe_features if 'volume' in f.lower()],
                    'technical_features': [f for f in self.safe_features if any(t in f.lower() for t in ['sma', 'rsi', 'macd', 'bb', 'atr'])],
                    'volatility_features': [f for f in self.safe_features if 'volatility' in f.lower()]
                }
            },
            'data_quality': {
                'null_values': data[self.safe_features].isnull().sum().to_dict(),
                'infinite_values': np.isinf(data[self.safe_features].select_dtypes(include=[np.number])).sum().to_dict()
            }
        }
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = f"{self.output_dir}/extended_data_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
            
        logger.info(f"Data quality report saved: {report_path}")
        
        return report
        
    def run_complete_integration(self) -> Tuple[str, str]:
        """ì „ì²´ ë°ì´í„° í†µí•© í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("Starting complete data integration process...")
        
        # 1. 5ë…„ SPY ë°ì´í„° ë¡œë“œ
        raw_data = self.load_extended_spy_data()
        
        # 2. ì•ˆì „í•œ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        data_with_features = self.calculate_safe_technical_indicators(raw_data)
        
        # 3. ì•ˆì „í•œ ì´ë²¤íŠ¸ ë¼ë²¨ ìƒì„±
        data_with_labels = self.create_safe_event_labels(data_with_features)
        
        # 4. ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬
        leakage_analysis = self.detect_data_leakage_features(data_with_labels)
        
        # 5. ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        data_path = self.save_processed_data(data_with_labels)
        
        # 6. ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
        report = self.create_data_report(data_with_labels)
        report_path = f"{self.output_dir}/extended_data_report.json"
        
        logger.info("Data integration completed successfully!")
        logger.info(f"Processed data: {data_path}")
        logger.info(f"Quality report: {report_path}")
        
        return data_path, report_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    integrator = ExtendedDataIntegrator()
    
    try:
        data_path, report_path = integrator.run_complete_integration()
        
        print("\n" + "="*60)
        print("ğŸ¯ EXTENDED DATA INTEGRATION COMPLETED")
        print("="*60)
        print(f"ğŸ“Š Processed Data: {data_path}")
        print(f"ğŸ“‹ Quality Report: {report_path}")
        print(f"âœ… Ready for Walk-Forward Validation")
        print("="*60)
        
    except Exception as e:
        logger.error(f"Data integration failed: {e}")
        raise


if __name__ == "__main__":
    main()