#!/usr/bin/env python3
"""
LSTM í”„ë¡œí† íƒ€ì…
- ì‹œê³„ì—´ ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ
- Sequence-to-one ì•„í‚¤í…ì²˜
"""

import pandas as pd
import numpy as np
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.linear_model import Ridge
import warnings
warnings.filterwarnings('ignore')

# TensorFlow ì„í¬íŠ¸ ì‹œë„
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.optimizers import Adam
    tf_available = True
    print("TensorFlow version:", tf.__version__)
except ImportError:
    tf_available = False
    print("âš ï¸ TensorFlow not available - using fallback approach")

def purged_kfold_cv(X, y, n_splits=5, purge_length=5, embargo_length=5):
    """Purged K-Fold Cross-Validation"""
    n_samples = len(X)
    fold_size = n_samples // n_splits
    indices = np.arange(n_samples)

    for i in range(n_splits):
        test_start = i * fold_size
        test_end = (i + 1) * fold_size if i < n_splits - 1 else n_samples
        test_indices = indices[test_start:test_end]

        purge_start = max(0, test_start - purge_length)
        embargo_end = min(n_samples, test_end + embargo_length)

        train_indices = np.concatenate([
            indices[:purge_start],
            indices[embargo_end:]
        ])

        yield train_indices, test_indices

def create_sequences(X, y, sequence_length=20):
    """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±"""
    X_seq, y_seq, indices = [], [], []

    for i in range(sequence_length, len(X)):
        X_seq.append(X[i-sequence_length:i])
        y_seq.append(y[i])
        indices.append(i)

    return np.array(X_seq), np.array(y_seq), np.array(indices)

print("="*70)
print("ğŸ”¬ LSTM í”„ë¡œí† íƒ€ì…")
print("="*70)

if not tf_available:
    print("\nâŒ TensorFlowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ LSTM ì‹¤í—˜ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("   ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ Ridge ëª¨ë¸ ë¹„êµë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")

# 1. ë°ì´í„° ë¡œë“œ
print("\n1ï¸âƒ£  ë°ì´í„° ë¡œë“œ...")
spy = yf.Ticker("SPY")
df = spy.history(start="2015-01-01", end="2024-12-31")
df.index = pd.to_datetime(df.index).tz_localize(None)
df['returns'] = np.log(df['Close'] / df['Close'].shift(1))

# íƒ€ê²Ÿ ìƒì„±
targets = []
for i in range(len(df)):
    if i + 5 < len(df):
        future_returns = df['returns'].iloc[i+1:i+6]
        targets.append(future_returns.std())
    else:
        targets.append(np.nan)
df['target_vol_5d'] = targets

# 2. íŠ¹ì„± ìƒì„±
print("\n2ï¸âƒ£  íŠ¹ì„± ìƒì„±...")

for window in [5, 10, 20, 60]:
    df[f'volatility_{window}d'] = df['returns'].rolling(window).std()

for lag in [1, 2, 3, 5, 10, 20]:
    df[f'vol_lag_{lag}'] = df['volatility_20d'].shift(lag)

df['vol_mean_5d'] = df['volatility_20d'].rolling(5).mean()
df['vol_mean_10d'] = df['volatility_20d'].rolling(10).mean()
df['vol_std_5d'] = df['volatility_20d'].rolling(5).std()
df['vol_std_10d'] = df['volatility_20d'].rolling(10).std()

for window in [5, 10, 20]:
    df[f'momentum_{window}d'] = df['returns'].rolling(window).sum()

df['returns_mean_5d'] = df['returns'].rolling(5).mean()
df['returns_mean_10d'] = df['returns'].rolling(10).mean()
df['returns_std_5d'] = df['returns'].rolling(5).std()
df['returns_std_10d'] = df['returns'].rolling(10).std()

df['vol_change_5d'] = df['volatility_20d'].pct_change(5)
df['vol_change_10d'] = df['volatility_20d'].pct_change(10)

df['extreme_returns'] = (df['returns'].abs() > 2 * df['volatility_20d']).astype(int)
df['extreme_count_20d'] = df['extreme_returns'].rolling(20).sum()

df = df.dropna()

feature_cols = [col for col in df.columns if col not in
                ['returns', 'target_vol_5d', 'Close', 'Open', 'High', 'Low',
                 'Volume', 'Dividends', 'Stock Splits']]

X = df[feature_cols].values
y = df['target_vol_5d'].values

print(f"   ë°ì´í„°: {len(df)} ìƒ˜í”Œ")
print(f"   íŠ¹ì„±: {len(feature_cols)}ê°œ")

# 3. Baseline (Ridge) ì„±ëŠ¥
print("\n" + "="*70)
print("ğŸ“Š Baseline (Ridge) ì„±ëŠ¥")
print("="*70)

X_df = df[feature_cols]
y_df = df['target_vol_5d']

fold_r2_baseline = []

for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X_df, y_df, n_splits=5, purge_length=5, embargo_length=5), 1):

    X_train = X_df.iloc[train_idx]
    y_train = y_df.iloc[train_idx]
    X_test = X_df.iloc[test_idx]
    y_test = y_df.iloc[test_idx]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = Ridge(alpha=1.0)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    fold_r2 = r2_score(y_test, y_pred)
    fold_r2_baseline.append(fold_r2)
    print(f"Fold {fold_idx}: RÂ² = {fold_r2:.4f}")

baseline_cv_r2 = np.mean(fold_r2_baseline)
baseline_cv_std = np.std(fold_r2_baseline)

print(f"\nBaseline Ridge CV RÂ² Mean: {baseline_cv_r2:.4f} (Â±{baseline_cv_std:.4f})")

if not tf_available:
    print("\n" + "="*70)
    print("âš ï¸ LSTM ì‹¤í—˜ ë¶ˆê°€")
    print("="*70)
    print("""
TensorFlowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ LSTM í”„ë¡œí† íƒ€ì…ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì„¤ì¹˜ ë°©ë²•:
  pip install tensorflow --break-system-packages

ì°¸ê³ :
  - LSTMì€ ê³„ì‚° ë¹„ìš©ì´ ë†’ê³  í•™ìŠµ ì‹œê°„ì´ ê¹€
  - Ridge ëª¨ë¸ (RÂ² = {:.4f})ì´ ì´ë¯¸ ìš°ìˆ˜í•œ ì„±ëŠ¥
  - ì¶”ê°€ ê°œì„  ê°€ëŠ¥ì„± ë‚®ìŒ

ê¶Œì¥:
  âœ… í˜„ì¬ Ridge ëª¨ë¸ ìœ ì§€
  âœ… í•™ìˆ  ë…¼ë¬¸ ì‘ì„±
  âŒ ì‹¤ì „ ë°°í¬ëŠ” ì¶”ê°€ ê²€ì¦ í•„ìš”
    """.format(baseline_cv_r2))

    import sys
    sys.exit(0)

# 4. LSTM ëª¨ë¸ êµ¬ì¶•
print("\n" + "="*70)
print("ğŸ§  LSTM ëª¨ë¸ êµ¬ì¶•")
print("="*70)

sequence_length = 20  # 20ì¼ lookback

print(f"\nSequence Length: {sequence_length}")
print(f"Input Shape: ({sequence_length}, {len(feature_cols)})")

# LSTM ì•„í‚¤í…ì²˜
def build_lstm_model(sequence_length, n_features):
    """LSTM ëª¨ë¸ êµ¬ì¶•"""
    model = Sequential([
        # LSTM Layer 1
        LSTM(64, return_sequences=True, input_shape=(sequence_length, n_features)),
        Dropout(0.2),
        BatchNormalization(),

        # LSTM Layer 2
        LSTM(32, return_sequences=False),
        Dropout(0.2),
        BatchNormalization(),

        # Dense Layers
        Dense(16, activation='relu'),
        Dropout(0.1),
        Dense(1, activation='linear')
    ])

    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

    return model

# 5. LSTM Cross-Validation
print("\n" + "="*70)
print("ğŸ”¬ LSTM Cross-Validation")
print("="*70)

# Scaler ì¤€ë¹„
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ì‹œí€€ìŠ¤ ìƒì„±
X_seq, y_seq, seq_indices = create_sequences(X_scaled, y, sequence_length)

print(f"\nì‹œí€€ìŠ¤ ë°ì´í„°: {len(X_seq)} ìƒ˜í”Œ")
print(f"ì‹œí€€ìŠ¤ Shape: {X_seq.shape}")

fold_r2_lstm = []
fold_idx_counter = 0

# Purged K-Fold with sequences
for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X_df, y_df, n_splits=3, purge_length=5, embargo_length=5), 1):  # 3-fold (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)

    fold_idx_counter += 1
    print(f"\nFold {fold_idx_counter}/3:")

    # ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ë§¤í•‘
    seq_train_mask = np.isin(seq_indices, train_idx)
    seq_test_mask = np.isin(seq_indices, test_idx)

    X_train_seq = X_seq[seq_train_mask]
    y_train_seq = y_seq[seq_train_mask]
    X_test_seq = X_seq[seq_test_mask]
    y_test_seq = y_seq[seq_test_mask]

    print(f"  Train: {len(X_train_seq)} ìƒ˜í”Œ, Test: {len(X_test_seq)} ìƒ˜í”Œ")

    # LSTM ëª¨ë¸ ë¹Œë“œ
    model = build_lstm_model(sequence_length, len(feature_cols))

    # Callbacks
    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=0)

    # í•™ìŠµ
    print("  í•™ìŠµ ì¤‘...")
    history = model.fit(
        X_train_seq, y_train_seq,
        validation_split=0.2,
        epochs=50,
        batch_size=32,
        callbacks=[early_stop, reduce_lr],
        verbose=0
    )

    # ì˜ˆì¸¡
    y_pred = model.predict(X_test_seq, verbose=0).flatten()

    # í‰ê°€
    fold_r2 = r2_score(y_test_seq, y_pred)
    fold_mae = mean_absolute_error(y_test_seq, y_pred)
    fold_rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))

    fold_r2_lstm.append(fold_r2)

    print(f"  RÂ² = {fold_r2:.4f}, MAE = {fold_mae:.6f}, RMSE = {fold_rmse:.6f}")
    print(f"  Epochs: {len(history.history['loss'])}, Best Epoch: {np.argmin(history.history['val_loss']) + 1}")

lstm_cv_r2 = np.mean(fold_r2_lstm)
lstm_cv_std = np.std(fold_r2_lstm)

print(f"\nLSTM CV RÂ² Mean: {lstm_cv_r2:.4f} (Â±{lstm_cv_std:.4f})")

# 6. ê²°ê³¼ ë¹„êµ
print("\n" + "="*70)
print("ğŸ“ˆ LSTM vs Ridge ë¹„êµ")
print("="*70)

# Baselineë„ 3-foldë¡œ ì¬ê³„ì‚° (ê³µì •í•œ ë¹„êµ)
fold_r2_baseline_3fold = fold_r2_baseline[:3]
baseline_cv_r2_3fold = np.mean(fold_r2_baseline_3fold)

print(f"\nCV RÂ² Mean (3-fold):")
print(f"  Ridge Baseline:  {baseline_cv_r2_3fold:.4f}")
print(f"  LSTM:            {lstm_cv_r2:.4f}")
print(f"  Î”:               {lstm_cv_r2 - baseline_cv_r2_3fold:+.4f} "
      f"({(lstm_cv_r2 / baseline_cv_r2_3fold - 1) * 100:+.1f}%)")

print(f"\nCV RÂ² Std (ì•ˆì •ì„±):")
print(f"  Ridge:  {np.std(fold_r2_baseline_3fold):.4f}")
print(f"  LSTM:   {lstm_cv_std:.4f}")

# 7. ê²°ê³¼ ì €ì¥
print("\n" + "="*70)
print("ğŸ’¾ ê²°ê³¼ ì €ì¥")
print("="*70)

import json
results = {
    "experiment": "LSTM Prototype",
    "date": pd.Timestamp.now().isoformat(),
    "architecture": {
        "sequence_length": sequence_length,
        "layers": ["LSTM(64)", "LSTM(32)", "Dense(16)", "Dense(1)"],
        "dropout": 0.2,
        "optimizer": "Adam(lr=0.001)",
        "epochs": 50,
        "batch_size": 32
    },
    "baseline_ridge": {
        "cv_r2_mean_5fold": float(baseline_cv_r2),
        "cv_r2_mean_3fold": float(baseline_cv_r2_3fold),
        "cv_r2_std": float(np.std(fold_r2_baseline_3fold)),
        "cv_scores": [float(r2) for r2 in fold_r2_baseline_3fold]
    },
    "lstm": {
        "cv_r2_mean": float(lstm_cv_r2),
        "cv_r2_std": float(lstm_cv_std),
        "cv_scores": [float(r2) for r2 in fold_r2_lstm]
    },
    "improvement": {
        "cv_r2_delta": float(lstm_cv_r2 - baseline_cv_r2_3fold),
        "cv_r2_pct": float((lstm_cv_r2 / baseline_cv_r2_3fold - 1) * 100),
    },
    "conclusion": "LSTM í”„ë¡œí† íƒ€ì… ì‹¤í—˜ ì™„ë£Œ"
}

with open('data/raw/lstm_prototype_results.json', 'w') as f:
    json.dump(results, f, indent=2)

print("\nâœ… ê²°ê³¼ ì €ì¥: data/raw/lstm_prototype_results.json")

# 8. ìµœì¢… ê²°ë¡ 
print("\n" + "="*70)
print("ğŸ¯ ìµœì¢… ê²°ë¡ ")
print("="*70)

if lstm_cv_r2 > baseline_cv_r2_3fold:
    improvement_pct = (lstm_cv_r2 / baseline_cv_r2_3fold - 1) * 100
    print(f"""
âœ… LSTM ì„±ê³µ!

ì„±ëŠ¥ ê°œì„ :
  - CV RÂ² Mean: {baseline_cv_r2_3fold:.4f} â†’ {lstm_cv_r2:.4f} ({improvement_pct:+.1f}%)
  - LSTMì´ Ridgeë³´ë‹¤ ìš°ìˆ˜

ì¥ì :
  - ë¹„ì„ í˜• ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
  - Sequence ì •ë³´ í™œìš©
  - ë”¥ëŸ¬ë‹ ì¥ì  í™œìš©

ë‹¨ì :
  - í•™ìŠµ ì‹œê°„ ê¹€ (Ridge ëŒ€ë¹„ 10-100ë°°)
  - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”
  - í•´ì„ ì–´ë ¤ì›€ (Black Box)

ê¶Œì¥ì‚¬í•­:
  âš ï¸ ì„±ëŠ¥ ê°œì„ ì´ ë¯¸ë¯¸í•˜ë©´ Ridge ìœ ì§€ (ë‹¨ìˆœì„±)
  âœ… ê°œì„ ì´ í¬ë©´ (>10%) LSTM ê³ ë ¤
  ğŸ” Production ë°°í¬ ì „ ì¶”ê°€ ê²€ì¦ í•„ìˆ˜
""")
else:
    decline_pct = (lstm_cv_r2 / baseline_cv_r2_3fold - 1) * 100
    print(f"""
âŒ LSTM ì‹¤íŒ¨ (ë˜ëŠ” ê°œì„  ë¯¸ë¯¸)

ì„±ëŠ¥ ë³€í™”:
  - CV RÂ² Mean: {baseline_cv_r2_3fold:.4f} â†’ {lstm_cv_r2:.4f} ({decline_pct:+.1f}%)
  - Ridgeê°€ ë” ìš°ìˆ˜í•˜ê±°ë‚˜ ìœ ì‚¬

ë¬¸ì œ ë¶„ì„:
  - ë°ì´í„° ë¶€ì¡± (LSTMì€ ëŒ€ëŸ‰ ë°ì´í„° í•„ìš”)
  - ê³¼ì í•© (ì‹œí€€ìŠ¤ ë°ì´í„° ë” ì ìŒ)
  - ë³€ë™ì„± ì˜ˆì¸¡ì€ ì„ í˜• ëª¨ë¸ë¡œ ì¶©ë¶„

ê¶Œì¥ì‚¬í•­:
  âœ… Ridge ëª¨ë¸ ìœ ì§€ (ë‹¨ìˆœí•˜ê³  í•´ì„ ê°€ëŠ¥)
  âœ… í•™ìˆ  ë…¼ë¬¸ ì‘ì„± (í˜„ì¬ ì„±ëŠ¥ìœ¼ë¡œ ì¶©ë¶„)
  âŒ ì¶”ê°€ ë³µì¡í™” ë¶ˆí•„ìš”

ìµœì¢… ëª¨ë¸: Ridge Regression (RÂ² = {baseline_cv_r2:.4f})
""")

print("\n" + "="*70)
print("ğŸ ëª¨ë“  ê°œì„  ì‹¤í—˜ ì™„ë£Œ")
print("="*70)

print("""
ì‹¤í—˜ ìš”ì•½:
1. âŒ Quick Win (Regime + Clipping): -13.8% ì•…í™”
2. âŒ VIX í†µí•©: -24.1% ì•…í™”
3. âŒ GARCH í†µí•©: -16.2% ì•…í™”
4. {} LSTM: {:+.1f}% ë³€í™”

ê²°ë¡ :
  - ê¸°ë³¸ Ridge ëª¨ë¸ (RÂ² = 0.31)ì´ ìµœì 
  - ì¶”ê°€ ë³µì¡í™”ëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ ì•…í™”
  - Simple is Better (Occam's Razor)

ìµœì¢… ê¶Œì¥:
  âœ… Ridge Regression (V0) ìµœì¢… ëª¨ë¸ë¡œ í™•ì •
  âœ… í•™ìˆ  ë…¼ë¬¸ ì‘ì„± (HAR ëŒ€ë¹„ 35.5ë°° ìš°ìˆ˜)
  âš ï¸ ì‹¤ì „ ë°°í¬ëŠ” ì¶”ê°€ ê²€ì¦ í›„ ì œí•œì  ì‚¬ìš©
  âŒ ë” ì´ìƒì˜ ë³µì¡í™” ë¶ˆí•„ìš”
""".format('âœ…' if lstm_cv_r2 > baseline_cv_r2_3fold else 'âŒ',
           (lstm_cv_r2 / baseline_cv_r2_3fold - 1) * 100))
