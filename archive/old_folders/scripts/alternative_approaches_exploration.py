#!/usr/bin/env python3
"""
ëŒ€ì•ˆì  ì ‘ê·¼ë²• íƒìƒ‰
ê¸°ì¡´ ë°©ë²•ê³¼ ì™„ì „íˆ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì‹œë„

1. Quantile Regression - ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡
2. Random Forest - ë¹„ì„ í˜• ì•™ìƒë¸”
3. Target ì¬ì„¤ê³„ - Realized Volatility
4. Feature Selection - ìµœì†Œ íŠ¹ì„±ìœ¼ë¡œ ìµœëŒ€ ì„±ëŠ¥
5. Ensemble Stacking - ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©
"""

import pandas as pd
import numpy as np
import yfinance as yf
from sklearn.linear_model import Ridge, QuantileRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
import warnings
warnings.filterwarnings('ignore')

def purged_kfold_cv(X, y, n_splits=5, purge_length=5, embargo_length=5):
    """Purged K-Fold Cross-Validation"""
    n_samples = len(X)
    fold_size = n_samples // n_splits
    indices = np.arange(n_samples)

    for i in range(n_splits):
        test_start = i * fold_size
        test_end = (i + 1) * fold_size if i < n_splits - 1 else n_samples
        test_indices = indices[test_start:test_end]

        purge_start = max(0, test_start - purge_length)
        embargo_end = min(n_samples, test_end + embargo_length)

        train_indices = np.concatenate([
            indices[:purge_start],
            indices[embargo_end:]
        ])

        yield train_indices, test_indices

print("="*70)
print("ğŸ”¬ ëŒ€ì•ˆì  ì ‘ê·¼ë²• íƒìƒ‰")
print("="*70)

# ë°ì´í„° ë¡œë“œ
print("\n1ï¸âƒ£  ë°ì´í„° ë¡œë“œ...")
spy = yf.Ticker("SPY")
df = spy.history(start="2015-01-01", end="2024-12-31")
df.index = pd.to_datetime(df.index).tz_localize(None)
df['returns'] = np.log(df['Close'] / df['Close'].shift(1))

# ê¸°ë³¸ íƒ€ê²Ÿ (V0 ë°©ì‹)
targets = []
for i in range(len(df)):
    if i + 5 < len(df):
        future_returns = df['returns'].iloc[i+1:i+6]
        targets.append(future_returns.std())
    else:
        targets.append(np.nan)
df['target_vol_5d'] = targets

# ê¸°ë³¸ íŠ¹ì„± ìƒì„±
print("\n2ï¸âƒ£  ê¸°ë³¸ íŠ¹ì„± ìƒì„±...")

for window in [5, 10, 20, 60]:
    df[f'volatility_{window}d'] = df['returns'].rolling(window).std()

for lag in [1, 2, 3, 5, 10, 20]:
    df[f'vol_lag_{lag}'] = df['volatility_20d'].shift(lag)

df['vol_mean_5d'] = df['volatility_20d'].rolling(5).mean()
df['vol_mean_10d'] = df['volatility_20d'].rolling(10).mean()
df['vol_std_5d'] = df['volatility_20d'].rolling(5).std()
df['vol_std_10d'] = df['volatility_20d'].rolling(10).std()

for window in [5, 10, 20]:
    df[f'momentum_{window}d'] = df['returns'].rolling(window).sum()

df['returns_mean_5d'] = df['returns'].rolling(5).mean()
df['returns_mean_10d'] = df['returns'].rolling(10).mean()
df['returns_std_5d'] = df['returns'].rolling(5).std()
df['returns_std_10d'] = df['returns'].rolling(10).std()

df['vol_change_5d'] = df['volatility_20d'].pct_change(5)
df['vol_change_10d'] = df['volatility_20d'].pct_change(10)

df['extreme_returns'] = (df['returns'].abs() > 2 * df['volatility_20d']).astype(int)
df['extreme_count_20d'] = df['extreme_returns'].rolling(20).sum()

df = df.dropna()

feature_cols = [col for col in df.columns if col not in
                ['returns', 'target_vol_5d', 'Close', 'Open', 'High', 'Low',
                 'Volume', 'Dividends', 'Stock Splits']]

X = df[feature_cols]
y = df['target_vol_5d']

print(f"   ë°ì´í„°: {len(df)} ìƒ˜í”Œ")
print(f"   íŠ¹ì„±: {len(feature_cols)}ê°œ")

# Baseline
print("\n" + "="*70)
print("ğŸ“Š Baseline (Ridge)")
print("="*70)

baseline_scores = []
for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X, y, n_splits=5, purge_length=5, embargo_length=5), 1):

    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = Ridge(alpha=1.0)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    r2 = r2_score(y_test, y_pred)
    baseline_scores.append(r2)
    print(f"Fold {fold_idx}: RÂ² = {r2:.4f}")

baseline_mean = np.mean(baseline_scores)
print(f"\nBaseline Mean RÂ²: {baseline_mean:.4f} (Â±{np.std(baseline_scores):.4f})")

# ==================== ëŒ€ì•ˆ 1: Quantile Regression ====================
print("\n" + "="*70)
print("ğŸ¯ ëŒ€ì•ˆ 1: Quantile Regression (ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡)")
print("="*70)
print("   ì „ëµ: ì  ì˜ˆì¸¡ ëŒ€ì‹  ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡ (10%, 50%, 90%)")

quantile_scores = []
for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X, y, n_splits=5, purge_length=5, embargo_length=5), 1):

    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Median ì˜ˆì¸¡ (quantile=0.5)
    model = QuantileRegressor(quantile=0.5, alpha=1.0, solver='highs')
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    r2 = r2_score(y_test, y_pred)
    quantile_scores.append(r2)
    print(f"Fold {fold_idx}: RÂ² = {r2:.4f}")

quantile_mean = np.mean(quantile_scores)
print(f"\nQuantile Mean RÂ²: {quantile_mean:.4f} (Â±{np.std(quantile_scores):.4f})")
print(f"vs Baseline: {quantile_mean - baseline_mean:+.4f} ({(quantile_mean/baseline_mean - 1)*100:+.1f}%)")

# ==================== ëŒ€ì•ˆ 2: Random Forest ====================
print("\n" + "="*70)
print("ğŸŒ² ëŒ€ì•ˆ 2: Random Forest (ë¹„ì„ í˜• ì•™ìƒë¸”)")
print("="*70)
print("   ì „ëµ: íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ë¡œ ë¹„ì„ í˜• ê´€ê³„ í¬ì°©")

rf_scores = []
for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X, y, n_splits=5, purge_length=5, embargo_length=5), 1):

    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]

    # Random Forest (ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš”)
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=50,
        min_samples_leaf=20,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    rf_scores.append(r2)
    print(f"Fold {fold_idx}: RÂ² = {r2:.4f}")

rf_mean = np.mean(rf_scores)
print(f"\nRandom Forest Mean RÂ²: {rf_mean:.4f} (Â±{np.std(rf_scores):.4f})")
print(f"vs Baseline: {rf_mean - baseline_mean:+.4f} ({(rf_mean/baseline_mean - 1)*100:+.1f}%)")

# ==================== ëŒ€ì•ˆ 3: Gradient Boosting ====================
print("\n" + "="*70)
print("ğŸš€ ëŒ€ì•ˆ 3: Gradient Boosting")
print("="*70)
print("   ì „ëµ: Boostingìœ¼ë¡œ ì”ì°¨ í•™ìŠµ")

gb_scores = []
for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X, y, n_splits=5, purge_length=5, embargo_length=5), 1):

    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]

    model = GradientBoostingRegressor(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.01,
        subsample=0.8,
        min_samples_split=50,
        min_samples_leaf=20,
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    gb_scores.append(r2)
    print(f"Fold {fold_idx}: RÂ² = {r2:.4f}")

gb_mean = np.mean(gb_scores)
print(f"\nGradient Boosting Mean RÂ²: {gb_mean:.4f} (Â±{np.std(gb_scores):.4f})")
print(f"vs Baseline: {gb_mean - baseline_mean:+.4f} ({(gb_mean/baseline_mean - 1)*100:+.1f}%)")

# ==================== ëŒ€ì•ˆ 4: Feature Selection ====================
print("\n" + "="*70)
print("ğŸ¯ ëŒ€ì•ˆ 4: Feature Selection (ìµœì†Œ íŠ¹ì„±)")
print("="*70)
print("   ì „ëµ: ê°€ì¥ ì¤‘ìš”í•œ 10ê°œ íŠ¹ì„±ë§Œ ì‚¬ìš©")

# Feature importance ë¶„ì„
X_train_full = X.iloc[:int(len(X)*0.8)]
y_train_full = y.iloc[:int(len(y)*0.8)]

selector = SelectKBest(score_func=mutual_info_regression, k=10)
selector.fit(X_train_full, y_train_full)

selected_features = X.columns[selector.get_support()].tolist()
print(f"\n   ì„ íƒëœ 10ê°œ íŠ¹ì„±:")
for i, feat in enumerate(selected_features, 1):
    print(f"   {i:2d}. {feat}")

X_selected = X[selected_features]

fs_scores = []
for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X_selected, y, n_splits=5, purge_length=5, embargo_length=5), 1):

    X_train, y_train = X_selected.iloc[train_idx], y.iloc[train_idx]
    X_test, y_test = X_selected.iloc[test_idx], y.iloc[test_idx]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = Ridge(alpha=1.0)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    r2 = r2_score(y_test, y_pred)
    fs_scores.append(r2)
    print(f"Fold {fold_idx}: RÂ² = {r2:.4f}")

fs_mean = np.mean(fs_scores)
print(f"\nFeature Selection Mean RÂ²: {fs_mean:.4f} (Â±{np.std(fs_scores):.4f})")
print(f"vs Baseline (26 features): {fs_mean - baseline_mean:+.4f} ({(fs_mean/baseline_mean - 1)*100:+.1f}%)")

# ==================== ëŒ€ì•ˆ 5: Simple Ensemble ====================
print("\n" + "="*70)
print("ğŸ­ ëŒ€ì•ˆ 5: Simple Ensemble (Ridge + RF)")
print("="*70)
print("   ì „ëµ: Ridgeì™€ Random Forest í‰ê· ")

ensemble_scores = []
for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X, y, n_splits=5, purge_length=5, embargo_length=5), 1):

    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]

    # Ridge
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    ridge = Ridge(alpha=1.0)
    ridge.fit(X_train_scaled, y_train)
    ridge_pred = ridge.predict(X_test_scaled)

    # Random Forest
    rf = RandomForestRegressor(
        n_estimators=100, max_depth=10, min_samples_split=50,
        min_samples_leaf=20, random_state=42, n_jobs=-1
    )
    rf.fit(X_train, y_train)
    rf_pred = rf.predict(X_test)

    # í‰ê· 
    y_pred = (ridge_pred + rf_pred) / 2

    r2 = r2_score(y_test, y_pred)
    ensemble_scores.append(r2)
    print(f"Fold {fold_idx}: RÂ² = {r2:.4f}")

ensemble_mean = np.mean(ensemble_scores)
print(f"\nEnsemble Mean RÂ²: {ensemble_mean:.4f} (Â±{np.std(ensemble_scores):.4f})")
print(f"vs Baseline: {ensemble_mean - baseline_mean:+.4f} ({(ensemble_mean/baseline_mean - 1)*100:+.1f}%)")

# ==================== ëŒ€ì•ˆ 6: Realized Volatility Target ====================
print("\n" + "="*70)
print("ğŸ“ ëŒ€ì•ˆ 6: Realized Volatility (ê³ ë¹ˆë„ ê³„ì‚°)")
print("="*70)
print("   ì „ëµ: ì¼ì¤‘ High-Lowë¡œ ë³€ë™ì„± ì¶”ì •")

# Parkinson's volatility (High-Low ê¸°ë°˜)
df['parkinson_vol'] = np.sqrt(
    1/(4*np.log(2)) * (np.log(df['High']/df['Low']))**2
)

# 5ì¼ í‰ê·  Parkinson volatilityë¥¼ íƒ€ê²Ÿìœ¼ë¡œ
targets_rv = []
for i in range(len(df)):
    if i + 5 < len(df):
        future_vol = df['parkinson_vol'].iloc[i+1:i+6].mean()
        targets_rv.append(future_vol)
    else:
        targets_rv.append(np.nan)

df['target_rv_5d'] = targets_rv
df_rv = df.dropna()

X_rv = df_rv[feature_cols]
y_rv = df_rv['target_rv_5d']

print(f"   Realized Vol ë°ì´í„°: {len(df_rv)} ìƒ˜í”Œ")

rv_scores = []
for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X_rv, y_rv, n_splits=5, purge_length=5, embargo_length=5), 1):

    X_train, y_train = X_rv.iloc[train_idx], y_rv.iloc[train_idx]
    X_test, y_test = X_rv.iloc[test_idx], y_rv.iloc[test_idx]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = Ridge(alpha=1.0)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    r2 = r2_score(y_test, y_pred)
    rv_scores.append(r2)
    print(f"Fold {fold_idx}: RÂ² = {r2:.4f}")

rv_mean = np.mean(rv_scores)
print(f"\nRealized Vol Mean RÂ²: {rv_mean:.4f} (Â±{np.std(rv_scores):.4f})")
print(f"vs Baseline: {rv_mean - baseline_mean:+.4f} ({(rv_mean/baseline_mean - 1)*100:+.1f}%)")

# ==================== ìµœì¢… ê²°ê³¼ ====================
print("\n" + "="*70)
print("ğŸ“Š ëª¨ë“  ëŒ€ì•ˆ ë¹„êµ")
print("="*70)

results = [
    ("Baseline (Ridge)", baseline_mean, np.std(baseline_scores)),
    ("Quantile Regression", quantile_mean, np.std(quantile_scores)),
    ("Random Forest", rf_mean, np.std(rf_scores)),
    ("Gradient Boosting", gb_mean, np.std(gb_scores)),
    ("Feature Selection (10ê°œ)", fs_mean, np.std(fs_scores)),
    ("Ensemble (Ridge+RF)", ensemble_mean, np.std(ensemble_scores)),
    ("Realized Volatility", rv_mean, np.std(rv_scores)),
]

results_sorted = sorted(results, key=lambda x: x[1], reverse=True)

print(f"\n{'ë°©ë²•':<30s} {'RÂ² Mean':>10s} {'Std':>10s} {'vs Baseline':>15s}")
print("-" * 70)
for name, mean, std in results_sorted:
    delta = mean - baseline_mean
    pct = (mean / baseline_mean - 1) * 100 if baseline_mean != 0 else 0
    symbol = "ğŸ†" if mean == max(r[1] for r in results) else "  "
    print(f"{symbol} {name:<28s} {mean:10.4f} {std:10.4f} {delta:+7.4f} ({pct:+6.1f}%)")

# ìŠ¹ì í™•ì¸
best = results_sorted[0]
print("\n" + "="*70)
print("ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸")
print("="*70)
print(f"\nëª¨ë¸: {best[0]}")
print(f"RÂ² Mean: {best[1]:.4f} (Â±{best[2]:.4f})")
print(f"ê°œì„ : {best[1] - baseline_mean:+.4f} ({(best[1]/baseline_mean - 1)*100:+.1f}%)")

if best[1] > baseline_mean * 1.05:  # 5% ì´ìƒ ê°œì„ 
    print("\nâœ… ì˜ë¯¸ ìˆëŠ” ê°œì„  ë°œê²¬!")
    print(f"   {best[0]} ëª¨ë¸ì„ ìµœì¢… ëª¨ë¸ë¡œ ê³ ë ¤ ê°€ëŠ¥")
elif best[1] > baseline_mean:
    print("\nâš ï¸ ë¯¸ë¯¸í•œ ê°œì„ ")
    print(f"   ê°œì„  í­ì´ ì‘ì•„ Baseline ìœ ì§€ ê¶Œì¥")
else:
    print("\nâŒ ê°œì„  ì—†ìŒ")
    print("   Baseline (Ridge) ìœ ì§€ ê¶Œì¥")

# ê²°ê³¼ ì €ì¥
import json
results_dict = {
    "experiment": "Alternative Approaches",
    "date": pd.Timestamp.now().isoformat(),
    "baseline": {
        "model": "Ridge",
        "r2_mean": float(baseline_mean),
        "r2_std": float(np.std(baseline_scores)),
        "scores": [float(s) for s in baseline_scores]
    },
    "alternatives": {
        "quantile_regression": {
            "r2_mean": float(quantile_mean),
            "r2_std": float(np.std(quantile_scores)),
            "improvement_pct": float((quantile_mean/baseline_mean - 1)*100)
        },
        "random_forest": {
            "r2_mean": float(rf_mean),
            "r2_std": float(np.std(rf_scores)),
            "improvement_pct": float((rf_mean/baseline_mean - 1)*100)
        },
        "gradient_boosting": {
            "r2_mean": float(gb_mean),
            "r2_std": float(np.std(gb_scores)),
            "improvement_pct": float((gb_mean/baseline_mean - 1)*100)
        },
        "feature_selection": {
            "r2_mean": float(fs_mean),
            "r2_std": float(np.std(fs_scores)),
            "features": selected_features,
            "improvement_pct": float((fs_mean/baseline_mean - 1)*100)
        },
        "ensemble": {
            "r2_mean": float(ensemble_mean),
            "r2_std": float(np.std(ensemble_scores)),
            "improvement_pct": float((ensemble_mean/baseline_mean - 1)*100)
        },
        "realized_volatility": {
            "r2_mean": float(rv_mean),
            "r2_std": float(np.std(rv_scores)),
            "improvement_pct": float((rv_mean/baseline_mean - 1)*100)
        }
    },
    "best_model": {
        "name": best[0],
        "r2_mean": float(best[1]),
        "r2_std": float(best[2]),
        "improvement": float(best[1] - baseline_mean),
        "improvement_pct": float((best[1]/baseline_mean - 1)*100)
    }
}

with open('data/raw/alternative_approaches_results.json', 'w') as f:
    json.dump(results_dict, f, indent=2)

print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: data/raw/alternative_approaches_results.json")
