#!/usr/bin/env python3
"""
ë³€ë™ì„± êµ¬ê°„ë³„ ì„±ëŠ¥ ì¬ê²€ì¦
ì‹¬ê°í•œ ìŒìˆ˜ RÂ² ë¬¸ì œ ì›ì¸ íŒŒì•…
"""

import pandas as pd
import numpy as np
import yfinance as yf
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

def purged_kfold_cv(X, y, n_splits=5, purge_length=5, embargo_length=5):
    """Purged K-Fold Cross-Validation"""
    n_samples = len(X)
    fold_size = n_samples // n_splits
    indices = np.arange(n_samples)

    for i in range(n_splits):
        test_start = i * fold_size
        test_end = (i + 1) * fold_size if i < n_splits - 1 else n_samples
        test_indices = indices[test_start:test_end]

        purge_start = max(0, test_start - purge_length)
        embargo_end = min(n_samples, test_end + embargo_length)

        train_indices = np.concatenate([
            indices[:purge_start],
            indices[embargo_end:]
        ])

        yield train_indices, test_indices

print("="*70)
print("ğŸ” ë³€ë™ì„± êµ¬ê°„ë³„ ì„±ëŠ¥ ì¬ê²€ì¦")
print("="*70)

# ë°ì´í„° ë¡œë“œ
spy = yf.Ticker("SPY")
df = spy.history(start="2015-01-01", end="2024-12-31")
df.index = pd.to_datetime(df.index).tz_localize(None)
df['returns'] = np.log(df['Close'] / df['Close'].shift(1))

# íƒ€ê²Ÿ ìƒì„±
targets = []
for i in range(len(df)):
    if i + 5 < len(df):
        future_returns = df['returns'].iloc[i+1:i+6]
        targets.append(future_returns.std())
    else:
        targets.append(np.nan)
df['target_vol_5d'] = targets

# íŠ¹ì„± ìƒì„±
for window in [5, 10, 20, 60]:
    df[f'volatility_{window}d'] = df['returns'].rolling(window).std()

for lag in [1, 2, 3, 5, 10, 20]:
    df[f'vol_lag_{lag}'] = df['volatility_20d'].shift(lag)

df['vol_mean_5d'] = df['volatility_20d'].rolling(5).mean()
df['vol_mean_10d'] = df['volatility_20d'].rolling(10).mean()
df['vol_std_5d'] = df['volatility_20d'].rolling(5).std()
df['vol_std_10d'] = df['volatility_20d'].rolling(10).std()

for window in [5, 10, 20]:
    df[f'momentum_{window}d'] = df['returns'].rolling(window).sum()

df['returns_mean_5d'] = df['returns'].rolling(5).mean()
df['returns_mean_10d'] = df['returns'].rolling(10).mean()
df['returns_std_5d'] = df['returns'].rolling(5).std()
df['returns_std_10d'] = df['returns'].rolling(10).std()

df['vol_change_5d'] = df['volatility_20d'].pct_change(5)
df['vol_change_10d'] = df['volatility_20d'].pct_change(10)

df['extreme_returns'] = (df['returns'].abs() > 2 * df['volatility_20d']).astype(int)
df['extreme_count_20d'] = df['extreme_returns'].rolling(20).sum()

df = df.dropna()

feature_cols = [col for col in df.columns if col not in
                ['returns', 'target_vol_5d', 'Close', 'Open', 'High', 'Low',
                 'Volume', 'Dividends', 'Stock Splits']]
feature_cols = feature_cols[:31]

X = df[feature_cols]
y = df['target_vol_5d']

print(f"\në°ì´í„°: {len(df)} ìƒ˜í”Œ")
print(f"íŠ¹ì„±: {len(feature_cols)}ê°œ")

# CV ìˆ˜í–‰
all_predictions = np.full(len(X), np.nan)
all_actuals = np.full(len(X), np.nan)

for fold_idx, (train_idx, test_idx) in enumerate(
    purged_kfold_cv(X, y, n_splits=5, purge_length=5, embargo_length=5), 1):

    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_test = X.iloc[test_idx]
    y_test = y.iloc[test_idx]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = Ridge(alpha=1.0)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    all_predictions[test_idx] = y_pred
    all_actuals[test_idx] = y_test.values

test_mask = ~np.isnan(all_predictions)
y_test_all = all_actuals[test_mask]
y_pred_all = all_predictions[test_mask]

print(f"\nì „ì²´ ì„±ëŠ¥:")
print(f"  RÂ² = {r2_score(y_test_all, y_pred_all):.4f}")

# ë³€ë™ì„± êµ¬ê°„ ë¶„ì„
print("\n" + "="*70)
print("ë³€ë™ì„± êµ¬ê°„ë³„ ìƒì„¸ ë¶„ì„")
print("="*70)

# 3ë¶„ìœ„
terciles = pd.qcut(y_test_all, q=3, labels=['Low', 'Medium', 'High'], duplicates='drop')

for tercile in ['Low', 'Medium', 'High']:
    mask = terciles == tercile
    actual = y_test_all[mask]
    pred = y_pred_all[mask]

    r2 = r2_score(actual, pred)
    rmse = np.sqrt(mean_squared_error(actual, pred))
    mae = np.mean(np.abs(actual - pred))

    # ì¶”ê°€ ë¶„ì„
    baseline_var = np.var(actual)
    residual_var = np.var(actual - pred)

    print(f"\n{tercile} Volatility:")
    print(f"  ìƒ˜í”Œ ìˆ˜: {mask.sum()}")
    print(f"  ì‹¤ì œ ë²”ìœ„: [{actual.min():.6f}, {actual.max():.6f}]")
    print(f"  ì‹¤ì œ í‰ê· : {actual.mean():.6f} (Â±{actual.std():.6f})")
    print(f"  ì˜ˆì¸¡ í‰ê· : {pred.mean():.6f} (Â±{pred.std():.6f})")
    print(f"  RÂ²: {r2:.4f}")
    print(f"  RMSE: {rmse:.6f}")
    print(f"  MAE: {mae:.6f}")
    print(f"  Baseline Var: {baseline_var:.8f}")
    print(f"  Residual Var: {residual_var:.8f}")

    # RÂ² ë¶„í•´
    ss_tot = np.sum((actual - actual.mean())**2)
    ss_res = np.sum((actual - pred)**2)
    print(f"  SS_tot: {ss_tot:.8f}")
    print(f"  SS_res: {ss_res:.8f}")
    print(f"  Ratio (SS_res/SS_tot): {ss_res/ss_tot:.4f}")

    # ìƒê´€ê³„ìˆ˜
    corr = np.corrcoef(actual, pred)[0, 1]
    print(f"  Correlation: {corr:.4f}")

# ë¬¸ì œ ì§„ë‹¨
print("\n" + "="*70)
print("ë¬¸ì œ ì§„ë‹¨")
print("="*70)

print("\n1. ì˜ˆì¸¡ê°’ ë²”ìœ„ í™•ì¸:")
print(f"   ì‹¤ì œ ì „ì²´ ë²”ìœ„: [{y_test_all.min():.6f}, {y_test_all.max():.6f}]")
print(f"   ì˜ˆì¸¡ ì „ì²´ ë²”ìœ„: [{y_pred_all.min():.6f}, {y_pred_all.max():.6f}]")

# Low Vol êµ¬ê°„ ìƒì„¸
low_mask = terciles == 'Low'
low_actual = y_test_all[low_mask]
low_pred = y_pred_all[low_mask]

print("\n2. Low Vol êµ¬ê°„ ë¬¸ì œ:")
print(f"   Low Vol ì‹¤ì œ ë²”ìœ„: [{low_actual.min():.6f}, {low_actual.max():.6f}]")
print(f"   Low Vol ì˜ˆì¸¡ ë²”ìœ„: [{low_pred.min():.6f}, {low_pred.max():.6f}]")
print(f"   Low Vol ì‹¤ì œ í‰ê· : {low_actual.mean():.6f}")
print(f"   Low Vol ì˜ˆì¸¡ í‰ê· : {low_pred.mean():.6f}")

# ê³¼ì í•©/ê³¼ì†Œì í•© í™•ì¸
low_baseline = np.mean(low_actual)
low_baseline_mse = np.mean((low_actual - low_baseline)**2)
low_model_mse = mean_squared_error(low_actual, low_pred)

print(f"\n   Baseline MSE (í‰ê·  ì˜ˆì¸¡): {low_baseline_mse:.8f}")
print(f"   Model MSE: {low_model_mse:.8f}")
print(f"   ë¹„ìœ¨: {low_model_mse / low_baseline_mse:.4f}")

if low_model_mse > low_baseline_mse:
    print(f"   âš ï¸ ëª¨ë¸ì´ ë‹¨ìˆœ í‰ê· ë³´ë‹¤ {(low_model_mse/low_baseline_mse - 1)*100:.1f}% ë” ë‚˜ì¨!")

# 3. ë³€ë™ì„± ë²”ìœ„ ë¬¸ì œ
print("\n3. ë³€ë™ì„± ë²”ìœ„ ë¶„ì„:")
vol_ranges = pd.cut(y_test_all, bins=10)
for i, (interval, group) in enumerate(vol_ranges.value_counts().sort_index().items(), 1):
    mask = vol_ranges == interval
    if mask.sum() > 0:
        r2 = r2_score(y_test_all[mask], y_pred_all[mask])
        print(f"   êµ¬ê°„ {i} [{interval.left:.4f}, {interval.right:.4f}]: {mask.sum():4d} ìƒ˜í”Œ, RÂ² = {r2:7.4f}")

print("\n" + "="*70)
print("ê²°ë¡ ")
print("="*70)

print("""
1. Low/Medium Volì—ì„œ RÂ² ìŒìˆ˜ ì›ì¸:
   - ëª¨ë¸ ì˜ˆì¸¡ì´ ë‹¨ìˆœ í‰ê· ë³´ë‹¤ ëª»í•¨
   - ë‚®ì€ ë³€ë™ì„± êµ¬ê°„ì—ì„œ ê³¼ì í•© ë°œìƒ
   - íŠ¹ì„±ì˜ ì‹ í˜¸ ëŒ€ë¹„ ë…¸ì´ì¦ˆ ë¹„ìœ¨ ê³¼ë‹¤

2. í•´ê²° ë°©ì•ˆ:
   - êµ¬ê°„ë³„ ë…ë¦½ ëª¨ë¸ í•™ìŠµ
   - Regularization ê°•í™” (alpha ì¦ê°€)
   - íŠ¹ì„± ì„ íƒ (Low Vol ì „ìš© íŠ¹ì„±)
   - Quantile Regression ì‚¬ìš©

3. í˜„ì¬ ëª¨ë¸ í•œê³„:
   - ì „ì²´ RÂ² 0.31ì€ High Vol êµ¬ê°„ ë•ë¶„
   - Low/Medium Vol ì˜ˆì¸¡ ì‚¬ì‹¤ìƒ ë¶ˆê°€ëŠ¥
   - ì‹¤ì „ ì‚¬ìš© ì‹œ êµ¬ê°„ í•„í„°ë§ í•„ìˆ˜
""")
