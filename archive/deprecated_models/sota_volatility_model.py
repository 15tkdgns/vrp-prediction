#!/usr/bin/env python3
"""
State-of-the-Art Volatility Model - RÂ² 0.25+ ëª©í‘œ
ìµœì‹  í•™ìˆ  ì—°êµ¬ ê¸°ë°˜ ê³ ì„±ëŠ¥ ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import warnings
import os
import json
from datetime import datetime
import matplotlib.pyplot as plt

# ë”¥ëŸ¬ë‹ ëª¨ë¸ (ì„ íƒì  import)
try:
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping
    import tensorflow as tf
    TENSORFLOW_AVAILABLE = True
    print("âœ… TensorFlow ì‚¬ìš© ê°€ëŠ¥ - LSTM ëª¨ë¸ í™œìš©")
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("âš ï¸ TensorFlow ì—†ìŒ - ì „í†µì ì¸ ML ëª¨ë¸ë§Œ ì‚¬ìš©")

warnings.filterwarnings('ignore')

class SOTAVolatilityPredictor:
    """State-of-the-Art ë³€ë™ì„± ì˜ˆì¸¡ê¸°"""

    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.performance_history = {}

    def load_comprehensive_data(self):
        """í¬ê´„ì  ë°ì´í„° ë¡œë“œ - VIX, ê²½ì œì§€í‘œ, ê³ ë¹ˆë„ í”„ë¡ì‹œ"""
        print("ğŸ“Š í¬ê´„ì  ë°ì´í„° ë¡œë“œ ì¤‘...")

        # SPY ê¸°ë³¸ ë°ì´í„° (ë” ê¸´ ê¸°ê°„)
        spy = yf.download('SPY', start='2010-01-01', end='2024-12-31', progress=False)
        spy['returns'] = spy['Close'].pct_change()

        # VIX (í•µì‹¬ ì§€í‘œ)
        vix = yf.download('^VIX', start='2010-01-01', end='2024-12-31', progress=False)
        spy['vix'] = vix['Close'].reindex(spy.index, method='ffill')

        # ì¶”ê°€ ì§€í‘œë“¤
        try:
            # 10ë…„ êµ­ì±„
            tnx = yf.download('^TNX', start='2010-01-01', end='2024-12-31', progress=False)
            spy['treasury_10y'] = tnx['Close'].reindex(spy.index, method='ffill')

            # ë‹¬ëŸ¬ ì§€ìˆ˜ (DXY ëŒ€ì‹  UUP ì‚¬ìš©)
            dxy = yf.download('UUP', start='2010-01-01', end='2024-12-31', progress=False)
            spy['dollar_index'] = dxy['Close'].reindex(spy.index, method='ffill')

            # ê¸ˆ (GLD)
            gold = yf.download('GLD', start='2010-01-01', end='2024-12-31', progress=False)
            spy['gold'] = gold['Close'].reindex(spy.index, method='ffill')

            # ì›ìœ  (USO)
            oil = yf.download('USO', start='2010-01-01', end='2024-12-31', progress=False)
            spy['oil'] = oil['Close'].reindex(spy.index, method='ffill')

        except Exception as e:
            print(f"âš ï¸ ì¼ë¶€ ì§€í‘œ ë¡œë“œ ì‹¤íŒ¨: {e}")

        spy = spy.dropna()
        print(f"âœ… í¬ê´„ì  ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(spy)} ê´€ì¸¡ì¹˜")
        return spy

    def create_realized_volatility_features(self, data):
        """Realized Volatility ê¸°ë°˜ ê³ ê¸‰ íŠ¹ì„± (HAR-RV ëª¨ë¸ ê¸°ë°˜)"""
        print("ğŸ”§ Realized Volatility íŠ¹ì„± ìƒì„± ì¤‘...")

        features = pd.DataFrame(index=data.index)
        returns = data['returns']
        prices = data['Close']
        high = data['High']
        low = data['Low']
        volume = data['Volume']

        # 1. Realized Volatility (ì—¬ëŸ¬ ìœˆë„ìš°)
        for window in [1, 5, 22]:  # ì¼, ì£¼, ì›”
            rv = returns.rolling(window).std() * np.sqrt(252)  # ì—°ìœ¨í™”
            features[f'rv_{window}'] = rv

        # 2. HAR-RV ëª¨ë¸ íŠ¹ì„±ë“¤
        features['rv_daily'] = features['rv_1']
        features['rv_weekly'] = features['rv_5']
        features['rv_monthly'] = features['rv_22']

        # ë¡œê·¸ ë³€í™˜ (HAR-RV ëª¨ë¸ì—ì„œ í‘œì¤€)
        for col in ['rv_daily', 'rv_weekly', 'rv_monthly']:
            features[f'log_{col}'] = np.log(features[col] + 1e-8)

        # 3. Garman-Klass ì¶”ì •ëŸ‰ (ê³ ë¹ˆë„ í”„ë¡ì‹œ)
        gk_vol = np.log(high / low) ** 2
        for window in [1, 5, 22]:
            features[f'gk_vol_{window}'] = gk_vol.rolling(window).mean()

        # 4. Parkinson ì¶”ì •ëŸ‰
        parkinson_vol = np.log(high / low) ** 2 / (4 * np.log(2))
        for window in [1, 5, 22]:
            features[f'parkinson_vol_{window}'] = parkinson_vol.rolling(window).mean()

        # 5. Rogers-Satchell ì¶”ì •ëŸ‰
        rs_vol = np.log(high / prices) * np.log(high / prices.shift(1)) + \
                 np.log(low / prices) * np.log(low / prices.shift(1))
        for window in [1, 5, 22]:
            features[f'rs_vol_{window}'] = rs_vol.rolling(window).mean()

        print(f"âœ… Realized Volatility íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
        return features

    def create_regime_switching_features(self, data):
        """ì‹œì¥ ì²´ì œ ë³€í™” íŠ¹ì„±"""
        print("ğŸ”§ Regime-Switching íŠ¹ì„± ìƒì„± ì¤‘...")

        features = pd.DataFrame(index=data.index)
        returns = data['returns']
        vix = data['vix']

        # 1. ë³€ë™ì„± ì²´ì œ ë¶„ë¥˜
        vol_rolling = returns.rolling(22).std()
        vol_percentile = vol_rolling.rolling(252).rank(pct=True)

        features['vol_regime_low'] = (vol_percentile < 0.3).astype(int)
        features['vol_regime_high'] = (vol_percentile > 0.7).astype(int)
        features['vol_regime_normal'] = ((vol_percentile >= 0.3) & (vol_percentile <= 0.7)).astype(int)

        # 2. VIX ì²´ì œ ë¶„ë¥˜
        vix_percentile = vix.rolling(252).rank(pct=True)
        features['vix_regime_low'] = (vix_percentile < 0.3).astype(int)
        features['vix_regime_high'] = (vix_percentile > 0.7).astype(int)

        # 3. íŠ¸ë Œë“œ ì²´ì œ
        price_ma_short = data['Close'].rolling(20).mean()
        price_ma_long = data['Close'].rolling(50).mean()
        features['trend_up'] = (price_ma_short > price_ma_long).astype(int)
        features['trend_down'] = (price_ma_short < price_ma_long).astype(int)

        # 4. ì‹œì¥ ìŠ¤íŠ¸ë ˆìŠ¤ ì§€í‘œ
        if 'vix' in data.columns and 'treasury_10y' in data.columns:
            stress_indicator = data['vix'] / data['treasury_10y']
            stress_percentile = stress_indicator.rolling(252).rank(pct=True)
            features['stress_regime'] = (stress_percentile > 0.8).astype(int)

        # 5. ì²´ì œ ì§€ì†ì„± (ìƒíƒœ ë³€í™” ê°ì§€)
        for col in ['vol_regime_high', 'vix_regime_high', 'trend_up']:
            if col in features.columns:
                features[f'{col}_persistence'] = features[col].rolling(5).sum() / 5

        print(f"âœ… Regime-Switching íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
        return features

    def create_sentiment_proxy_features(self, data):
        """ê°ì • ì§€í‘œ í”„ë¡ì‹œ íŠ¹ì„± (VIX ê¸°ë°˜)"""
        print("ğŸ”§ ê°ì • ì§€í‘œ í”„ë¡ì‹œ íŠ¹ì„± ìƒì„± ì¤‘...")

        features = pd.DataFrame(index=data.index)
        vix = data['vix']
        returns = data['returns']

        # 1. VIX êµ¬ì¡° íŠ¹ì„±
        features['vix_level'] = vix / 100
        for window in [5, 10, 22]:
            features[f'vix_ma_{window}'] = vix.rolling(window).mean() / 100

        # 2. VIX ê¸°ê°„ êµ¬ì¡° (í”„ë¡ì‹œ)
        features['vix_term_structure'] = features['vix_level'] / features['vix_ma_22']

        # 3. VIXì˜ ë³€ë™ì„± (ê³µí¬ì˜ ë³€ë™ì„±)
        vix_vol = vix.pct_change().rolling(22).std()
        features['vix_vol'] = vix_vol

        # 4. VIX-ì‹¤í˜„ë³€ë™ì„± ìŠ¤í”„ë ˆë“œ
        realized_vol = returns.rolling(22).std() * np.sqrt(252) / 100
        features['vix_rv_spread'] = features['vix_level'] - realized_vol

        # 5. ì‹œì¥ ìŠ¤íŠ¸ë ˆìŠ¤ í•©ì„± ì§€í‘œ
        if 'treasury_10y' in data.columns:
            features['financial_stress'] = (vix / data['treasury_10y']).rolling(5).mean()

        # 6. ê°ì • ëª¨ë©˜í…€
        features['vix_momentum'] = vix.pct_change().rolling(5).mean()

        print(f"âœ… ê°ì • ì§€í‘œ í”„ë¡ì‹œ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
        return features

    def create_cross_asset_features(self, data):
        """Cross-Asset íŠ¹ì„±"""
        print("ğŸ”§ Cross-Asset íŠ¹ì„± ìƒì„± ì¤‘...")

        features = pd.DataFrame(index=data.index)

        # 1. ìì‚° ê°„ ìƒê´€ê´€ê³„
        assets = ['Close', 'vix', 'treasury_10y', 'dollar_index', 'gold', 'oil']
        available_assets = [asset for asset in assets if asset in data.columns]

        # 2. ë¡¤ë§ ìƒê´€ê´€ê³„
        for i, asset1 in enumerate(available_assets):
            for asset2 in available_assets[i+1:]:
                corr = data[asset1].pct_change().rolling(22).corr(data[asset2].pct_change())
                features[f'corr_{asset1}_{asset2}'] = corr

        # 3. ì•ˆì „ìì‚° ìŠ¤í”„ë ˆë“œ
        if 'gold' in data.columns and 'Close' in data.columns:
            gold_equity_ratio = data['gold'] / data['Close']
            flight_to_quality = gold_equity_ratio.pct_change().rolling(5).mean()
            if isinstance(flight_to_quality, pd.Series):
                features['flight_to_quality'] = flight_to_quality
            else:
                features['flight_to_quality'] = flight_to_quality.iloc[:, 0] if not flight_to_quality.empty else np.nan

        # 4. ë‹¬ëŸ¬ ê°•ì„¸ íš¨ê³¼
        if 'dollar_index' in data.columns:
            dollar_strength = data['dollar_index'].pct_change().rolling(10).mean()
            if isinstance(dollar_strength, pd.Series):
                features['dollar_strength'] = dollar_strength
            else:
                features['dollar_strength'] = dollar_strength.iloc[:, 0] if not dollar_strength.empty else np.nan

        print(f"âœ… Cross-Asset íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
        return features

    def create_enhanced_targets(self, data):
        """í–¥ìƒëœ íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ê³„"""
        print("ğŸ¯ í–¥ìƒëœ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì¤‘...")

        targets = pd.DataFrame(index=data.index)
        returns = data['returns']
        high = data['High']
        low = data['Low']
        prices = data['Close']

        # 1. ê¸°ë³¸ ë¯¸ë˜ ë³€ë™ì„± (1, 3, 5ì¼)
        for horizon in [1, 3, 5]:
            vol_values = []
            for i in range(len(returns)):
                if i + horizon < len(returns):
                    future_returns = returns.iloc[i+1:i+1+horizon]
                    vol_values.append(future_returns.std() * np.sqrt(252))  # ì—°ìœ¨í™”
                else:
                    vol_values.append(np.nan)
            targets[f'target_rv_{horizon}d'] = vol_values

        # 2. Garman-Klass ê¸°ë°˜ ë¯¸ë˜ ë³€ë™ì„±
        for horizon in [1, 3, 5]:
            gk_values = []
            for i in range(len(returns)):
                if i + horizon < len(returns):
                    future_high = high.iloc[i+1:i+1+horizon]
                    future_low = low.iloc[i+1:i+1+horizon]
                    future_close = prices.iloc[i+1:i+1+horizon]
                    future_gk = np.mean(np.log(future_high / future_low) ** 2)
                    gk_values.append(future_gk)
                else:
                    gk_values.append(np.nan)
            targets[f'target_gk_{horizon}d'] = gk_values

        # 3. ë¡œê·¸ ë³€í™˜ëœ íƒ€ê²Ÿ (HAR-RV ìŠ¤íƒ€ì¼)
        for col in targets.columns:
            if 'target_rv' in col:
                targets[f'log_{col}'] = np.log(targets[col] + 1e-8)

        print(f"âœ… í–¥ìƒëœ íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ: {len(targets.columns)}ê°œ")
        return targets

    def create_lstm_model(self, input_shape):
        """LSTM ëª¨ë¸ ìƒì„± (TensorFlow ì‚¬ìš© ê°€ëŠ¥ ì‹œ)"""
        if not TENSORFLOW_AVAILABLE:
            return None

        model = Sequential([
            LSTM(64, return_sequences=True, input_shape=input_shape),
            Dropout(0.2),
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            Dense(16, activation='relu'),
            Dropout(0.1),
            Dense(1)
        ])

        model.compile(optimizer=Adam(learning_rate=0.001),
                     loss='mse',
                     metrics=['mae'])
        return model

    def prepare_lstm_data(self, X, y, sequence_length=22):
        """LSTMìš© ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„"""
        X_seq = []
        y_seq = []

        for i in range(sequence_length, len(X)):
            X_seq.append(X[i-sequence_length:i])
            y_seq.append(y[i])

        return np.array(X_seq), np.array(y_seq)

    def train_sota_models(self, X, y, target_name):
        """SOTA ëª¨ë¸ë“¤ í›ˆë ¨"""
        print(f"ğŸ¤– SOTA ëª¨ë¸ í›ˆë ¨ ì¤‘: {target_name}")

        models = {}

        # 1. ê°•í™”ëœ ì„ í˜• ëª¨ë¸ë“¤
        models['HAR_Ridge'] = Ridge(alpha=1.0)
        models['HAR_ElasticNet'] = ElasticNet(alpha=0.01, l1_ratio=0.7, max_iter=3000)

        # 2. ì•™ìƒë¸” ëª¨ë¸ë“¤
        models['RF_Enhanced'] = RandomForestRegressor(
            n_estimators=200, max_depth=10, min_samples_split=20,
            min_samples_leaf=10, random_state=42
        )

        models['GBM_Enhanced'] = GradientBoostingRegressor(
            n_estimators=200, max_depth=6, learning_rate=0.05,
            min_samples_split=20, min_samples_leaf=10, random_state=42
        )

        # 3. ì‹ ê²½ë§
        models['MLP_Enhanced'] = MLPRegressor(
            hidden_layer_sizes=(100, 50, 25), activation='relu',
            alpha=0.01, learning_rate='adaptive', max_iter=1000,
            random_state=42
        )

        # ë°ì´í„° ì¤€ë¹„
        combined_data = pd.concat([X, y], axis=1).dropna()
        X_clean = combined_data[X.columns]
        y_clean = combined_data[y.name]

        print(f"í›ˆë ¨ ë°ì´í„°: {len(X_clean)} ìƒ˜í”Œ, {len(X_clean.columns)} íŠ¹ì„±")

        # ìŠ¤ì¼€ì¼ë§
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X_clean)

        # ëª¨ë¸ í›ˆë ¨
        trained_models = {}
        for name, model in models.items():
            try:
                print(f"  í›ˆë ¨ ì¤‘: {name}")
                model.fit(X_scaled, y_clean)
                trained_models[name] = model

                # ê°„ë‹¨í•œ ì„±ëŠ¥ ì²´í¬
                y_pred = model.predict(X_scaled)
                train_r2 = r2_score(y_clean, y_pred)
                print(f"    í›ˆë ¨ RÂ²: {train_r2:.4f}")

            except Exception as e:
                print(f"    {name} í›ˆë ¨ ì‹¤íŒ¨: {e}")

        # 4. LSTM ëª¨ë¸ (TensorFlow ì‚¬ìš© ê°€ëŠ¥ ì‹œ)
        if TENSORFLOW_AVAILABLE and len(X_clean) > 100:
            try:
                print("  í›ˆë ¨ ì¤‘: LSTM_Enhanced")

                # LSTM ë°ì´í„° ì¤€ë¹„
                X_lstm, y_lstm = self.prepare_lstm_data(X_scaled, y_clean.values)

                if len(X_lstm) > 50:
                    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
                    split_idx = int(len(X_lstm) * 0.8)
                    X_train_lstm = X_lstm[:split_idx]
                    y_train_lstm = y_lstm[:split_idx]
                    X_val_lstm = X_lstm[split_idx:]
                    y_val_lstm = y_lstm[split_idx:]

                    # LSTM ëª¨ë¸ ìƒì„±
                    lstm_model = self.create_lstm_model((X_lstm.shape[1], X_lstm.shape[2]))

                    # ì¡°ê¸° ì¤‘ë‹¨
                    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

                    # í›ˆë ¨
                    history = lstm_model.fit(
                        X_train_lstm, y_train_lstm,
                        epochs=50,
                        batch_size=32,
                        validation_data=(X_val_lstm, y_val_lstm),
                        callbacks=[early_stopping],
                        verbose=0
                    )

                    trained_models['LSTM_Enhanced'] = lstm_model
                    print(f"    LSTM í›ˆë ¨ ì™„ë£Œ: {len(history.history['loss'])} epochs")

            except Exception as e:
                print(f"    LSTM í›ˆë ¨ ì‹¤íŒ¨: {e}")

        self.models[target_name] = trained_models
        self.scalers[target_name] = scaler

        print(f"âœ… {len(trained_models)}ê°œ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        return trained_models

    def comprehensive_walkforward_validation(self, X, y, target_name):
        """í¬ê´„ì  Walk-Forward ê²€ì¦"""
        print(f"ğŸš€ í¬ê´„ì  Walk-Forward ê²€ì¦: {target_name}")

        if target_name not in self.models:
            print(f"âŒ {target_name}ì— ëŒ€í•œ í›ˆë ¨ëœ ëª¨ë¸ ì—†ìŒ")
            return {}

        models = self.models[target_name]
        scaler = self.scalers[target_name]

        combined_data = pd.concat([X, y], axis=1).dropna()
        X_clean = combined_data[X.columns]
        y_clean = combined_data[y.name]

        # Walk-Forward ì„¤ì • (ë” ì—„ê²©í•˜ê²Œ)
        initial_window = 1000  # 4ë…„
        step_size = 60         # 3ê°œì›”
        test_window = 20       # 1ê°œì›”

        results = {}
        for model_name in models.keys():
            results[model_name] = []

        current_start = 0
        fold = 0

        while current_start + initial_window + test_window < len(X_clean):
            fold += 1

            train_end = current_start + initial_window
            test_start = train_end
            test_end = min(test_start + test_window, len(X_clean))

            X_train = X_clean.iloc[current_start:train_end]
            y_train = y_clean.iloc[current_start:train_end]
            X_test = X_clean.iloc[test_start:test_end]
            y_test = y_clean.iloc[test_start:test_end]

            print(f"  Fold {fold}: í›ˆë ¨ {len(X_train)}, í…ŒìŠ¤íŠ¸ {len(X_test)} ({X_test.index[0].strftime('%Y-%m')})")

            # ê° ëª¨ë¸ í…ŒìŠ¤íŠ¸
            for model_name, model in models.items():
                try:
                    if model_name == 'LSTM_Enhanced' and TENSORFLOW_AVAILABLE:
                        # LSTM ë³„ë„ ì²˜ë¦¬
                        scaler_fold = RobustScaler()
                        X_train_scaled = scaler_fold.fit_transform(X_train)
                        X_test_scaled = scaler_fold.transform(X_test)

                        X_train_lstm, y_train_lstm = self.prepare_lstm_data(X_train_scaled, y_train.values)
                        X_test_lstm, _ = self.prepare_lstm_data(X_test_scaled, y_test.values)

                        if len(X_train_lstm) > 30 and len(X_test_lstm) > 0:
                            # LSTM ì¬í›ˆë ¨
                            lstm_fold = self.create_lstm_model((X_train_lstm.shape[1], X_train_lstm.shape[2]))
                            lstm_fold.fit(X_train_lstm, y_train_lstm, epochs=30, batch_size=16, verbose=0)

                            y_pred_lstm = lstm_fold.predict(X_test_lstm, verbose=0)
                            y_pred = y_pred_lstm.flatten()
                            y_test_adj = y_test.iloc[22:22+len(y_pred)]  # ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì¡°ì •
                        else:
                            continue

                    else:
                        # ì „í†µì ì¸ ëª¨ë¸ë“¤
                        scaler_fold = RobustScaler()
                        X_train_scaled = scaler_fold.fit_transform(X_train)
                        X_test_scaled = scaler_fold.transform(X_test)

                        model.fit(X_train_scaled, y_train)
                        y_pred = model.predict(X_test_scaled)
                        y_test_adj = y_test

                    # ì„±ëŠ¥ ê³„ì‚°
                    r2 = r2_score(y_test_adj, y_pred)
                    rmse = np.sqrt(mean_squared_error(y_test_adj, y_pred))
                    mae = mean_absolute_error(y_test_adj, y_pred)

                    results[model_name].append({
                        'fold': fold,
                        'r2': r2,
                        'rmse': rmse,
                        'mae': mae,
                        'test_period': f"{X_test.index[0].strftime('%Y-%m')} ~ {X_test.index[-1].strftime('%Y-%m')}"
                    })

                    print(f"    {model_name:20}: RÂ²={r2:7.4f}, RMSE={rmse:.5f}")

                except Exception as e:
                    print(f"    {model_name:20}: ì˜¤ë¥˜ - {e}")
                    results[model_name].append({
                        'fold': fold,
                        'r2': -999,
                        'rmse': 999,
                        'mae': 999,
                        'error': str(e)
                    })

            current_start += step_size

        # ê²°ê³¼ ìš”ì•½
        summary = {}
        print(f"\nğŸ“Š {target_name} Walk-Forward ê²°ê³¼:")
        print(f"{'Model':<20} {'Mean RÂ²':<10} {'Std RÂ²':<10} {'Max RÂ²':<10} {'Valid'}")
        print("-" * 70)

        for model_name, fold_results in results.items():
            valid_r2s = [r['r2'] for r in fold_results if r['r2'] != -999]

            if valid_r2s:
                mean_r2 = np.mean(valid_r2s)
                std_r2 = np.std(valid_r2s)
                max_r2 = np.max(valid_r2s)
                valid_count = len(valid_r2s)

                summary[model_name] = {
                    'mean_r2': mean_r2,
                    'std_r2': std_r2,
                    'max_r2': max_r2,
                    'valid_folds': valid_count,
                    'total_folds': len(fold_results)
                }

                print(f"{model_name:<20} {mean_r2:<10.4f} {std_r2:<10.4f} {max_r2:<10.4f} {valid_count}/{len(fold_results)}")
            else:
                summary[model_name] = {'mean_r2': -999}
                print(f"{model_name:<20} {'FAILED':<10}")

        self.performance_history[target_name] = summary
        return summary

def main():
    """ë©”ì¸ SOTA ëª¨ë¸ í•¨ìˆ˜"""
    print("ğŸš€ State-of-the-Art Volatility Model - RÂ² 0.25+ ëª©í‘œ")
    print("=" * 80)
    print("ìµœì‹  í•™ìˆ  ì—°êµ¬ ê¸°ë°˜ ê³ ì„±ëŠ¥ ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸")
    print("=" * 80)

    predictor = SOTAVolatilityPredictor()

    # 1. í¬ê´„ì  ë°ì´í„° ë¡œë“œ
    data = predictor.load_comprehensive_data()

    # 2. ê³ ê¸‰ íŠ¹ì„± ìƒì„±
    print("\nğŸ”§ ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...")

    rv_features = predictor.create_realized_volatility_features(data)
    regime_features = predictor.create_regime_switching_features(data)
    sentiment_features = predictor.create_sentiment_proxy_features(data)
    cross_asset_features = predictor.create_cross_asset_features(data)

    # ëª¨ë“  íŠ¹ì„± ê²°í•©
    all_features = pd.concat([
        rv_features,
        regime_features,
        sentiment_features,
        cross_asset_features
    ], axis=1)

    print(f"ì „ì²´ íŠ¹ì„± ìˆ˜: {len(all_features.columns)}")

    # 3. í–¥ìƒëœ íƒ€ê²Ÿ ìƒì„±
    targets = predictor.create_enhanced_targets(data)

    # 4. í•µì‹¬ íƒ€ê²Ÿë“¤ì— ëŒ€í•´ ëª¨ë¸ í›ˆë ¨ ë° ê²€ì¦
    key_targets = ['target_rv_1d', 'target_rv_3d', 'target_rv_5d', 'log_target_rv_5d']

    best_performance = {'target': None, 'model': None, 'r2': -999}

    for target_name in key_targets:
        if target_name not in targets.columns:
            continue

        print(f"\n" + "="*60)
        print(f"ğŸ¯ íƒ€ê²Ÿ: {target_name}")
        print("="*60)

        target_series = targets[target_name]

        # íŠ¹ì„± ì„ ë³„ (ìƒê´€ê´€ê³„ ê¸°ë°˜)
        combined = pd.concat([all_features, target_series], axis=1).dropna()

        if len(combined) < 500:
            print(f"âŒ {target_name}: ë°ì´í„° ë¶€ì¡± ({len(combined)} ìƒ˜í”Œ)")
            continue

        correlations = combined[all_features.columns].corrwith(combined[target_name]).abs().sort_values(ascending=False)

        # ìƒìœ„ 30ê°œ íŠ¹ì„± ì„ íƒ (ë” ë§ì€ ì •ë³´ í™œìš©)
        top_features = correlations.head(30).index
        selected_features = all_features[top_features]

        print(f"ì„ íƒëœ íŠ¹ì„±: {len(selected_features.columns)}ê°œ")
        print("ìƒìœ„ 10ê°œ íŠ¹ì„±:")
        for i, (feature, corr) in enumerate(correlations.head(10).items()):
            print(f"  {i+1:2d}. {feature:30}: {corr:.4f}")

        # 5. ëª¨ë¸ í›ˆë ¨
        trained_models = predictor.train_sota_models(selected_features, target_series, target_name)

        if not trained_models:
            continue

        # 6. Walk-Forward ê²€ì¦
        summary = predictor.comprehensive_walkforward_validation(selected_features, target_series, target_name)

        # ìµœê³  ì„±ëŠ¥ ì¶”ì 
        if summary:
            for model_name, stats in summary.items():
                if stats.get('mean_r2', -999) > best_performance['r2']:
                    best_performance = {
                        'target': target_name,
                        'model': model_name,
                        'r2': stats['mean_r2'],
                        'std_r2': stats.get('std_r2', 0),
                        'max_r2': stats.get('max_r2', 0)
                    }

    # 7. ìµœì¢… ê²°ê³¼ ë° í‰ê°€
    print(f"\n" + "="*80)
    print(f"ğŸ† ìµœì¢… ê²°ê³¼")
    print("="*80)

    if best_performance['target']:
        print(f"ìµœê³  ì„±ëŠ¥:")
        print(f"  íƒ€ê²Ÿ: {best_performance['target']}")
        print(f"  ëª¨ë¸: {best_performance['model']}")
        print(f"  í‰ê·  RÂ²: {best_performance['r2']:.4f}")
        print(f"  í‘œì¤€í¸ì°¨: {best_performance['std_r2']:.4f}")
        print(f"  ìµœê³  RÂ²: {best_performance['max_r2']:.4f}")

        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        target_r2 = 0.25
        if best_performance['r2'] >= target_r2:
            print(f"\nğŸ‰ ëª©í‘œ ë‹¬ì„±! RÂ² â‰¥ {target_r2}")
            improvement = (best_performance['r2'] - 0.0145) / 0.0145 * 100
            print(f"   ì´ì „ ìµœê³  ëŒ€ë¹„ ê°œì„ : +{improvement:.1f}%")
        else:
            print(f"\nğŸ“ˆ ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: RÂ² â‰¥ {target_r2})")
            gap = target_r2 - best_performance['r2']
            print(f"   ëª©í‘œê¹Œì§€ ê°­: {gap:.4f}")

        # ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
        print(f"\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ë¹„êµ:")
        print(f"   ìš°ë¦¬ ëª¨ë¸: RÂ² = {best_performance['r2']:.4f}")
        print(f"   HAR-RV ëª¨ë¸: RÂ² = 0.119")
        print(f"   í•˜ì´ë¸Œë¦¬ë“œ HAR-PSO-ESN: RÂ² = 0.635 (1ì¼)")

        if best_performance['r2'] > 0.119:
            improvement_vs_har = (best_performance['r2'] - 0.119) / 0.119 * 100
            print(f"   HAR-RV ëŒ€ë¹„: +{improvement_vs_har:.1f}%")

    else:
        print("âŒ ëª¨ë“  ëª¨ë¸ì—ì„œ ìœ íš¨í•œ ì„±ëŠ¥ì„ ì–»ì§€ ëª»í•¨")

    # 8. ê²°ê³¼ ì €ì¥
    os.makedirs('results', exist_ok=True)

    sota_results = {
        'version': 'SOTA_Volatility_Model',
        'timestamp': datetime.now().isoformat(),
        'goal': 'RÂ² â‰¥ 0.25',
        'best_performance': best_performance,
        'all_performances': predictor.performance_history,
        'total_features_engineered': len(all_features.columns),
        'targets_tested': key_targets,
        'methodology': [
            'Realized Volatility (HAR-RV based)',
            'Regime-Switching Detection',
            'Sentiment Proxy Features',
            'Cross-Asset Features',
            'Enhanced Neural Networks',
            'LSTM with Walk-Forward'
        ]
    }

    with open('results/sota_volatility_model.json', 'w') as f:
        json.dump(sota_results, f, indent=2, default=str)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: results/sota_volatility_model.json")
    print("="*80)

if __name__ == "__main__":
    main()