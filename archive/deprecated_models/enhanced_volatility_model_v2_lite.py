#!/usr/bin/env python3
"""
Enhanced Volatility Model V2 Lite - íš¨ìœ¨ì ì¸ ë²„ì „
Phase 1: ë°ì´í„° í™•ì¥ ë° í•µì‹¬ ê²½ì œ ì§€í‘œë§Œ ì¶”ê°€
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso, ElasticNet, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
import warnings
import os
import json
from datetime import datetime

warnings.filterwarnings('ignore')

def load_extended_data_lite():
    """íš¨ìœ¨ì ì¸ í™•ì¥ ë°ì´í„° ë¡œë“œ"""
    print("ğŸ“Š í™•ì¥ëœ ë°ì´í„° ë¡œë“œ ì¤‘ (2010-2024)...")

    # SPY ë°ì´í„°
    spy = yf.download('SPY', start='2010-01-01', end='2024-12-31', progress=False)
    spy['returns'] = spy['Close'].pct_change()

    # VIX ë°ì´í„°
    vix = yf.download('^VIX', start='2010-01-01', end='2024-12-31', progress=False)
    spy['vix'] = vix['Close'].reindex(spy.index, method='ffill')

    # 10ë…„ êµ­ì±„ ê¸ˆë¦¬
    try:
        treasury = yf.download('^TNX', start='2010-01-01', end='2024-12-31', progress=False)
        spy['treasury_10y'] = treasury['Close'].reindex(spy.index, method='ffill')
        print("âœ… 10ë…„ êµ­ì±„ ê¸ˆë¦¬ ì¶”ê°€")
    except:
        spy['treasury_10y'] = 2.5
        print("âš ï¸ êµ­ì±„ ê¸ˆë¦¬ ê¸°ë³¸ê°’ ì‚¬ìš©")

    spy = spy.dropna()
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(spy)} ê´€ì¸¡ì¹˜")
    return spy

def create_core_features_v2(data):
    """í•µì‹¬ íŠ¹ì„±ë§Œ ìƒì„± (íš¨ìœ¨ì„± ìš°ì„ )"""
    print("ğŸ”§ í•µì‹¬ íŠ¹ì„± ìƒì„± ì¤‘...")

    features = pd.DataFrame(index=data.index)
    returns = data['returns']
    prices = data['Close']
    high = data['High']
    low = data['Low']
    volume = data['Volume']

    # 1. í•µì‹¬ ë³€ë™ì„± íŠ¹ì„±
    for window in [5, 10, 20, 50]:
        features[f'volatility_{window}'] = returns.rolling(window).std()
        features[f'realized_vol_{window}'] = features[f'volatility_{window}'] * np.sqrt(252)

    # 2. VIX íŠ¹ì„± (ê¸°ì¡´ + í•µì‹¬ë§Œ)
    if 'vix' in data.columns:
        vix = data['vix']
        features['vix_level'] = vix
        features['vix_change'] = vix.pct_change()
        for window in [5, 10, 20]:
            features[f'vix_ma_{window}'] = vix.rolling(window).mean()
            features[f'vix_std_{window}'] = vix.rolling(window).std()

        # VIX ê¸°ê°„êµ¬ì¡° í”„ë¡ì‹œ
        features['vix_term_structure'] = vix / features['vix_ma_20']

    # 3. ê²½ì œ ì§€í‘œ (ì‹ ê·œ - í•µì‹¬ë§Œ)
    if 'treasury_10y' in data.columns:
        treasury = data['treasury_10y']
        features['treasury_10y'] = treasury
        features['treasury_change'] = treasury.diff()
        features['treasury_ma_20'] = treasury.rolling(20).mean()
        features['treasury_vol_20'] = treasury.rolling(20).std()

        # VIX-ê¸ˆë¦¬ ìŠ¤í”„ë ˆë“œ
        features['vix_treasury_spread'] = features['vix_level'] - treasury

    # 4. ì§€ìˆ˜ ê°€ì¤‘ ë³€ë™ì„± (í•µì‹¬ë§Œ)
    for span in [10, 20]:
        features[f'ewm_vol_{span}'] = returns.ewm(span=span).std()

    # 5. ê³ ê¸‰ ë³€ë™ì„± (í•µì‹¬ë§Œ)
    for window in [5, 10, 20]:
        # Garman-Klass
        gk_vol = np.log(high / low) ** 2
        features[f'garman_klass_{window}'] = gk_vol.rolling(window).mean()

        # ì¼ì¤‘ ë³€ë™ì„±
        intraday_range = (high - low) / prices
        features[f'intraday_vol_{window}'] = intraday_range.rolling(window).mean()

    # 6. ë˜ê·¸ íŠ¹ì„± (í•µì‹¬ë§Œ)
    for lag in [1, 2, 3, 5]:
        features[f'return_lag_{lag}'] = returns.shift(lag)
        features[f'vol_lag_{lag}'] = features['volatility_5'].shift(lag)

    # 7. ë³¼ë¥¨ íŠ¹ì„± (í•µì‹¬ë§Œ)
    volume_ma_20 = volume.rolling(20).mean()
    features['volume_ratio'] = volume / (volume_ma_20 + 1e-8)

    # 8. ë³€ë™ì„± ë¹„ìœ¨ (í•µì‹¬ë§Œ)
    features['vol_ratio_5_20'] = features['volatility_5'] / (features['volatility_20'] + 1e-8)
    features['vol_ratio_10_50'] = features['volatility_10'] / (features['volatility_50'] + 1e-8)

    # 9. Z-score (í•µì‹¬ë§Œ)
    mean_ret_20 = returns.rolling(20).mean()
    std_ret_20 = returns.rolling(20).std()
    features['return_zscore_20'] = (returns - mean_ret_20) / (std_ret_20 + 1e-8)

    mean_vol_50 = features['volatility_5'].rolling(50).mean()
    std_vol_50 = features['volatility_5'].rolling(50).std()
    features['vol_zscore_50'] = (features['volatility_5'] - mean_vol_50) / (std_vol_50 + 1e-8)

    print(f"âœ… í•µì‹¬ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features.columns)}ê°œ")
    return features

def create_simple_interactions(base_features, n_top=12):
    """ê°„ë‹¨í•œ ìƒí˜¸ì‘ìš© (ìƒìœ„ íŠ¹ì„±ë§Œ)"""
    print(f"ğŸ”§ ê°„ë‹¨í•œ ìƒí˜¸ì‘ìš© ìƒì„± ì¤‘...")

    selected_features = base_features.iloc[:, :n_top]
    interactions = pd.DataFrame(index=base_features.index)

    # í•µì‹¬ ìƒí˜¸ì‘ìš©ë§Œ (ê³„ì‚° íš¨ìœ¨ì„±)
    important_pairs = [
        ('vix_level', 'intraday_vol_5'),
        ('vix_level', 'treasury_10y'),
        ('vix_level', 'ewm_vol_10'),
        ('treasury_10y', 'volatility_10'),
        ('vix_term_structure', 'volatility_5')
    ]

    for col1, col2 in important_pairs:
        if col1 in selected_features.columns and col2 in selected_features.columns:
            interactions[f'{col1}_x_{col2}'] = selected_features[col1] * selected_features[col2]
            interactions[f'{col1}_div_{col2}'] = selected_features[col1] / (selected_features[col2] + 1e-8)

    print(f"âœ… ìƒí˜¸ì‘ìš© íŠ¹ì„± {len(interactions.columns)}ê°œ ìƒì„±")
    return interactions

def create_future_volatility_targets(data):
    """ë¯¸ë˜ ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„±"""
    print("ğŸ¯ ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„± ì¤‘...")

    targets = pd.DataFrame(index=data.index)
    returns = data['returns']

    for window in [5]:  # 5ì¼ë§Œ ì§‘ì¤‘
        vol_values = []
        for i in range(len(returns)):
            if i + window < len(returns):
                future_window = returns.iloc[i+1:i+1+window]
                vol_values.append(future_window.std())
            else:
                vol_values.append(np.nan)
        targets[f'target_vol_{window}d'] = vol_values

    print(f"âœ… íƒ€ê²Ÿ ìƒì„± ì™„ë£Œ")
    return targets

def test_lite_models(X, y):
    """ë¼ì´íŠ¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ¤– V2 Lite ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # ë°ì´í„° ì •ë¦¬
    combined_data = pd.concat([X, y], axis=1).dropna()
    print(f"ìœ íš¨ ìƒ˜í”Œ ìˆ˜: {len(combined_data)}")

    if len(combined_data) < 200:
        print("âš ï¸ ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±")
        return {}

    X_clean = combined_data[X.columns]
    y_clean = combined_data[y.name]

    # êµì°¨ ê²€ì¦
    tscv = TimeSeriesSplit(n_splits=3)
    results = {}

    models = {
        'Lasso (Î±=0.0001)': Lasso(alpha=0.0001, max_iter=2000),
        'Lasso (Î±=0.0005)': Lasso(alpha=0.0005, max_iter=2000),
        'Lasso (Î±=0.001)': Lasso(alpha=0.001, max_iter=2000),
        'ElasticNet (Î±=0.0005)': ElasticNet(alpha=0.0005, l1_ratio=0.7, max_iter=2000),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=8)
    }

    for name, model in models.items():
        scores = []

        for train_idx, test_idx in tscv.split(X_clean):
            X_train, X_test = X_clean.iloc[train_idx], X_clean.iloc[test_idx]
            y_train, y_test = y_clean.iloc[train_idx], y_clean.iloc[test_idx]

            if 'Forest' not in name:
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
            else:
                X_train_scaled = X_train.values
                X_test_scaled = X_test.values

            try:
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
                score = r2_score(y_test, y_pred)
                scores.append(score)
            except:
                scores.append(-999)

        avg_score = np.mean(scores)
        std_score = np.std(scores)

        results[name] = {
            'mean_r2': avg_score,
            'std_r2': std_score
        }

        print(f"{name:25}: RÂ² = {avg_score:7.4f} Â± {std_score:.4f}")

    return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Enhanced Volatility Model V2 Lite")
    print("=" * 50)

    # 1. í™•ì¥ëœ ë°ì´í„° ë¡œë“œ
    spy_data = load_extended_data_lite()

    # 2. í•µì‹¬ íŠ¹ì„± ìƒì„±
    core_features = create_core_features_v2(spy_data)

    # 3. íƒ€ê²Ÿ ìƒì„±
    targets = create_future_volatility_targets(spy_data)

    # 4. ìƒê´€ê´€ê³„ ë¶„ì„
    if 'target_vol_5d' in targets.columns:
        combined = pd.concat([core_features, targets[['target_vol_5d']]], axis=1).dropna()

        if len(combined) > 200:
            print(f"\nğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„ (ìƒ˜í”Œ ìˆ˜: {len(combined)})")

            correlations = combined[core_features.columns].corrwith(
                combined['target_vol_5d']
            ).abs().sort_values(ascending=False)

            print("ìƒìœ„ 15ê°œ íŠ¹ì„±:")
            for i, (feature, corr) in enumerate(correlations.head(15).items()):
                print(f"  {i+1:2d}. {feature:25}: {corr:.4f}")

            # 5. ìƒìœ„ íŠ¹ì„± + ìƒí˜¸ì‘ìš©
            top_12_features = correlations.head(12).index
            top_features_df = core_features[top_12_features]

            interaction_features = create_simple_interactions(top_features_df)
            final_features = pd.concat([top_features_df, interaction_features], axis=1)

            print(f"\nğŸ“Š ìµœì¢… íŠ¹ì„± ìˆ˜: {len(final_features.columns)}")

            # 6. ëª¨ë¸ í…ŒìŠ¤íŠ¸
            results = test_lite_models(final_features, targets['target_vol_5d'])

            if results:
                valid_results = {k: v for k, v in results.items() if v['mean_r2'] > -900}
                if valid_results:
                    best_model = max(valid_results.items(), key=lambda x: x[1]['mean_r2'])
                    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model[0]}")
                    print(f"   RÂ² = {best_model[1]['mean_r2']:.4f} Â± {best_model[1]['std_r2']:.4f}")

                    # ê°œì„  ë¶„ì„
                    baseline_r2 = 0.0988
                    improvement = (best_model[1]['mean_r2'] - baseline_r2) / abs(baseline_r2) * 100
                    print(f"\nğŸ“ˆ ì„±ëŠ¥ ê°œì„ :")
                    print(f"   ê¸°ì¡´ RÂ²: {baseline_r2:.4f}")
                    print(f"   V2 RÂ²:   {best_model[1]['mean_r2']:.4f}")
                    print(f"   ê°œì„ :    {improvement:+.1f}%")

                    # ê²°ê³¼ ì €ì¥
                    os.makedirs('results', exist_ok=True)

                    v2_lite_results = {
                        'version': 'V2_Lite',
                        'timestamp': datetime.now().isoformat(),
                        'data_period': '2010-2024',
                        'samples': len(combined),
                        'features': len(final_features.columns),
                        'best_model': {
                            'name': best_model[0],
                            'r2_mean': best_model[1]['mean_r2'],
                            'r2_std': best_model[1]['std_r2']
                        },
                        'improvement_vs_baseline': improvement,
                        'top_features': top_12_features.tolist(),
                        'all_results': results
                    }

                    with open('results/enhanced_model_v2_lite.json', 'w') as f:
                        json.dump(v2_lite_results, f, indent=2, default=str)

                    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: results/enhanced_model_v2_lite.json")

    print("=" * 50)

if __name__ == "__main__":
    main()