#!/usr/bin/env python3
"""
ê³ ê¸‰ LLM ê¸°ë°˜ íŠ¸ìœ„í„°/ë‰´ìŠ¤ ê°ì„±ë¶„ì„ íŒŒì´í”„ë¼ì¸
ëª©í‘œ: ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° íŒ¨í„´ ë°œê²¬ì„ í†µí•œ ìˆ˜ìµë¥  ì˜ˆì¸¡ ê°€ëŠ¥ì„± íƒìƒ‰

ì „ëµ:
1. ë‹¤ì¤‘ ì†ŒìŠ¤ ë‰´ìŠ¤ ë°ì´í„° (ì‹¤ì‹œê°„ì„± ì¤‘ìš”)
2. ê³ ê¸‰ NLP íŠ¹ì„± (í† í”½, ì—”í‹°í‹°, ê°ì • ê°•ë„, ì‹œê°„ì  íŒ¨í„´)
3. ë¹„ì„ í˜• ëª¨ë¸ë¡œ ë³µì¡í•œ íŒ¨í„´ í¬ì°©
"""

import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
from textblob import TextBlob
import warnings
warnings.filterwarnings('ignore')

class AdvancedNewsTwitterPipeline:
    """ê³ ê¸‰ LLM ê¸°ë°˜ ê°ì„±ë¶„ì„ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, ticker="SPY", start_date="2015-01-01", end_date="2024-12-31"):
        self.ticker = ticker
        self.start_date = start_date
        self.end_date = end_date
        self.price_data = None
        self.news_data = []
        self.advanced_features = None

    def fetch_price_data(self):
        """ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ (ê³ í•´ìƒë„ - ë¶„/ì‹œê°„ ë‹¨ìœ„)"""
        print(f"ğŸ“ˆ {self.ticker} ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        try:
            spy = yf.Ticker(self.ticker)

            # ì¼ë´‰ ë°ì´í„°
            daily_data = spy.history(start=self.start_date, end=self.end_date)
            daily_data.index = pd.to_datetime(daily_data.index, utc=True).tz_localize(None)

            # ê¸°ë³¸ íŠ¹ì„±
            daily_data['returns'] = np.log(daily_data['Close'] / daily_data['Close'].shift(1))
            daily_data['returns_1h'] = daily_data['returns']  # 1ì‹œê°„ ìˆ˜ìµë¥  (í”„ë¡ì‹œ)
            daily_data['returns_4h'] = daily_data['returns'].rolling(4).sum()  # 4ì‹œê°„
            daily_data['intraday_volatility'] = (daily_data['High'] - daily_data['Low']) / daily_data['Close']

            # ê±°ë˜ëŸ‰ íŒ¨í„´
            daily_data['volume_surge'] = daily_data['Volume'] / daily_data['Volume'].rolling(20).mean()
            daily_data['volume_surge'] = daily_data['volume_surge'].fillna(1.0)

            self.price_data = daily_data.dropna()
            print(f"âœ… ê°€ê²© ë°ì´í„°: {len(self.price_data)} ìƒ˜í”Œ")

            return True

        except Exception as e:
            print(f"âŒ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return False

    def simulate_realtime_news(self):
        """ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ API ëŒ€ì²´)"""
        print("ğŸ“° ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")

        try:
            # ì‹¤ì œ í™˜ê²½: Twitter API, NewsAPI, Reddit API ì‚¬ìš©
            # ì—¬ê¸°ì„œëŠ” VIX + ì‹œê°„ íŒ¨í„´ ê¸°ë°˜ ê³ ê¸‰ í”„ë¡ì‹œ ìƒì„±

            # VIX ë°ì´í„°
            vix = yf.Ticker("^VIX")
            vix_data = vix.history(start=self.start_date, end=self.end_date)
            vix_data.index = pd.to_datetime(vix_data.index, utc=True).tz_localize(None)

            dates = self.price_data.index

            for date in dates:
                # VIX ë ˆë²¨
                vix_val = vix_data.loc[vix_data.index <= date, 'Close'].iloc[-1] if len(vix_data.loc[vix_data.index <= date]) > 0 else 15.0

                # SPY ìˆ˜ìµë¥  (íƒ€ì´ë° í…ŒìŠ¤íŠ¸ìš©)
                spy_return = self.price_data.loc[date, 'returns'] if date in self.price_data.index else 0.0

                # ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ ìƒì„± (ì¥ ì‹œì‘ ì „, ì¥ì¤‘, ì¥ ë§ˆê° í›„)
                # íŒ¨í„´ ê°€ì •: ì¥ ì‹œì‘ ì „ ë‰´ìŠ¤ê°€ ë‹¹ì¼ ìˆ˜ìµë¥ ì— ì˜í–¥
                for hour in [7, 10, 13, 16]:  # 7AM, 10AM, 1PM, 4PM ET

                    # ê°ì„± ì ìˆ˜ ìƒì„± (VIX ê¸°ë°˜ + ë…¸ì´ì¦ˆ)
                    base_sentiment = -np.tanh((vix_val - 15) / 10)  # VIX ë†’ìœ¼ë©´ ë¶€ì •ì 

                    # ì‹œê°„ëŒ€ë³„ ì°¨ë³„í™”
                    if hour == 7:  # ì¥ ì‹œì‘ ì „: ë¯¸ë˜ ì§€í–¥ì 
                        sentiment = base_sentiment + np.random.normal(0, 0.3)
                        news_impact = 'high'  # ë†’ì€ ì˜í–¥ë ¥
                    elif hour == 10:  # ì¥ ì´ˆë°˜
                        sentiment = base_sentiment * 0.8 + np.random.normal(0, 0.2)
                        news_impact = 'medium'
                    elif hour == 13:  # ì¥ ì¤‘ë°˜
                        sentiment = base_sentiment * 0.6 + np.random.normal(0, 0.15)
                        news_impact = 'low'
                    else:  # ì¥ ë§ˆê°
                        sentiment = spy_return * 0.3 + np.random.normal(0, 0.1)  # ì‹¤ì  ë°˜ì˜
                        news_impact = 'post-market'

                    sentiment = np.clip(sentiment, -1, 1)

                    # ë‰´ìŠ¤ ì—”í‹°í‹° (ê¸°ì—…, ì¸ë¬¼, ì´ë²¤íŠ¸)
                    entities = self._generate_entities(vix_val, spy_return)

                    # í† í”½ (ê²½ì œ, ì •ì¹˜, ê¸°ìˆ  ë“±)
                    topics = self._generate_topics(vix_val)

                    self.news_data.append({
                        'date': date,
                        'hour': hour,
                        'timestamp': pd.Timestamp(date) + pd.Timedelta(hours=hour),
                        'sentiment': sentiment,
                        'sentiment_strength': abs(sentiment),  # ê°ì • ê°•ë„
                        'vix_level': vix_val,
                        'news_impact': news_impact,
                        'entities': entities,
                        'topics': topics,
                        'source': 'twitter' if hour in [7, 13] else 'news',
                        'virality': np.random.exponential(100) if abs(sentiment) > 0.7 else np.random.exponential(30)  # í™•ì‚° ì†ë„
                    })

            print(f"âœ… ë‰´ìŠ¤ ë°ì´í„° ìƒì„±: {len(self.news_data)}ê°œ")
            return True

        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _generate_entities(self, vix_val, spy_return):
        """ë‰´ìŠ¤ ì—”í‹°í‹° ìƒì„± (ê¸°ì—…, ì¸ë¬¼, ì´ë²¤íŠ¸)"""
        entities = []

        # VIX ë†’ìœ¼ë©´ ë¦¬ìŠ¤í¬ ê´€ë ¨ ì—”í‹°í‹°
        if vix_val > 20:
            entities.extend(['Federal Reserve', 'Interest Rate', 'Recession'])

        # ìˆ˜ìµë¥  ë°©í–¥ì— ë”°ë¼
        if spy_return > 0.01:
            entities.extend(['Earnings Beat', 'Economic Growth'])
        elif spy_return < -0.01:
            entities.extend(['Trade War', 'Inflation'])
        else:
            entities.extend(['Market Neutral', 'Consolidation'])

        return entities[:3]  # ìµœëŒ€ 3ê°œ

    def _generate_topics(self, vix_val):
        """í† í”½ ìƒì„±"""
        topics = []

        if vix_val > 25:
            topics = ['market_volatility', 'risk_management', 'safe_haven']
        elif vix_val > 15:
            topics = ['economic_outlook', 'corporate_earnings', 'fed_policy']
        else:
            topics = ['market_rally', 'tech_sector', 'innovation']

        return topics

    def create_advanced_nlp_features(self):
        """ê³ ê¸‰ NLP íŠ¹ì„± ìƒì„±"""
        print("ğŸ§  ê³ ê¸‰ NLP íŠ¹ì„± ìƒì„± ì¤‘...")

        try:
            news_df = pd.DataFrame(self.news_data)

            # ë‚ ì§œë³„ ì§‘ê³„
            daily_features = []

            for date in self.price_data.index:
                day_news = news_df[news_df['date'] == date]

                if len(day_news) == 0:
                    continue

                # ê¸°ë³¸ ê°ì„± ì§€í‘œ
                features = {
                    'date': date,

                    # ê°ì„± í†µê³„
                    'sentiment_mean': day_news['sentiment'].mean(),
                    'sentiment_std': day_news['sentiment'].std() if len(day_news) > 1 else 0,
                    'sentiment_max': day_news['sentiment'].max(),
                    'sentiment_min': day_news['sentiment'].min(),

                    # ê°ì • ê°•ë„
                    'sentiment_strength_mean': day_news['sentiment_strength'].mean(),
                    'sentiment_strength_max': day_news['sentiment_strength'].max(),

                    # ì‹œê°„ëŒ€ë³„ ê°ì„±
                    'sentiment_premarket': day_news[day_news['hour'] == 7]['sentiment'].mean() if len(day_news[day_news['hour'] == 7]) > 0 else 0,
                    'sentiment_intraday': day_news[day_news['hour'].isin([10, 13])]['sentiment'].mean() if len(day_news[day_news['hour'].isin([10, 13])]) > 0 else 0,
                    'sentiment_postmarket': day_news[day_news['hour'] == 16]['sentiment'].mean() if len(day_news[day_news['hour'] == 16]) > 0 else 0,

                    # ì†ŒìŠ¤ë³„ ê°ì„±
                    'sentiment_twitter': day_news[day_news['source'] == 'twitter']['sentiment'].mean() if len(day_news[day_news['source'] == 'twitter']) > 0 else 0,
                    'sentiment_news': day_news[day_news['source'] == 'news']['sentiment'].mean() if len(day_news[day_news['source'] == 'news']) > 0 else 0,

                    # í™•ì‚° ì†ë„
                    'virality_mean': day_news['virality'].mean(),
                    'virality_max': day_news['virality'].max(),

                    # VIX ë ˆë²¨
                    'vix_level': day_news['vix_level'].mean(),

                    # ë‰´ìŠ¤ ë³¼ë¥¨
                    'news_count': len(day_news),
                    'news_count_high_impact': len(day_news[day_news['news_impact'] == 'high']),

                    # ê°ì„± ë³€í™”ìœ¨
                    'sentiment_change': day_news['sentiment'].iloc[-1] - day_news['sentiment'].iloc[0] if len(day_news) > 1 else 0,

                    # ê·¹ë‹¨ ê°ì„± ë¹„ìœ¨
                    'extreme_positive_ratio': len(day_news[day_news['sentiment'] > 0.7]) / len(day_news),
                    'extreme_negative_ratio': len(day_news[day_news['sentiment'] < -0.7]) / len(day_news),
                }

                daily_features.append(features)

            self.advanced_features = pd.DataFrame(daily_features)
            self.advanced_features.set_index('date', inplace=True)

            # ì‹œê³„ì—´ íŠ¹ì„± ì¶”ê°€
            self.advanced_features['sentiment_momentum_3d'] = self.advanced_features['sentiment_mean'].diff(3)
            self.advanced_features['sentiment_acceleration'] = self.advanced_features['sentiment_momentum_3d'].diff()
            self.advanced_features['virality_surge'] = self.advanced_features['virality_max'] / self.advanced_features['virality_mean']

            self.advanced_features = self.advanced_features.fillna(0)

            print(f"âœ… ê³ ê¸‰ íŠ¹ì„± ìƒì„±: {self.advanced_features.shape[1]}ê°œ íŠ¹ì„±")
            print(f"   ì‹œê°„ëŒ€ë³„ ê°ì„±, ì†ŒìŠ¤ë³„ ê°ì„±, í™•ì‚° ì†ë„, ê°ì„± ë³€í™”ìœ¨ í¬í•¨")

            return True

        except Exception as e:
            print(f"âŒ íŠ¹ì„± ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def create_integrated_dataset(self):
        """ê°€ê²© + ê³ ê¸‰ NLP íŠ¹ì„± í†µí•©"""
        print("ğŸ”— ê³ ê¸‰ í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")

        try:
            # ë‚ ì§œ ì •ê·œí™”
            self.price_data.index = pd.to_datetime(self.price_data.index).tz_localize(None).normalize()
            self.advanced_features.index = pd.to_datetime(self.advanced_features.index).tz_localize(None).normalize()

            # ê³µí†µ ë‚ ì§œ
            common_dates = self.price_data.index.intersection(self.advanced_features.index)

            # ì •ë ¬
            price_aligned = self.price_data.loc[common_dates]
            features_aligned = self.advanced_features.loc[common_dates]

            # í†µí•©
            integrated = pd.concat([price_aligned, features_aligned], axis=1)

            # íƒ€ê²Ÿ ë³€ìˆ˜ (ë‹¤ì–‘í•œ ì‹œê°„ëŒ€)
            integrated['target_return_1d'] = integrated['returns'].shift(-1)
            integrated['target_return_1h_ahead'] = integrated['returns'].shift(-1)  # 1ì‹œê°„ í›„ (í”„ë¡ì‹œ)
            integrated['target_return_4h_ahead'] = integrated['returns_4h'].shift(-1)  # 4ì‹œê°„ í›„
            integrated['target_direction_1d'] = (integrated['target_return_1d'] > 0).astype(int)

            # ê·¹ë‹¨ ìˆ˜ìµë¥  (í° ì›€ì§ì„ë§Œ ì˜ˆì¸¡)
            returns_std = integrated['returns'].std()
            extreme_mask = integrated['target_return_1d'].abs() > returns_std
            integrated['target_extreme_move'] = (extreme_mask * np.sign(integrated['target_return_1d'])).fillna(0).astype(int)

            # ì •ë¦¬
            integrated = integrated.dropna()

            print(f"\nâœ… ê³ ê¸‰ í†µí•© ë°ì´í„°ì…‹ ì™„ì„±:")
            print(f"   ğŸ“Š ìƒ˜í”Œ ìˆ˜: {len(integrated):,}")
            print(f"   ğŸ“Š íŠ¹ì„± ìˆ˜: {integrated.shape[1]}")
            print(f"   ğŸ“Š ê¸°ê°„: {integrated.index.min()} ~ {integrated.index.max()}")
            print(f"\n   íƒ€ê²Ÿ ë³€ìˆ˜:")
            print(f"     - target_return_1d: 1ì¼ í›„ ìˆ˜ìµë¥ ")
            print(f"     - target_extreme_move: ê·¹ë‹¨ ì›€ì§ì„ (Â±1Ïƒ ì´ìƒ)")
            print(f"     - target_direction_1d: ë°©í–¥ì„± (0/1)")

            # ì €ì¥
            output_path = "data/training/advanced_news_twitter_dataset.csv"
            integrated.to_csv(output_path)
            print(f"\nğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥: {output_path}")

            # ë©”íƒ€ë°ì´í„°
            import json
            metadata = {
                'dataset_info': {
                    'ticker': self.ticker,
                    'samples': len(integrated),
                    'features': integrated.shape[1],
                    'start_date': str(integrated.index.min()),
                    'end_date': str(integrated.index.max()),
                    'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                },
                'feature_categories': {
                    'basic_sentiment': ['sentiment_mean', 'sentiment_std', 'sentiment_max', 'sentiment_min'],
                    'sentiment_strength': ['sentiment_strength_mean', 'sentiment_strength_max'],
                    'timing_features': ['sentiment_premarket', 'sentiment_intraday', 'sentiment_postmarket'],
                    'source_features': ['sentiment_twitter', 'sentiment_news'],
                    'virality': ['virality_mean', 'virality_max', 'virality_surge'],
                    'temporal_patterns': ['sentiment_momentum_3d', 'sentiment_acceleration'],
                    'extreme_sentiment': ['extreme_positive_ratio', 'extreme_negative_ratio']
                },
                'target_variables': [
                    'target_return_1d',
                    'target_extreme_move',
                    'target_direction_1d'
                ],
                'hypothesis': 'LLM ê³ ê¸‰ ê°ì„±ë¶„ì„ìœ¼ë¡œ ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° íŒ¨í„´ ë°œê²¬ ê°€ëŠ¥'
            }

            with open("data/raw/advanced_news_metadata.json", 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: data/raw/advanced_news_metadata.json")

            return integrated

        except Exception as e:
            print(f"âŒ í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None

    def run_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("="*60)
        print("ğŸš€ ê³ ê¸‰ LLM ë‰´ìŠ¤/íŠ¸ìœ„í„° ê°ì„±ë¶„ì„ íŒŒì´í”„ë¼ì¸")
        print("="*60)
        print("ëª©í‘œ: ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° íŒ¨í„´ ë°œê²¬ìœ¼ë¡œ ìˆ˜ìµë¥  ì˜ˆì¸¡ ê°€ëŠ¥ì„± íƒìƒ‰\n")

        steps = [
            ("ê°€ê²© ë°ì´í„° ìˆ˜ì§‘", self.fetch_price_data),
            ("ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì‹œë®¬ë ˆì´ì…˜", self.simulate_realtime_news),
            ("ê³ ê¸‰ NLP íŠ¹ì„± ìƒì„±", self.create_advanced_nlp_features),
            ("í†µí•© ë°ì´í„°ì…‹ ìƒì„±", self.create_integrated_dataset)
        ]

        for step_name, step_func in steps:
            print(f"\n{'='*60}")
            print(f"ğŸ”„ {step_name}")
            print(f"{'='*60}")

            result = step_func()
            if result is False:
                print(f"\nâŒ {step_name} ì‹¤íŒ¨")
                return False

        print("\n" + "="*60)
        print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("="*60)
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("  1. XGBoost/LSTM ë¹„ì„ í˜• ëª¨ë¸ í•™ìŠµ")
        print("  2. íŒ¨í„´ íƒì§€ ë° íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
        print("  3. ë¯¸ì‹œêµ¬ì¡° íš¨ê³¼ ê²€ì¦")

        return True

if __name__ == "__main__":
    pipeline = AdvancedNewsTwitterPipeline(
        ticker="SPY",
        start_date="2015-01-01",
        end_date="2024-12-31"
    )

    pipeline.run_pipeline()
