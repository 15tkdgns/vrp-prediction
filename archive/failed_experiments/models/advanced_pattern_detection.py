#!/usr/bin/env python3
"""
ê³ ê¸‰ íŒ¨í„´ íƒì§€: XGBoost + íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
ëª©í‘œ: LLM ê°ì„± ì§€í‘œì—ì„œ ë¯¸ì„¸í•œ ì˜ˆì¸¡ íŒ¨í„´ ë°œê²¬
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score
import xgboost as xgb
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class AdvancedPatternDetection:
    """XGBoost ê¸°ë°˜ ê³ ê¸‰ íŒ¨í„´ íƒì§€"""

    def __init__(self, dataset_path="data/training/advanced_news_twitter_dataset.csv"):
        self.dataset_path = dataset_path
        self.data = None
        self.X = None
        self.y = None
        self.feature_names = None
        self.model = None
        self.results = {}

    def load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„"""
        print("ğŸ“‚ ê³ ê¸‰ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")

        try:
            self.data = pd.read_csv(self.dataset_path, index_col=0, parse_dates=True)
            self.data = self.data.dropna()

            print(f"âœ… ë°ì´í„°: {self.data.shape}")
            print(f"   ê¸°ê°„: {self.data.index.min()} ~ {self.data.index.max()}")

            # ê³ ê¸‰ NLP íŠ¹ì„±ë§Œ ì„ íƒ (íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸)
            exclude_targets = ['target_return', 'target_direction', 'target_extreme']
            nlp_features = [col for col in self.data.columns if
                           ('sentiment' in col or 'virality' in col or 'news_count' in col or 'extreme' in col) and
                           not any(exc in col for exc in exclude_targets)]

            # ê°€ê²© íŠ¹ì„± ì¶”ê°€ (ë³´ì¡°)
            price_features = ['returns', 'intraday_volatility', 'volume_surge']

            self.feature_names = nlp_features + [f for f in price_features if f in self.data.columns]

            print(f"\nğŸ“Š ì„ íƒëœ íŠ¹ì„± ({len(self.feature_names)}ê°œ):")
            print(f"   NLP íŠ¹ì„±: {len(nlp_features)}ê°œ")
            print(f"   ê°€ê²© íŠ¹ì„±: {len([f for f in price_features if f in self.data.columns])}ê°œ")

            # X, y ë¶„ë¦¬
            self.X = self.data[self.feature_names]
            self.y = self.data['target_return_1d']

            return True

        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def train_xgboost_model(self, target='return'):
        """XGBoost ëª¨ë¸ í•™ìŠµ (ì‹œê³„ì—´ êµì°¨ ê²€ì¦)"""
        print(f"\nğŸŒ³ XGBoost ëª¨ë¸ í•™ìŠµ ì¤‘ (íƒ€ê²Ÿ: {target})...")

        try:
            # íƒ€ê²Ÿ ì„ íƒ
            if target == 'return':
                y = self.data['target_return_1d']
                task = 'regression'
            elif target == 'direction':
                y = self.data['target_direction_1d']
                task = 'classification'
            elif target == 'extreme':
                # -1, 0, 1 â†’ 0, 1, 2ë¡œ ë³€í™˜ (XGBoost ë¶„ë¥˜ ìš”êµ¬ì‚¬í•­)
                y = self.data['target_extreme_move'] + 1
                task = 'multiclass'
            else:
                raise ValueError(f"Unknown target: {target}")

            # ì‹œê³„ì—´ ë¶„í•  (5-fold)
            tscv = TimeSeriesSplit(n_splits=5)

            cv_scores = []
            feature_importances = []

            for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(self.X), 1):
                X_train, X_test = self.X.iloc[train_idx], self.X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

                print(f"\n   Fold {fold_idx}/5:")
                print(f"     í•™ìŠµ: {len(train_idx)} ìƒ˜í”Œ")
                print(f"     í…ŒìŠ¤íŠ¸: {len(test_idx)} ìƒ˜í”Œ")

                # XGBoost ì„¤ì •
                if task == 'regression':
                    model = xgb.XGBRegressor(
                        n_estimators=200,
                        max_depth=5,
                        learning_rate=0.05,
                        subsample=0.8,
                        colsample_bytree=0.8,
                        random_state=42 + fold_idx,
                        objective='reg:squarederror'
                    )
                elif task == 'multiclass':
                    model = xgb.XGBClassifier(
                        n_estimators=200,
                        max_depth=5,
                        learning_rate=0.05,
                        subsample=0.8,
                        colsample_bytree=0.8,
                        random_state=42 + fold_idx,
                        objective='multi:softmax',
                        num_class=3
                    )
                else:
                    model = xgb.XGBClassifier(
                        n_estimators=200,
                        max_depth=5,
                        learning_rate=0.05,
                        subsample=0.8,
                        colsample_bytree=0.8,
                        random_state=42 + fold_idx,
                        objective='binary:logistic'
                    )

                # í•™ìŠµ
                model.fit(X_train, y_train, verbose=False)

                # ì˜ˆì¸¡
                y_pred = model.predict(X_test)

                # ì„±ëŠ¥ í‰ê°€
                if task == 'regression':
                    r2 = r2_score(y_test, y_pred)
                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    mae = mean_absolute_error(y_test, y_pred)
                    print(f"     RÂ²: {r2:.4f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}")
                    cv_scores.append({'r2': r2, 'rmse': rmse, 'mae': mae})
                else:
                    acc = accuracy_score(y_test, y_pred)
                    print(f"     Accuracy: {acc:.4f}")
                    cv_scores.append({'accuracy': acc})

                # íŠ¹ì„± ì¤‘ìš”ë„
                feature_importances.append(model.feature_importances_)

            # í‰ê·  ì„±ëŠ¥
            if task == 'regression':
                mean_r2 = np.mean([s['r2'] for s in cv_scores])
                std_r2 = np.std([s['r2'] for s in cv_scores])
                mean_rmse = np.mean([s['rmse'] for s in cv_scores])
                mean_mae = np.mean([s['mae'] for s in cv_scores])

                print(f"\nğŸ“Š TimeSeriesSplit CV í‰ê·  ì„±ëŠ¥:")
                print(f"   RÂ² = {mean_r2:.4f} Â± {std_r2:.4f}")
                print(f"   RMSE = {mean_rmse:.6f}")
                print(f"   MAE = {mean_mae:.6f}")

                self.results[f'{target}_regression'] = {
                    'mean_r2': float(mean_r2),
                    'std_r2': float(std_r2),
                    'mean_rmse': float(mean_rmse),
                    'mean_mae': float(mean_mae),
                    'cv_scores': cv_scores
                }
            else:
                mean_acc = np.mean([s['accuracy'] for s in cv_scores])
                std_acc = np.std([s['accuracy'] for s in cv_scores])

                print(f"\nğŸ“Š TimeSeriesSplit CV í‰ê·  ì„±ëŠ¥:")
                print(f"   Accuracy = {mean_acc:.4f} Â± {std_acc:.4f}")

                self.results[f'{target}_classification'] = {
                    'mean_accuracy': float(mean_acc),
                    'std_accuracy': float(std_acc),
                    'cv_scores': cv_scores
                }

            # í‰ê·  íŠ¹ì„± ì¤‘ìš”ë„
            mean_importance = np.mean(feature_importances, axis=0)
            feature_importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': mean_importance
            }).sort_values('importance', ascending=False)

            print(f"\nğŸ” Top 10 ì¤‘ìš” íŠ¹ì„±:")
            for idx, row in feature_importance_df.head(10).iterrows():
                print(f"   {row['feature']:35s}: {row['importance']:.4f}")

            self.results[f'{target}_feature_importance'] = feature_importance_df.to_dict('records')

            return True

        except Exception as e:
            print(f"âŒ XGBoost í•™ìŠµ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def detect_microstructure_patterns(self):
        """ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° íŒ¨í„´ íƒì§€"""
        print(f"\nğŸ”¬ ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° íŒ¨í„´ íƒì§€ ì¤‘...")

        try:
            # íŒ¨í„´ 1: ì¥ ì‹œì‘ ì „ ê°ì„± vs ë‹¹ì¼ ìˆ˜ìµë¥ 
            premarket_corr = self.data[['sentiment_premarket', 'returns']].corr().iloc[0, 1]
            print(f"\n   íŒ¨í„´ 1: ì¥ ì‹œì‘ ì „ ê°ì„± â†” ë‹¹ì¼ ìˆ˜ìµë¥ ")
            print(f"     ìƒê´€ê³„ìˆ˜: {premarket_corr:.4f}")

            # íŒ¨í„´ 2: íŠ¸ìœ„í„° vs ë‰´ìŠ¤ ê°ì„± ì°¨ì´
            twitter_news_diff = self.data['sentiment_twitter'] - self.data['sentiment_news']
            diff_return_corr = twitter_news_diff.corr(self.data['target_return_1d'])
            print(f"\n   íŒ¨í„´ 2: íŠ¸ìœ„í„°-ë‰´ìŠ¤ ê°ì„± ì°¨ì´ â†” ë¯¸ë˜ ìˆ˜ìµë¥ ")
            print(f"     ìƒê´€ê³„ìˆ˜: {diff_return_corr:.4f}")

            # íŒ¨í„´ 3: í™•ì‚° ì†ë„ vs ê·¹ë‹¨ ì›€ì§ì„
            virality_extreme = self.data[self.data['target_extreme_move'] != 0]['virality_max'].mean()
            virality_normal = self.data[self.data['target_extreme_move'] == 0]['virality_max'].mean()
            print(f"\n   íŒ¨í„´ 3: í™•ì‚° ì†ë„ vs ê·¹ë‹¨ ì›€ì§ì„")
            print(f"     ê·¹ë‹¨ ì›€ì§ì„ ì‹œ í™•ì‚° ì†ë„: {virality_extreme:.2f}")
            print(f"     ì •ìƒ ì‹œ í™•ì‚° ì†ë„: {virality_normal:.2f}")
            print(f"     ì°¨ì´: {virality_extreme - virality_normal:.2f}")

            # íŒ¨í„´ 4: ê°ì„± ê°•ë„ vs ë°©í–¥ì„± ì •í™•ë„
            high_strength = self.data[self.data['sentiment_strength_mean'] > 0.7]
            low_strength = self.data[self.data['sentiment_strength_mean'] <= 0.3]

            if len(high_strength) > 0:
                high_acc = ((high_strength['sentiment_mean'] > 0) == (high_strength['target_return_1d'] > 0)).mean()
                print(f"\n   íŒ¨í„´ 4: ê°ì„± ê°•ë„ vs ë°©í–¥ì„± ì •í™•ë„")
                print(f"     ê³ ê°•ë„ ê°ì„±(>0.7) ë°©í–¥ì„± ì •í™•ë„: {high_acc:.4f}")

            if len(low_strength) > 0:
                low_acc = ((low_strength['sentiment_mean'] > 0) == (low_strength['target_return_1d'] > 0)).mean()
                print(f"     ì €ê°•ë„ ê°ì„±(â‰¤0.3) ë°©í–¥ì„± ì •í™•ë„: {low_acc:.4f}")

            # íŒ¨í„´ 5: ê·¹ë‹¨ ê°ì„± ë¹„ìœ¨ vs ë°˜ì „ í™•ë¥ 
            extreme_positive = self.data[self.data['extreme_positive_ratio'] > 0.3]
            if len(extreme_positive) > 0:
                reversal_prob = (extreme_positive['target_return_1d'] < 0).mean()
                print(f"\n   íŒ¨í„´ 5: ê·¹ë‹¨ ê¸ì • ê°ì„± vs ë°˜ì „ í™•ë¥ ")
                print(f"     ê·¹ë‹¨ ê¸ì •(>30%) í›„ í•˜ë½ í™•ë¥ : {reversal_prob:.4f}")

            extreme_negative = self.data[self.data['extreme_negative_ratio'] > 0.3]
            if len(extreme_negative) > 0:
                bounce_prob = (extreme_negative['target_return_1d'] > 0).mean()
                print(f"     ê·¹ë‹¨ ë¶€ì •(>30%) í›„ ìƒìŠ¹ í™•ë¥ : {bounce_prob:.4f}")

            # ê²°ê³¼ ì €ì¥
            self.results['microstructure_patterns'] = {
                'premarket_correlation': float(premarket_corr),
                'twitter_news_diff_correlation': float(diff_return_corr),
                'virality_extreme_vs_normal': {
                    'extreme': float(virality_extreme),
                    'normal': float(virality_normal),
                    'difference': float(virality_extreme - virality_normal)
                }
            }

            return True

        except Exception as e:
            print(f"âŒ íŒ¨í„´ íƒì§€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        try:
            self.results['metadata'] = {
                'experiment': 'advanced_pattern_detection',
                'dataset': self.dataset_path,
                'n_samples': len(self.X),
                'n_features': len(self.feature_names),
                'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

            output_path = "data/raw/advanced_pattern_detection_results.json"
            with open(output_path, 'w') as f:
                json.dump(self.results, f, indent=2)

            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
            return True

        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("="*60)
        print("ğŸ”¬ ê³ ê¸‰ íŒ¨í„´ íƒì§€ ë¶„ì„")
        print("="*60)
        print("ëª©í‘œ: XGBoostë¡œ LLM ê°ì„± ì§€í‘œì˜ ë¯¸ì„¸ íŒ¨í„´ ë°œê²¬\n")

        if not self.load_and_prepare_data():
            return False

        # 1. ìˆ˜ìµë¥  ì˜ˆì¸¡ (íšŒê·€)
        if not self.train_xgboost_model(target='return'):
            return False

        # 2. ë°©í–¥ì„± ì˜ˆì¸¡ (ë¶„ë¥˜)
        if not self.train_xgboost_model(target='direction'):
            return False

        # 3. ê·¹ë‹¨ ì›€ì§ì„ ì˜ˆì¸¡ (ë¶„ë¥˜)
        if not self.train_xgboost_model(target='extreme'):
            return False

        # 4. ë¯¸ì‹œêµ¬ì¡° íŒ¨í„´ íƒì§€
        if not self.detect_microstructure_patterns():
            return False

        # 5. ê²°ê³¼ ì €ì¥
        if not self.save_results():
            return False

        print("\n" + "="*60)
        print("âœ… ë¶„ì„ ì™„ë£Œ!")
        print("="*60)

        # ìµœì¢… ìš”ì•½
        print(f"\nğŸ“‹ ìµœì¢… ê²°ê³¼ ìš”ì•½:")

        if 'return_regression' in self.results:
            r2 = self.results['return_regression']['mean_r2']
            print(f"   ìˆ˜ìµë¥  ì˜ˆì¸¡ (íšŒê·€): RÂ² = {r2:.4f}")

        if 'direction_classification' in self.results:
            acc = self.results['direction_classification']['mean_accuracy']
            print(f"   ë°©í–¥ì„± ì˜ˆì¸¡ (ë¶„ë¥˜): Accuracy = {acc:.4f}")

        if 'extreme_classification' in self.results:
            acc = self.results['extreme_classification']['mean_accuracy']
            print(f"   ê·¹ë‹¨ ì›€ì§ì„ ì˜ˆì¸¡: Accuracy = {acc:.4f}")

        print(f"\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
        if 'return_regression' in self.results:
            r2 = self.results['return_regression']['mean_r2']
            if r2 > 0.1:
                print(f"   âœ… XGBoostë¡œ íŒ¨í„´ ë°œê²¬ (RÂ² = {r2:.4f})")
            elif r2 > 0.05:
                print(f"   âš ï¸  ì•½í•œ íŒ¨í„´ ì¡´ì¬ (RÂ² = {r2:.4f})")
            else:
                print(f"   âŒ ìœ ì˜ë¯¸í•œ íŒ¨í„´ ì—†ìŒ (RÂ² = {r2:.4f})")

        return True

if __name__ == "__main__":
    detector = AdvancedPatternDetection(
        dataset_path="data/training/advanced_news_twitter_dataset.csv"
    )

    detector.run_analysis()
