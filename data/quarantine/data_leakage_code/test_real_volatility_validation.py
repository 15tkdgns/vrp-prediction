#!/usr/bin/env python3
"""
ì‹¤ì œ SPY ë°ì´í„°ë¡œ ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸ì˜ RÂ² = 0.2136 ë‹¬ì„± ê°€ëŠ¥ì„± ê²€ì¦
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

def create_volatility_target(df):
    """ì‹¤ì œ ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„±"""
    # ì¼ì¼ ìˆ˜ìµë¥ ì´ ì´ë¯¸ ìˆë‹¤ê³  ê°€ì •
    if 'Returns' not in df.columns:
        df['Returns'] = df['Close'].pct_change()

    # ë‹¤ìŒë‚  ë³€ë™ì„± ê³„ì‚° (ì ˆëŒ€ê°’ ê¸°ì¤€)
    df['next_day_volatility'] = np.abs(df['Returns'].shift(-1))

    return df

def test_volatility_prediction():
    """ì‹¤ì œ ë°ì´í„°ë¡œ ë³€ë™ì„± ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ì‹¤ì œ SPY ë°ì´í„°ë¡œ ë³€ë™ì„± ì˜ˆì¸¡ ê²€ì¦")
    print("="*60)

    # 1. ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('/root/workspace/data/training/sp500_leak_free_dataset.csv')
    df['Date'] = pd.to_datetime(df['Date'])
    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ: {df.shape}")

    # 2. ë³€ë™ì„± íƒ€ê²Ÿ ìƒì„±
    df = create_volatility_target(df)

    # 3. íŠ¹ì„± ì„ íƒ (í˜„ì¬ì‹œìŠ¤í…œìƒíƒœ2.txtì—ì„œ ì–¸ê¸‰ëœ 7ê°œ íŠ¹ì„± ì‚¬ìš©)
    safe_features = [
        'MA_20', 'MA_50', 'RSI', 'Volatility_20',
        'Volume_ratio_20', 'Returns_lag_1', 'Returns_lag_2'
    ]

    # ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±ë§Œ ì„ íƒ
    available_features = [col for col in safe_features if col in df.columns]
    print(f"ğŸ”§ ì‚¬ìš© íŠ¹ì„±: {len(available_features)}ê°œ")
    for feature in available_features:
        print(f"   âœ… {feature}")

    # 4. ë°ì´í„° ì¤€ë¹„
    X = df[available_features].copy()
    y = df['next_day_volatility'].copy()

    # NaN ì œê±°
    mask = ~(X.isnull().any(axis=1) | y.isnull())
    X_clean = X[mask]
    y_clean = y[mask]

    print(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {len(X_clean)} ìƒ˜í”Œ")
    print(f"ğŸ¯ íƒ€ê²Ÿ í†µê³„:")
    print(f"   í‰ê· : {y_clean.mean():.6f}")
    print(f"   í‘œì¤€í¸ì°¨: {y_clean.std():.6f}")
    print(f"   ë²”ìœ„: {y_clean.min():.6f} ~ {y_clean.max():.6f}")

    # 5. ì‹œê³„ì—´ ë¶„í•  (í˜„ì¬ì‹œìŠ¤í…œìƒíƒœ2.txtì—ì„œ ì–¸ê¸‰ëœ ë°©ì‹)
    print(f"\nğŸ”„ ì‹œê³„ì—´ ë¶„í• ë¡œ êµì°¨ê²€ì¦...")

    tscv = TimeSeriesSplit(n_splits=5)
    cv_scores = []

    scaler = StandardScaler()

    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_clean)):
        X_train, X_val = X_clean.iloc[train_idx], X_clean.iloc[val_idx]
        y_train, y_val = y_clean.iloc[train_idx], y_clean.iloc[val_idx]

        # ì •ê·œí™”
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)

        # ëª¨ë¸ í›ˆë ¨ (í˜„ì¬ì‹œìŠ¤í…œìƒíƒœ2.txtì—ì„œ ì–¸ê¸‰ëœ íŒŒë¼ë¯¸í„°)
        model = ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=42)
        model.fit(X_train_scaled, y_train)

        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = model.predict(X_val_scaled)
        r2 = r2_score(y_val, y_pred)
        cv_scores.append(r2)

        print(f"   Fold {fold+1}: RÂ² = {r2:.4f}")

    # 6. ìµœì¢… ëª¨ë¸ í›ˆë ¨ (ì „ì²´ ë°ì´í„°ì˜ 80%ë¡œ)
    print(f"\nğŸ¤– ìµœì¢… ëª¨ë¸ í›ˆë ¨...")

    split_idx = int(len(X_clean) * 0.8)
    X_train = X_clean.iloc[:split_idx]
    X_test = X_clean.iloc[split_idx:]
    y_train = y_clean.iloc[:split_idx]
    y_test = y_clean.iloc[split_idx:]

    # ì •ê·œí™”
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ëª¨ë¸ í›ˆë ¨
    final_model = ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=42)
    final_model.fit(X_train_scaled, y_train)

    # ì˜ˆì¸¡
    y_train_pred = final_model.predict(X_train_scaled)
    y_test_pred = final_model.predict(X_test_scaled)

    # í‰ê°€
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

    # 7. ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    print(f"="*40)
    print(f"ğŸ“Š êµì°¨ê²€ì¦ RÂ²:")
    print(f"   í‰ê· : {np.mean(cv_scores):.4f}")
    print(f"   í‘œì¤€í¸ì°¨: {np.std(cv_scores):.4f}")
    print(f"   ë²”ìœ„: {np.min(cv_scores):.4f} ~ {np.max(cv_scores):.4f}")

    print(f"\nğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:")
    print(f"   í›ˆë ¨ RÂ²: {train_r2:.4f}")
    print(f"   í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.4f}")
    print(f"   í…ŒìŠ¤íŠ¸ MAE: {test_mae:.6f}")
    print(f"   í…ŒìŠ¤íŠ¸ RMSE: {test_rmse:.6f}")

    # 8. ì£¼ì¥ ê²€ì¦
    claimed_r2 = 0.2136
    print(f"\nğŸ” ì£¼ì¥ ê²€ì¦:")
    print(f"   í˜„ì¬ì‹œìŠ¤í…œìƒíƒœ2.txt ì£¼ì¥: RÂ² = {claimed_r2:.4f}")
    print(f"   ì‹¤ì œ ë‹¬ì„±: RÂ² = {test_r2:.4f}")

    if test_r2 >= claimed_r2 * 0.8:  # 80% ì´ìƒì´ë©´ ë‹¬ì„± ê°€ëŠ¥
        print(f"   âœ… ì£¼ì¥ ë‹¬ì„± ê°€ëŠ¥: {test_r2:.4f} >= {claimed_r2 * 0.8:.4f}")
        is_achievable = True
    else:
        print(f"   âŒ ì£¼ì¥ ë‹¬ì„± ë¶ˆê°€: {test_r2:.4f} < {claimed_r2 * 0.8:.4f}")
        is_achievable = False

    # 9. íŠ¹ì„± ì¤‘ìš”ë„
    print(f"\nğŸ”§ íŠ¹ì„± ì¤‘ìš”ë„:")
    feature_importance = np.abs(final_model.coef_)
    feature_names = available_features

    for i, (feature, importance) in enumerate(zip(feature_names, feature_importance)):
        print(f"   {feature:15s}: {importance:.4f}")

    return {
        'cv_r2_mean': np.mean(cv_scores),
        'cv_r2_std': np.std(cv_scores),
        'test_r2': test_r2,
        'train_r2': train_r2,
        'claimed_r2': claimed_r2,
        'is_achievable': is_achievable,
        'sample_count': len(X_clean),
        'feature_count': len(available_features)
    }

if __name__ == "__main__":
    result = test_volatility_prediction()

    print(f"\nğŸ‰ ê²€ì¦ ì™„ë£Œ!")
    print(f"   ìƒ˜í”Œ ìˆ˜: {result['sample_count']}")
    print(f"   íŠ¹ì„± ìˆ˜: {result['feature_count']}")
    print(f"   ë‹¬ì„± ê°€ëŠ¥: {result['is_achievable']}")