#!/usr/bin/env python3
"""
ì‚¬ì´í‚·ëŸ° ê¸°ë°˜ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (TensorFlow ì—†ì´)
"""

import os
import json
import numpy as np
import pandas as pd
import joblib
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')


def test_sklearn_models():
    """ì‚¬ì´í‚·ëŸ° ê¸°ë°˜ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”„ ì‚¬ì´í‚·ëŸ° ê¸°ë°˜ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    models_dir = "data/models"
    data_dir = "data/raw"
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ...")
    try:
        features_df = pd.read_csv(f"{data_dir}/training_features.csv")
        labels_df = pd.read_csv(f"{data_dir}/event_labels.csv")
        
        # ë°ì´í„° ë¶„í•  (ê°„ë‹¨íˆ 80/20)
        split_idx = int(len(features_df) * 0.8)
        X_test = features_df.iloc[split_idx:].drop(['timestamp', 'symbol'], axis=1, errors='ignore')
        y_test = labels_df.iloc[split_idx:]['has_event'].values
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ ìƒ˜í”Œ")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ë° ì ìš©
        scaler = joblib.load(f"{models_dir}/scaler.pkl")
        X_test_scaled = scaler.transform(X_test)
        
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤
        model_files = {
            'Random Forest': 'random_forest_model.pkl',
            'Gradient Boosting': 'gradient_boosting_model.pkl',
            'XGBoost': 'xgboost_model.pkl',
            'Random Forest (Improved)': 'random_forest_improved_model.pkl',
            'Gradient Boosting (Improved)': 'gradient_boosting_improved_model.pkl'
        }
        
        results = {}
        
        for model_name, filename in model_files.items():
            model_path = f"{models_dir}/{filename}"
            
            if os.path.exists(model_path):
                print(f"\nğŸ” {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
                
                try:
                    # ëª¨ë¸ ë¡œë“œ
                    model = joblib.load(model_path)
                    
                    # ì˜ˆì¸¡
                    if hasattr(model, 'predict_proba'):
                        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
                        y_pred = (y_pred_proba > 0.5).astype(int)
                    else:
                        y_pred = model.predict(X_test_scaled)
                        y_pred_proba = None
                    
                    # ë©”íŠ¸ë¦­ ê³„ì‚°
                    accuracy = accuracy_score(y_test, y_pred)
                    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
                    
                    model_results = {
                        'accuracy': accuracy,
                        'precision': precision,
                        'recall': recall,
                        'f1_score': f1,
                        'predictions': len(y_pred),
                        'positive_predictions': int(np.sum(y_pred)),
                        'actual_positives': int(np.sum(y_test))
                    }
                    
                    if y_pred_proba is not None:
                        try:
                            auc = roc_auc_score(y_test, y_pred_proba)
                            model_results['auc'] = auc
                        except:
                            model_results['auc'] = None
                    
                    results[model_name] = model_results
                    
                    # ê²°ê³¼ ì¶œë ¥
                    print(f"  âœ… ì •í™•ë„: {accuracy:.3f}")
                    print(f"  âœ… ì •ë°€ë„: {precision:.3f}")
                    print(f"  âœ… ì¬í˜„ìœ¨: {recall:.3f}")
                    print(f"  âœ… F1 ì ìˆ˜: {f1:.3f}")
                    if 'auc' in model_results and model_results['auc']:
                        print(f"  âœ… AUC: {model_results['auc']:.3f}")
                    print(f"  ğŸ“Š ì˜ˆì¸¡: {model_results['positive_predictions']}/{len(y_pred)} ê¸ì •")
                    
                except Exception as e:
                    print(f"  âŒ ì˜¤ë¥˜: {str(e)}")
            else:
                print(f"  âš ï¸  ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {filename}")
        
        # ê²°ê³¼ ì €ì¥
        results_file = f"{data_dir}/sklearn_model_test_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        if results:
            best_model = max(results.items(), key=lambda x: x[1]['f1_score'])
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model[0]}")
            print(f"   F1 ì ìˆ˜: {best_model[1]['f1_score']:.3f}")
            
        return results
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return None


if __name__ == "__main__":
    test_sklearn_models()