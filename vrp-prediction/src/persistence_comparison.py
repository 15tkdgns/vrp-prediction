#!/usr/bin/env python3
"""
Persistence Model ë¹„êµ ë¶„ì„
===========================

LSTM/MLPê°€ ë‹¨ìˆœíˆ "ì–´ì œ ê°’ ë³µì‚¬"ë³´ë‹¤ ìš°ìˆ˜í•œì§€ ê²€ì¦
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
from sklearn.linear_model import ElasticNet
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import yfinance as yf
from pathlib import Path
import json
from datetime import datetime

SEED = 42
np.random.seed(SEED)


def download_data(ticker, start='2015-01-01', end='2025-01-01'):
    """ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
    try:
        data = yf.download(ticker, start=start, end=end, progress=False)
        if isinstance(data.columns, pd.MultiIndex):
            data.columns = data.columns.get_level_values(0)
        return data
    except:
        return None


def prepare_data(ticker='GLD'):
    """ë°ì´í„° ì¤€ë¹„"""
    vix = download_data('^VIX')
    asset = download_data(ticker)
    
    if vix is None or asset is None:
        return None, None
    
    df = asset[['Close']].copy()
    df['VIX'] = vix['Close'].reindex(df.index).ffill().bfill()
    df['returns'] = df['Close'].pct_change()
    
    # ì‹¤í˜„ë³€ë™ì„±
    df['RV_22d'] = df['returns'].rolling(22).std() * np.sqrt(252) * 100
    
    # VIX-RV Spread (ì´ì „ ëª…ì¹­: VRP)
    df['Spread'] = df['VIX'] - df['RV_22d']
    
    # íƒ€ê²Ÿ: 22ì¼ í›„ ì‹¤í˜„ë³€ë™ì„±
    df['RV_future'] = df['RV_22d'].shift(-22)
    df['Spread_true'] = df['VIX'] - df['RV_future']
    
    # íŠ¹ì„±
    df['VIX_lag1'] = df['VIX'].shift(1)
    df['Spread_lag1'] = df['Spread'].shift(1)
    df['Spread_lag5'] = df['Spread'].shift(5)
    
    df = df.dropna()
    
    feature_cols = ['RV_22d', 'VIX_lag1', 'Spread_lag1', 'Spread_lag5']
    
    return df, feature_cols


def persistence_model_comparison():
    """Persistence Model vs ML ëª¨ë¸ ë¹„êµ"""
    print("\n" + "=" * 70)
    print("Persistence Model ë¹„êµ ë¶„ì„")
    print("=" * 70)
    print("\nëª©ì : LSTMì´ ë‹¨ìˆœíˆ 'ì–´ì œ ê°’ ë³µì‚¬'ë³´ë‹¤ ìš°ìˆ˜í•œì§€ ê²€ì¦\n")
    
    df, feature_cols = prepare_data('GLD')
    
    if df is None:
        return {'error': 'ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨'}
    
    # VIX-RV Spread (Target)
    target = df['Spread_true'].values
    
    # Train/Test ë¶„í•  (22ì¼ Gap í¬í•¨)
    split = int(len(df) * 0.8)
    gap = 22
    
    train_idx = slice(0, split)
    test_idx = slice(split + gap, len(df))
    
    y_train = target[train_idx]
    y_test = target[test_idx]
    
    print(f"ë°ì´í„°: Train {len(y_train)}, Test {len(y_test)}")
    
    results = {}
    
    # ========================================
    # 1. Persistence Model (Naive: y_t = y_{t-1})
    # ========================================
    spread_lag1 = df['Spread_lag1'].values
    y_pred_naive = spread_lag1[test_idx]
    
    r2_naive = r2_score(y_test, y_pred_naive)
    mae_naive = mean_absolute_error(y_test, y_pred_naive)
    rmse_naive = np.sqrt(mean_squared_error(y_test, y_pred_naive))
    
    results['Naive (y_{t-1})'] = {
        'r2': r2_naive,
        'mae': mae_naive,
        'rmse': rmse_naive
    }
    
    print(f"\n1. Naive Model (ì–´ì œ ê°’ ë³µì‚¬):")
    print(f"   RÂ² = {r2_naive:.4f}")
    print(f"   MAE = {mae_naive:.2f}%")
    print(f"   RMSE = {rmse_naive:.2f}%")
    
    # ========================================
    # 2. ElasticNet
    # ========================================
    X = df[feature_cols].values
    y_rv = df['RV_future'].values
    vix_arr = df['VIX'].values
    
    X_train, X_test = X[train_idx], X[test_idx]
    y_rv_train = y_rv[train_idx]
    vix_test = vix_arr[test_idx]
    
    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)
    
    en = ElasticNet(alpha=0.01, random_state=SEED)
    en.fit(X_train_s, y_rv_train)
    spread_pred_en = vix_test - en.predict(X_test_s)
    
    r2_en = r2_score(y_test, spread_pred_en)
    mae_en = mean_absolute_error(y_test, spread_pred_en)
    rmse_en = np.sqrt(mean_squared_error(y_test, spread_pred_en))
    
    results['ElasticNet'] = {
        'r2': r2_en,
        'mae': mae_en,
        'rmse': rmse_en
    }
    
    print(f"\n2. ElasticNet:")
    print(f"   RÂ² = {r2_en:.4f}")
    print(f"   MAE = {mae_en:.2f}%")
    print(f"   RMSE = {rmse_en:.2f}%")
    
    # ========================================
    # 3. MLP
    # ========================================
    mlp = MLPRegressor(hidden_layer_sizes=(64,), max_iter=500, 
                       random_state=SEED, early_stopping=True)
    mlp.fit(X_train_s, y_rv_train)
    spread_pred_mlp = vix_test - mlp.predict(X_test_s)
    
    r2_mlp = r2_score(y_test, spread_pred_mlp)
    mae_mlp = mean_absolute_error(y_test, spread_pred_mlp)
    rmse_mlp = np.sqrt(mean_squared_error(y_test, spread_pred_mlp))
    
    results['MLP (64)'] = {
        'r2': r2_mlp,
        'mae': mae_mlp,
        'rmse': rmse_mlp
    }
    
    print(f"\n3. MLP (64):")
    print(f"   RÂ² = {r2_mlp:.4f}")
    print(f"   MAE = {mae_mlp:.2f}%")
    print(f"   RMSE = {rmse_mlp:.2f}%")
    
    # ========================================
    # 4. LSTM (TensorFlow)
    # ========================================
    try:
        import tensorflow as tf
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import LSTM, Dense, Dropout
        from tensorflow.keras.callbacks import EarlyStopping
        tf.random.set_seed(SEED)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        lookback = 22
        X_seq_train, y_seq_train = [], []
        X_seq_test, y_seq_test = [], []
        vix_seq_test = []
        
        X_all = df[feature_cols].values
        y_rv_all = df['RV_future'].values
        vix_all = df['VIX'].values
        target_all = df['Spread_true'].values
        
        for i in range(lookback, split):
            X_seq_train.append(X_all[i-lookback:i])
            y_seq_train.append(y_rv_all[i])
        
        for i in range(split + gap + lookback, len(df)):
            X_seq_test.append(X_all[i-lookback:i])
            y_seq_test.append(target_all[i])
            vix_seq_test.append(vix_all[i])
        
        X_seq_train = np.array(X_seq_train)
        y_seq_train = np.array(y_seq_train)
        X_seq_test = np.array(X_seq_test)
        y_seq_test = np.array(y_seq_test)
        vix_seq_test = np.array(vix_seq_test)
        
        # ìŠ¤ì¼€ì¼ë§
        X_flat_train = X_seq_train.reshape(-1, X_seq_train.shape[-1])
        X_flat_test = X_seq_test.reshape(-1, X_seq_test.shape[-1])
        scaler_lstm = StandardScaler()
        X_flat_train_s = scaler_lstm.fit_transform(X_flat_train)
        X_flat_test_s = scaler_lstm.transform(X_flat_test)
        X_seq_train_s = X_flat_train_s.reshape(X_seq_train.shape)
        X_seq_test_s = X_flat_test_s.reshape(X_seq_test.shape)
        
        # LSTM ëª¨ë¸
        model = Sequential([
            LSTM(64, input_shape=(lookback, len(feature_cols))),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(1)
        ])
        model.compile(optimizer='adam', loss='mse')
        
        early_stop = EarlyStopping(patience=10, restore_best_weights=True)
        model.fit(X_seq_train_s, y_seq_train, epochs=100, batch_size=32,
                  validation_split=0.1, callbacks=[early_stop], verbose=0)
        
        rv_pred_lstm = model.predict(X_seq_test_s, verbose=0).flatten()
        spread_pred_lstm = vix_seq_test - rv_pred_lstm
        
        r2_lstm = r2_score(y_seq_test, spread_pred_lstm)
        mae_lstm = mean_absolute_error(y_seq_test, spread_pred_lstm)
        rmse_lstm = np.sqrt(mean_squared_error(y_seq_test, spread_pred_lstm))
        
        results['LSTM (64)'] = {
            'r2': r2_lstm,
            'mae': mae_lstm,
            'rmse': rmse_lstm
        }
        
        print(f"\n4. LSTM (64):")
        print(f"   RÂ² = {r2_lstm:.4f}")
        print(f"   MAE = {mae_lstm:.2f}%")
        print(f"   RMSE = {rmse_lstm:.2f}%")
        
    except ImportError:
        print("\n4. LSTM: TensorFlow ë¯¸ì„¤ì¹˜")
        r2_lstm = None
    
    # ========================================
    # ë¹„êµ ìš”ì•½
    # ========================================
    print("\n" + "=" * 70)
    print("ë¹„êµ ìš”ì•½")
    print("=" * 70)
    
    print(f"\n{'ëª¨ë¸':<20} | {'RÂ²':>10} | {'vs Naive':>15}")
    print("-" * 50)
    
    for model_name, metrics in results.items():
        improvement = ((metrics['r2'] - r2_naive) / abs(r2_naive) * 100) if r2_naive != 0 else 0
        sign = "+" if improvement > 0 else ""
        print(f"{model_name:<20} | {metrics['r2']:>10.4f} | {sign}{improvement:>13.1f}%")
    
    # í•µì‹¬ ê²°ë¡ 
    print("\n" + "=" * 70)
    print("í•µì‹¬ ê²°ë¡ ")
    print("=" * 70)
    
    if r2_naive > 0:
        if r2_en > r2_naive or r2_mlp > r2_naive or (r2_lstm and r2_lstm > r2_naive):
            print("\nâœ“ ML ëª¨ë¸ì´ Naive Modelë³´ë‹¤ ìš°ìˆ˜í•¨ â†’ ì˜ˆì¸¡ë ¥ ìœ íš¨")
        else:
            print("\nâœ— ML ëª¨ë¸ì´ Naive Modelë³´ë‹¤ ì—´ë“±í•¨ â†’ ë‹¨ìˆœ ë³µì‚¬ì™€ ë‹¤ë¥¼ ë°” ì—†ìŒ")
    else:
        print("\nâš  Naive Model RÂ² < 0 â†’ VIX-RV Spreadì— ìê¸°ìƒê´€ ì—†ìŒ")
    
    # ì €ì¥
    Path('data/results').mkdir(parents=True, exist_ok=True)
    output = {
        'models': {k: {kk: float(vv) for kk, vv in v.items()} for k, v in results.items()},
        'baseline': 'Naive (y_{t-1})',
        'timestamp': datetime.now().isoformat()
    }
    
    with open('data/results/persistence_comparison.json', 'w') as f:
        json.dump(output, f, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: data/results/persistence_comparison.json")
    
    return results


if __name__ == '__main__':
    persistence_model_comparison()
