#!/usr/bin/env python3
"""
ë…¼ë¬¸ìš© í†µê³„ ë¶„ì„ ë° í‘œ ìƒì„±
===========================

Diebold-Mariano ê²€ì •, ê¸°ìˆ í†µê³„ëŸ‰ ë“± ë…¼ë¬¸ì— í•„ìš”í•œ í†µê³„ ë¶„ì„ ìˆ˜í–‰
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
from scipy import stats
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import yfinance as yf
from pathlib import Path
import json

SEED = 42
np.random.seed(SEED)


def load_data():
    """ë°ì´í„° ë¡œë“œ"""
    print("=" * 60)
    print("[1] ë°ì´í„° ë¡œë“œ")
    print("=" * 60)
    
    csv_path = Path('data/raw/spy_data_2020_2025.csv')
    if csv_path.exists():
        spy = pd.read_csv(csv_path, index_col=0, parse_dates=True)
    else:
        spy = yf.download('SPY', start='2020-01-01', end='2025-01-01', progress=False)
        if isinstance(spy.columns, pd.MultiIndex):
            spy.columns = spy.columns.get_level_values(0)
    
    # VIX ë¡œë“œ
    vix = yf.download('^VIX', start='2020-01-01', end='2025-01-01', progress=False)
    if isinstance(vix.columns, pd.MultiIndex):
        vix.columns = vix.columns.get_level_values(0)
    spy['VIX'] = vix['Close']
    spy = spy.ffill().dropna()
    
    print(f"  âœ“ SPY ë°ì´í„°: {len(spy)} í–‰")
    return spy


def compute_descriptive_stats(spy):
    """ê¸°ìˆ í†µê³„ëŸ‰ ê³„ì‚°"""
    print("\n" + "=" * 60)
    print("[2] ê¸°ìˆ í†µê³„ëŸ‰")
    print("=" * 60)
    
    spy['returns'] = spy['Close'].pct_change() * 100  # í¼ì„¼íŠ¸
    spy['volatility_5d'] = spy['returns'].rolling(5).std() * np.sqrt(252)
    
    spy = spy.dropna()
    
    # ê¸°ìˆ í†µê³„ëŸ‰ í…Œì´ë¸”
    stats_df = pd.DataFrame({
        'ë³€ìˆ˜': ['ì¼ë³„ ìˆ˜ìµë¥  (%)', '5ì¼ ì‹¤í˜„ë³€ë™ì„± (ì—°ìœ¨í™”)', 'VIX'],
        'í‰ê· ': [
            spy['returns'].mean(),
            spy['volatility_5d'].mean(),
            spy['VIX'].mean()
        ],
        'í‘œì¤€í¸ì°¨': [
            spy['returns'].std(),
            spy['volatility_5d'].std(),
            spy['VIX'].std()
        ],
        'ìµœì†Œê°’': [
            spy['returns'].min(),
            spy['volatility_5d'].min(),
            spy['VIX'].min()
        ],
        'ìµœëŒ€ê°’': [
            spy['returns'].max(),
            spy['volatility_5d'].max(),
            spy['VIX'].max()
        ],
        'ì™œë„': [
            stats.skew(spy['returns'].dropna()),
            stats.skew(spy['volatility_5d'].dropna()),
            stats.skew(spy['VIX'].dropna())
        ],
        'ì²¨ë„': [
            stats.kurtosis(spy['returns'].dropna()),
            stats.kurtosis(spy['volatility_5d'].dropna()),
            stats.kurtosis(spy['VIX'].dropna())
        ]
    }).round(4)
    
    print("\ní‘œ 1: SPY ë°ì´í„° ê¸°ìˆ í†µê³„ëŸ‰ (2020-2024)")
    print(stats_df.to_string(index=False))
    
    return spy, stats_df


def create_features_and_target(spy):
    """íŠ¹ì„± ë° íƒ€ê²Ÿ ìƒì„±"""
    print("\n" + "=" * 60)
    print("[3] íŠ¹ì„± ë° íƒ€ê²Ÿ ìƒì„±")
    print("=" * 60)
    
    spy['returns_raw'] = spy['Close'].pct_change()
    
    # ë³€ë™ì„± íŠ¹ì„±
    for w in [5, 10, 20, 50]:
        spy[f'volatility_{w}'] = spy['returns_raw'].rolling(w).std()
        spy[f'realized_vol_{w}'] = spy[f'volatility_{w}'] * np.sqrt(252)
    
    # VIX íŠ¹ì„±
    spy['vix_lag_1'] = spy['VIX'].shift(1)
    spy['vix_lag_5'] = spy['VIX'].shift(5)
    spy['vix_change'] = spy['VIX'].pct_change()
    spy['vix_zscore'] = (spy['VIX'] - spy['VIX'].rolling(20).mean()) / (spy['VIX'].rolling(20).std() + 1e-8)
    
    # Regime íŠ¹ì„±
    vix_lagged = spy['VIX'].shift(1)
    spy['regime_high_vol'] = (vix_lagged >= 25).astype(int)
    spy['regime_crisis'] = (vix_lagged >= 35).astype(int)
    spy['vol_in_high_regime'] = spy['regime_high_vol'] * spy['volatility_5']
    spy['vol_in_crisis'] = spy['regime_crisis'] * spy['volatility_5']
    spy['vix_excess_25'] = np.maximum(vix_lagged - 25, 0)
    spy['vix_excess_35'] = np.maximum(vix_lagged - 35, 0)
    
    # ìˆ˜ìµë¥  í†µê³„
    for w in [5, 10, 20]:
        spy[f'mean_return_{w}'] = spy['returns_raw'].rolling(w).mean()
        spy[f'skew_{w}'] = spy['returns_raw'].rolling(w).skew()
        spy[f'kurt_{w}'] = spy['returns_raw'].rolling(w).kurt()
    
    # ë˜ê·¸ ë³€ìˆ˜
    for lag in [1, 2, 3, 5]:
        spy[f'return_lag_{lag}'] = spy['returns_raw'].shift(lag)
        spy[f'vol_lag_{lag}'] = spy['volatility_5'].shift(lag)
    
    # ëª¨ë©˜í…€
    for w in [5, 10, 20]:
        spy[f'momentum_{w}'] = spy['returns_raw'].rolling(w).sum()
    
    # ë¹„ìœ¨ ë° Z-score
    spy['vol_ratio_5_20'] = spy['volatility_5'] / (spy['volatility_20'] + 1e-8)
    spy['zscore_20'] = (spy['returns_raw'] - spy['returns_raw'].rolling(20).mean()) / (spy['returns_raw'].rolling(20).std() + 1e-8)
    
    # HAR íŠ¹ì„±
    returns_sq = spy['returns_raw'] ** 2
    spy['har_rv_d'] = returns_sq.shift(1)
    spy['har_rv_w'] = returns_sq.rolling(5).mean().shift(1)
    spy['har_rv_m'] = returns_sq.rolling(22).mean().shift(1)
    
    # íƒ€ê²Ÿ: 5ì¼ ë¯¸ë˜ ë³€ë™ì„±
    vol_values = []
    returns = spy['returns_raw'].values
    for i in range(len(returns)):
        if i + 5 < len(returns):
            future_window = returns[i+1:i+6]
            vol_values.append(pd.Series(future_window).std())
        else:
            vol_values.append(np.nan)
    spy['target'] = vol_values
    
    spy = spy.dropna()
    print(f"  âœ“ ìµœì¢… ë°ì´í„°: {len(spy)} í–‰")
    
    return spy


def train_all_models(spy):
    """ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
    print("\n" + "=" * 60)
    print("[4] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")
    print("=" * 60)
    
    # íŠ¹ì„± ì»¬ëŸ¼
    feature_cols = [c for c in spy.columns if c.startswith((
        'volatility_', 'realized_vol_', 'vix_', 'regime_', 'vol_in_',
        'mean_return_', 'skew_', 'kurt_', 'return_lag_', 'vol_lag_',
        'momentum_', 'vol_ratio_', 'zscore_', 'har_'
    ))]
    
    X = spy[feature_cols].values
    y = spy['target'].values
    
    # ë¶„í• 
    split_idx = int(len(spy) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)
    
    print(f"  Train: {len(X_train)}, Test: {len(X_test)}")
    
    results = {}
    predictions = {}
    
    # 1. ElasticNet + VIX + Regime
    print("\n  â†’ ElasticNet + VIX + Regime")
    en = ElasticNet(alpha=0.0005, l1_ratio=0.5, random_state=SEED, max_iter=10000)
    en.fit(X_train_s, y_train)
    y_pred_en = en.predict(X_test_s)
    predictions['ElasticNet+VIX+Regime'] = y_pred_en
    results['ElasticNet+VIX+Regime'] = {
        'r2': r2_score(y_test, y_pred_en),
        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_en)),
        'mae': mean_absolute_error(y_test, y_pred_en),
        'params': len([c for c in en.coef_ if abs(c) > 1e-8])
    }
    print(f"    RÂ² = {results['ElasticNet+VIX+Regime']['r2']:.4f}")
    
    # 2. HAR-RV
    print("  â†’ HAR-RV")
    har_cols = ['har_rv_d', 'har_rv_w', 'har_rv_m']
    X_har = spy[[c for c in har_cols if c in spy.columns]].values
    X_har_train, X_har_test = X_har[:split_idx], X_har[split_idx:]
    X_har_train_s = scaler.fit_transform(X_har_train)
    X_har_test_s = scaler.transform(X_har_test)
    
    har = Ridge(alpha=1.0, random_state=SEED)
    har.fit(X_har_train_s, y_train)
    y_pred_har = har.predict(X_har_test_s)
    predictions['HAR-RV'] = y_pred_har
    results['HAR-RV'] = {
        'r2': r2_score(y_test, y_pred_har),
        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_har)),
        'mae': mean_absolute_error(y_test, y_pred_har),
        'params': 4
    }
    print(f"    RÂ² = {results['HAR-RV']['r2']:.4f}")
    
    # 3. Ridge
    print("  â†’ Ridge")
    ridge = Ridge(alpha=1.0, random_state=SEED)
    ridge.fit(X_train_s, y_train)
    y_pred_ridge = ridge.predict(X_test_s)
    predictions['Ridge'] = y_pred_ridge
    results['Ridge'] = {
        'r2': r2_score(y_test, y_pred_ridge),
        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_ridge)),
        'mae': mean_absolute_error(y_test, y_pred_ridge),
        'params': len(feature_cols)
    }
    print(f"    RÂ² = {results['Ridge']['r2']:.4f}")
    
    # 4. GradientBoosting
    print("  â†’ GradientBoosting")
    gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=SEED)
    gb.fit(X_train_s, y_train)
    y_pred_gb = gb.predict(X_test_s)
    predictions['GradientBoosting'] = y_pred_gb
    results['GradientBoosting'] = {
        'r2': r2_score(y_test, y_pred_gb),
        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_gb)),
        'mae': mean_absolute_error(y_test, y_pred_gb),
        'params': 1000
    }
    print(f"    RÂ² = {results['GradientBoosting']['r2']:.4f}")
    
    # 5. ê¸°ë³¸ ElasticNet (VIX/Regime ì—†ì´)
    print("  â†’ ElasticNet (baseline)")
    baseline_cols = [c for c in feature_cols if not c.startswith(('vix_', 'regime_', 'vol_in_'))]
    X_base = spy[baseline_cols].values
    X_base_train, X_base_test = X_base[:split_idx], X_base[split_idx:]
    X_base_train_s = scaler.fit_transform(X_base_train)
    X_base_test_s = scaler.transform(X_base_test)
    
    en_base = ElasticNet(alpha=0.001, l1_ratio=0.1, random_state=SEED, max_iter=10000)
    en_base.fit(X_base_train_s, y_train)
    y_pred_base = en_base.predict(X_base_test_s)
    predictions['ElasticNet_baseline'] = y_pred_base
    results['ElasticNet_baseline'] = {
        'r2': r2_score(y_test, y_pred_base),
        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_base)),
        'mae': mean_absolute_error(y_test, y_pred_base),
        'params': 31
    }
    print(f"    RÂ² = {results['ElasticNet_baseline']['r2']:.4f}")
    
    return results, predictions, y_test, feature_cols, en.coef_


def diebold_mariano_test(e1, e2, h=1):
    """
    Diebold-Mariano ê²€ì •
    H0: ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë™ì¼í•˜ë‹¤
    """
    d = e1**2 - e2**2
    d_mean = np.mean(d)
    d_var = np.var(d, ddof=1)
    
    # Newey-West í‘œì¤€ì˜¤ì°¨ (autocorrelation correction)
    n = len(d)
    gamma0 = np.sum((d - d_mean)**2) / n
    
    gamma = 0
    for k in range(1, h):
        gamma += 2 * np.sum((d[k:] - d_mean) * (d[:-k] - d_mean)) / n
    
    var_d = (gamma0 + gamma) / n
    
    if var_d <= 0:
        var_d = d_var / n
    
    dm_stat = d_mean / np.sqrt(var_d)
    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))
    
    return dm_stat, p_value


def perform_statistical_tests(predictions, y_test):
    """í†µê³„ì  ê²€ì • ìˆ˜í–‰"""
    print("\n" + "=" * 60)
    print("[5] Diebold-Mariano ê²€ì •")
    print("=" * 60)
    
    base_model = 'ElasticNet+VIX+Regime'
    base_pred = predictions[base_model]
    base_error = y_test - base_pred
    
    dm_results = []
    
    for model_name, pred in predictions.items():
        if model_name == base_model:
            continue
        
        error = y_test - pred
        dm_stat, p_value = diebold_mariano_test(base_error, error)
        
        if p_value < 0.001:
            sig = '***'
        elif p_value < 0.01:
            sig = '**'
        elif p_value < 0.05:
            sig = '*'
        else:
            sig = ''
        
        conclusion = 'ìœ ì˜í•˜ê²Œ ìš°ìˆ˜' if dm_stat > 0 and p_value < 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'
        
        dm_results.append({
            'Model': model_name,
            'DM_stat': dm_stat,
            'p_value': p_value,
            'sig': sig,
            'conclusion': conclusion
        })
        
        print(f"  vs {model_name}: DM = {dm_stat:.2f}, p = {p_value:.4f} {sig}")
    
    return dm_results


def analyze_feature_importance(feature_cols, coef):
    """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
    print("\n" + "=" * 60)
    print("[6] íŠ¹ì„± ì¤‘ìš”ë„ (ElasticNet ê³„ìˆ˜)")
    print("=" * 60)
    
    importance = pd.DataFrame({
        'feature': feature_cols,
        'coefficient': np.abs(coef)
    }).sort_values('coefficient', ascending=False)
    
    print("\nìƒìœ„ 10ê°œ íŠ¹ì„±:")
    for i, row in importance.head(10).iterrows():
        print(f"  {row['feature']:25s}: {row['coefficient']:.6f}")
    
    return importance


def save_results(results, dm_results, stats_df):
    """ê²°ê³¼ ì €ì¥"""
    print("\n" + "=" * 60)
    print("[7] ê²°ê³¼ ì €ì¥")
    print("=" * 60)
    
    output = {
        'model_performance': results,
        'dm_test': dm_results,
        'descriptive_stats': stats_df.to_dict('records')
    }
    
    output_path = Path('paper/statistical_analysis_results.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"  âœ“ ê²°ê³¼ ì €ì¥: {output_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "ğŸ”¬" * 30)
    print("ë…¼ë¬¸ìš© í†µê³„ ë¶„ì„")
    print("ğŸ”¬" * 30)
    
    # 1. ë°ì´í„° ë¡œë“œ
    spy = load_data()
    
    # 2. ê¸°ìˆ í†µê³„ëŸ‰
    spy, stats_df = compute_descriptive_stats(spy)
    
    # 3. íŠ¹ì„± ìƒì„±
    spy = create_features_and_target(spy)
    
    # 4. ëª¨ë¸ í•™ìŠµ
    results, predictions, y_test, feature_cols, coef = train_all_models(spy)
    
    # 5. í†µê³„ì  ê²€ì •
    dm_results = perform_statistical_tests(predictions, y_test)
    
    # 6. íŠ¹ì„± ì¤‘ìš”ë„
    importance = analyze_feature_importance(feature_cols, coef)
    
    # 7. ê²°ê³¼ ì €ì¥
    save_results(results, dm_results, stats_df)
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    print("\nëª¨ë¸ ì„±ëŠ¥:")
    for model, perf in sorted(results.items(), key=lambda x: x[1]['r2'], reverse=True):
        print(f"  {model:25s}: RÂ² = {perf['r2']:.4f}, RMSE = {perf['rmse']:.6f}")
    
    print("\nâœ… í†µê³„ ë¶„ì„ ì™„ë£Œ!")


if __name__ == '__main__':
    main()
